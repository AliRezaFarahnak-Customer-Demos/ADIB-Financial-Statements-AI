[
{"url": "https://cohere.com", "page_title": "Cohere | The leading AI platform for enterprise", "headers": ["Build conversational apps with RAG", "Give your technology language", "Chat with RAG", "Powerful, accurate semantic search", "Search performance improvement", "Accelerate the next wave of productivity", "The enterprise LLM"], "content": ["Try now", "Cohere\u2019s Retrieval Augmented Generation (RAG) toolkit allows LLMs to accurately answer questions and solve tasks using enterprise data as the source of truth", "Cohere\u2019s world-class large language models (LLMs) help enterprises build powerful, secure applications that search, understand meaning and converse in text.", "Coral", "Cohere\u2019s Coral lets you build powerful chatbots and knowledge assistants. Coral uses RAG (Retrieval Augmented Generation) to deliver accurate conversations grounded by your enterprise data.", "Search", "Cohere\u2019s Embed model allows enterprises to build powerful search solutions.\u00a0Embed is the industry\u2019s highest performing embedding model in English and over 100 other languages, ensuring relevant results.", "Rerank", "Rerank greatly improves the relevance of search results from existing lexical and semantic search tools, such as Elasticsearch, OpenSearch, or Solr, and is customizable by domain for better performance.", "Today\u2019s language models already show productivity gains in white-collar tasks of over\u00a050%.\u00a0The coming Intelligent Assistants will understand your enterprise data, giving your employees the tools to make decisions far more quickly than we can imagine today. ", " ", "CUSTOMIZABLE MODELS", "Cohere offers sophisticated customization (fine-tuning) tools and capabilities that give superior model performance at industry-leading inference cost", "PERFORMANCE AND SCALABILITY", "Cohere\u2019s models are packaged with inference engines that deliver better runtime performance at a lower cost than open-source equivalents", "FLEXIBLE DEPLOYMENT OPTIONS", "Cohere models are accessible through a SaaS API, cloud services (e.g., OCI, AWS SageMaker, Bedrock), and private deployments (VPC and on-prem)", "PRIVACY", "Customer data is never used in training base models, and customers have complete control over customization and model inputs/outputs", "CUSTOMIZABLE MODELS", "Cohere offers sophisticated customization (fine-tuning) tools and capabilities that give superior model performance at industry-leading inference cost", "PERFORMANCE AND SCALABILITY", "Cohere\u2019s models are packaged with inference engines that deliver better runtime performance at a lower cost than open-source equivalents", "FLEXIBLE DEPLOYMENT OPTIONS", "Cohere models are accessible through a SaaS API, cloud services (e.g., OCI, AWS SageMaker, Bedrock), and private deployments (VPC and on-prem)", "PRIVACY", "Customer data is never used in training base models, and customers have complete control over customization and model inputs/outputs", "LLM University", "Curious about large language models?", "LLM University offers an approachable and structured\u2028 curriculum to get you speaking our language.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/slo", "page_title": "SaaS Agreement | Cohere", "headers": ["Cohere API SaaS Service"], "content": ["Try now", "Service Level Objective (SLO)", "During the Term , Cohere will use commercially reasonable efforts to make the Cohere API SaaS Services available on a 24 x 7 x 365 basis with an objective of achieving the following \u201cMonthly Uptime Percentages \u201dfor the corresponding \u201cCovered Service \u201das set out in the following table (the \"Service Level Objective \"or \"SLO \"):", "If Cohere fails to meet the SLO in any calendar month during the Term, and if Customer meets its obligations under the Agreement, Customer will be entitled to receive the following credits as applicable (the \u201cService Level Credits \u201d):", "*Credited %: Percentage of the monthly invoice for the respective Covered Service which does not meet the SLO that will be credited to the next monthly invoice of Customer.", "The Service Level Credits are Customer \u2019s sole and exclusive remedy and Cohere \u2019s sole and entire obligation and liability in respect of any failure of Cohere to meet the SLO. In order to receive any Service Level Credits, Customer must notify Cohere at support@cohere.ai within 30 days from the time that Customer becomes entitled to receive a Service Level Credit. Customer must provide Cohere with the date and time upon which the SLO failure occurred, and any additional information requested by Cohere. If Customer does not comply with these requirements, Customer forfeits its right to receive a Service Level Credit. Based on the information provided by Customer and Cohere \u2019s internal system logs, Cohere will make a determination, in its sole discretion, whether Customer will receive a Service Level Credit.", "Any Service Level Credits issued by Cohere apply to outstanding or future invoices only and are forfeited upon termination or expiration of the Agreement. Cohere is not required to issue refunds or to make payments against such credits under any circumstances, including without limitation after termination or expiration of the Agreement.", "Cohere may unilaterally amend this SLO, in whole or in part, by giving Customer prior notice of such amendment or posting notice of such amendment on the Website.", "The SLO does not apply to any features within the Covered Services that are marked as Alpha or Beta or any features excluded from the Covered Services as stated in Cohere \u2019s Documentation.", "Capitalized terms used but not defined herein, will have the meanings set out in the Cohere Software as a Service Agreement or Terms of Use (collectively, the \u201cAgreement \u201d).", "\u201cCovered Service \u201dmeans services that comprise the Cohere API SaaS Services available at the following Websites: (i) os.cohere.ai; and (ii) api.cohere.ai.", "\u201cMonthly Uptime Percentage \u201dmeans the total percentage of availability of a Covered Service within any given month, excluding unavailability of such Covered Service due to one or more of the following: (i) acts or omissions of Customer or its Permitted Users, including any modifications made to the Covered Service or any breach of the terms of the Agreement; (ii) Customer 's failure to adhere to Cohere \u2019s recommendations, including hardware or software configuration necessary to meet minimum system requirements for the Covered Service; (iii) acts or omissions of Cohere when complying with the request or acting under the direction of Customer; (iv) spikes in demand for system resources driven by Customer for which Customer and Cohere did not previously agree in writing; (v) scheduled or emergency maintenance of the Covered Service; (vi) downtime of third party service providers; (vii) Force Majeure; or (viii) suspension permitted under the Agreement.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/partners", "page_title": "Partners | Cohere", "headers": ["Our Partners", "Product, cloud, and distribution partner", "Strategic consulting collaboration", "Cloud and distribution partner", "Cloud and distribution partner", "Vector Database Partners", "Weaviate", "Pinecone", "Qdrant", "Milvus", "Zilliz", "Community & Research Partners", "Vector Institute", "Mila"], "content": ["Try now", "Cohere offers best-in-class data security, performance, and flexible hosting thanks to our partners.", "Oracle delivers a comprehensive AI portfolio integrated in its cloud applications on a best-in-class AI infrastructure and with state-of-the-art generative AI innovations.", "McKinsey is a global management consulting firm and Cohere\u2019s strategic collaborator. McKinsey and Cohere will use AI to transform client businesses, while prioritizing responsible and secure implementation of this technology.", "Get complete control over your environment and data privacy, while benefiting from the power of language AI.", "Access our powerful natural language processing solution on Google\u2019s flexible and scalable cloud platform.", "The open-source Weaviate vector search engine stores both objects and vectors. The text2vec-cohere module allows you to use Cohere embeddings directly in the Weaviate vector search engine as a vectorization module.", "The Pinecone vector database makes it easy to build high-performance vector search applications. Use Cohere to generate language embeddings, then store them in Pinecone and use them for semantic search.", "Qdrant is an open-source vector search engine. When used with Cohere, you\u2019ll gain a comprehensive solution for specific text analysis use cases.", "Milvus is a highly flexible, reliable, and blazing-fast cloud-native, open-source vector database. It powers embedding similarity search and AI applications and strives to make vector databases accessible to every organization. Milvus is a graduated-stage project of the LF AI & Data Foundation.", "Zilliz Cloud is a cloud-native vector database that stores, indexes, and searches for billions of embedding vectors to power enterprise-grade similarity search, recommender systems, anomaly detection, and more. Zilliz Cloud provides a fully-managed Milvus service, made by the creators of Milvus that allows for easy integration with vectorizers from Cohere and other popular models. Purpose-built to solve the challenge of managing billions of embeddings, Zilliz Cloud makes it easy to build applications for scale.", "Curious about Large Language Models?", "LLM University offers an approachable and structured curriculum to get you speaking our language.  ", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/data-usage-policy", "page_title": "Data Usage Policy | Cohere", "headers": ["Cohere Data Usage Policy", "Common API Data Usage FAQs", "Privacy FAQs"], "content": ["Try now", "Last Update: October 30, 2023", "Cohere maintains robust data usage and retention practices that recognize and respect our customers\u2019 rights regarding their data.", "For Cohere\u2019s commercial customers using our API services, we will:", "API data is data provided or generated by commercial customers when directly calling the API, specifically customer prompt, model output, embeddings, documents, and fine-tune data. For clarity, this does not include data sent through our free services or metadata, such as an aggregate count of input prompt tokens. It also does not include our end-user products, such as the Playground or Coral.", "When opted out, commercial customers\u2019 API data is not used for model training and purposes related to improving our models or enhancing Cohere\u2019s offerings. We do not use raw input prompts or model generations to profile how our models are being used. We may use aggregated metadata, such as a count of input prompt tokens, to understand usage and improve our system performance. For up to 30 days, unless required by law or to meet our contractual commitments, we will also retain API data to monitor for compliance with our terms and acceptable use guidelines, e.g., for abuse and misuses of our services.", "Additionally, for highly sensitive scenarios, Cohere may grant a zero data retention opt-out after review. This removes our ability to log any data. Contact us at ", " for more information.", "Opting in to share your data helps improve our models for you. API data undergoes a sanitization process before storage. Before being fed into any training models, our team removes common sources of personal information.", "Opt-out flags are managed at an organization level within Cohere\u2019s services. You may have multiple Cohere organizations, and you may manage the opt-out flags independently. Opt out selections made prior to August 10, 2023 will remain the same.", "Please review our ", " for more information on how we handle the personal information we collect and process via our website. We do not process personal information through our Playground and/or API and we strongly recommend that our customers do not upload any personal information when using our services.", "Commercial customers can access opt out flags in the ", " Check with your organization\u2019s administrator if you do not have access to the settings page, or contact us at ", ".", "Cohere collects and retains API data up to 30 days, unless required by law or to meet our contractual commitments, to monitor for compliance with our terms and acceptable use guidelines, e.g., for abuse and misuse of our services. After 30 days, commercial customers' API data will be automatically deleted unless requested otherwise. For more information on Cohere\u2019s organizational data retention practices, please contact us at ", ".", "All of our services are hosted by Google Cloud Platform, which is located in the United States. We only train and host our models in the United States.", "Commercial customers\u2019 API data may be opted out of model training and for purposes related to improving our models or enhancing Cohere\u2019s offerings. We may use aggregated metadata, such as a count of input prompt tokens, to understand usage and improve our system performance.", "When you are opted in to model training, API data undergoes a sanitization process upon storage. Before being fed into any training models, our team removes common sources of personal information.", "All data stored for any purpose is subject to our SOC2 controls regarding storage and processing. You can request a copy of the SOC2 Type II report at our ", "For more information on our privacy practices, please read our ", "We only keep your personal information as long as it is operationally or legally necessary. After that, we will either destroy or anonymize the information.", "Yes, upon request, and subject to Cohere\u2019s legal and regulatory obligations, we can delete your personal information. For more information on our privacy practices, including making data deletion requests, please read our ", "Yes, we can provide our DPA to potential customers for review. Please contact ", " for more details.", "Yes, Cohere is SOC 2 Type I and Type II compliant. If you would like a copy of our SOC 2 Type I and Type II report, please visit our ", "Cohere complies with all applicable privacy laws as they relate to personal information under the company\u2019s control. We also proactively work with our enterprise customers to ensure that we support them with their privacy compliance.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events", "page_title": "Events - AI Conferences, NLP Webinars, LLM Hackathons | Cohere", "headers": ["Cohere For AI - Geo Regional Africa Kick-off Event", "Upcoming Events", "Cohere\u2019s  past events"], "content": ["Try now", "Cohere Event \u2014 Jan 23, 2024", "Cohere for AI -  Geo Regional Africa Kick-off Event", "All types", "All cities", "Jan 23, 2024 \u2014 Online", "Cohere For AI - Geo Regional Africa Kick-off Event", "Jan 24, 2024 \u2014 Online", "Cohere For AI Fireside Chat: Jacob Hilton", "Jan 31, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Jorge Ruiz Reyes, Political Scientist", "Feb 02, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Lewis Tunstall, Engineer @ Hugging Face", "Feb 05, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Muhammad Uzair Khattak, MBZUAI Graduate", "Feb 07, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Timoth\u00e9e Darcet, PhD @ Meta AI & INRIA", "Feb 13, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Akifumi Wachi, Sr Research Scientist", "Feb 21, 2024 \u2014 Online", "Cohere For AI - Guest Speaker:Arthur Douillard, Sr. Research @DeepMind", "Feb 24, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Weijia Shi, PhD student at UW", "Mar 06, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Adil Salim, Sr. researcher at Microsoft", "Mar 20, 2024 \u2014 Online", "Cohere For AI - Guest Speaker: Sanae Lotfi, PhD student at NYU", "Recommended", "Here are some of the highlights from past events, hosted by Cohere. ", "Join our NLP community and attend Cohere\u2019s events", "Network and learn from professionals who \nare forging the future path for NLP. ", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/careers", "page_title": "Careers | Cohere", "headers": ["Our team of ML/AI experts is passionate about helping developers solve real-world problems. From our offices in Toronto, London, and Palo Alto, we work at the cutting edge of machine learning to unlock the power of language AI for all developers.", "With great power comes great responsibility", "We lead with trust and assume positive intent", "We are all co:owners, and we act like it", "We create more value than we capture", "We embrace a Beginner\u2019s Mind", "A high bar is our floor", "Work isn\u2019t the opposite of play", "With great power comes great responsibility", "We lead with trust and assume positive intent", "We are all co:owners, and we act like it", "We create more value than we capture", "We embrace a Beginner\u2019s Mind", "A high bar is our floor", "Work isn\u2019t the opposite of play", "Our Benefits"], "content": ["Try now", "We are Cohere", "Our Values", "Cohere was founded by Nick Frosst, Ivan Zhang and Aidan Gomez, a co-author of the groundbreaking paper (with over 100k+ citations), \u201cAttention Is All You Need,\u201d which revolutionized NLP. As our company continues to grow, we are guided by our 7 values.", "Responsibility means more than getting the technology right. It\u2019s about staying tethered to the societal impact of our work and anticipating risks \u2014 both in what we build and who we share it with.", "We lead with empathy, we give trust freely and assume positive intent. We interact with each other humbly and judiciously and actively look for ways to make our coworkers and users feel special.", "We show up and do what needs to be done; we do our work without ego; we ask for help when we need it and give help whenever we have a chance.", "Real-world applications and solving real user problems are our north star. They tell us where they want to go, and we figure out how to get them there.", "While we aim to be masters of our craft, we approach problems with a beginner\u2019s mindset: humbly, and with curiosity and eagerness to learn.", "We want to be excellent across our product and all of our disciplines. No matter where we start, we continually look for opportunities to learn and improve, always pursuing excellence.", "We inject fun and lightness into our work to create delight and excitement from the time we log on. We can be serious without taking ourselves too seriously.", "Responsibility means more than getting the technology right. It\u2019s about staying tethered to the societal impact of our work and anticipating risks \u2014 both in what we build and who we share it with.", "We lead with empathy, we give trust freely and assume positive intent. We interact with each other humbly and judiciously and actively look for ways to make our coworkers and users feel special.", "We show up and do what needs to be done; we do our work without ego; we ask for help when we need it and give help whenever we have a chance.", "Real-world applications and solving real user problems are our north star. They tell us where they want to go, and we figure out how to get them there.", "While we aim to be masters of our craft, we approach problems with a beginner\u2019s mindset: humbly, and with curiosity and eagerness to learn.", "We want to be excellent across our product and all of our disciplines. No matter where we start, we continually look for opportunities to learn and improve, always pursuing excellence.", "We inject fun and lightness into our work to create delight and excitement from the time we log on. We can be serious without taking ourselves too seriously.", "We want Cohere to be the place where everyone does the best work of their career. So we make thoughtful decisions about how we work, the perks we provide, and how we create a diverse and inclusive work environment. We also foster technical creativity and innovation through internal hackathons, demos, tech talks, and achievement recognition programs that form a cornerstone of our culture at Cohere.", "Join us and work alongside some of the world\u2019s best talent from the likes of Apple, Meta AI, Amazon and Google Brain, as well as full-time staff adjunct professors from Stanford, Oxford, and University College London.", "01", ":", "We cover 100% of premiums across health, dental, vision & travel.", "02", ":", "Mental health is a priority for us all. We offer additional coverage for accessing mental health providers/services. As well as access to an EAP (employee assistance program) and a Headspace account for employees and up to 5 family members.", "03", ":", "We offer RRSP, 401K, and Pension Scheme contributions.", "04", ":", "We also offer the following benefits: a one-time workspace improvement allowance (for your at home workspace setup), free daily lunch credit, a monthly fitness/wellness allowance, an annual arts and culture allowance (for hobbies and things that enrich you as a person like improv lessons or art classes), and a monthly quality time allowance (to help improve the quality of the time you spend outside of work like a meal delivery or cleaning service).", "01", ":", "We want everyone who contributes to our success to get a \u2018piece of the pie.\u2019", "01", ":", "We are remote-first culture and have folks located all around the world! We have working spaces in Toronto, Ontario, Palo Alto and San Francisco as well as London, UK. To remain inclusive, we ensure that all company events, meetings, and opportunities are set up for our distributed teams. If you\u2019re not located near one of our working spaces, we offer a co-working allowance that can be used towards working at your favorite co-working space.", "02", ":", "Every employee at Cohere gets 6 weeks of paid time off and locally observed\u00a0 holidays. We also provide unlimited sick days.", "01", ":", "All new parents (including those who adopt or go through a surrogate journey) are eligible to receive 100% of their salary for 6 months.", "02", ":", "The decision and journey to having kids is different for each person and family. Part of our commitment to diversity is being able to support the widest variety of these scenarios as we can so we offer financial support for egg freezing and IVF for those in the US, Canada, and the UK.", "01", ":", "We cover education and continued learning for full-time employees. The benefit can be put towards attending a conference or taking courses.", "We are well", "Backed", "We raised our Series C from some of the leading AI investors in North America, have partnerships with AWS, Oracle Cloud Infrastructure, Google Cloud, allowing enterprises to host securely on their preferred environment.", "We are", "Ambitious", "We believe that language AI will fundamentally change the relationship between humans and machines. Our goal is to build products that help any developer make this future a reality, faster.", "We are", "Diverse & Inclusive", "We are focused on creating a work environment where everyone can thrive, whether they had a previous life at Google Brain, Apple, a startup, or as an online poker champion (true story).", "We are well", "Backed", "We raised our Series C from some of the leading AI investors in North America, have partnerships with AWS, Oracle Cloud Infrastructure, Google Cloud, allowing enterprises to host securely on their preferred environment.", "We are", "Ambitious", "We believe that language AI will fundamentally change the relationship between humans and machines. Our goal is to build products that help any developer make this future a reality, faster.", "We are", "Diverse & Inclusive", "We are focused on creating a work environment where everyone can thrive, whether they had a previous life at Google Brain, Apple, a startup, or as an online poker champion (true story).", "Ready to get started? ", "We\u2019re always looking for new people to join our team. Check out our listings below.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research", "page_title": "Research | Cohere For AI ", "headers": ["Cohere For AI", "Cohere For AI", "Fundamental research lab", "Open Science Initiative", "Our programs", "AYA research project", "Research grant", "Scholars program", "AYA research project", "Research grant", "Scholars program", "Spotlight papers", "Past events and videos ", "Meet our research team", "Frequently Asked Questions"], "content": ["Try now", "Cohere For AI is a non-profit research lab that seeks to solve complex machine learning problems. We support fundamental research that explores the unknown, and are focused on creating more points of entry into machine learning research.", "Cohere For AI is a non-profit research lab that seeks to solve complex machine learning problems. We support fundamental research that explores the unknown, and are focused on creating more points of entry into machine learning research.", "We work at the frontier of AI progress with the goal of solving cutting edge scientific problems. We see contributions to traditional conferences and publications in journals as an important part of our work, but also support efforts that go \u201cbeyond the research paper\u201d and encourage scientific communication through different mediums. We drive the creation of new research spaces and breakthroughs that changes where, how and by whom research is done. We believe that technology is powerful, and empowering different perspectives ensures responsible innovation.", "We\u2019re not just another research group. We are a hybrid lab with both a dedicated research staff and support for open science initiatives. We collaborate openly with independent researchers all over the world to conduct top-tier ML research.", "Our open science research community is a space where researchers, engineers, linguists, social scientists, and lifelong learners connect and collaborate with each other. We come together from over 100 countries around the world and support large and small scale research collaborations.", "Advancing the NLP space through our programs.", "making models multilingual", "About", "Aya is a global project that aims to build a multilingual language model via instruction tuning that harnesses the collective wisdom and contributions of people from all over the world. It is the largest open science initiative to-date in AI involving 100+ independent researchers all over the world. Aya is open to anyone who is passionate about advancing the field of natural language processing and is committed to promoting open science. Learn more about the project in ", ".", "Join the ", " and ", " in your language today.", "academic support", "Benefits", " are designed to support academic partners who are conducting research with the goal of releasing a peer-reviewed scientific artifact. Our program provides academic partners, developers, researchers, and other members of our community with subsidized access to the Cohere API.", "Exploring the unknown together", "About", "Our Scholars Program provides the opportunity to work alongside some of the best research and engineering experts in the world. We have created an open and supportive environment that provides an alternative point of entry into machine learning research.", "making models multilingual", "About", "Aya is a global project that aims to build a multilingual language model via instruction tuning that harnesses the collective wisdom and contributions of people from all over the world. It is the largest open science initiative to-date in AI involving 100+ independent researchers all over the world. Aya is open to anyone who is passionate about advancing the field of natural language processing and is committed to promoting open science. Learn more about the project in ", ".", "Join the ", " and ", " in your language today.", "academic support", "Benefits", " are designed to support academic partners who are conducting research with the goal of releasing a peer-reviewed scientific artifact. Our program provides academic partners, developers, researchers, and other members of our community with subsidized access to the Cohere API.", "Exploring the unknown together", "About", "Our Scholars Program provides the opportunity to work alongside some of the best research and engineering experts in the world. We have created an open and supportive environment that provides an alternative point of entry into machine learning research.", "Research is inherently a human endeavor, and our event series provide insights from beginning to breakthrough.", "Video", "Fireside Chat: Colin Raffel", "Video", "8-bit Methods for Efficient Deep Learning", "Video", "Mechanistic Interpretability: Getting Started", "Video", "Fireside Chat: Pablo Samuel Castro", "Video", "Career creation for non-standard candidates", "Video", "Fireside Chat: Samy Bengio", "Our staff brings together machine learning experts to contribute to progress in machine learning through fundamental research. We are committed to open collaboration, and empowering more points of entry into machine learning research through our scholars program.", "Sara hooker", "head, Cohere for ai", "Marzieh Fadaee", "Senior Research Scientist", "Julia Kreutzer", "SENIOR RESEARCH SCIENTIST", "Ahmet \u00dcst\u00fcn", "Research Scientist", "Beyza Ermis", "Research Scientist", "Madeline Smith", "Operations and Community Lead", "Brittawnya Prince", "Operations Associate", "Luiza Pozzobon", "Research Scholar", "Viraat Aryabumi", "Research Scholar", "Nikolas Gritsch", "Research Scholar", "John Dang", "Research Scholar", "Lu\u00edsa Shimabucoro", "Research Scholar", "Ayomide Odumakinde", "Research Scholar", "Younesse Kaddar", "Research Scholar", " Aakanksha", "Research Scholar", "Arash Ahmadian Dehkordi", "Research Scholar", "In 2017, a team of friends, classmates, and engineers started a distributed research collaboration, with a focus on creating a medium for early-career AI enthusiasts to engage with experienced researchers \u2013 they called it \u201cfor.ai.\u201d Two of those co-founding members, Aidan Gomez and Ivan Zhang, later went on to co-found Cohere, and many of the original members went on to do exciting things (pursuing PhDs, working at industry and academic labs).", "At the time, For AI was one of the first community-driven research groups to support independent researchers around the world. Today, Cohere is proud to reintroduce For AI as Cohere For AI, a dedicated research lab and community for exploring the unknown, together. Watch the C4AI history video ", ".", "We do not charge for participating in any of our programs, and are committed to supporting educational outreach programs, which include compute resources and infrastructure needed to participate in machine learning research.", "Our full list of positions are listed ", ".", "To stay up to date on upcoming talks, ", ".", "You can also apply to ", " or follow us on ", " and ", ".", "Join our open science community", "Collaborate with researchers, engineers, linguists, social scientists, and lifelong learners from 100+ countries on top-tier ML research.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/about", "page_title": "About | Cohere", "headers": ["We\u2019re building the future of language AI", "We\u2019re building the future of language AI", "We\u2019re driven by cutting-edge research", "Pioneering the future of language AI for business", "We\u2019re a collaborative team of experts", "We\u2019re driven by cutting-edge research", "Pioneering the future of language AI for business", "We\u2019re a collaborative team of experts", "Our founders & leaders carry our vision forward", "Executive Leadership Team", "ML Leaders", "Functional Leaders", "Our Investors", "Our Offices"], "content": ["Try now", "Cohere empowers every developer and enterprise to build amazing products and capture true business value with language AI.\n", "Cohere empowers every developer and enterprise to build amazing products and capture true business value with language AI.\n", "At Cohere, we believe that the union of research and product will realize a world where technology commands language in a way that\u2019s as compelling and coherent as ourselves. We live at the forefront of ML/AI research to bring the latest advancements in language AI to our platform. ", "Cohere\u2019s cutting-edge large language models are built on Transformer architecture and trained on supercomputers, providing NLP solutions that don\u2019t need expensive ML development. With a world-class team of experts, we're dedicated to helping companies revolutionize their operations and maximize potential in real-world business applications.", "We are ML/AI engineers, thinkers, and champions who are passionate about exploring the potential of language AI to make our world a better place. With diverse experience and perspectives, we work together to bring advancements in language AI to developers everywhere.\n", "At Cohere, we believe that the union of research and product will realize a world where technology commands language in a way that\u2019s as compelling and coherent as ourselves. We live at the forefront of ML/AI research to bring the latest advancements in language AI to our platform. ", "Cohere\u2019s cutting-edge large language models are built on Transformer architecture and trained on supercomputers, providing NLP solutions that don\u2019t need expensive ML development. With a world-class team of experts, we're dedicated to helping companies revolutionize their operations and maximize potential in real-world business applications.", "We are ML/AI engineers, thinkers, and champions who are passionate about exploring the potential of language AI to make our world a better place. With diverse experience and perspectives, we work together to bring advancements in language AI to developers everywhere.\n", "Aidan Gomez", "Co-founder  & CEO", "Nick Frosst", "co-founder", "Ivan Zhang", "co-Founder", "Martin Kon", "\u2028President & CoO", "Saurabh Baji", "SVP Eng", "Nora Beatty", "VP people", "Jaron Waldman", "CPO", "Phil Blunsom", "VP Science", "Nils Reimers", "Dir. ML", "Sara Hooker", "VP Cohere For AI", "Rodrigue Hajjar", "VP ENG", "Meini Heuberger", "VP Revenue", "Laura Moss", "VP Finance", "Kosta Starostin", "Dir. Legal", "Toronto, CA", "Palo Alto, US", "San Francisco, US", "London, UK", "\u201cVery large language models are now giving computers a much better understanding of human communication. The team at Cohere is building technology that will make this revolution in natural language understanding much more widely available.\u201d", "PRESS", "Check out the latest publications featuring Cohere", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "All Use Cases", "Restaurant Customer Inquiries", "Get a visualization for all the FAQs for your restaurant ", "Transcript Summarization", "Extract the main point of a conversation", "Spelling and Grammar Check", "Given a list of text, it will remove improper capitalization and transform the sentence to Capitalized Text", "Headline Market Analysis", "Given a news headline, identify if the category of the article is technology, economy, or health", "Email Copy", "Generate an Email given a description", "Customer Persona", "Describe a potential customer persona for a product", "Blog Post Body Paragraph", "Generate a body paragraph for a blog post", "Instagram Caption", "Generate a caption with a call to action", "Email Keyword Extraction", "Extract product name and return address from an email", "Article Summarization", "Summarize the main point of a paragraph into a single sentence", "Product Description to Benefits", "Turns a product description into a list of functional, emotional, and social benefits.", "Blog Title Alternatives Generator", "Suggest three alternative titles with a better marketing copy", "Insurance FAQ Classification", "Using our classification model, customer questions can be easily classified into the correct category.", "Business Model Canvas", "Generate a business model canvas for a satellite ", "Spam Detection", "Given email titles, identify if the email is spam or not spam", "Interview Question Generator", "Generate a list of 5 interview questions for a senior software engineer", "Contract Entity Extraction", "Use generative models to easily parse out entities from text", "LinkedIn Post Generator", "Uplevel your networking game with LinkedIn posts about your job", "Subreddit Titles", " Identify unique clusters from posts made on the subreddits", "Product Classifier", "Given a set of sample products, identify if it is a type of power tool, screw, or cement board", "Ad Headline", "Generate promotional ad copy variants", "Invoice Entity Extraction", "Given an invoice, the model is able to extract the named entities in the contract", "Blog Post Outline", "Generate an outline for a blog post given topic and audience", "Tweet Classification", "Given a set of tweets, determine if the tweet is about a demo, a piece of news, or an event", "Y-combinator Thread Titles", "Identify 5 distinct groupings from this dataset of text", "Concept Relationships", "A is to B as C is to D relationships", "Product Description", "Create a product description given keywords and a product name", "Startup Idea Generator", "Come up with a startup problem and solution for an industry", "Correct Errors in Voice to Text", "Correct all the grammatical errors within an excerpt of a transcript", "Blog Post Intro Paragraph", "Generate an intro paragraph for a blog post", "Ad Copy AIDA Framework", "Generate an ad copy following the AIDA Framework", " Article Title Classifier", "Given an article title, identify if the article is about business news, cooking, or arts & culture", "Blog Post Title", "Generate 5 titles with topic, tone, and audience", "Real Estate Listing", "Convert a description into a textual advertisement", "NeurIPS22 Accepted Papers", "Identify 4 unique clusters from the dataset of Arxiv titles and their abstracts", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/classify", "page_title": "Classify - Advanced Text Classification with LLMs | Cohere", "headers": ["Discover insights and patterns in your text  ", "Discover insights and patterns in your text  ", "Discover insights and patterns in your text  ", "What's possible with Classify", "Why Classify", "Integrate large language models into your builds", "Multiple platform support", "World-class models", "Integrate large language models into your builds", "Multiple platform support", "World-class models", "Classify resources", "No need to know ML or AI \u2014 try Cohere now!", "No need to know ML or AI \u2014 try Cohere now!"], "content": ["Try now", "classify", "Classify organizes information for more effective content moderation, analysis, and chatbot experiences.\n", "classify", "Classify organizes information for more effective content moderation, analysis, and chatbot experiences.\n", "classify", "Classify organizes information for more effective content moderation, analysis, and chatbot experiences.\n", "Customer support tagging", "Sentiment analysis", "Content moderation", "Classify text with just a few examples", "No matter your level of experience with ML/AI, the Cohere Platform makes it easy to classify text in your applications.", "Our API can be used in different libraries that fit every stack. We make it easy to build machine learning into your apps with our SDKs. Our API allows you to leverage classifiers on top of large language models without the need for additional training or hosting.", "Cohere works with all common languages through native SDK support or encapsulated REST calls.", "Our large language models have been trained on billions of words, allowing them to understand nuance and context.", "Our API can be used in different libraries that fit every stack. We make it easy to build machine learning into your apps with our SDKs. Our API allows you to leverage classifiers on top of large language models without the need for additional training or hosting.", "Cohere works with all common languages through native SDK support or encapsulated REST calls.", "Our large language models have been trained on billions of words, allowing them to understand nuance and context.", "Get started with Cohere today!", "Reach out to us and let\u2019s discuss your text classification needs.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/security", "page_title": "Security - Protecting Your Data | Cohere", "headers": ["Security", "Information Security Program", "Third-Party Audits", "Third-Party Penetration Testing", "Roles and Responsibilities", "Security Awareness Training", "Confidentiality", "Background Checks", "Cloud SecurityCloud Infrastructure Security", "Data Hosting Security", "Encryption at Rest", "Encryption in Transit", "Vulnerability Scanning", "Logging and Monitoring", "Business Continuity and Disaster Recovery", "Incident Response", "Access Security", "Permissions and Authentication", "Least Privilege Access Control", "Quarterly Access Reviews", "Password Requirements", "Password Managers", "Vendor and Risk Management", "Annual Risk Assessments", "Vendor Risk Management", "Quarterly Access Reviews", "Contact Us"], "content": ["Try now", "We have an Information Security Program in place that is communicated throughout the organization. Our Information Security Program follows the criteria set forth by the SOC 2 Framework. SOC 2 is a widely known information security auditing procedure created by the American Institute of Certified Public Accountants.", "Our organization undergoes independent third-party assessments to test our security and compliance controls.", "We perform an independent third-party penetration at least annually to ensure that the security posture of our services is uncompromised.", "Roles and responsibilities related to our Information Security Program and the protection of our customer\u2019s data are well defined and documented. Our team members are required to review and accept all of the security policies.", "Our team members are required to go through employee security awareness training covering industry standard practices and information security topics such as phishing and password management.", "All team members are required to sign and adhere to an industry standard confidentiality agreement prior to their first day of work.", "We perform background checks on all new team members in accordance with local laws.", "All of our services are hosted with Google Cloud Platform (GCP). They employ a robust security program with multiple certifications. For more information on our provider\u2019s security processes, please visit ", ".", "All of our data is hosted on Google Cloud Platform (GCP) databases. These databases are all located in the United States. Please reference the above vendor specific documentation linked above for more information.", "All databases are encrypted at rest.", "Our applications encrypt in transit with TLS/SSL only.", "We perform vulnerability scanning and actively monitor for threats.", "We actively monitor and log various cloud services.", "We use our data hosting provider\u2019s backup services to reduce any risk of data loss in the event of a hardware failure. We utilize monitoring services to alert the team in the event of any failures affecting users.", "We have a process for handling information security events which includes escalation procedures, rapid mitigation and communication.", "Access to cloud infrastructure and other sensitive tools are limited to authorized employees who require it for their role. Where available we have Single Sign-on (SSO), 2-factor authentication (2FA) and strong password policies to ensure access to cloud services are protected.", "We follow the principle of least privilege with respect to identity and access management.", "We perform quarterly access reviews of all team members with access to sensitive systems.", "All team members are required to adhere to a minimum set of password requirements and complexity for access.", "All company issued laptops utilize a password manager for team members to manage passwords and maintain password complexity.", "We undergo at least annual risk assessments to identify any potential threats, including considerations for fraud.", "Vendor risk is determined and the appropriate vendor reviews are performed prior to authorizing a new vendor.", "We perform quarterly access reviews of all team members with access to sensitive systems.", "If you have any questions, comments or concerns or if you wish to report a potential security issue, please contact", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/responsibility", "page_title": "Responsibility - Developing Safer Language Models | Cohere", "headers": ["Responsibility", "Developing safer language models", "Responsibility is an active research area at Cohere that requires interdisciplinary collaboration.", "Responsibility principles", "Our process", "Usage Guidelines"], "content": ["Try now", "Large language models help us build powerful systems that augment human capabilities, but they can also be used in ways that cause harm. We believe language model providers must develop their technologies responsibly. This means proactively working to build safer products and accepting a duty of care to users, the environment, and society.", "We\u2019ve invested in technical and non-technical measures to mitigate potential harm and make our development processes transparent. We've also established an advisory Responsibility Council empowered to inform our product and business decisions.", "We are excited about the potential for algorithmic language understanding to improve accessibility, enable human computer interaction, and allow for broader human-to-human dialogue. If you want to use our API to help create a better world, or stay informed of new developments, let us know.", "as we hope it will become.", " and listen to those affected.", " commensurate with expected and actual impacts.", " the societal impacts of our work.", "We believe that no technology can be made absolutely safe; machine learning is no exception. This requires anticipating and accounting for risks during our development process. We run adversarial attacks, filter our training data for harmful text, and measure our models against safety research benchmarks. We also evaluate evolving risks with monitoring tools designed to identify harmful model outputs.", "We recognize that misuse of powerful language models will disproportionately impact the most vulnerable, so we aim to balance safety considerations and equity of access. This is an ongoing process. As we release early versions of our technology, we\u2019ll work closely with our partners and users to ensure its safe and responsible use.", "We require our users to abide by Cohere\u2019s Usage Guidelines. Access will be revoked if these terms are not followed. If you spot the Cohere Platform being used in a harmful or otherwise unproductive way, please report it to us.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/embeddings", "page_title": "Embeddings - Text Embeddings with Advanced Language Models | Cohere", "headers": ["Uncover trends and compare languages easily", "Uncover trends and compare languages easily", "Uncover trends and compare languages easily", "What's possible with Embeddings", "Why Embeddings", "Embeddings performance", "Multilingual support", "Scalability", "Flexible deployment", "Embeddings performance", "Multilingual support", "Scalability", "Flexible deployment", "Embeddings resources ", "Don\u2019t want to code?  Try our playground instead", "Don\u2019t want to code?  Try our playground instead"], "content": ["Try now", "embeddings", "For ML teams looking to build their own text analysis applications, Embeddings offers high-performance and accuracy in English and 100+ languages.", "embeddings", "For ML teams looking to build their own text analysis applications, Embeddings offers high-performance and accuracy in English and 100+ languages.", "embeddings", "For ML teams looking to build their own text analysis applications, Embeddings offers high-performance and accuracy in English and 100+ languages.", "Semantic search", "Topic modeling", "Recommendations", "Multilingual Embeddings", "\"It's next to impossible to gain access to Language AI and the experts building the technology. That\u2019s why working with Cohere has been such a great experience. Anytime we have a new idea, their incredible team works with us to drive projects forward.\"", "Cohere\u2019s Embed model leads the industry in accuracy and performance, and works well with noisy datasets", "Over 100 languages are supported, so the same topics, products and issues are identified the same way in each", "Cohere Embed supports data compression, reducing storage and compute requirements", "Cohere models can be accessed through a SaaS API, on cloud services (e.g. OCI, AWS SageMaker, Bedrock) and soon through private deployments (VPC and on-premise)", "Cohere\u2019s Embed model leads the industry in accuracy and performance, and works well with noisy datasets", "Over 100 languages are supported, so the same topics, products and issues are identified the same way in each", "Cohere Embed supports data compression, reducing storage and compute requirements", "Cohere models can be accessed through a SaaS API, on cloud services (e.g. OCI, AWS SageMaker, Bedrock) and soon through private deployments (VPC and on-premise)", "Simple APIs, powerful results", "No matter your level of experience with ML/AI, the Cohere Platform makes it easy to classify text in your applications.", "Get started with Cohere today!", "Reach out to us and let\u2019s discuss your embedding needs.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/saas-agreement", "page_title": "SaaS Agreement | Cohere", "headers": ["Commercial SaaS Agreement "], "content": ["Try now", "Last updated: April 8, 2022", "\n        ", "\n        ", "\n      ", "\n      ", "\n    ", "\n      ", "\n      ", "\n    ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n    ", "\n      ", "\n      ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n      ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n      ", "\n      ", "\n      ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n      ", "\n      ", "\n    ", "\n      ", "\n    ", "\n        ", "\n      ", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/privacy", "page_title": "Privacy Policy | Cohere", "headers": ["Cohere Privacy Policy", "COLLECTION AND USE OF PERSONAL INFORMATION", "Information collected through the Cohere Services:", "Information collected through our Website or other interactions with you:", "SHARING OF PERSONAL INFORMATION", "INFORMATION ABOUT OUR WEBSITE", "SAFEGUARDS", "RETENTION", "ACCESS TO INFORMATION", "INTERNATIONAL TRANSFERS OF INFORMATION", "UPDATES TO THE PRIVACY POLICY", "CONTACT US"], "content": ["Try now", "Last Update: Aug 4, 2023", "Cohere Inc. (\u201cCohere\u201d) values and respects your privacy. We have prepared this privacy policy to explain the manner in which we collect, use, and disclose personal information through our website located at Cohere.com (\u201cWebsite\u201d) and through our AI-powered services offered via our Website, including our language model platform APIs (\u201cServices\u201d). For information on how we collect, use, retain or store non-personal customer data, please review our ", ".", "Generally speaking, our Services are not intended to process personal information and we recommend that our customers not upload any personal information when using our Services. However, in certain circumstances, our customers may choose to upload personal information. In those instances, our customers are responsible for complying with applicable privacy laws when collecting, using, or disclosing personal information through the Services for the purposes set out above, including by providing and obtaining all necessary notices and consents. Information we collect or generate is retained in accordance with our contractual commitments to our customers.", "If you have any questions regarding the personal information we process on behalf of one of our customers, we encourage you to first contact the customer directly and/or review their applicable privacy policy.", "If you signed up to receive marketing communications from one of our customers and now wish to unsubscribe, please click on the \u201cunsubscribe\u201d link in the marketing communication or contact the customer directly.", "We may collect information about you when you:", "Requesting Access to the Services:\u00a0Before accessing or\u00a0using the Services, you may be asked to complete the Cohere API application. As part of this application, we will collect your business contact information, information about your organization (including the type of organization and the jurisdiction in which it is located), and information about your intended use of the Services (including a description of your use case, your previous experiences using similar services, and estimates of the frequency with which you will use the Services).\u00a0We use this information to understand the purposes for which you are seeking to use the Services, to assess whether the Services are right for your organization and the intended purposes, and to customize your user experience.", ":\u00a0If your application is approved, you will be required to create an account in order to access and use the Services. To create your account, we collect your first and last name, email address, and a password that you create. We use this information to create and administer your account and facilitate your access to and use of the Services. We strongly recommend that you do not disclose your password to anyone. We will never ask you for your password in any unsolicited communication (such as letters, phone calls, or email messages). If you become aware of any unauthorized access to or use of your account, you are required to notify us immediately.", ":\u00a0We may send you communications (including by mail and email) regarding our platform and other products and services that we believe are of interest to you. You can unsubscribe at any time by clicking the \u201cunsubscribe\u201d link included at the bottom of each email or by contacting us at the contact information provided in the \u201cContact Us\u201d section below. Please note that you may continue to receive transactional or account-related communications from us after you unsubscribe.", ":\u00a0From time to time, we may offer you the opportunity to participate in one of our surveys. The information obtained through our surveys is used in an aggregated, de-identified form. We use this information to help us understand our users and to enhance our Website.", ":\u00a0If you apply for a job with us, we may collect certain personal information about you (such as information that would be contained in a resume, cover letter, or other employment-related materials). We use this information for the purpose of processing, evaluating, and responding to your application. We may also collect information from third parties about you, such as references you list, for the same purpose.", ": When you contact us with a comment, question, or complaint, you may be asked for information that identifies you, such as your name, address, and a telephone number, along with additional information that we need to help us promptly answer your question or respond to your comment. We may retain this information to assist you in the future and to improve our customer service and service offerings.", "We do not rent, sell, or disclose your personal information to third parties without your consent, except as set forth below or as required or permitted by law", ": Your personal information may be transferred (or otherwise made available) to our affiliates and business partners to meet our legal and compliance requirements and operational needs.", ":\u00a0Your personal information will be transferred (or otherwise made available) to certain third parties that provide services on our behalf. We use service providers to provide services, such as\u00a0cloud computing, data storage, publishing, and analytics.\u00a0Our service providers are only provided with the information they need to perform their designated functions and are not authorized to use or disclose personal information for their own marketing or other purposes.\u00a0Our service providers may be located in the U.S., Canada, or other foreign jurisdictions", ":\u00a0We\u00a0and our Canadian, U.S., and other foreign service providers may provide your personal information in response to a search warrant to other legally valid inquiry or order, or to another organization for the purposes of investigating a breach of an agreement or contravention of law or detecting, suppressing, or preventing fraud, or as otherwise may be required or permitted by applicable Canadian, U.S., or other law or legal process, which may include lawful access by U.S. or foreign courts, law enforcement, or other government authorities. Your personal information may also be disclosed where necessary for the establishment, exercise, or defence of legal claims, and to investigate or prevent actual or suspected loss or harm to persons or property.", ": We may transfer any information we have about you as an asset in connection with a proposed or completed merger, acquisition, or sale (including transfers made as part of insolvency or bankruptcy proceedings) involving all or part of Cohere.com or as part of a corporate reorganization or other change in corporate control.", "Visiting our Website:\u00a0In general, you can visit our Website without telling us who you are or submitting any personal information. However, we collect the IP (Internet protocol) addresses of all visitors to our Website and other related information, such as page requests, browser type, operating system, and average time spent on our Website. We use this information to help us understand our Website activity and to monitor and improve our Website.", ":\u00a0Our Website uses cookies, pixel tags (also called web beacons), and other tracking technologies (which we collectively refer to as \"cookies\"). A cookie is a tiny element of data that our Website sends to a user\u2019s browser, which may then be stored on the user\u2019s hard drive so that we can recognize the user\u2019s computer or device when they return. You may set your browser to notify you when you receive a cookie or to not accept certain cookies. However, if you decide not to accept cookies from our Website, you may not be able to take advantage of all of the Website features. We also use third-party tools that use cookies and other technologies to collect information about your device and your behaviour on our Website in order to show us how you interact with our Website. The information collected includes information on what you see during your visit and actions you take when navigating our Website (e.g., clicks, mouse movements, hovers, page visits, scrolling, typing, form fills) as well as your device\u2019s IP address, device screen size, device type, browser information, session duration, city and country, and preferred language. We use this information to optimize the user experience on our website, measure your engagement and help us identify errors on our Website.", ":\u00a0We may use a third party, such as Google Analytics, to help us gather and analyze information about the areas visited on the Website (such as the pages most read, time spent, search terms, and other engagement data) in order to evaluate, derive insights from, and improve the user experience and the Website (including the organization from which you access the website). These third parties may use cookies and other tracking technologies. For more information about Google Analytics or to prevent the storage and processing of this data (including your IP address) by Google, you can download and install the browser plug-in available at the following link: https://tools.google.com/dlpage/gaoptout?hl=en . You can also obtain additional information on Google Analytics\u2019 data privacy and security at the following links:", "\u00a0and\u00a0", ": Our Website may contain links to other websites that\u00a0Cohere does not own or operate. We provide links to third-party websites as a convenience to the user. These links are not intended as an endorsement of or referral to the linked websites. The linked websites have separate and independent privacy policies, notices, and terms of use. We do not have any control over such websites, and therefore we have no responsibility or liability for the manner in which the organizations that operate such linked websites may collect, use or disclose, secure and otherwise treat personal information. We encourage you to read the privacy policy of every website you visit.", "We have implemented reasonable administrative, technical, and physical measures in an effort to safeguard the personal information in our custody and control against theft, loss, and unauthorized access, use, modification, and disclosure. We restrict access to personal information on a need-to-know basis to employees and authorized service providers who require access to fulfill their job requirements.", "We only keep your personal\u00a0 information as long as it is operationally or legally necessary. After that, we will either destroy or anonymize the information.", "If we receive a request from an individual to access or update personal information that we maintain on behalf of a customer, we will direct that individual to the relevant customer. We will assist our customers wherever possible in responding to individual access requests.", "Subject to applicable law and your location, you may have the right to access, update or correct , delete, transfer, and object to processing \u00a0 your personal information in our control. Where we rely on your consent as a lawful basis to process your personal information, you have the right to withdraw your consent at any time. However, in some cases, withdrawing consent will mean that we can no longer provide you with certain services or perform certain tasks where the information is required to do so.", "Data subject requests, including what data subject rights you may be entitled to, with respect to personal information in our control can be made by emailing or writing to us at the contact information set out below. We may request certain personal information for the purpose of verifying the identity of the individual seeking access to their personal information records.", "Cohere is a global organization with affiliates, partners, and service providers located in many countries around the world. For that reason, Cohere may transfer, store, and/or receive certain personal information across geographical borders to and/or from Cohere affiliates or service providers in other countries working on our behalf in accordance with applicable law. Examples of countries we transfer personal information to include, but are not limited to, Canada, the United States, and the United Kingdom. As a result, in certain circumstances, other foreign governments, courts, law enforcement agencies, or regulatory agencies may be entitled to access the personal information collected and held by Cohere, its affiliates, partners, and service providers.", "We may update this privacy policy periodically to reflect changes to our privacy practices. We encourage you to periodically review this page to ensure that you are familiar with those changes. We will indicate at the top of this privacy policy when it was most recently updated.", "If you have any questions or comments about this privacy policy or the manner in which we or our service providers treat your personal information, or to make data subject requests , please contact our Privacy Officer at:", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/summarize", "page_title": "Summarize - Instant Text Summarization with LLMs | Cohere", "headers": ["Instantly encapsulate the key points of a document", "Instantly encapsulate the key points of a document", "Instantly encapsulate the key points of a document", "What's possible with Summarize", "Why Summarize", "Accurate summaries in seconds", "Informed, well-written summaries ", "Summarize your way", "Accurate summaries in seconds", "Informed, well-written summaries ", "Summarize your way", "See how Cohere works. Try our playground.", "See how Cohere works. Try our playground."], "content": ["Try now", "summarize", "Powered by our large language models, Summarize provides text summarization capabilities at scale.", "summarize", "Powered by our large language models, Summarize provides text summarization capabilities at scale.", "summarize", "Powered by our large language models, Summarize provides text summarization capabilities at scale.", "News articles", "Summarization Docs", "Simple APIs, powerful results", "No matter your level of experience with ML/AI, the Cohere Platform makes it easy to summarize text in your applications.", "Get high-quality summaries that accurately capture the most important information from your documents.", "Generate precise and coherent summaries that are well-written, informed, and grammatically correct.", "Input up to 100,000 characters, choose your preferred formatting options, and receive concise TLDRs.", "Get high-quality summaries that accurately capture the most important information from your documents.", "Generate precise and coherent summaries that are well-written, informed, and grammatically correct.", "Input up to 100,000 characters, choose your preferred formatting options, and receive concise TLDRs.", "Get started with Cohere today!", "Reach out to us and let\u2019s discuss your text summarization needs.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Sanae-Lotfi-2023", "page_title": "Cohere For AI - Guest Speaker: Sanae Lotfi, PhD student at NYU", "headers": ["Cohere For AI - Guest Speaker: Sanae Lotfi, PhD student at NYU"], "content": ["Try now", "Date", ": ", "Mar 20, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", " I am a PhD student at the Center for Data Science at NYU advised by Professor Andrew Gordon Wilson and a Visiting Researcher in the Fundamental AI Research (FAIR) group at Meta AI where I work with Brandon Amos. I work on the foundations of deep learning. My goal is to understand and quantify generalization in deep learning, and use this understanding to build more robust and reliable machine learning models.\u00a0", " Modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. We provide the first non-vacuous generalization bounds for pretrained large language models (LLMs), indicating that language models are capable of discovering regularities that generalize to unseen data. In particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, accelerating bound computation on massive datasets. To achieve the extreme level of compression required for non-vacuous generalization bounds, we devise SubLoRA, a low-dimensional non-linear parameterization. Using this approach, we find that larger models have better generalization bounds and are more compressible than smaller models", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/pricing", "page_title": "Pricing - Affordable NLP & Language Model Solutions | Cohere", "headers": ["Scalable, affordable pricing", "Free", "Free", "Production", "Production", "Enterprise", "Enterprise", "Frequently Asked Questions"], "content": ["Try now", "Access our models directly through our API and pay for what you use, or deploy on Amazon SageMaker. ", "Get free, rate-limited usage for learning and prototyping. Usage is free until you go into production", "Power your business with hosted language models.", "Features", "Default Model", "Input", "Output", "Command", "Input", "$", "1.00", "/", "1M tokens", "Output", "$", "2.00", "/", "1M tokens", "Default Model", "Input", "Output", "Command Light", "Input", "$", "0.30", "/", "1M tokens", "Output", "$", "0.60", "/", "1M tokens", "Fine-tuned Model", "Training", "Command Light", "Training", "$", "1.00", "/", "1M tokens", "Fine-tuned Model", "Input", "Output", "Input", "$", "0.30", "/", "1M tokens", "Output", "$", "0.60", "/", "1M tokens", "We charge differently for input and output tokens. You are charged based on the sum of tokens processed. Only Command Light is available for fine-tuning at this moment. Fine-tuning inference costs will be the same as our base models.", "If you require dedicated model instances, dedicated support channels, or custom deployment options, get in touch with our sales team", "\"Cohere excels at delivering high quality, low-latency language AI models and really supporting them. Having Cohere's team as an extension of ours lets us go 10x faster.\"", "Language models understand \u201ctokens\u201d rather than characters or bytes. The number of tokens per word depends on the complexity of the text. Simple text may approach 1 token per word on average, while complex texts may use less common words that require 3-4 tokens per word on average. For more details on tokens, refer to ", ".", "Want to try it first?", "Create an account and build with Cohere", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/terms-of-use", "page_title": "Terms of Use | Cohere", "headers": ["Terms Of Use."], "content": ["Try now", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "\n      ", "\n    ", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Adil-Salim-2024", "page_title": "Cohere For AI - Guest Speaker: Adil Salim, Sr. researcher at Microsoft", "headers": ["Cohere For AI - Guest Speaker: Adil Salim, Sr. researcher at Microsoft"], "content": ["Try now", "Date", ": ", "Mar 06, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", "Adil is a Senior researcher at Microsoft Research at Machine Learning Foundations Group focusing on Language Models.\u00a0", "PHI-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/deployment-options/oracle", "page_title": "Oracle | Cohere", "headers": ["Oracle resources"], "content": ["Try now", "powerful and secure generative ai", "Cohere\u2019s generative models are available via Oracle Cloud Infrastructure, helping customers solve their most pressing business challenges.", "Customers can use a native, generative AI service powered by specially trained Cohere foundational models and built on Oracle Cloud Infrastructure (OCI). The solution enables them to securely incorporate their own data to train and deploy specific Cohere models through OCI and experience immediate business benefits in their applications.", "Oracle is also integrating Cohere models into its portfolio of business applications, including Oracle Fusion Cloud Applications and Oracle NetSuite. Our collaboration will help organizations worldwide automate end-to-end business processes, improve decision-making, and enhance customer experiences.", "Learn more about the Cohere and Oracle partnership on both company websites.", "ORACLE PRESS RELEASE", "Oracle Cloud Infrastructure Delivers Enterprise Generative AI Service", "ORACLE ", "Generative AI at Oracle", "ORACLE PRESS RELEASE", "Oracle to Deliver Powerful and Secure Generative AI Services for Business", "ORACLE ", "Oracle's AI Capabilities", "ORACLE", "What Customers Are Saying About Oracle AI", "\"Using Cohere\u2019s foundational models, customers can securely incorporate their own data to train specific models, deploy them on best-in-class AI infrastructure through OCI, and experience the business benefits immediately in their applications.\"", "READY TO GET STARTED?", "Sign up for a free trial to try a range of Oracle AI and ML services.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/deployment-options/aws", "page_title": "Cohere on AWS", "headers": ["Cohere Models on AWS", "AWS resources"], "content": ["Try now", "AMAZON WEB SERVICES", "Access Cohere\u2019s generative and representative models in private Amazon Bedrock and Amazon SageMaker environments.", "Cohere's models are available on ", " and ", ", allowing AWS customers to access Cohere\u2019s models as an API, eliminating the need to manage the underlying infrastructure.", "Customers can easily access Cohere\u2019s models and incorporate AI features into their applications and workflows without the need to train models or build the infrastructure required for deploying and managing LLMs.", "With Cohere on AWS, customer data remains securely managed and protected. AWS provides enterprise-grade security and privacy tools integrated into the environment, enabling encryption, access control, and privacy.", "Amazon Bedrock and Amazon SageMaker are optimized for AI workloads, with high-performance compute, storage, and networking resources. It\u2019s fully managed, which makes it easy to deploy Cohere\u2019s models at scale.", "Cohere\u2019s models are trained from scratch from known, purchased, or public domain data sources and are subject to extensive adversarial testing and bias mitigation.", "Generative AI capabilities for secure enterprise environments", "*available with fine-tuning", "Deepen your understanding of Cohere\u2019s collaboration with AWS.", "AWS Machine Learning Blog", "Build financial search applications using multilingual embedding model", "AWS News blog", "Bedrock now provides access to Cohere Command Light and Embed models", "AWS Machine learning blog", "Announcing New Tools to Help Every Business Embrace Generative AI", "GitHub", "Cohere AWS SDK", "Cohere docs", "Cohere on AWS", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/real-estate-listing", "page_title": "Real Estate Listing", "headers": ["Real Estate Listing", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Convert a description into a textual advertisement", "Generate a creative ad for House with panoramic mountain views in Joshua Tree", "Come explore this stunning House with panoramic mountain views in Joshua Tree! This 3 bedroom, 2.5 bathroom House features a spacious open floor plan with plenty of natural light. The living room, which is located just off the kitchen, features a beautiful fireplace and large windows that provide sweeping mountain and desert views. The kitchen is well-equipped with stainless steel appliances and a large island with a breakfast bar. The Master bedroom is located downstairs and features a walk-in closet and an en-suite bathroom with a soaking tub and a separate shower. Two additional bedrooms are located upstairs, each with their own bathroom. The House also features a large laundry room and a 2 car garage. This is a must-see!", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/article-title-classifier", "page_title": " Article Title Classifier", "headers": [" Article Title Classifier", "Examples", "Request", "Response", "API Request"], "content": ["Try now", "Given an article title, identify if the article is about business news, cooking, or arts & culture", "Any decent trader spends 90% of their efforts exploring how they could be wrong. That should apply to everyone\u2019s decision making.", "Value", "Confidence level", "Any decent trader spends 90% of their efforts exploring how they could be wrong. That should apply to everyone\u2019s decision making.", "Confidence level", "Business news", "89", "%", "Arts & Culture", "7", "%", "Cooking", "4", "%", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "On the Fairness Impacts of Hardware Selection in Machine Learning", "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation", "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI", "Locally Differentially Private Document Generation Using Zero Shot Prompting", "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation", "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models", "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale", "The Presidio Recommendations on Responsible Generative AI - World Economic Forum", "Evaluating the Social Impact of Generative AI Systems in Systems and Society", "Intriguing Properties of Quantization at Scale", "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting ", "BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model\t", "Lifting the Veil on Hyper-parameters for Value-based Deep Reinforcement Learning", "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research", "PASHA: Efficient HPO and NAS with Progressive Resource Allocation", "Efficient Methods for Natural Language Processing: A Survey", "MTEB: Massive Text Embedding Benchmark\t", "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics ", "Dec 06, 2023", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Nov 29, 2023", "Evaluation", "Reproducibility", "Language", "Generative Models", "Evaluation", "Reproducibility", "Language", "Generative Models", "Oct 25, 2023", "Responsible AI", "Safety", "AI Policy", "Data", "Responsible AI", "Safety", "AI Policy", "Data", "Oct 24, 2023", "Safety", "Privacy", "Safety", "Privacy", "Oct 22, 2023", "Evaluation", "Efficiency", "Language", "Generative Models", "Evaluation", "Efficiency", "Language", "Generative Models", "Oct 11, 2023", "Safety", "Generative Models", "Language Models", "Safety", "Generative Models", "Language Models", "Sep 12, 2023", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Sep 08, 2023", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Jun 14, 2023", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Jun 12, 2023", "Responsible AI", "Responsible AI", "May 30, 2023", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "May 07, 2023", "Safety", "Reasoning", "Interpretability", "Safety", "Reasoning", "Interpretability", "May 05, 2023", "Collaboration", "BigScience", "Collaboration", "BigScience", "May 05, 2023", "Reinforcement Learning", "Reinforcement Learning", "Apr 24, 2023", "Safety", "Reproducibility", "Responsible AI", "Safety", "Reproducibility", "Responsible AI", "Apr 11, 2023", "Efficiency", "Optimization", "Neural Architecture Search", "Efficiency", "Optimization", "Neural Architecture Search", "Mar 24, 2023", "Efficiency", "Generative Models", "Efficiency", "Generative Models", "Mar 19, 2023", "Representation Learning", "Representation Learning", "Mar 02, 2023", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/neurips22-accepted-papers", "page_title": "NeurIPS22 Accepted Papers", "headers": ["NeurIPS22 Accepted Papers", "Dataset", "Sample Response", "API Request"], "content": ["Try now", "Identify 4 unique clusters from the dataset of Arxiv titles and their abstracts", "#", "TEXT", "1.", "On Kernel-Target Alignment. We describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems, then combine the solutions of these with message passing algorithms. We show empirically that these methods excel in avoiding local minima and produce better solutions with fewer function evaluations than existing global optimization methods. To develop these methods, we introduce a notion of coupling between variables of optimization that generalizes the notion of coupling that arises from factoring functions into terms that involve small subsets of the variables. It therefore subsumes the notion of independence between random variables in statistics, sparseness of the Hessian in nonlinear optimization, and the generalized distributive law. Despite being more general, this notion of coupling is easier to verify empirically -- making structure estimation easy -- yet it allows us to migrate well-established inference methods on graphical models to the setting of global optimization.", "2.", "On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms. Zeroth-order (ZO) optimization is widely used to handle challenging tasks, such as query-based black-box adversarial attacks and reinforcement learning. Various attempts have been made to integrate prior information into the gradient estimation procedure based on finite differences, with promising empirical results. However, their convergence properties are not well understood. This paper makes an attempt to fill up this gap by analyzing the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. We provide a convergence guarantee for the prior-guided random gradient-free (PRGF) algorithms. Moreover, to further accelerate over greedy descent methods, we present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. Finally, our theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks.", "3.", "Communication Complexity of Distributed Convex Learning and Optimization. We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.", "4.", "Straggler Mitigation in Distributed Optimization Through Data Encoding. Slow running or straggler tasks can significantly reduce computation speed in distributed computation. Recently, coding-theory-inspired approaches have been applied to mitigate the effect of straggling, through embedding redundancy in certain linear computational steps of the optimization algorithm, thus completing the computation without waiting for the stragglers. In this paper, we propose an alternate approach where we embed the redundancy directly in the data itself, and allow the computation to proceed completely oblivious to encoding. We propose several encoding schemes, and demonstrate that popular batch algorithms, such as gradient descent and L-BFGS, applied in a coding-oblivious manner, deterministically achieve sample path linear convergence to an approximate solution of the original problem, using an arbitrarily varying subset of the nodes at each iteration. Moreover, this approximation can be controlled by the amount of redundancy and the number of nodes used in each iteration. We provide experimental results demonstrating the advantage of the approach over uncoded and data replication strategies.", "5.", "Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods. We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\\sqrt{\\dim}$ in convergence rate over traditional stochastic gradient methods, where $\\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.", "6.", "Efficient Pure Exploration in Adaptive Round model. In the adaptive setting, many multi-armed bandit applications allow the learner to adaptively draw samples and adjust sampling strategy in rounds. In many real applications, not only the query complexity but also the round complexity need to be optimized. In this paper, we study both PAC and exact top-$k$ arm identification problems and design efficient algorithms considering both round complexity and query complexity. For PAC problem, we achieve optimal query complexity and use only $O(\\log_{\\frac{k}{\\delta}}^*(n))$ rounds, which matches the lower bound of round complexity, while most of existing works need $\\Theta(\\log \\frac{n}{k})$ rounds. For exact top-$k$ arm identification, we improve the round complexity factor from $\\log n$ to $\\log_{\\frac{1}{\\delta}}^*(n)$, and achieve near optimal query complexity. In experiments, our algorithms conduct far fewer rounds, and outperform state of the art by orders of magnitude with respect to query cost.", "7.", "On Top-k Selection in Multi-Armed Bandits and Hidden Bipartite Graphs. This paper discusses how to efficiently choose from $n$ unknowndistributions the $k$ ones whose means are the greatest by a certainmetric, up to a small relative error. We study the topic under twostandard settings---multi-armed bandits and hidden bipartitegraphs---which differ in the nature of the input distributions. In theformer setting, each distribution can be sampled (in the i.i.d.manner) an arbitrary number of times, whereas in the latter, eachdistribution is defined on a population of a finite size $m$ (andhence, is fully revealed after $m$ samples). For both settings, weprove lower bounds on the total number of samples needed, and proposeoptimal algorithms whose sample complexities match those lower bounds.", "8.", "MaxGap Bandit: Adaptive Algorithms for Approximate Ranking. This paper studies the problem of adaptively sampling from K distributions (arms) in order to identify the largest gap between any two adjacent means. We call this the MaxGap-bandit problem. This problem arises naturally in approximate ranking, noisy sorting, outlier detection, and top-arm identification in bandits. The key novelty of the MaxGap bandit problem is that it aims to adaptively determine the natural partitioning of the distributions into a subset with larger means and a subset with smaller means, where the split is determined by the largest gap rather than a pre-specified rank or threshold. Estimating an arm\u2019s gap requires sampling its neighboring arms in addition to itself, and this dependence results in a novel hardness parameter that characterizes the sample complexity of the problem. We propose elimination and UCB-style algorithms and show that they are minimax optimal. Our experiments show that the UCB-style algorithms require 6-8x fewer samples than non-adaptive sampling to achieve the same error.", "9.", "A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem. Bandit learning is characterized by the tension between long-term exploration and short-term exploitation. However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings, one might like to run a ``greedy'' algorithm, which always makes the optimal decision for the individuals at hand --- but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm.", "10.", "Identifying Outlier Arms in Multi-Armed Bandit. We study a novel problem lying at the intersection of two areas: multi-armed bandit and outlier detection. Multi-armed bandit is a useful tool to model the process of incrementally collecting data for multiple objects in a decision space. Outlier detection is a powerful method to narrow down the attention to a few objects after the data for them are collected. However, no one has studied how to detect outlier objects while incrementally collecting data for them, which is necessary when data collection is expensive. We formalize this problem as identifying outlier arms in a multi-armed bandit. We propose two sampling strategies with theoretical guarantee, and analyze their sampling efficiency. Our experimental results on both synthetic and real data show that our solution saves 70-99% of data collection cost from baseline while having nearly perfect accuracy.", "11.", "Combinatorial Pure Exploration with Bottleneck Reward Function. In this paper, we study the Combinatorial Pure Exploration problem with the Bottleneck reward function (CPE-B) under the fixed-confidence (FC) and fixed-budget (FB) settings.In CPE-B, given a set of base arms and a collection of subsets of base arms (super arms) following a certain combinatorial constraint, a learner sequentially plays a base arm and observes its random reward, with the objective of finding the optimal super arm with the maximum bottleneck value, defined as the minimum expected reward of the base arms contained in the super arm.CPE-B captures a variety of practical scenarios such as network routing in communication networks, and its unique challenges fall on how to utilize the bottleneck property to save samples and achieve the statistical optimality. None of the existing CPE studies (most of them assume linear rewards) can be adapted to solve such challenges, and thus we develop brand-new techniques to handle them.For the FC setting, we propose novel algorithms with optimal sample complexity for a broad family of instances and establish a matching lower bound to demonstrate the optimality (within a logarithmic factor).For the FB setting, we design an algorithm which achieves the state-of-the-art error probability guarantee and is the first to run efficiently on fixed-budget path instances, compared to existing CPE algorithms. Our experimental results on the top-$k$, path and matching instances validate the empirical superiority of the proposed algorithms over their baselines.", "12.", "CryptoNAS: Private Inference on a ReLU Budget. Machine learning as a service has given raise to privacy concerns surrounding clients' data and providers' models and has catalyzed research in private inference (PI): methods to process inferences without disclosing inputs. Recently, researchers have adapted cryptographic techniques to show PI is possible, however all solutions increase inference latency beyond practical limits. This paper makes the observation that existing models are ill-suited for PI and proposes a novel NAS method, named CryptoNAS, for finding and tailoring models to the needs of PI. The key insight is that in PI operator latency cost are inverted: non-linear operations (e.g., ReLU) dominate latency, while linear layers become effectively free. We develop the idea of a ReLU budget as a proxy for inference latency and use CryptoNAS to build models that maximize accuracy within a given budget. CryptoNAS improves accuracy by 3.4% and latency by 2.4x over the state-of-the-art.", "13.", "Evaluating Efficient Performance Estimators of Neural Architectures. Conducting efficient performance estimations of neural architectures is a major challenge in neural architecture search (NAS). To reduce the architecture training costs in NAS, one-shot estimators (OSEs) amortize the architecture training costs by sharing the parameters of one supernet between all architectures. Recently, zero-shot estimators (ZSEs) that involve no training are proposed to further reduce the architecture evaluation cost. Despite the high efficiency of these estimators, the quality of such estimations has not been thoroughly studied. In this paper, we conduct an extensive and organized assessment of OSEs and ZSEs on five NAS benchmarks: NAS-Bench-101/201/301, and NDS ResNet/ResNeXt-A. Specifically, we employ a set of NAS-oriented criteria to study the behavior of OSEs and ZSEs, and reveal their biases and variances. After analyzing how and why the OSE estimations are unsatisfying, we explore how to mitigate the correlation gap of OSEs from three perspectives. Through our analysis, we give out suggestions for future application and development of efficient architecture performance estimators. Furthermore, the analysis framework proposed in our work could be utilized in future research to give a more comprehensive understanding of newly designed architecture performance estimators. The code is available at https://github.com/walkerning/aw_nas.", "14.", "Discovering Neural Wirings. The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training -- as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1we boost the ImageNet accuracy by 10% at ~41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery with a combinatorial number of subnetworks. Code and pretrained models are available at https://github.com/allenai/dnw while additional visualizations may be found at https://mitchellnw.github.io/blog/2019/dnw/.", "15.", "Hierarchical Neural Architecture Search for Deep Stereo Matching. To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to allow the network the ability to choose among a set of operations (\\eg convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first \\emph{end-to-end} hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (\\ie, feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015, and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. Code available at https://github.com/XuelianCheng/LEAStereo", "16.", "PyGlove: Symbolic Programming for Automated Machine Learning. Neural networks are sensitive to hyper-parameter and architecture choices. Automated Machine Learning (AutoML) is a promising paradigm for automating these choices. Current ML software libraries, however, are quite limited in handling the dynamic interactions among the components of AutoML. For example, efficient NAS algorithms, such as ENAS and DARTS, typically require an implementation coupling between the search space and search algorithm, the two key components in AutoML. Furthermore, implementing a complex search flow, such as searching architectures within a loop of searching hardware configurations, is difficult. To summarize, changing the search space, search algorithm, or search flow in current ML libraries usually requires a significant change in the program logic.", "17.", "Theory-Inspired Path-Regularized Differential Network Architecture Search. Despite its high search efficiency, differential architecture search (DARTS) often selects network architectures with dominated skip connections which lead to performance degradation. However, theoretical understandings on this issue remain absent, hindering the development of more advanced methods in a principled way. In this work, we solve this problem by theoretically analyzing the effects of various types of operations, e.g. convolution, skip connection and zero operation, to the network optimization. We prove that the architectures with more skip connections can converge faster than the other candidates, and thus are selected by DARTS. This result, for the first time, theoretically and explicitly reveals the impact of skip connections to fast network optimization and its competitive advantage over other types of operations in DARTS. Then we propose a theory-inspired path-regularized DARTS that consists of two key modules: (i) a differential group-structured sparse binary gate introduced for each operation to avoid unfair competition among operations, and (ii) a path-depth-wise regularization used to incite search exploration for deep architectures that often converge slower than shallow ones as shown in our theory and are not well explored during search. Experimental results on image classification tasks validate its advantages. Codes and models will be released.", "18.", "BRP-NAS: Prediction-based NAS using GCNs. Neural architecture search (NAS) enables researchers to automatically explore broad design spaces in order to improve efficiency of neural networks. This efficiency is especially important in the case of on-device deployment, where improvements in accuracy should be balanced out with computational demands of a model. In practice, performance metrics of model are computationally expensive to obtain. Previous work uses a proxy (e.g., number of operations) or a layer-wise measurement of neural network layers to estimate end-to-end hardware performance but the imprecise prediction diminishes the quality of NAS. To address this problem, we propose BRP-NAS, an efficient hardware-aware NAS enabled by an accurate performance predictor-based on graph convolutional network (GCN). What is more, we investigate prediction quality on different metrics and show that sample efficiency of the predictor-based NAS can be improved by considering binary relations of models and an iterative data selection strategy. We show that our proposed method outperforms all prior methods on NAS-Bench-101, NAS-Bench-201 and DARTS. Finally, to raise awareness of the fact that accurate latency estimation is not a trivial task, we release LatBench -- a latency dataset of NAS-Bench-201 models running on a broad range of devices", "19.", "Adapting Neural Architectures Between Domains. Neural architecture search (NAS) has demonstrated impressive performance in automatically designing high-performance neural networks. The power of deep neural networks is to be unleashed for analyzing a large volume of data (e.g. ImageNet), but the architecture search is often executed on another smaller dataset (e.g. CIFAR-10) to finish it in a feasible time. However, it is hard to guarantee that the optimal architecture derived on the proxy task could maintain its advantages on another more challenging dataset. This paper aims to improve the generalization of neural architectures via domain adaptation. We analyze the generalization bounds of the derived architecture and suggest its close relations with the validation error and the data distribution distance on both domains. These theoretical analyses lead to AdaptNAS, a novel and principled approach to adapt neural architectures between domains in NAS. Our experimental evaluation shows that only a small part of ImageNet will be sufficient for AdaptNAS to extend its architecture success to the entire ImageNet and outperform state-of-the-art comparison algorithms.", "20.", "Statistical Active Learning Algorithms. We describe a framework for designing efficient active learning algorithms that are tolerant to random classification noise. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of filtered random examples. It builds on the powerful statistical query framework of Kearns (1993). We show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of uncorrelated\" noise. The complexity of the resulting algorithms has information-theoretically optimal quadratic dependence on $1/(1-2\\eta)$, where $\\eta$ is the noise rate. We demonstrate the power of our framework by showing that commonly studied concept classes including thresholds, rectangles, and linear separators can be efficiently actively learned in our framework. These results combined with our generic conversion lead to the first known computationally-efficient algorithms for actively learning some of these concept classes in the presence of random classification noise that provide exponential improvement in the dependence on the error $\\epsilon$ over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efficient active differentially-private algorithms. This leads to the first differentially-private active learning algorithms with exponential label savings over the passive case.\"", "21.", "Active Inference in Concept Learning. People are active experimenters, not just passive observers, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that maximize the expected information gain, given current beliefs (Fedorov 1972, MacKay 1992, Oaksford & Chater 1994). In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept task (Tenenbaum 2000). The results of the experiment are analyzed in terms of the expected information gain of the questions asked by subjects.", "22.", "Adaptive Active Hypothesis Testing under Limited Information. We consider the problem of active sequential hypothesis testing where a Bayesian decision maker must infer the true hypothesis from a set of hypotheses. The decision maker may choose for a set of actions, where the outcome of an action is corrupted by independent noise. In this paper we consider a special case where the decision maker has limited knowledge about the distribution of observations for each action, in that only a binary value is observed. Our objective is to infer the true hypothesis with low error, while minimizing the number of action sampled. Our main results include the derivation of a lower bound on sample size for our system under limited knowledge and the design of an active learning policy that matches this lower bound and outperforms similar known algorithms.", "23.", "Active Information Retrieval. In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the sys(cid:173) tem responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of infor(cid:173) mation that can be presented to the user per iteration.", "24.", "Latent Structured Active Learning. In this paper we present active learning algorithms in the context of structured prediction problems. To reduce the amount of labeling necessary to learn good models, our algorithms only label subsets of the output. To this end, we query examples using entropies of local marginals, which are a good surrogate for uncertainty. We demonstrate the effectiveness of our approach in the task of 3D layout prediction from single images, and show that good models are learned when labeling only a handful of random variables. In particular, the same performance as using the full training set can be obtained while only labeling ~10\\% of the random variables.", "25.", "Buy-in-Bulk Active Learning. In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time. This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch. In this work, we study the label complexity of active learning algorithms that request labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once. In particular, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increase the total number of labels requested, it reduces the total cost required for learning.", "26.", "Faster Rates in Regression via Active Learning. This paper presents a rigorous statistical analysis characterizing regimes in which active learning significantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra flexibility leads to significantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including field estimation using wireless sensor networks and fault line detection.", "27.", "Lower Bounds for Passive and Active Learning. We develop unified information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander's capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of \"disagreement coefficient.\" For passive learning, our lower bounds match the upper bounds of Gine and Koltchinskii up to constants and generalize analogous results of Massart and Nedelec. For active learning, we provide first known lower bounds based on the capacity function rather than the disagreement coefficient.", "28.", "Optimal Decision Tree with Noisy Outcomes. A fundamental task in active learning involves performing a sequence of tests to identify an unknown hypothesis that is drawn from a known distribution. This problem, known as optimal decision tree induction, has been widely studied for decades and the asymptotically best-possible approximation algorithm has been devised for it. We study a generalization where certain test outcomes are noisy, even in the more general case when the noise is persistent, i.e., repeating the test on the scenario gives the same noisy output, disallowing simple repetition as a way to gain confidence. We design new approximation algorithms for both the non-adaptive setting, where the test sequence must be fixed a-priori, and the adaptive setting where the test sequence depends on the outcomes of prior tests. Previous work in the area assumed at most a constant number of noisy outcomes per test and per scenario and provided approximation ratios that were problem dependent (such as the minimum probability of a hypothesis). Our new approximation algorithms provide guarantees that are nearly best-possible and work for the general case of a large number of noisy outcomes per test or per hypothesis where the performance degrades smoothly with this number. Our results adapt and generalize methods used for submodular ranking and stochastic set cover. We evaluate the performance of our algorithms on two natural applications with noise: toxic chemical identification and active learning of linear classifiers. Despite our logarithmic theoretical approximation guarantees, our methods give solutions with cost very close to the information theoretic minimum, demonstrating the effectiveness of our methods.", "29.", "Active Regression by Stratification. We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of O(1/epsilon) cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches a the optimal risk using piecewise constant approximations.", "30.", "Convergence Rates of Active Learning for Maximum Likelihood Estimation. An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting -- maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extraround of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation. On the empirical side, the recent work in (Gu et al. 2012) and (Gu et al. 2014) (on active linear and logistic regression) shows the promise of this approach.", "31.", "Online Active Learning with Surrogate Loss Functions. We derive a novel active learning algorithm in the streaming setting for binary classification tasks. The algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a resulting set of labeled and weak-labeled points. Our algorithm jointly admits two crucial properties: theoretical guarantees in the general agnostic setting and a strong empirical performance. Our theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while our empirical study on 18 real-world datasets demonstrate that the algorithm outperforms standard baselines, including the Margin Algorithm, or Uncertainty Sampling, a high-performing active learning algorithm favored by practitioners.", "32.", "A rational decision making framework for inhibitory control. Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems.", "33.", "Inverse Optimal Control Adapted to the Noise Characteristics of the Human Sensorimotor System. Computational level explanations based on optimal feedback control with signal-dependent noise have been able to account for a vast array of phenomena in human sensorimotor behavior. However, commonly a cost function needs to be assumed for a task and the optimality of human behavior is evaluated by comparing observed and predicted trajectories. Here, we introduce inverse optimal control with signal-dependent noise, which allows inferring the cost function from observed behavior. To do so, we formalize the problem as a partially observable Markov decision process and distinguish between the agent\u2019s and the experimenter\u2019s inference problems. Specifically, we derive a probabilistic formulation of the evolution of states and belief states and an approximation to the propagation equation in the linear-quadratic Gaussian problem with signal-dependent noise. We extend the model to the case of partial observability of state variables from the point of view of the experimenter. We show the feasibility of the approach through validation on synthetic data and application to experimental data. Our approach enables recovering the costs and benefits implicit in human sequential sensorimotor behavior, thereby reconciling normative and descriptive approaches in a computational framework.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/blog-post-title", "page_title": "Blog Post Title", "headers": ["Blog Post Title", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate 5 titles with topic, tone, and audience", "Write 5 titles for a blog ideas for the keywords \"large language model\" or \"text generation\"", "The Benefits of Large Language Models for Text Generation", "The Impact of Large Language Models on Text Generation", "The Role of Large Language Models in Text Generation", "The Future of Large Language Models for Text Generation", "Large Language Models: The Next Big Thing in Text Generation", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/ad-copy-aida-framework", "page_title": "Ad Copy AIDA Framework", "headers": ["Ad Copy AIDA Framework", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate an ad copy following the AIDA Framework", "Generate an ad copy for the product: Wireless Earbuds.", "The copy consists of four parts, following the AIDA Framework. ", "1 - Attention", "2 - Interest", "3 - Desire", "4 - Action", "The copy for each part is clear and concise.", "1 - Attention: \"Looking for a pair of wireless earbuds?\"", "2 - Interest: \"These earbuds are perfect for working out.\"", "3 - Desire: \"You can use these earbuds to listen to music or take calls.\"", "4 - Action: \"Buy these earbuds today!\"", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/correct-errors-in-voice-to-text", "page_title": "Correct Errors in Voice to Text", "headers": ["Correct Errors in Voice to Text", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Correct all the grammatical errors within an excerpt of a transcript", "This is voice-to-text transcription corrector. Given a transcribed excerpt with errors, the model responds with the correct version of the excerpt.", "Incorrect transcription: I am balling into hay to read port missing credit card. I lost by card when I what's at the grocery store and I need to see sent a new one.", "Correct transcription: I am calling in today to report a missing credit card. I lost my card when I was at the grocery store and I need to be sent a new one.", "--", "Incorrect transcription: Can you repeat the dates for the three and five dear fixed mortgages? I want to compare them a gain the dates I was quoted by a other broker.", "Correct transcription: Can you repeat the rates for the three and five year fixed mortgages? I want to compare them against the rates I was quoted by another broker.", "--", "Incorrect transcription: I got got charged interest on ly credit card but I paid my pull balance one day due date. I not missed a pavement year yet. Man you reverse the interest charge?", "Correct transcription:", "I got charged interest on my credit card but I paid my full balance one day before the due date. I have not missed a payment yet. Can you reverse the interest charge?", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/product-description", "page_title": "Product Description", "headers": ["Product Description", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Create a product description given keywords and a product name", "Write a creative product description for a wireless headphone product named the CO-1T, with the keywords \"bluetooth\", \"wireless\", \"fast charging\" for a software developer who works in noisy offices, and describe benefits of this product.", "The CO-1T is a wireless headphone product that uses Bluetooth technology to connect to your devices. It has a long battery life and can be quickly charged using the included USB cable. The headphone is lightweight and comfortable, ideal for long periods of use. It has a built-in microphone for making calls, and a button to control the volume. The CO-1T is a great choice for anyone looking for a wireless headphone product with great battery life.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/concept-relationships", "page_title": "Concept Relationships", "headers": ["Concept Relationships", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "A is to B as C is to D relationships", "These are examples of a is to b as c is to d relationships ", "-- ", "Example: cat, kitten ", "Input: dog ", "Output: puppy. ", "Cat is to kitten as dog is to puppy. ", "-- ", "Example: big, small ", "Input: fast, slow ", "Output: slow. ", "Big is to fast as small is to slow. ", "-- ", "Example: breeze, wind ", "Input: stream ", "Output: current ", "Breeze is to wind as stream is to current. ", "-- ", "Example: guitar, music ", "Input: programmer ", "Output:", "Output: code. ", "Guitar is to music as programmer is to code.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/blog-post-intro-paragraph", "page_title": "Blog Post Intro Paragraph", "headers": ["Blog Post Intro Paragraph", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate an intro paragraph for a blog post", "Generate an intro paragraph of a blog post geared towards athletes who are looking to learn how to rock climb. The post should be enthusiastic and speak to people who are inactive and nervous.", "If you're an athlete who's looking to learn how to rock climb, then you've come to the right place! This blog post will give you all the information you need to know about how to get started in the sport. Rock climbing is a great way to stay active and challenge yourself in a new way. It's also a great way to make new friends and explore new places. So, what are you waiting for? Get out there and start climbing!", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/startup-idea-generator", "page_title": "Startup Idea Generator", "headers": ["Startup Idea Generator", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Come up with a startup problem and solution for an industry", "Industry: Workplace", "Problem to solve: Too much time spent making slide decks.", "Startup Idea: A platform that generates slide deck contents automatically based on a given outline.", "Startup Name: Deckerize", "--", "Industry: Home Decor", "Problem to solve: Not eveyone has the design skills and aesthetic taste in indoor landscaping.", "Startup Idea: An app that calculates the best position of your indoor plants for your apartment.", "Startup Name: Planteasy", "--", "Industry: Healthcare", "Problem to solve: Hearing aid battery lasts too short.", "Startup Idea: A hearing aid for the elderly that automatically adjusts its levels and with a battery lasting a whole week.", "Startup Name: Hearspan", "--", "Industry: Education", "Problem to solve: Students are bored and not engaged with the traditional school system.", "Startup Idea: An online school that lets students mix and match their own curriculum based on their interests and goals.", "Startup Name: Prime Age", "--", "Industry: Productivity", "Problem to solve:", "Problem to solve: Too much time is wasted to doing things not related to work. Startup Idea: A platform that allows you to outsource mundane tasks like housekeeping and errands. ", "Startup Name: Eden", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/newsroom", "page_title": "Newsroom - Press Releases & Press Kit | Cohere", "headers": ["Newsroom", "Resources", "Logo", "Team", "Culture"], "content": ["Try now", "The Cohere logos, assets and media resources are for editorial purposes only. For non-editorial uses, please request permission at press@cohere.com.", "01:", "Download logo lockups and symbols for digital and printed applications here.", "02:", "High-quality images of our founders and leaders.  ", "Aidan Gomez", "Co-Founder & CEo", "Nick Frosst\u2028", "Co-Founder", "IVAN ZHANG", "Co-Founder", "Sara hooker", "Head of c4ai", "Martin Kon", "President & COO", "03:", "An assortment of photographs of our offices and the people that make up Cohere. ", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/tweet-classification", "page_title": "Tweet Classification", "headers": ["Tweet Classification", "Examples", "Request", "Response", "API Request"], "content": ["Try now", "Given a set of tweets, determine if the tweet is about a demo, a piece of news, or an event", "You don\u2019t want to miss this live AMA session with Cohere CEO and Co-founder @aidangomezzz!\ud83d\udd25 Sign up here: https://hubs.li/Q01lLz9J0 If you have any questions for Aidan in advance, submit them here: https://hubs.li/Q01lLFz_0#ai #nlproc #ama", "Value", "Confidence level", "You don\u2019t want to miss this live AMA session with Cohere CEO and Co-founder @aidangomezzz!\ud83d\udd25 Sign up here: https://hubs.li/Q01lLz9J0 If you have any questions for Aidan in advance, submit them here: https://hubs.li/Q01lLFz_0#ai #nlproc #ama", "Confidence level", "Event", "100", "%", "Demo", "0", "%", "News", "0", "%", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/y-combinator-thread-titles", "page_title": "Y-combinator Thread Titles", "headers": ["Y-combinator Thread Titles", "Dataset", "Sample Response", "API Request"], "content": ["Try now", "Identify 5 distinct groupings from this dataset of text", "#", "TEXT", "1.", "Who is seeking a cofounder?", "2.", "Did your YC (or other incubator) startup fail?", "3.", "What are you doing now?", "4.", "Obtaining initial users for a startupWhich successful startups were rejected by YC?", "5.", "Is there VC appetite for defence related startups?", "6.", "Did reading HN bring anything valuable into your life?", "7.", "What are some of the best HN comments that you've read?", "8.", "What is your favorite HN post?", "9.", "Did reading HN bring anything valuable into your life?", "10.", "What is your favourite tech talk?", "11.", "What's the best technical eBook you have read?", "12.", "Do you know any good audio books for developers?", "13.", "As a technical founder what is the best business book you've read?", "14.", "Is it just me or did Google search recently get a lot worse?", "15.", "Is Someone Hijacking Google Images?", "16.", "What's Up with Google?", "17.", "Google removed my site from search results, what can I do?", "18.", "Passive income ideas?", "19.", "How do you create productive habits?", "20.", "What is your passive income 2021?", "21.", "What are your passive income sources?", "22.", "Do you know any good audio books for developers?", "23.", "Interesting (Non software) books?", "24.", "Non-tech books that have helped you grow professionally?", "25.", "Which books teach mental models?", "26.", "Anxiety is limiting my enjoyment of a wonderful career. Can you relate?", "27.", "Should I quit the field of software development?", "28.", "Did your life as a parent affected your life as a developer?", "29.", "Is it normal to fall out of love with coding?", "30.", "I made $24k over the last month. Now what?", "31.", "What is a way of making residual income with $5K a month?", "32.", "What kind of personal financial investment do you do?", "33.", "I sold my company last month for $5m. What do I do with the money?", "34.", "How do you all deal with lack of motivation?", "35.", "How do I stay motivated to learn?", "36.", "How to keep yourself accountable?", "37.", "How are you getting through (and back from) burning out?", "38.", "What are your favourite non-fiction books of all time?", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/blog-post-outline", "page_title": "Blog Post Outline", "headers": ["Blog Post Outline", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate an outline for a blog post given topic and audience", "Write a blog outline for a blog titled \"How Transformers made Large Language models possible\"", "The history of Transformers and how they made large language models possible", "How Transformers work and how they were able to create such large models", "The benefits of using Transformers for large language models", "The challenges of using Transformers for large language models", "The future of Transformers and large language models", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/invoice-entity-extraction", "page_title": "Invoice Entity Extraction", "headers": ["Invoice Entity Extraction", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Given an invoice, the model is able to extract the named entities in the contract", "Given a banking invoice, please return the name of the vendor.", "Bank Invoice: PURCHASE #0521 MERCHANT ALLBIRDS ACC XXX3846", "Vendor: ALLBIRDS", "--", "Bank Invoice: PURCHASE #6781 MERCHANT SHOPPERS ACC XXX9877", "Vendor: SHOPPERS", "--", "Bank Invoice: PURCHASE #0777 CN TOWER ACC XXX3846", "Vendor:", "CN TOWER", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/ad-headline", "page_title": "Ad Headline", "headers": ["Ad Headline", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate promotional ad copy variants", "Write a short ad headline for the 25% off everything holiday sale for my homemade jewelry", "Looking for a great deal on some handmade jewelry? Check out our holiday sale! All jewelry is 25% off!", "Come one, come all! Our holiday sale is here! All jewelry is 25% off!", "Don't miss out on this great deal! All jewelry is 25% off!", "Looking for a great deal on some handmade jewelry? Check out our holiday sale! All jewelry is 25% off!", "Come one, come all! Our holiday sale is here! All jewelry is 25% off!", "Don't miss out on this great deal! All jewelry is 25% off!", "Looking for a great deal on some handmade jewelry? Check out our holiday sale! All jewelry is 25% off!", "Come one, come all! Our holiday sale is here! All jewelry is 25% off!", "Don't miss out on this great deal! All jewelry is 25% off!", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/product-classifier", "page_title": "Product Classifier", "headers": ["Product Classifier", "Examples", "Request", "Response", "API Request"], "content": ["Try now", "Given a set of sample products, identify if it is a type of power tool, screw, or cement board", "1,200-Piece 8 x 2-1/2-inch Brown Deck Screws", "Value", "Confidence level", "1,200-Piece 8 x 2-1/2-inch Brown Deck Screws", "Confidence level", "Screws", "100", "%", "Cement boards", "0", "%", "Power tools", "0", "%", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/subreddit-titles", "page_title": "Subreddit Titles", "headers": ["Subreddit Titles", "Dataset", "Sample Response", "API Request"], "content": ["Try now", " Identify unique clusters from posts made on the subreddits", "#", "TEXT", "1.", "Russia to withdraw troops from key city of Kherson /r/worldnews", "2.", "NATO Parliamentary Assembly declares Russia to be a \u2018terrorist state\u2019 /r/worldnews", "3.", "Fifa and Qatar in urgent talks after Wales rainbow hats confiscated | Fifa and the Qataris were in talks on the matter on Tuesday, where Fifa reminded their hosts of their assurances before the tournament that everyone was welcome and rainbow flags would be allowed. /r/worldnewsQatar", "4.", "Bans Beer Sales at World Cup Stadiums /r/worldnews", "5.", "Biden calls 'emergency' meeting after missile hits Poland /r/worldnews", "6.", "Germany to force plastic makers to help pay for cleanups /r/worldnews", "7.", "President Zelensky: Today is a historic day, we are regaining Kherson /r/worldnews", "8.", "Ukraine suffered a comms outage when 1,300 SpaceX satellite units went offline over funding issues /r/worldnews", "9.", "Protests against Xi and his zero covid policy broke out in Shanghai and other cities in China /r/worldnews", "10.", "Vladimir Putin's black belt revoked by international taekwondo organization /r/worldnews", "11.", "Queen Elizabeth II has died, Buckingham Palace announces /r/worldnews", "12.", "Macron wins French presidential election /r/worldnews", "13.", "Greenland permanently bans all oil and gas exploration /r/worldnews", "14.", "Climate activists fill golf holes with cement after water ban exemption /r/worldnews", "15.", "\u201cHarshest Sanctions Ever,\u201d EU to Freeze Russian Assets and Stop Russian Bank Access to EU Markets /r/worldnews", "16.", "Boris Johnson to resign as prime minister /r/worldnews", "17.", "Shell consultant quits, says company causes \u2018extreme harm\u2019 to planet /r/worldnews", "18.", "Spain makes it a crime for pro-lifers to harrass women outside abortion clinics /r/worldnews", "19.", "Canada to Ban Foreigners From Buying Homes as Prices Soar /r/worldnews", "20.", "News that the world's first commercial octopus farm is closer to becoming reality has been met with dismay by scientists and conservationists. They argue such intelligent \"sentient\" creatures - considered able to feel pain and emotions - should never be commercially reared for food. /r/worldnews", "21.", "Shinzo Abe, former Japanese prime minister, dies after being shot while giving speech, state broadcaster says /r/worldnews", "22.", "Liz Truss to resign as Prime Minister /r/worldnews", "23.", "UK: Russia needs to be isolated \u2018completely, diplomatically and financially\u2019 /r/worldnews", "24.", "Best approach to buying a new mattress? /r/personalfinancecanada", "25.", "Almost half of Gen Z and millennials living paycheque-to-paycheque, global survey finds /r/personalfinancecanada", "26.", "Housing is never going to get any better. /r/personalfinancecanada", "27.", "Are you not annoyed that taxes are not built into price tags in Canada? /r/personalfinancecanada", "28.", "What is one thing that you consider to be an absolute waste of money? /r/personalfinancecanada", "29.", "Non-sufficient funds (NSF) fees are ludicrous and our government should have outlawed them years ago. /r/personalfinancecanada", "30.", "Why does anyone do food delivery? /r/personalfinancecanada", "31.", "My mom (60F) is still pressuring me (25F)to add me as a third person on her new $820k mortgage. I really don\u2019t want to as my husband and I are looking to buy our own home. She states doing this will build my credit for a new mortgage down the line /r/personalfinancecanada", "32.", "Bank of Canada increases policy interest rate by 75 basis points, continues quantitative tightening /r/personalfinancecanada", "33.", "Bragging about RESP /r/personalfinancecanada", "34.", "The only person benefitting from you not discussing your wage with co-workers is your employer! /r/personalfinancecanada", "35.", "It's genuinely shocking to see how many people misunderstand income tax brackets /r/personalfinancecanada", "36.", "I (30F) caught my husband (31M) in an affair and I don\u2019t know how to move forward. /r/relationships", "37.", "I'm [28F] unsure if it's time to draw boundaries with my boyfriend [28M] and his coworker [late 20sF] who he got close to very quickly /r/relationships", "38.", "Me [36 M] married to my Wife [36 F] 11 years, and I can't stop thinking about a girl I dated for 1 month 16 years ago. /r/relationships", "39.", "My (28F) colleague (30F) didn't invite me to her wedding, and it's completely unravelled our friendship /r/relationships", "40.", "My [26 F] with my husband [29 M] 1 year, he has been diagnosed with terminal cancer, how to make it count? /r/relationships", "41.", "Husband and I are having our longest fight ever and I don't know what to do /r/relationships", "42.", "I (26F) want to propose to my boyfriend (25M) but just found out he plans to propose as well /r/relationships", "43.", "My (30F) husband (32M) impulse bought a dog. /r/relationships", "44.", "I [29F] am traveling overseas to meet my online \"boyfriend\" [38M] in a week. He just said he had to cancel /r/relationships", "45.", "My wife [28F] found out that my mother [59F] and I [28M] have been lying to her about our (me and my wife's) baby [0M] for months /r/relationships", "46.", "United Airlines stock down over 5% premarket trading /r/investing", "47.", "Bitcoin was nearly $20,000 a year ago today /r/investing", "48.", "If in 2001, you bought $399 of Apple stock instead of buying the original iPod, today that stock would be worth ~$62,000. /r/investing", "49.", "Historically it's way better to invest at market close than at market open, most gains occur overnight/r/investing", "50.", "Jim Cramer Gave an Interview in 2006 on how the Hedge Funds Manipulate the Markets /r/investing", "51.", "Uber Fined $649 Million for Saying Drivers Aren\u2019t Employees /r/investing", "52.", "TIL if you had bought EA stock after they were voted \"The Worst Company in America\" your investment would be up by more than 378% today /r/investing", "53.", "Long Island Ice Tea Corp, a beverage company, was up 500% premarket today after changing its name to Long Blockchain Corp, despite having no relation to blockchain in its business /r/investing", "54.", "By popular demand: official \u201cI hate Robinhood and want a new broker thread\u201d /r/investing", "55.", "Gamestop Big Picture: The Short Singularity Pt 3 - WTF edition /r/investing", "56.", "PSA: There's an extremely high quality MIT Finance course on Youtube by Andrew Lo, filmed during the height of the subprime mortgage crisis /r/investing", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?task=Generate%20Content", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "Generate Content", "All Use Cases", "Transcript Summarization", "Extract the main point of a conversation", "Spelling and Grammar Check", "Given a list of text, it will remove improper capitalization and transform the sentence to Capitalized Text", "Email Copy", "Generate an Email given a description", "Customer Persona", "Describe a potential customer persona for a product", "Blog Post Body Paragraph", "Generate a body paragraph for a blog post", "Instagram Caption", "Generate a caption with a call to action", "Email Keyword Extraction", "Extract product name and return address from an email", "Article Summarization", "Summarize the main point of a paragraph into a single sentence", "Product Description to Benefits", "Turns a product description into a list of functional, emotional, and social benefits.", "Blog Title Alternatives Generator", "Suggest three alternative titles with a better marketing copy", "Business Model Canvas", "Generate a business model canvas for a satellite ", "Interview Question Generator", "Generate a list of 5 interview questions for a senior software engineer", "Contract Entity Extraction", "Use generative models to easily parse out entities from text", "LinkedIn Post Generator", "Uplevel your networking game with LinkedIn posts about your job", "Ad Headline", "Generate promotional ad copy variants", "Invoice Entity Extraction", "Given an invoice, the model is able to extract the named entities in the contract", "Blog Post Outline", "Generate an outline for a blog post given topic and audience", "Concept Relationships", "A is to B as C is to D relationships", "Product Description", "Create a product description given keywords and a product name", "Startup Idea Generator", "Come up with a startup problem and solution for an industry", "Correct Errors in Voice to Text", "Correct all the grammatical errors within an excerpt of a transcript", "Blog Post Intro Paragraph", "Generate an intro paragraph for a blog post", "Ad Copy AIDA Framework", "Generate an ad copy following the AIDA Framework", "Blog Post Title", "Generate 5 titles with topic, tone, and audience", "Real Estate Listing", "Convert a description into a textual advertisement", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?task=Embed", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "Embed", "All Use Cases", "Restaurant Customer Inquiries", "Get a visualization for all the FAQs for your restaurant ", "Subreddit Titles", " Identify unique clusters from posts made on the subreddits", "Y-combinator Thread Titles", "Identify 5 distinct groupings from this dataset of text", "NeurIPS22 Accepted Papers", "Identify 4 unique clusters from the dataset of Arxiv titles and their abstracts", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/associative-memory-augmented-asynchronous-spatiotemporal-representation-learning-for-event-based-perception-2023-03-02", "page_title": "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "headers": ["Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception"], "content": ["Try now", "AUTHORS", "Uday Kamal, Saurabh Dash, Saibal Mukhopahdyay", "ABSTRACT", "We propose EventFormer\u2013 a computationally efficient event-based representation learning framework for asynchronously processing event camera data. EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only where these events occur and update them only when they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0.5% and 9% better accuracy with 30000\u00d7 and 200\u00d7 less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/efficient-methods-for-natural-language-processing-a-survey-2023-03-24", "page_title": "Efficient Methods for Natural Language Processing: A Survey", "headers": ["Efficient Methods for Natural Language Processing: A Survey"], "content": ["Try now", "AUTHORS", "Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, Andr\u00e9 F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, Roy Schwartz", "ABSTRACT", "Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/mteb-massive-text-embedding-benchmark-2023-03-19", "page_title": "MTEB: Massive Text Embedding Benchmark\t", "headers": ["MTEB: Massive Text Embedding Benchmark\t"], "content": ["Try now", "AUTHORS", "Niklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, Nils Reimers", "ABSTRACT", "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at ", ".", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/pasha-efficient-hpo-and-nas-with-progressive-resource-allocation-2023-04-11", "page_title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation", "headers": ["PASHA: Efficient HPO and NAS with Progressive Resource Allocation"], "content": ["Try now", "AUTHORS", "Ondrej Bohdal, Lukas Balles, Martin Wistuba, Beyza Ermis, Cedric Archambeau, Giovanni Zappella", "ABSTRACT", "Hyperparameter optimization (HPO) and neural architecture search (NAS) are methods of choice to obtain the best-in-class machine learning models, but in practice they can be costly to run. When models are trained on large datasets, tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners, even when efficient multi-fidelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach, named PASHA, extends ASHA and is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identifies well-performing hyperparameter configurations and architectures while consuming significantly fewer computational resources than ASHA.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Copywriting", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Copywriting", "Email Copy", "Generate an Email given a description", "Customer Persona", "Describe a potential customer persona for a product", "Blog Post Body Paragraph", "Generate a body paragraph for a blog post", "Instagram Caption", "Generate a caption with a call to action", "Product Description to Benefits", "Turns a product description into a list of functional, emotional, and social benefits.", "Blog Title Alternatives Generator", "Suggest three alternative titles with a better marketing copy", "LinkedIn Post Generator", "Uplevel your networking game with LinkedIn posts about your job", "Ad Headline", "Generate promotional ad copy variants", "Blog Post Outline", "Generate an outline for a blog post given topic and audience", "Product Description", "Create a product description given keywords and a product name", "Correct Errors in Voice to Text", "Correct all the grammatical errors within an excerpt of a transcript", "Blog Post Intro Paragraph", "Generate an intro paragraph for a blog post", "Ad Copy AIDA Framework", "Generate an ad copy following the AIDA Framework", "Blog Post Title", "Generate 5 titles with topic, tone, and audience", "Real Estate Listing", "Convert a description into a textual advertisement", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Creative%20Writing", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Creative Writing", "Concept Relationships", "A is to B as C is to D relationships", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/on-the-challenges-of-using-black-box-apis-for-toxicity-evaluation-in-research-2023-04-24", "page_title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research", "headers": ["On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research"], "content": ["Try now", "AUTHORS", "Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker", "ABSTRACT", "Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at ", ".", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/lifting-the-veil-on-hyper-parameters-for-value-based-deep-reinforcement-learning-2023-05-05", "page_title": "Lifting the Veil on Hyper-parameters for Value-based Deep Reinforcement Learning", "headers": ["Lifting the Veil on Hyper-parameters for Value-based Deep Reinforcement Learning"], "content": ["Try now", "AUTHORS", "Jo\u00e3o G.M. Ara\u00fajo, Johan S. Obando-Ceron, Pablo Samuel Castro", "ABSTRACT", "Successful applications of deep reinforcement learning (deep RL) combine algorithmic design and careful hyper-parameter selection. The former often comes from iterative improvements over existing algorithms, while the latter is either inherited from prior methods or tuned for the specific method being introduced. Although critical to a method\u2019s performance, the effect of the various hyper-parameter choices are often overlooked in favour of algorithmic advances. In this paper, we perform an initial empirical investigation into a number of often-overlooked hyperparameters for value-based deep RL agents, demonstrating their varying levels of importance. We conduct this study on a varied set of classic control environments which helps highlight the effect each environment has on an algorithm\u2019s hyper-parameter sensitivity.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/bigscience-a-case-study-in-the-social-construction-of-a-multilingual-large-language-model-2023-05-05", "page_title": "BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model\t", "headers": ["BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model\t"], "content": ["Try now", "AUTHORS", "Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gall\u00e9, Thomas Wolf, Suzana Ilic, Yacine Jernite", "ABSTRACT", "The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/language-models-don-t-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting-2023-05-07", "page_title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting ", "headers": ["Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting "], "content": ["Try now", "AUTHORS", "Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman", "ABSTRACT", "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always \"(A)\" -- which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations supporting those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. CoT is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Clustering", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Clustering", "Restaurant Customer Inquiries", "Get a visualization for all the FAQs for your restaurant ", "Subreddit Titles", " Identify unique clusters from posts made on the subreddits", "Y-combinator Thread Titles", "Identify 5 distinct groupings from this dataset of text", "NeurIPS22 Accepted Papers", "Identify 4 unique clusters from the dataset of Arxiv titles and their abstracts", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Business", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Business", "Spelling and Grammar Check", "Given a list of text, it will remove improper capitalization and transform the sentence to Capitalized Text", "Business Model Canvas", "Generate a business model canvas for a satellite ", "Interview Question Generator", "Generate a list of 5 interview questions for a senior software engineer", "Startup Idea Generator", "Come up with a startup problem and solution for an industry", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/the-presidio-recommendations-on-responsible-generative-ai-world-economic-forum-2023-06-14", "page_title": "The Presidio Recommendations on Responsible Generative AI - World Economic Forum", "headers": ["The Presidio Recommendations on Responsible Generative AI - World Economic Forum"], "content": ["Try now", "AUTHORS", "Sara Hooker, and over 100 other thought leaders.", "ABSTRACT", "Generative AI has the potential to transform industries and society, but responsible design and collaboration among stakeholders are crucial.", "The \"Responsible AI Leadership: A Global Summit on Generative AI\" was held in April 2023 to guide experts and policymakers in developing and governing generative AI systems responsibly. Over 100 thought leaders and practitioners participated, discussing key recommendations for responsible development, open innovation, and social progress. These 30 action-oriented recommendations aim to navigate AI complexities and harness its potential ethically. By implementing them, we can shape a more innovative, equitable, and prosperous future while mitigating risks.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/when-less-is-more-investigating-data-pruning-for-pretraining-llms-at-scale-2023-09-08", "page_title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale", "headers": ["When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"], "content": ["Try now", "AUTHORS", "Max Marion, Ahmet \u00dcst\u00fcn, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, Sara Hooker", "ABSTRACT", "Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. We improve over our no-pruning baseline while training on as little as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/intriguing-properties-of-quantization-at-scale-2023-05-30", "page_title": "Intriguing Properties of Quantization at Scale", "headers": ["Intriguing Properties of Quantization at Scale"], "content": ["Try now", "AUTHORS", "Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Stephen Gou, Phil Blunsom, Ahmet \u00dcst\u00fcn, Sara Hooker", "ABSTRACT", "Emergent properties have been widely adopted as a term to describe behavior not present in smaller models but observed in larger models. Recent work suggests that the trade-off incurred by quantization is also an emergent property, with sharp drops in performance in models over 6B parameters. In this work, we ask \"are quantization cliffs in performance solely a factor of scale?\" Against a backdrop of increased research focus on why certain emergent properties surface at scale, this work provides a useful counter-example. We posit that it is possible to optimize for a quantization friendly training recipe that suppresses large activation magnitude outliers. Here, we find that outlier dimensions are not an inherent product of scale, but rather sensitive to the optimization conditions present during pre-training. This both opens up directions for more efficient quantization, and poses the question of whether other emergent properties are inherent or can be altered and conditioned by optimization and architecture design choices. We successfully quantize models ranging in size from 410M to 52B with minimal degradation in performance.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/evaluating-the-social-impact-of-generative-ai-systems-in-systems-and-society-2023-06-12", "page_title": "Evaluating the Social Impact of Generative AI Systems in Systems and Society", "headers": ["Evaluating the Social Impact of Generative AI Systems in Systems and Society"], "content": ["Try now", "AUTHORS", "Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daum\u00e9 III, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, Apostol Vassilev", "ABSTRACT", "Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm. We are concurrently crafting an evaluation repository for the AI research community to contribute existing evaluations along the given categories. This version will be updated following a CRAFT session at ACM FAccT 2023.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/pushing-mixture-of-experts-to-the-limit-extremely-parameter-efficient-moe-for-instruction-tuning-2023-09-11", "page_title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "headers": ["Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"], "content": ["Try now", "AUTHORS", "Ted Zadouri, Ahmet \u00dcst\u00fcn, Arash Ahmadian, Beyza Ermi\u015f, Acyr Locatelli, Sara Hooker", "ABSTRACT", "The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts. Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints. Our code used in all the experiments is publicly available here: ", ".", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Computer%20Vision", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling", "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics", "Studying the Impact of Magnitude Pruning on Contrastive Learning Methods\t"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Mar 02, 2023", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Mar 01, 2023", "Computer Vision", "Responsible AI", "Computer Vision", "Responsible AI", "Sep 20, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Jul 01, 2022", "Computer Vision", "Contrastive Learning", "Computer Vision", "Contrastive Learning", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Extraction", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Extraction", "Email Keyword Extraction", "Extract product name and return address from an email", "Contract Entity Extraction", "Use generative models to easily parse out entities from text", "Invoice Entity Extraction", "Given an invoice, the model is able to extract the named entities in the contract", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Neural%20Architecture%20Search", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "PASHA: Efficient HPO and NAS with Progressive Resource Allocation", "\u03b1NAS: Neural Architecture Search using Property Guided Synthesis"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Apr 11, 2023", "Efficiency", "Optimization", "Neural Architecture Search", "Efficiency", "Optimization", "Neural Architecture Search", "Nov 10, 2022", "Neural Architecture Search", "Neural Architecture Search", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Generative%20Models", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation", "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation", "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale", "Intriguing Properties of Quantization at Scale", "Efficient Methods for Natural Language Processing: A Survey", "Intriguing Properties of Compression on Multilingual Models", "Large Language Models are not Zero Shot Communicators", "Predicting Twitter Engagement With Deep Language Models"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Nov 29, 2023", "Evaluation", "Reproducibility", "Language", "Generative Models", "Evaluation", "Reproducibility", "Language", "Generative Models", "Oct 22, 2023", "Evaluation", "Efficiency", "Language", "Generative Models", "Evaluation", "Efficiency", "Language", "Generative Models", "Oct 11, 2023", "Safety", "Generative Models", "Language Models", "Safety", "Generative Models", "Language Models", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Sep 08, 2023", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "May 30, 2023", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Mar 24, 2023", "Efficiency", "Generative Models", "Efficiency", "Generative Models", "Nov 26, 2022", "Efficiency", "Compute", "Language", "Generative Models", "Efficiency", "Compute", "Language", "Generative Models", "Oct 26, 2022", "Interpretability", "Language", "Generative Models", "Interpretability", "Language", "Generative Models", "Sep 26, 2020", "Generative Models", "Interpretability", "Generative Models", "Interpretability", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Transformers", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale", "Intriguing Properties of Quantization at Scale", "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "Exploring Low Rank Training of Deep Neural Networks", "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 08, 2023", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "May 30, 2023", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Mar 02, 2023", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Sep 27, 2022", "Model Compression", "Transformers", "Efficiency", "Model Compression", "Transformers", "Efficiency", "Sep 26, 2022", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Optimization", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "PASHA: Efficient HPO and NAS with Progressive Resource Allocation"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Apr 11, 2023", "Efficiency", "Optimization", "Neural Architecture Search", "Efficiency", "Optimization", "Neural Architecture Search", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Representation%20Learning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Intriguing Properties of Quantization at Scale", "MTEB: Massive Text Embedding Benchmark\t", "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "Improving Policy Learning via Language Dynamics Distillation\t", "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "May 30, 2023", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Mar 19, 2023", "Representation Learning", "Representation Learning", "Mar 02, 2023", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Sep 30, 2022", "Representation Learning", "Reinforcement Learning", "Language Models", "Representation Learning", "Reinforcement Learning", "Language Models", "Sep 26, 2022", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Efficiency", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "PASHA: Efficient HPO and NAS with Progressive Resource Allocation", "Efficient Methods for Natural Language Processing: A Survey", "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "Intriguing Properties of Compression on Multilingual Models", "Exploring Low Rank Training of Deep Neural Networks", "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics", "Robust Distillation for Worst-class Performance", "Scalable Training of Language Models using PAX pjit and TPUv4"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Oct 22, 2023", "Evaluation", "Efficiency", "Language", "Generative Models", "Evaluation", "Efficiency", "Language", "Generative Models", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Apr 11, 2023", "Efficiency", "Optimization", "Neural Architecture Search", "Efficiency", "Optimization", "Neural Architecture Search", "Mar 24, 2023", "Efficiency", "Generative Models", "Efficiency", "Generative Models", "Mar 02, 2023", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Transformers", "Representation Learning", "Efficiency", "Computer Vision", "Nov 26, 2022", "Efficiency", "Compute", "Language", "Generative Models", "Efficiency", "Compute", "Language", "Generative Models", "Sep 27, 2022", "Model Compression", "Transformers", "Efficiency", "Model Compression", "Transformers", "Efficiency", "Sep 20, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Jun 13, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Apr 13, 2022", "Efficiency", "Frameworks", "Tooling", "Efficiency", "Frameworks", "Tooling", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Responsible%20AI", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "On the Fairness Impacts of Hardware Selection in Machine Learning", "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI", "The Presidio Recommendations on Responsible Generative AI - World Economic Forum", "Evaluating the Social Impact of Generative AI Systems in Systems and Society", "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research", "FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling", "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics", "Robust Distillation for Worst-class Performance", "No News is Good News: A Critique of the One Billion Word Benchmark"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Dec 06, 2023", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Oct 25, 2023", "Responsible AI", "Safety", "AI Policy", "Data", "Responsible AI", "Safety", "AI Policy", "Data", "Jun 14, 2023", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Jun 12, 2023", "Responsible AI", "Responsible AI", "Apr 24, 2023", "Safety", "Reproducibility", "Responsible AI", "Safety", "Reproducibility", "Responsible AI", "Mar 01, 2023", "Computer Vision", "Responsible AI", "Computer Vision", "Responsible AI", "Sep 20, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Jun 13, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Oct 25, 2021", "Responsible AI", "Evaluation", "Responsible AI", "Evaluation", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Interpretability", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "The Presidio Recommendations on Responsible Generative AI - World Economic Forum", "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting ", "Large Language Models are not Zero Shot Communicators", "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics", "Robust Distillation for Worst-class Performance", "Predicting Twitter Engagement With Deep Language Models"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Jun 14, 2023", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Interpretability", "Safety", "Responsible AI", "AI Policy", "May 07, 2023", "Safety", "Reasoning", "Interpretability", "Safety", "Reasoning", "Interpretability", "Oct 26, 2022", "Interpretability", "Language", "Generative Models", "Interpretability", "Language", "Generative Models", "Sep 20, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Jun 13, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Sep 26, 2020", "Generative Models", "Interpretability", "Generative Models", "Interpretability", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Reinforcement%20Learning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Lifting the Veil on Hyper-parameters for Value-based Deep Reinforcement Learning", "Improving Policy Learning via Language Dynamics Distillation\t"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "May 05, 2023", "Reinforcement Learning", "Reinforcement Learning", "Sep 30, 2022", "Representation Learning", "Reinforcement Learning", "Language Models", "Representation Learning", "Reinforcement Learning", "Language Models", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Reasoning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting "], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "May 07, 2023", "Safety", "Reasoning", "Interpretability", "Safety", "Reasoning", "Interpretability", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Collaboration", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model\t"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "May 05, 2023", "Collaboration", "BigScience", "Collaboration", "BigScience", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/studying-the-impact-of-magnitude-pruning-on-contrastive-learning-methods-2022-07-01", "page_title": "Studying the Impact of Magnitude Pruning on Contrastive Learning Methods\t", "headers": ["Studying the Impact of Magnitude Pruning on Contrastive Learning Methods\t"], "content": ["Try now", "AUTHORS", "Francesco Corti, Rahim Entezari, Sara Hooker, Davide Bacciu, Olga Sauk", "ABSTRACT", "We study the impact of different pruning techniques on the representation learned by deep neural networks trained with contrastive loss functions. Our work finds that at high sparsity levels, contrastive learning results in a higher number of misclassified examples relative to models trained with traditional cross-entropy loss. To understand this pronounced difference, we use metrics such as the number of PIEs (Hooker et al., 2019), Q-Score (Kalibhat et al., 2022) and PDScore (Baldock et al., 2021) to measure the impact of pruning on the learned representation quality. Our analysis suggests the schedule of the pruning method implementation matters. We find that the negative impact of sparsity on the quality of the learned representation is the highest when pruning is introduced early-on in training phase.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=BigScience", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model\t"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "May 05, 2023", "Collaboration", "BigScience", "Collaboration", "BigScience", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/anas-neural-architecture-search-using-property-guided-synthesis-2022-11-10", "page_title": "\u03b1NAS: Neural Architecture Search using Property Guided Synthesis", "headers": ["\u03b1NAS: Neural Architecture Search using Property Guided Synthesis"], "content": ["Try now", "AUTHORS", "Charles Jin, Phitchaya Mangpo Phothilimthana, Sudip Roy", "ABSTRACT", "In the past few years, neural architecture search (NAS) has become an increasingly important tool within the deep learning community. Despite the many recent successes of NAS, however, most existing approaches operate within highly structured design spaces, and hence explore only a small fraction of the full search space of neural architectures while also requiring significant manual effort from domain experts. In this work, we develop techniques that enable efficient NAS in a significantly larger design space. To accomplish this, we propose to perform NAS in an abstract search space of program properties. Our key insights are as follows: (1) the abstract search space is significantly smaller than the original search space, and (2) architectures with similar program properties also have similar performance; thus, we can search more efficiently in the abstract search space. To enable this approach, we also propose a novel efficient synthesis procedure, which accepts a set of promising program properties, and returns a satisfying neural architecture. We implement our approach, \u03b1NAS, within an evolutionary framework, where the mutations are guided by the program properties. Starting with a ResNet-34 model, \u03b1NAS produces a model with slightly improved accuracy on CIFAR-10 but 96% fewer parameters. On ImageNet, \u03b1NAS is able to improve over Vision Transformer (30% fewer FLOPS and parameters), ResNet-50 (23% fewer FLOPS, 14% fewer parameters), and EfficientNet (7% fewer FLOPS and parameters) without any degradation in accuracy.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Reproducibility", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation", "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Nov 29, 2023", "Evaluation", "Reproducibility", "Language", "Generative Models", "Evaluation", "Reproducibility", "Language", "Generative Models", "Apr 24, 2023", "Safety", "Reproducibility", "Responsible AI", "Safety", "Reproducibility", "Responsible AI", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Compute", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "Intriguing Properties of Compression on Multilingual Models"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 12, 2023", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Nov 26, 2022", "Efficiency", "Compute", "Language", "Generative Models", "Efficiency", "Compute", "Language", "Generative Models", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/metadata-archaeology-unearthing-data-subsets-by-leveraging-training-dynamics-2022-09-20", "page_title": "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics", "headers": ["Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics"], "content": ["Try now", "AUTHORS", "Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, Sara Hooker", "ABSTRACT", "Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in \u2018untidy\u2019 or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology \u2013 uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Model%20Compression", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Intriguing Properties of Quantization at Scale", "Exploring Low Rank Training of Deep Neural Networks"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "May 30, 2023", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Generative Models", "Representation Learning", "Transformers", "Model Compression", "Sep 27, 2022", "Model Compression", "Transformers", "Efficiency", "Model Compression", "Transformers", "Efficiency", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/fair-ensemble-when-fairness-naturally-emerges-from-deep-ensembling-2023-03-01", "page_title": "FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling", "headers": ["FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling"], "content": ["Try now", "AUTHORS", "Wei-Yin Ko, Daniel D\u2019souza, Karina Nguyen, Randall Balestriero, Sara Hooker", "ABSTRACT", "Ensembling independent deep neural networks (DNNs) is a simple and effective way to improve top-line metrics and to outperform larger single models. In this work, we go beyond top-line metrics and instead explore the impact of ensembling on subgroup performances. Surprisingly, even with a simple homogenous ensemble \u2013 all the individual models share the same training set, architecture, and design choices \u2013 we find compelling and powerful gains in worst-k and minority group performance, i.e. fairness naturally emerges from ensembling. We show that the gains in performance from ensembling for the minority group continue for far longer than for the majority group as more models are added. Our work establishes that simple DNN ensembles can be a powerful tool for alleviating disparate impact from DNN classifiers, thus curbing algorithmic harm. We also explore why this is the case. We find that even in homogeneous ensembles, varying the sources of stochasticity through parameter initialization, mini-batch sampling, and the data-augmentation realizations, results in different fairness outcomes.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/predicting-twitter-engagement-with-deep-language-models-2020-09-26", "page_title": "Predicting Twitter Engagement With Deep Language Models", "headers": ["Predicting Twitter Engagement With Deep Language Models"], "content": ["Try now", "AUTHORS", "Maksim N Volkovs, Zhaoyue Cheng, Mathieu Ravaut, Hojin Yang, Kevin Shen, Jinpeng Zhou, Anson Wong, Saba Zuberi, Ivan Zhang, Nick Frosst, Helen Ngo, Carol Chen, Bharat Venkitesh, Stephen Gou, Aidan N. Gomez", "ABSTRACT", "Twitter has become one of the main information sharing platforms for millions of users world-wide. Numerous tweets are created daily, many with highly time sensitive content such as breaking news, new multimedia content or personal updates. Consequently, accurately recommending relevant tweets to users in a timely manner is a highly important and challenging problem. The 2020 ACM RecSys Challenge is aimed at benchmarking leading recommendation models for this task. The challenge is based on a large and recent dataset of over 200M tweet engagements released by Twitter with content in over 50 languages. In this work we present our approach where we leverage recent advances in deep language modeling and attention architectures, to combine information from extracted features, user engagement history and target tweet content. We first fine-tune leading multilingual language models M-BERT and XLM-R for Twitter data. Embeddings from these models are used to extract tweet and user history representations. We then combine all components together and jointly train them to maximize engagement prediction accuracy. Our approach achieves highly competitive performance placing 2\u2019nd on the final private leaderboard. Full code is available here:\u00a0https://github.com/layer6ai-labs/RecSys2020.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/prioritized-training-on-points-that-are-learnable-worth-learning-and-not-yet-learnt-2022-09-26", "page_title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "headers": ["Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"], "content": ["Try now", "AUTHORS", "S\u00f6ren Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N. Gomez, Adrien Morisot, Sebastian Farquhar, Yarin Gal", "ABSTRACT", "Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/large-language-models-are-not-zero-shot-communicators-2022-10-26", "page_title": "Large Language Models are not Zero Shot Communicators", "headers": ["Large Language Models are not Zero Shot Communicators"], "content": ["Try now", "AUTHORS", "Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rockt\u00e4schel, Edward Grefenstette", "ABSTRACT", "Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response \"I wore gloves\" to the question \"Did you leave fingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random. Models adapted to be \"aligned with human intent\" perform much better, but still show a significant gap with human performance. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/exploring-low-rank-training-of-deep-neural-networks-2022-09-27", "page_title": "Exploring Low Rank Training of Deep Neural Networks", "headers": ["Exploring Low Rank Training of Deep Neural Networks"], "content": ["Try now", "AUTHORS", "Siddhartha Rao Kamalakara, Acyr Locatelli, Bharat Venkitesh, Jimmy Ba, Yarin Gal, Aidan N. Gomez", "ABSTRACT", "Training deep neural networks in low rank, i.e. with factorised layers, is of particular interest to the community: it offers efficiency over unfactorised training in terms of both memory consumption and training time. Prior work has focused on low rank approximations of pre-trained networks and training in low rank space with additional objectives, offering various ad hoc explanations for chosen practice. We analyse techniques that work well in practice, and through extensive ablations on models such as GPT2 we provide evidence falsifying common beliefs in the field, hinting in the process at exciting research opportunities that still need answering.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/no-news-is-good-news-a-critique-of-the-one-billion-word-benchmark-2021-10-25", "page_title": "No News is Good News: A Critique of the One Billion Word Benchmark", "headers": ["No News is Good News: A Critique of the One Billion Word Benchmark"], "content": ["Try now", "AUTHORS", "Helen Ngo, Jo\u00e3o G.M. Ara\u00fajo, Jeffrey Hui, Nicholas Frosst", "ABSTRACT", "The One Billion Word Benchmark is a dataset derived from the WMT 2011 News Crawl, commonly used to measure language modeling ability in natural language processing. We train models solely on Common Crawl web scrapes partitioned by year, and demonstrate that they perform worse on this task over time due to distributional shift. Analysis of this corpus reveals that it contains several examples of harmful text, as well as outdated references to current events. We suggest that the temporal nature of news and its distribution shift over time makes it poorly suited for measuring language modeling ability, and discuss potential impact and considerations for researchers building language models and evaluation datasets.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/intriguing-properties-of-compression-on-multilingual-models-2022-11-26", "page_title": "Intriguing Properties of Compression on Multilingual Models", "headers": ["Intriguing Properties of Compression on Multilingual Models"], "content": ["Try now", "AUTHORS", "Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian Gehrmann, Sara Hooker, Julia Kreutzer", "ABSTRACT", "Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance for low-resource languages. It is thus crucial to understand the trade-offs between scale, multilingualism, and compression. In this work, we propose an experimental framework to characterize the impact of sparsifying multilingual pre-trained language models during fine-tuning. Applying this framework to mBERT named entity recognition models across 40 languages, we find that compression confers several intriguing and previously unknown generalization properties. In contrast to prior findings, we find that compression may improve model robustness over dense models. We additionally observe that under certain sparsification regimes compression may aid, rather than disproportionately impact the performance of low-resource languages.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/scalable-training-of-language-models-using-pax-pjit-and-tpu-v-four-2022-04-13", "page_title": "Scalable Training of Language Models using PAX pjit and TPUv4", "headers": ["Scalable Training of Language Models using PAX pjit and TPUv4"], "content": ["Try now", "AUTHORS", "Joanna Yoo, Kuba Perlin, Siddhartha Rao Kamalakara, Jo\u00e3o G.M. Ara\u00fajo", "ABSTRACT", "Modern large language models require distributed training strategies due to their size. The challenges of efficiently and robustly training them are met with rapid developments on both software and hardware frontiers. In this technical report, we explore challenges and design decisions associated with developing a scalable training framework, and present a quantitative analysis of efficiency improvements coming from adopting new software and hardware solutions.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/robust-distillation-for-worst-class-performance-2022-06-13", "page_title": "Robust Distillation for Worst-class Performance", "headers": ["Robust Distillation for Worst-class Performance"], "content": ["Try now", "AUTHORS", "Serena Wang, Harikrishna Narasimhan, Yichen Zhou, Sara Hooker, Michal Lukasik, Aditya Krishna Menon", "ABSTRACT", "Knowledge distillation has proven to be an effective technique in improving the performance a student model using predictions from a teacher model. However, recent work has shown that gains in average efficiency are not uniform across subgroups in the data, and in particular can often come at the cost of accuracy on rare subgroups and classes. To preserve strong performance across classes that may follow a long-tailed distribution, we develop distillation techniques that are tailored to improve the student\u2019s worst-class performance. Specifically, we introduce robust optimization objectives in different combinations for the teacher and student, and further allow for training with any tradeoff between the overall accuracy and the robust worst-class objective. We show empirically that our robust distillation techniques not only achieve better worst-class performance, but also lead to Pareto improvement in the tradeoff between overall performance and worstclass performance compared to other baseline methods. Theoretically, we provide insights into what makes a good teacher when the goal is to train a robust student.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Contrastive%20Learning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Studying the Impact of Magnitude Pruning on Contrastive Learning Methods\t"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Jul 01, 2022", "Computer Vision", "Contrastive Learning", "Computer Vision", "Contrastive Learning", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Language", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation", "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "Intriguing Properties of Compression on Multilingual Models", "Large Language Models are not Zero Shot Communicators"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Nov 29, 2023", "Evaluation", "Reproducibility", "Language", "Generative Models", "Evaluation", "Reproducibility", "Language", "Generative Models", "Oct 22, 2023", "Evaluation", "Efficiency", "Language", "Generative Models", "Evaluation", "Efficiency", "Language", "Generative Models", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Nov 26, 2022", "Efficiency", "Compute", "Language", "Generative Models", "Efficiency", "Compute", "Language", "Generative Models", "Oct 26, 2022", "Interpretability", "Language", "Generative Models", "Interpretability", "Language", "Generative Models", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/improving-policy-learning-via-language-dynamics-distillation-2022-09-30", "page_title": "Improving Policy Learning via Language Dynamics Distillation\t", "headers": ["Improving Policy Learning via Language Dynamics Distillation\t"], "content": ["Try now", "AUTHORS", "Victor Zhong, Jesse Mu, Luke Zettlemoyer, Edward Grefenstette, Tim Rockt\u00e4schel", "ABSTRACT", "Recent work has shown that augmenting environments with language descriptions improves policy learning. However, for environments with complex language abstractions, learning how to ground language to observations is difficult due to sparse, delayed rewards. We propose Language Dynamics Distillation (LDD), which pretrains a model to predict environment dynamics given demonstrations with language descriptions, and then fine-tunes these language-aware pretrained representations via reinforcement learning (RL). In this way, the model is trained to both maximize expected reward and retain knowledge about how language relates to environment dynamics. On SILG, a benchmark of five tasks with language descriptions that evaluate distinct generalization challenges on unseen environments (NetHack, ALFWorld, RTFM, Messenger, and Touchdown), LDD outperforms tabula-rasa RL, VAE pretraining, and methods that learn from unlabeled demonstrations in inverse RL and reward shaping with pretrained experts. In our analyses, we show that language descriptions in demonstrations improve sample-efficiency and generalization across environments, and that dynamics modelling with expert demonstrations is more effective than with non-experts.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Data%20Pruning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 08, 2023", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Large-Scale%20Pretraining", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 08, 2023", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Data%20Efficiency", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 08, 2023", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Transformers", "Generative Models", "Data Pruning", "Large-Scale Pretraining", "Data Efficiency", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Mixture%20of%20Experts", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Transfer%20Learning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 11, 2023", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Mixture of Experts", "Efficiency", "Transfer Learning", "Language", "Generative Models", "Compute", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/the-grand-illusion-the-myth-of-software-portability-and-implications-for-ml-progress-2023-09-12", "page_title": "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress", "headers": ["The Grand Illusion: The Myth of Software Portability and Implications for ML Progress"], "content": ["Try now", "AUTHORS", "Fraser Mince, Dzung Dinh, Jonas Kgomo, Neil Thompson, Sara Hooker", "ABSTRACT", "Pushing the boundaries of machine learning often requires exploring different hardware and software combinations. However, the freedom to experiment across different tooling stacks can be at odds with the drive for efficiency, which has produced increasingly specialized AI hardware and incentivized consolidation around a narrow set of ML frameworks. Exploratory research can be restricted if software and hardware are co-evolving, making it even harder to stray away from mainstream ideas that work well with popular tooling stacks. While this friction increasingly impacts the rate of innovation in machine learning, to our knowledge the lack of portability in tooling has not been quantified. In this work, we ask: How portable are popular ML software frameworks? We conduct a large-scale study of the portability of mainstream ML frameworks across different hardware types. Our findings paint an uncomfortable picture \u2013 frameworks can lose more than 40% of their key functions when ported to other hardware. Worse, even when functions are portable, the slowdown in their performance can be extreme and render performance untenable. Collectively, our results reveal how costly straying from a narrow set of hardware-software combinations can be - and suggest that specialization of hardware impedes innovation in machine learning research.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Safety", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI", "Locally Differentially Private Document Generation Using Zero Shot Prompting", "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models", "The Presidio Recommendations on Responsible Generative AI - World Economic Forum", "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting ", "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research", "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics", "Robust Distillation for Worst-class Performance", "Mitigating Harm in Language Models with Conditional-Likelihood Filtration"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Oct 25, 2023", "Responsible AI", "Safety", "AI Policy", "Data", "Responsible AI", "Safety", "AI Policy", "Data", "Oct 24, 2023", "Safety", "Privacy", "Safety", "Privacy", "Oct 11, 2023", "Safety", "Generative Models", "Language Models", "Safety", "Generative Models", "Language Models", "Jun 14, 2023", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Interpretability", "Safety", "Responsible AI", "AI Policy", "May 07, 2023", "Safety", "Reasoning", "Interpretability", "Safety", "Reasoning", "Interpretability", "Apr 24, 2023", "Safety", "Reproducibility", "Responsible AI", "Safety", "Reproducibility", "Responsible AI", "Sep 20, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Computer Vision", "Jun 13, 2022", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Efficiency", "Interpretability", "Safety", "Responsible AI", "Nov 28, 2021", "Safety", "Safety", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=AI%20Policy", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI", "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress", "The Presidio Recommendations on Responsible Generative AI - World Economic Forum"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Oct 25, 2023", "Responsible AI", "Safety", "AI Policy", "Data", "Responsible AI", "Safety", "AI Policy", "Data", "Sep 12, 2023", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "Jun 14, 2023", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Interpretability", "Safety", "Responsible AI", "AI Policy", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/goodtriever-adaptive-toxicity-mitigation-with-retrieval-augmented-models-2023-10-11", "page_title": "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models", "headers": ["Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models"], "content": ["Try now", "AUTHORS", "Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker", "ABSTRACT", "Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language's evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43% relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Tooling", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "On the Fairness Impacts of Hardware Selection in Machine Learning", "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress", "Scalable Training of Language Models using PAX pjit and TPUv4"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Dec 06, 2023", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Sep 12, 2023", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "Apr 13, 2022", "Efficiency", "Frameworks", "Tooling", "Efficiency", "Frameworks", "Tooling", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/which-prompts-make-the-difference-data-prioritization-for-efficient-human-llm-evaluation-2023-10-22", "page_title": "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation", "headers": ["Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation"], "content": ["Try now", "AUTHORS", "Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, Sara Hooker", "ABSTRACT", "Human evaluation is increasingly critical for assessing large language models, capturing linguistic nuances, and reflecting user preferences more accurately than traditional automated metrics. However, the resource-intensive nature of this type of annotation process poses significant challenges. The key question driving our work: \"is it feasible to minimize human-in-the-loop feedback by prioritizing data instances which most effectively distinguish between models?\" We evaluate several metric-based methods and find that these metrics enhance the efficiency of human evaluations by minimizing the number of required annotations, thus saving time and cost, while ensuring a robust performance evaluation. We show that our method is effective across widely used model families, reducing instances of indecisive (or \"tie\") outcomes by up to 54% compared to a random sample when focusing on the top-20 percentile of prioritized instances. This potential reduction in required human effort positions our approach as a valuable strategy in future large language model evaluations.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Unsupervised%20Learning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 26, 2022", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Frameworks", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Scalable Training of Language Models using PAX pjit and TPUv4"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Apr 13, 2022", "Efficiency", "Frameworks", "Tooling", "Efficiency", "Frameworks", "Tooling", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Evaluation", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation", "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation", "No News is Good News: A Critique of the One Billion Word Benchmark"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Nov 29, 2023", "Evaluation", "Reproducibility", "Language", "Generative Models", "Evaluation", "Reproducibility", "Language", "Generative Models", "Oct 22, 2023", "Evaluation", "Efficiency", "Language", "Generative Models", "Evaluation", "Efficiency", "Language", "Generative Models", "Oct 25, 2021", "Responsible AI", "Evaluation", "Responsible AI", "Evaluation", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/locally-differentially-private-document-generation-using-zero-shot-prompting-2023-10-24", "page_title": "Locally Differentially Private Document Generation Using Zero Shot Prompting", "headers": ["Locally Differentially Private Document Generation Using Zero Shot Prompting"], "content": ["Try now", "AUTHORS", "Saiteja Utpala, Sara Hooker, Pin Yu Chen", "ABSTRACT", "Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of deanonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46% reduction in author identification F1 score against static attackers and a 26% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Supervised%20Learning", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "On the Fairness Impacts of Hardware Selection in Machine Learning", "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Dec 06, 2023", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Sep 26, 2022", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Transformers", "Representation Learning", "Supervised Learning", "Unsupervised Learning", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Language%20Models", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models", "Improving Policy Learning via Language Dynamics Distillation\t"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Oct 11, 2023", "Safety", "Generative Models", "Language Models", "Safety", "Generative Models", "Language Models", "Sep 30, 2022", "Representation Learning", "Reinforcement Learning", "Language Models", "Representation Learning", "Reinforcement Learning", "Language Models", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/the-data-provenance-initiative-a-large-scale-audit-of-dataset-licensing-and-attribution-in-ai-2023-10-25", "page_title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI", "headers": ["The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI"], "content": ["Try now", "AUTHORS", "Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi Wu, Enrico Shippole Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, Deb Roy, Sara Hooker", "ABSTRACT", "The race to train language models on vast, diverse, and inconsistently documented datasets has raised pressing concerns about the legal and ethical risks for practitioners. To remedy these practices, threatening data transparency and understanding, we convene a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace 1800+ finetuning datasets. Our landscape analysis highlights the sharp divides in composition and focus of commercially open vs closed datasets, with closed datasets monopolizing important categories: lower resource languages, more creative tasks, richer topic variety, newer and more synthetic training data. This points to a deepening divide in the types of data that are made available under different license conditions, and heightened implications for jurisdictional legal interpretations of copyright and fair use. We also observe frequent miscategorization of licenses on widely used dataset hosting sites, with license omission of 72%+ and error rates of 50%+. This points to a crisis in misattribution, and informed use of the most popular datasets, driving many recent breakthroughs. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire audit, with an interactive UI, the Data Provenance Explorer, which allows practitioners to trace and filter on data provenance for the most popular open source finetuning data collections: www.dataprovenance.org.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/on-the-fairness-impacts-of-hardware-selection-in-machine-learning-2023-12-06", "page_title": "On the Fairness Impacts of Hardware Selection in Machine Learning", "headers": ["On the Fairness Impacts of Hardware Selection in Machine Learning"], "content": ["Try now", "AUTHORS", "Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto", "ABSTRACT", "In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/elo-uncovered-robustness-and-best-practices-in-language-model-evaluation-2023-11-29", "page_title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation", "headers": ["Elo Uncovered: Robustness and Best Practices in Language Model Evaluation"], "content": ["Try now", "AUTHORS", "Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, Marzieh Fadaee", "ABSTRACT", "In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking players in dynamic games such as chess, is increasingly being used to evaluate Large Language Models (LLMs) through \"A vs B\" paired comparisons. However, while popular, the system's suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored. We study two fundamental axioms that evaluation methods should adhere to: reliability and transitivity. We conduct extensive evaluation of Elo behaviour, illustrating that individual Elo computations exhibit volatility and delving into the impact of varying the Elo rating system's hyperparameters. We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs. If the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM evaluation methods, suggesting a need for reassessment of existing comparative approaches.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Hardware", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "On the Fairness Impacts of Hardware Selection in Machine Learning", "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Dec 06, 2023", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Hardware", "Responsible AI", "Tooling", "Supervised Learning", "Sep 12, 2023", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?task=Classify%20Data", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "Classify Data", "All Use Cases", "Headline Market Analysis", "Given a news headline, identify if the category of the article is technology, economy, or health", "Insurance FAQ Classification", "Using our classification model, customer questions can be easily classified into the correct category.", "Spam Detection", "Given email titles, identify if the email is spam or not spam", "Product Classifier", "Given a set of sample products, identify if it is a type of power tool, screw, or cement board", "Tweet Classification", "Given a set of tweets, determine if the tweet is about a demo, a piece of news, or an event", " Article Title Classifier", "Given an article title, identify if the article is about business news, cooking, or arts & culture", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Topic%20Classification", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Topic Classification", "Headline Market Analysis", "Given a news headline, identify if the category of the article is technology, economy, or health", "Insurance FAQ Classification", "Using our classification model, customer questions can be easily classified into the correct category.", "Spam Detection", "Given email titles, identify if the email is spam or not spam", "Product Classifier", "Given a set of sample products, identify if it is a type of power tool, screw, or cement board", "Tweet Classification", "Given a set of tweets, determine if the tweet is about a demo, a piece of news, or an event", " Article Title Classifier", "Given an article title, identify if the article is about business news, cooking, or arts & culture", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers/mitigating-harm-in-language-models-with-conditional-likelihood-filtration-2021-11-28", "page_title": "Mitigating Harm in Language Models with Conditional-Likelihood Filtration", "headers": ["Mitigating Harm in Language Models with Conditional-Likelihood Filtration"], "content": ["Try now", "AUTHORS", "Helen Ngo, Cooper Raterink, Jo\u00e3o G.M. Ara\u00fajo, Ivan Zhang, Carol Chen, Adrien Morisot, Nicholas Frosst", "ABSTRACT", "Language models trained on large-scale unfiltered datasets curated from the open web acquire systemic biases, prejudices, and harmful views from their training data. We present a methodology for programmatically identifying and removing harmful text from web-scale datasets. A pretrained language model is used to assess the loglikelihood of researcher-written trigger phrases conditioned on a specific document, which is used to identify and filter documents from the dataset. We demonstrate that models trained on this filtered dataset exhibit lower propensity to generate harmful text, with a marginal decrease in performance on standard language modeling benchmarks compared to unfiltered baselines. We provide a partial explanation for this performance gap by surfacing examples of hate speech and other undesirable content from standard language modeling benchmarks. Finally, we discuss the generalization of this method and how trigger phrases reflecting specific values can be used by researchers to build language models which are more closely aligned with their values.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/linkedin-post-generator", "page_title": "LinkedIn Post Generator", "headers": ["LinkedIn Post Generator", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Uplevel your networking game with LinkedIn posts about your job", "Write a LinkedIn post about how to start a career in tech:", "If you're interested in starting a career in tech, there are a few things you can do to get started. First, make sure you have the necessary skills and knowledge. Second, find a company or organization that is willing to train you in your chosen field. Third, take advantage of online resources and communities to connect with other professionals and learn about potential job opportunities. Finally, don't be afraid to reach out to potential employers and ask about available positions. With a little hard work and networking, you can start a successful career in the tech industry.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/contract-entity-extraction", "page_title": "Contract Entity Extraction", "headers": ["Contract Entity Extraction", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Use generative models to easily parse out entities from text", "Extract the band name from the contract: ", "This Music Recording Agreement (\"Agreement\") is made effective as of the 13 day of December, 2021 by and between Good Kid, a Toronto-based musical group (\u201cArtist\u201d) and Universal Music Group, a record label with license number 545345 (\u201cRecording Label\"). Artist and Recording Label may each be referred to in this Agreement individually as a \"Party\" and collectively as the \"Parties.\" Work under this Agreement shall begin on March 15, 2022.", "Good Kid", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/business-model-canvas", "page_title": "Business Model Canvas", "headers": ["Business Model Canvas", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate a business model canvas for a satellite ", "Generate a Business Model Canvas consisting of the following nine building blocks.", "1 - Customer Segments", "2 - Value Propositions", "3 - Channels", "4 - Customer Relationships", "5 - Revenue Streams", "6 - Key Resources", "7 - Key Activities", "8 - Key Partners", "9 - Cost Structure", "Business Model Canvas", "Product: Satellite broadband service", "1 - Customer Segments:", "-Residential", "-Business", "-Government", "-Military", "-Education", "2 - Value Propositions: ", "-High-speed internet service", "-Flexible data plans", "-24/7 customer support", "3 - Channels:", "-Online", "-TV", "-Radio", "-Billboards", "4 - Customer Relationships:", "-One-time purchase", "-Recurring subscription", "-Lease", "-Licensing", "5 - Revenue Streams:", "-Service fees", "-Equipment sales", "-Advertising ", "-Referral fees", "6 - Key Resources:", "-Employees", "-Equipment", "-Software", "-Data", "7 - Key Activities:", "-Installation", "-Maintenance", "-Technical support", "-Marketing", "8 - Key Partners:", "-Suppliers", "-Distributors", "-Advertisers", "-Referral sources", "9 - Cost Structure:", "-Labor", "-Equipment", "-Software", "-Data", "-Advertising- Refer", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/interview-question-generator", "page_title": "Interview Question Generator", "headers": ["Interview Question Generator", "Input", "Response", "API Request", "Parameters"], "content": ["Try now", "Generate a list of 5 interview questions for a senior software engineer", "Generate a list of 5 interview questions for a senior software engineer", "1. What is your experience with software engineering best practices? ", "2. What programming languages are you comfortable working with? ", "3. Tell me about a time when you had to manage a complex software project. ", "4. What would you say is your greatest strength as a software engineer? ", "5. Have you ever been part of a team that developed a software product from scratch?", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Open%20Source", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Sep 12, 2023", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "AI Policy", "Compute", "Open Source", "Tooling", "Hardware", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/spam-detection", "page_title": "Spam Detection", "headers": ["Spam Detection", "Examples", "Request", "Response", "API Request"], "content": ["Try now", "Given email titles, identify if the email is spam or not spam", "Confirm your email address", "Value", "Confidence level", "Confirm your email address", "Confidence level", "Not Spam", "76", "%", "Spam", "24", "%", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Privacy", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "Locally Differentially Private Document Generation Using Zero Shot Prompting"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Oct 24, 2023", "Safety", "Privacy", "Safety", "Privacy", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/blog-title-alternatives-generator", "page_title": "Blog Title Alternatives Generator", "headers": ["Blog Title Alternatives Generator", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Suggest three alternative titles with a better marketing copy", "Suggest three alternative titles with a better marketing copy for the following blog.", "The tone of the alternative titles is: Inspirational", "Blog Title: Learning to Play Chess", "Alternative Blog Titles:", "1. \"How I Learned to Play Chess and You Can, Too!\"", "2. \"Chess: A Game of Strategy and Intrigue\"", "3. \"The Beginner's Guide to Chess\"", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/insurance-faq-classification", "page_title": "Insurance FAQ Classification", "headers": ["Insurance FAQ Classification", "Examples", "Request", "Response", "API Request"], "content": ["Try now", "Using our classification model, customer questions can be easily classified into the correct category.", "I want to change my password", "Value", "Confidence level", "I want to change my password", "Confidence level", "Change account settings", "99", "%", "Cancelling coverage", "1", "%", "Finding policy details", "0", "%", "Filing a claim and viewing status", "0", "%", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/article-summarization", "page_title": "Article Summarization", "headers": ["Article Summarization", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Summarize the main point of a paragraph into a single sentence", "Passage: Is Wordle getting tougher to solve? Players seem to be convinced that the game has gotten harder in recent weeks ever since The New York Times bought it from developer Josh Wardle in late January. The Times has come forward and shared that this likely isn\u2019t the case. That said, the NYT did mess with the back end code a bit, removing some offensive and sexual language, as well as some obscure words There is a viral thread claiming that a confirmation bias was at play. One Twitter user went so far as to claim the game has gone to \u201cthe dusty section of the dictionary\u201d to find its latest words.", "TLDR: Wordle has not gotten more difficult to solve.", "--", "Passage: ArtificialIvan, a seven-year-old, London-based payment and expense management software company, has raised $190 million in Series C funding led by ARG Global, with participation from D9 Capital Group and Boulder Capital. Earlier backers also joined the round, including Hilton Group, Roxanne Capital, Paved Roads Ventures, Brook Partners, and Plato Capital.", "TLDR: ArtificialIvan has raised $190 million in Series C funding.", "--", "Passage: The National Weather Service announced Tuesday that a freeze warning is in effect for the Bay Area, with freezing temperatures expected in these areas overnight. Temperatures could fall into the mid-20s to low 30s in some areas. In anticipation of the hard freeze, the weather service warns people to take action now.", "TLDR:", "A freeze warning is in effect for the Bay Area, with temperatures expected to fall into the mid-20s to low 30s in some areas.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/email-keyword-extraction", "page_title": "Email Keyword Extraction", "headers": ["Email Keyword Extraction", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Extract product name and return address from an email", "Extract the product and return address from this email:", "Dear Elliott, ", "Unfortunately, your item, Supreme The North Face Statue of Liberty Baltoro Jacket, was not accepted for verification. ", "We will be shipping this back to 171 John Street, Toronto ON, M5T 1X2.", "Product:", "Supreme The North Face Statue of Liberty Baltoro Jacket", "Address: 171 John Street, Toronto ON, M5T 1X2", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/instagram-caption", "page_title": "Instagram Caption", "headers": ["Instagram Caption", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate a caption with a call to action", "Write a formal instagram caption without hashtags for working professionals advertising beginner hip hop, ballet, and gymnastics classes at my dance studio", "Looking for a fun way to get your little one moving? Look no further than my dance studio! We offer beginner hip hop, ballet, and gymnastics classes that are sure to please. Our professional instructors are dedicated to providing a positive learning experience for your child. So come on in and check us out!", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/product-description-to-benefits", "page_title": "Product Description to Benefits", "headers": ["Product Description to Benefits", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Turns a product description into a list of functional, emotional, and social benefits.", "Turn the following product feature into a list of benefits. Group them into three types of benefits: Functional Benefits, Emotional Benefits, and Social Benefits.", "Product Feature:", "Our app automatically transcribes your meetings. It uses state-of-the-art speech-to-text technology that works even in noisy backgrounds. Once the transcription is done, our app creates its summary and automatically emails it to the meeting attendees.", "Functional Benefits:", "- Save time by not having to take notes during meetings", "- Increase accuracy by not having to rely on your own memory", "- Reduce stress by not having to worry about forgetting something important", "Emotional Benefits:", "- Feel more engaged and included in the meeting", "- Get a better understanding of what was discussed", "- Feel like your time was well-spent", "Social Benefits:", "- Increase collaboration and teamwork", "- Encourage employees to share their ideas", "- Make it easier for people to stay up-to-date on what's happening in the company", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/research/papers?tag=Data", "page_title": "Research papers | Cohere For AI", "headers": ["Research papers", "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI"], "content": ["Try now", "Work by Cohere For AI and Technical Staff at Cohere", "Search papers", "Filter papers", "Topics (1)", "Remove All Filters", "Oct 25, 2023", "Responsible AI", "Safety", "AI Policy", "Data", "Responsible AI", "Safety", "AI Policy", "Data", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/customer-persona", "page_title": "Customer Persona", "headers": ["Customer Persona", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Describe a potential customer persona for a product", "Describe a potential customer persona for the product: ", "Sleep Tracker.", "For that persona, describe the following information. ", "1. Psychographic", "2. Demographic", "3. Day-in-the-life", "4. Struggles and pains", "5. Jobs-to-be-done", "6. Others", "1. Psychographic:", "1. Psychographic: That person is interested in tracking their sleep because they want to improve their sleep quality.", "2. Demographic: That person is between the ages of 18 and 29.", "3. Day-in-the-life: That person is a student who is currently living away from home.", "4. Struggles and pains: That person is struggling to get enough sleep because they have a lot of work to do and they are often up late.", "5. Jobs-to-be-done: That person is looking for a way to improve their sleep quality so that they can feel more rested and be more productive during the day.", "6. Others: That person has tried other sleep tracking apps in the past, but they were too difficult to use.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/blog-post-body-paragraph", "page_title": "Blog Post Body Paragraph", "headers": ["Blog Post Body Paragraph", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate a body paragraph for a blog post", "Write a body paragraph about \"Shopify is a great case study\" in a blog post titled \"Tips from the most successful companies", "Shopify is a great case study of a company that has successfully implemented tips from the most successful companies. They have a great product and a very successful business model. ", "Shopify is a web-based e-commerce platform that allows users to create and operate online stores. The company was founded in 2004 by a group of friends who were looking for a way to sell their online product ideas. They quickly realized that there was a gap in the market for a simple, easy-to-use e-commerce platform, and so they set about developing one. ", "Since then, Shopify has grown to become one of the most popular e-commerce platforms in the world. The company has over 300,000 active users, and they process over $2 billion in sales each year. Shopify has also been named as one of the fastest-growing companies in Canada. ", "This case study is a great example of how implementing tips from the most successful companies can lead to great.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/headline-market-analysis", "page_title": "Headline Market Analysis", "headers": ["Headline Market Analysis", "Examples", "Input", "Response", "API Request"], "content": ["Try now", "Given a news headline, identify if the category of the article is technology, economy, or health", "How Robots Can Assist Students with Disabilities", "Value", "Confidence level", "How Robots Can Assist Students with Disabilities", "Confidence level", "Technology", "48", "%", "Health", "44", "%", "Economy", "8", "%", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/transcript-summarization", "page_title": "Transcript Summarization", "headers": ["Transcript Summarization", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Extract the main point of a conversation", "Summarize this dialogue:", "Customer: Please connect me with a support agent.", "AI: Hi there, how can I assist you today?", "Customer: I forgot my password and lost access to the email affiliated to my account. Can you please help me?", "AI: Yes of course. First I'll need to confirm your identity and then I can connect you with one of our support agents.", "TLDR: A customer lost access to their account.", "--", "Summarize this dialogue:", "AI: Hi there, how can I assist you today?", "Customer: I want to book a product demo.", "AI: Sounds great. What country are you located in?", "Customer: I'll connect you with a support agent who can get something scheduled for you.", "TLDR: A customer wants to book a product demo.", "--", "Summarize this dialogue:", "AI: Hi there, how can I assist you today?", "Customer: I want to get more information about your pricing.", "AI: I can pull this for you, just a moment.", "TLDR:", "A customer wants to get more information about pricing.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/email-copy", "page_title": "Email Copy", "headers": ["Email Copy", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Generate an Email given a description", "Write a cold outreach email introducing myself as Susan, a business development manager at CoolCompany, to Amy who is a product manager at Microsoft asking if they'd be interested in speaking about an integration to add autocomplete to Microsoft Office.", "Hi Amy,", "I hope this email finds you well. I am Susan, a business development manager at CoolCompany, and we are interested in discussing an integration to add autocomplete to Microsoft Office.", "Autocomplete is a useful feature that can help users quickly find what they are looking for in Microsoft Office. It can also help reduce spelling errors and improve efficiency.", "We would be happy to discuss our proposal with you to see if there is any interest in our autocomplete integration. Please let me know if you would be available for a meeting or phone call sometime in the next week to discuss.", "Thank you,", "Susan", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Weijia-Shi-2023", "page_title": "Cohere For AI - Guest Speaker: Weijia Shi, PhD student at UW", "headers": ["Cohere For AI - Guest Speaker: Weijia Shi, PhD student at UW"], "content": ["Try now", "Date", ": ", "Feb 24, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", " I am a PhD student in Computer Science at the\u00a0University of Washington advised by Prof.\u00a0Luke Zettlemoyer and Prof. Noah A. Smith. I have been a visiting research at Meta AI, working with Scott Yih. Prior to UW, I graduated from UCLA with a B.S. in Computer Science and Minor in Math. Twitter: ", " Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm.abs: ", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/restaurant-customer-inquiries", "page_title": "Restaurant Customer Inquiries", "headers": ["Restaurant Customer Inquiries", "Dataset", "Sample Response", "API Request"], "content": ["Try now", "Get a visualization for all the FAQs for your restaurant ", "#", "TEXT", "1.", "When are you open?", "2.", "When do you close?", "3.", "What are the hours?", "4.", "Are you open on weekends?", "5.", "Are you available on holidays?", "6.", "How much is a burger?", "7.", "What's the price of a meal?", "8.", "How much for a few burgers?", "9.", "Do you have a vegan option?", "10.", "Do you have vegetarian?", "11.", "Do you serve non-meat alternatives?", "12.", "Do you have milkshakes?", "13.", "Milkshake", "14.", "Do you have desert?", "15.", "Can I bring my child?", "16.", "Are you kid friendly?", "17.", "Do you have booster seats?", "18.", "Do you do delivery?", "19.", "Is there takeout?", "20.", "Do you deliver?", "21.", "Can I have it delivered?", "22.", "Can you bring it to me?", "23.", "Do you have space for a party?", "24.", "Can you accommodate large groups?", "25.", "Can I book a party here?", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples/spelling-and-grammar-check", "page_title": "Spelling and Grammar Check", "headers": ["Spelling and Grammar Check", "Input", "Sample Response", "API Request", "Parameters"], "content": ["Try now", "Given a list of text, it will remove improper capitalization and transform the sentence to Capitalized Text", "This is a spell check generator that capitalizes samples of text.", "Sample: a new type OF aurora FounD on saturn resolves a planetary mystery", "Capitalized Text: A New Type Of Aurora Found On Saturn Resolves A Planetary Mystery", "--", "Sample: online Shopping is ReSHaping Real-world Cities", "Title Case: Online Shopping Is Reshaping Real-World Cities", "--", "Sample: When you close 100 TAbs AFter Finding THE SoluTion To A BuG", "Title Case: When You Close 100 Tabs After Finding The Solution To A Bug", "--", "Sample: mastering DYNAmIC ProGrammING", "Title Case:", "Mastering Dynamic Programming", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Arthur-Douillard-2023", "page_title": "Cohere For AI - Guest Speaker:Arthur Douillard, Sr. Research @DeepMind", "headers": ["Cohere For AI - Guest Speaker:Arthur Douillard, Sr. Research @DeepMind"], "content": ["Try now", "Date", ": ", "Feb 21, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", " \"Arthur is a Senior Research Scientist at DeepMind, he got his PhD in 2022 from Sorbonne, Paris, for his work on Continual Learning. Now living in London, he's working on making IA training more collaborative through distributed training. @Ar_Douillard on Twitter.\"\u00a0", " Large language models (LLM) have become a critical component in many applications of machine learning. However, standard approaches to training LLM require a large number of tightly interconnected accelerators, with devices exchanging gradients and other intermediate states at each optimization step. While it is difficult to build and maintain a single computing cluster hosting many accelerators, it might be easier to find several computing clusters each hosting a smaller number of devices. In this work, we propose a distributed optimization algorithm, Distributed Low-Communication (DiLoCo), that enables training of language models on islands of devices that are poorly connected. The approach is a variant of federated averaging, where the number of inner steps is large, the inner optimizer is AdamW, and the outer optimizer is Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8 workers performs as well as fully synchronous optimization while communicating 500 times less. DiLoCo exhibits great robustness to the data distribution of each worker. It is also robust to resources becoming unavailable over time, and vice versa, it can seamlessly leverage resources that become available during training. abs: ", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Akifumi-Wachi-2023", "page_title": "Cohere For AI - Guest Speaker: Akifumi Wachi, Sr Research Scientist", "headers": ["Cohere For AI - Guest Speaker: Akifumi Wachi, Sr Research Scientist"], "content": ["Try now", "Date", ": ", "Feb 13, 2024", "Time: ", "8:00 AM - 9:00 AM ", "Location: ", "Online", "Join our Reinforcement Learning group as they host\u00a0Akifumi Wachi to present \"Safe RL.\"", "Speaker's Bio: Akifumi Wachi is a Chief Research Scientist at LY Corporation. His research interests lie primarily in reinforcement learning and span the entire theory-to-application spectrum from fundamental advances to deployment in real-world systems. He is especially interested in how a policy should and can be trained and deployed in safety-critical problems. HP: ", " Description: Safety is an essential problem when deploying reinforcement learning (RL) to real applications. Consequently, safe RL emerges as a fundamental yet powerful paradigm for optimizing an agent's policy from experimental data, which is typically formulated as a problem where the expected cumulative reward is maximized under safety constraints. The talk will first give an overview of safe RL and then discuss a few algorithms we have recently proposed including SNO-MDP (", ") and MASE (", "). ", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Muhammad-Uzair-Khattak-2024", "page_title": "Cohere For AI - Guest Speaker: Muhammad Uzair Khattak, MBZUAI Graduate", "headers": ["Cohere For AI - Guest Speaker: Muhammad Uzair Khattak, MBZUAI Graduate"], "content": ["Try now", "Date", ": ", "Feb 05, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", " Muhammad Uzair Khattak About Speaker: I am Muhammad Uzair, recently graduated from MBZUAI with a master's degree in Computer Vision. During my master's, I was fortunate to work under the supervision of Dr. Salman Khan and Dr. Fahad Khan at the IVAL lab. I am also grateful to have been supervised and mentored by Dr. Muzammal Naseer. My research focus is on adapting foundational multi-modal models for vision tasks, including image recognition, object detection, and video action recognition. The goal is to guide these foundational models for downstream tasks with limited data (few-/zero-shot) while maintaining their pre-trained generalization for novel tasks.", " ProText: Prompt Learning with Text Only Supervision, allows one to finetune CLIP by leveraging contextual knowledge derived from Large Language Models (LLMs) without relying on visual samples. ProText exhibits strong transferability towards unseen datasets and classes and effectively overcomes the transferability limitations of LLM-based Prompt Ensembling methods. Through text-only training, ProText improves over previous prompt ensembling and image-supervised methods in challenging cross-dataset transfer settings. We have open-sourced our checkpoints and source code. ProText offers a drop-in replacement for CLIP\u2019s text transformer. We look forward to the impact of ProText on applications beyond what we tested in the manuscript. abs: https://arxiv.org/abs/2401.02418", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Jacob%20Hilton-2024", "page_title": "Cohere For AI Fireside Chat: Jacob Hilton", "headers": ["Cohere For AI Fireside Chat: Jacob Hilton"], "content": ["Try now", "Date", ": ", "Jan 24, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", "Cohere For AI Fireside Chats bring together leading researchers and rising stars in the field of machine learning to discuss their research learning journeys. Research is inherently a human endeavour, and this discussion series provides insights from beginning to breakthrough.", "This Fireside Chat features Jacob Hilton, Researcher at the Alignment Research Center. Beyza Ermis, Research Scientist at Cohere For AI, will sit down with Jacob for a conversation on \"Discussion on AI Truthfulness, Falsehoods and Hallucinations.\"", " ", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Jorge-Ruiz-Reyes-2024", "page_title": "Cohere For AI - Guest Speaker: Jorge Ruiz Reyes, Political Scientist", "headers": ["Cohere For AI - Guest Speaker: Jorge Ruiz Reyes, Political Scientist"], "content": ["Try now", "Date", ": ", "Jan 31, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", "Disappearances committed by state and non-state armed actors have become widespread in Mexico since late 2006 due to the use of armed forces to counter drug-related activities in the country. Consequently, groups of families with missing persons and authorities are carrying out search brigades to locate missing persons in different regions. A fraction of these persons have been found in clandestine or hidden graves.", "In this context, Data Civica, CentroGeo, Universidad Iberoamericana, and Elementa DDHH, with the support of Amnesty International, developed a geospatial model that delimits areas in Mexico where it is likely that new clandestine graves will be located. This new approach includes three methods: Point pattern analysis, accessibility combined with visibility analysis, and hyperspectral analysis.", "This session will show the methods to develop this tool and highlight the latest results and the impact of its implementation on the field.", " Jorge Ruiz Reyes holds a master's degree in Social Data Science from the Univerisity of Oxford and an undergraduate degree in Political Science from Universidad Iberoamericana (Ibero-Mexico). He was the lead researcher of the Human Rights Programme of Universidad Iberoamericana (2016-2021). He has also worked as Sr. Data Analyst and Project Coordinator (2021-2022) at Data C\u00edvica, a Mexican NGO that works with data and technology to support human rights advocacy. He is finishing a consultancy period for Amnesty International's Crisis Evidence Lab.", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Lewis-Tunstall-2024", "page_title": "Cohere For AI - Guest Speaker: Lewis Tunstall, Engineer @ Hugging Face", "headers": ["Cohere For AI - Guest Speaker: Lewis Tunstall, Engineer @ Hugging Face"], "content": ["Try now", "Date", ": ", "Feb 02, 2024", "Time: ", "6:00 PM - 7:00 PM ", "Location: ", "Online", "Event description: We are excited to announce a speaker session featuring Lewis Tunstall , Co-author of \"NLP with Transformers\" book. He has had an interesting journey to get into AI research. He finished his PhD in Theoretical Physics. Now he is a machine learning engineer at Hugging Face, working on reinforcement learning from human feedback. Coming from a non-CS background, it's always interesting to learn how he transitioned to machine learning research. Besides his journey, we will explore Hugging Face. Hugging Face is a platform where the machine learning community collaborates on models, datasets, and applications. We will learn about how to utilize hugging face as an experimentation platform.", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Timothee-Darcet-2023", "page_title": "Cohere For AI - Guest Speaker: Timoth\u00e9e Darcet, PhD @ Meta AI & INRIA", "headers": ["Cohere For AI - Guest Speaker: Timoth\u00e9e Darcet, PhD @ Meta AI & INRIA"], "content": ["Try now", "Date", ": ", "Feb 07, 2024", "Time: ", "4:00 PM - 5:00 PM ", "Location: ", "Online", "Description:", "Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.", "abs:\u00a0https://arxiv.org/abs/2309.16588v1", "Intro: I am a PhD student at FAIR Paris (Meta) and INRIA Grenoble, co-advised by Piotr Bojanowski, Maxime Oquab, Julien Mairal and (formerly) Armand Joulin. My work focuses on self-supervised learning (DINOv2) and understanding ViTs (vision transformers need registers). Prior to that, I studied at \u00c9cole polytechnique, and at ENS (MVA master).", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/events/c4ai-Geo-Regional-Africa-2024", "page_title": "Cohere For AI - Geo Regional Africa Kick-off Event", "headers": ["Cohere For AI - Geo Regional Africa Kick-off Event"], "content": ["Try now", "Date", ": ", "Jan 23, 2024", "Time: ", "3:00 PM - 4:00 PM ", "Location: ", "Online", "A New Beginning! Join us as we inaugurate the Geo Regional Africa group!", "This is more than just a meeting \u2013 it's a social gateway to connect with fellow enthusiasts around the world.", "This gathering is your platform to mingle, share ideas, and voice your thoughts on what you'd like to see in future events and speakers. Let's together shape the direction of our Geo Regional Africa group, making it a hub of innovation and collaboration.", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/examples?useCase=Summarization", "page_title": "Examples - Inspiring NLP Use Cases and Applications | Cohere", "headers": ["Examples"], "content": ["Try now", "With Cohere, you can build NLP-powered solutions that deliver an ongoing flow of content for the business.", "Search examples", "Filter Examples", "All Endpoints", "Summarization", "Transcript Summarization", "Extract the main point of a conversation", "Article Summarization", "Summarize the main point of a paragraph into a single sentence", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/generate", "page_title": "Generate - AI-Powered Text Generation with LLMs | Cohere", "headers": ["Write copy for any context", "Write copy for any context", "Write copy for any context", "What's possible with Generate", "Why Generate", "Your data, your way", "Your goals, Cohere\u2019s AI", "Your business, our commitment", "Powered by Cohere\u2019s flagship model: Command", "Your data, your way", "Your goals, Cohere\u2019s AI", "Your business, our commitment", "Powered by Cohere\u2019s flagship model: Command", "Generate resources", "See how Cohere works. Try our playground.", "See how Cohere works. Try our playground."], "content": ["Try now", "generate", "Generate produces unique content for emails, landing pages, product descriptions, and more. \n", "generate", "Generate produces unique content for emails, landing pages, product descriptions, and more. \n", "generate", "Generate produces unique content for emails, landing pages, product descriptions, and more. \n", "Ad content", "Blog copy", "Product descriptions", "\"Cohere excels at delivering high quality, low-latency language AI models and really supporting them.  Having Cohere\u2019s team as an extension of ours lets us go 10x faster.\"", "Simple APIs, powerful results", "No matter your level of experience with ML/AI, Cohere\u2019s Command model makes it easy to generate text in your applications.", "Deliver exceptional experiences wherever your customers interact with your business, whether it be public, private, or hybrid cloud.", "Our state-of-the-art models are not only accurate, but they also support your specific tasks and drive impact for your business.", "We collaborate with your preferred partners and adapt to any environment to ensure that safety and security always come first.", "Cohere\u2019s text generation model is tuned to follow user commands and deliver leading edge performance with continuous updates.", "Deliver exceptional experiences wherever your customers interact with your business, whether it be public, private, or hybrid cloud.", "Our state-of-the-art models are not only accurate, but they also support your specific tasks and drive impact for your business.", "We collaborate with your preferred partners and adapt to any environment to ensure that safety and security always come first.", "Cohere\u2019s text generation model is tuned to follow user commands and deliver leading edge performance with continuous updates.", "Generate documentation", "Learn how to integrate text generation capabilities into your apps.", "Get started with Cohere today!", "Reach out to us and let\u2019s discuss your text generation needs.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/contact-sales", "page_title": "Contact Sales - Get in Touch with Cohere's Team | Cohere", "headers": ["Get in touch"], "content": ["Try now", "Have a specific question or want to speak directly with someone? Please provide your information and someone from our team will get back to you shortly.\u2028", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/", "page_title": "Cohere | The leading AI platform for enterprise", "headers": ["Build conversational apps with RAG", "Give your technology language", "Chat with RAG", "Powerful, accurate semantic search", "Search performance improvement", "Accelerate the next wave of productivity", "The enterprise LLM"], "content": ["Try now", "Cohere\u2019s Retrieval Augmented Generation (RAG) toolkit allows LLMs to accurately answer questions and solve tasks using enterprise data as the source of truth", "Cohere\u2019s world-class large language models (LLMs) help enterprises build powerful, secure applications that search, understand meaning and converse in text.", "Coral", "Cohere\u2019s Coral lets you build powerful chatbots and knowledge assistants. Coral uses RAG (Retrieval Augmented Generation) to deliver accurate conversations grounded by your enterprise data.", "Search", "Cohere\u2019s Embed model allows enterprises to build powerful search solutions.\u00a0Embed is the industry\u2019s highest performing embedding model in English and over 100 other languages, ensuring relevant results.", "Rerank", "Rerank greatly improves the relevance of search results from existing lexical and semantic search tools, such as Elasticsearch, OpenSearch, or Solr, and is customizable by domain for better performance.", "Today\u2019s language models already show productivity gains in white-collar tasks of over\u00a050%.\u00a0The coming Intelligent Assistants will understand your enterprise data, giving your employees the tools to make decisions far more quickly than we can imagine today. ", " ", "CUSTOMIZABLE MODELS", "Cohere offers sophisticated customization (fine-tuning) tools and capabilities that give superior model performance at industry-leading inference cost", "PERFORMANCE AND SCALABILITY", "Cohere\u2019s models are packaged with inference engines that deliver better runtime performance at a lower cost than open-source equivalents", "FLEXIBLE DEPLOYMENT OPTIONS", "Cohere models are accessible through a SaaS API, cloud services (e.g., OCI, AWS SageMaker, Bedrock), and private deployments (VPC and on-prem)", "PRIVACY", "Customer data is never used in training base models, and customers have complete control over customization and model inputs/outputs", "CUSTOMIZABLE MODELS", "Cohere offers sophisticated customization (fine-tuning) tools and capabilities that give superior model performance at industry-leading inference cost", "PERFORMANCE AND SCALABILITY", "Cohere\u2019s models are packaged with inference engines that deliver better runtime performance at a lower cost than open-source equivalents", "FLEXIBLE DEPLOYMENT OPTIONS", "Cohere models are accessible through a SaaS API, cloud services (e.g., OCI, AWS SageMaker, Bedrock), and private deployments (VPC and on-prem)", "PRIVACY", "Customer data is never used in training base models, and customers have complete control over customization and model inputs/outputs", "LLM University", "Curious about large language models?", "LLM University offers an approachable and structured\u2028 curriculum to get you speaking our language.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/search", "page_title": "Semantic Search - Advanced Semantic Search for Any Language | Cohere", "headers": ["Build better search for any language", "Build better search for any language", "Build better search for any language", "What\u2019s possible with Semantic Search", "Why Semantic Search", "Embeddings performance", "Multilingual support", "Scalability", "Flexible deployment options", "Embeddings performance", "Multilingual support", "Scalability", "Flexible deployment options", "Semantic Search resources"], "content": ["Try now", "semantic search", "Semantic Search provides powerful semantic search capabilities that find text, documents, and articles based on meaning, not just keywords.", "semantic search", "Semantic Search provides powerful semantic search capabilities that find text, documents, and articles based on meaning, not just keywords.", "semantic search", "Semantic Search provides powerful semantic search capabilities that find text, documents, and articles based on meaning, not just keywords.", "Vector search: English", "Vector search: Multilingual", "\"At ML6, we see that multilingualism remains a major challenge in an English-centric NLP landscape \u2014 especially in Europe. Naturally, we are actively on the lookout for solutions and have been impressed by what we\u2019ve seen from Cohere thus far!\"", "Simple APIs, powerful results", "No matter your level of experience with ML/AI, the Cohere Platform makes it easy to add search to your applications.", "Cohere\u2019s Embed model leads the industry in accuracy and performance, meaning the highest search relevance, even with noisy datasets", "Over 100 languages are supported, so the same topics, products and issues are identified the same way in each", "Cohere Embed supports data compression, reducing storage and compute requirements", "Cohere models are accessible through a SaaS API, cloud services (e.g., OCI, AWS SageMaker, Bedrock), and private deployments (VPC and on-prem)", "Cohere\u2019s Embed model leads the industry in accuracy and performance, meaning the highest search relevance, even with noisy datasets", "Over 100 languages are supported, so the same topics, products and issues are identified the same way in each", "Cohere Embed supports data compression, reducing storage and compute requirements", "Cohere models are accessible through a SaaS API, cloud services (e.g., OCI, AWS SageMaker, Bedrock), and private deployments (VPC and on-prem)", "Semantic search documentation", "Learn how to implement semantic search using Cohere\u2019s embedding models.", "Get started with Cohere today!", "Reach out to us and let\u2019s discuss your search needs.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/coral", "page_title": "Coral | Cohere", "headers": ["Start building with Coral", "Start building with Coral", "Ground responses in your data", "Make enterprise data useful", "Keep your data secure and private", "Frequently Asked Questions"], "content": ["Try now", "CONVERSATIONAL AI TOOLKIT", " Coral is a conversational AI toolkit for enterprises to build RAG-enabled knowledge assistants for their employees and customers.", "CONVERSATIONAL AI TOOLKIT", " Coral is a conversational AI toolkit for enterprises to build RAG-enabled knowledge assistants for their employees and customers.", "VERIFIABLE ANSWERS", "Our language models can search multiple datastores and using retrieval-augmented generation (RAG), dynamically incorporate relevant information into model responses (citations included). This improves the accuracy of responses and minimizes hallucinations.", "Give me step-by-step instructions for creating a new LLC in California", "1. Choose a name", "2. Pick a business structure", "3. Choose a registered agent", "4. File your Articles of Organization", "5. Create an operating agreement", "6. Get an EIN", "7. Open a business bank account", "8. Get business insurance", "source: ", " ", "USA_CA-Biz_Guidelines.doc", "Give me step-by-step instructions for creating a new LLC in California", "Give me step-by-step instructions for creating a new LLC in California", "Give me step-by-step instructions for creating a new LLC in California", "Our language models can search multiple datastores and using retrieval-augmented generation (RAG), dynamically incorporate relevant information into model responses (citations included). This improves the accuracy of responses and minimizes hallucinations.", "What are the key takeaways from our 2024 annual plan?", "How do I get access to our ticket management software?", "What\u2019s the latest on the supply-chain optimization project?", "AUGMENT COMMAND\u2019S KNOWLEDGE", "Build a tailored knowledge assistant for unique job functions. Connectors can be built into 100+ datastores across CRMs, collaboration tools, databases, and more. Make your teams more productive.", "Connect to 100+ datastores like:", "Data stays with you", "We offer industry leading deployment options for our models to meet enterprise security requirements. We can deploy through cloud partners (e.g. AWS, GCP, OCI, Azure) or virtual private clouds.", "Cohere's latest Command model - our top performing generative model that is retrained weekly - sits at the core of the Coral toolkit.", "Yes. Command can provide responses grounded on your data and documents. Our API provides the option to display citations which denote sources.", "Connectors allow businesses to augment Command\u2019s knowledge with data from their business, making certain responses more relevant and accurate.", "Explore the future of AI with Coral", "Coral is currently in open beta. Use the toolkit to start building. If your organization has a large scale project, talk to our team.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/rerank", "page_title": "Rerank - Optimize Your Search With One Line of Code | Cohere", "headers": ["Improve search performance with a single line of code", "Improve search performance with a single line of code", "Improve search performance with a single line of code", "What's possible with Rerank", "Why Rerank", "Industry-leading accuracy", "Customization (fine-tuning)", "Scalability", "Flexible deployment", "Industry-leading accuracy", "Customization (fine-tuning)", "Scalability", "Flexible deployment", "Rerank resources"], "content": ["Try now", "rerank", "Rerank provides a powerful semantic boost to the search quality of any keyword or vector search system without requiring any overhaul or replacement. \n", "rerank", "Rerank provides a powerful semantic boost to the search quality of any keyword or vector search system without requiring any overhaul or replacement. \n", "rerank", "Rerank provides a powerful semantic boost to the search quality of any keyword or vector search system without requiring any overhaul or replacement. \n", "Improve search results", "Optimize eCommerce search", "Boost knowledge base search", "Implement Rerank in just a few minutes", "Implementation can take as little as ten minutes (we\u2019ve seen it!) and only a few lines of code. The code here implements Rerank.", "Cohere\u2019s embedding performance ensures accurate reranking, even with noisy datasets", "Cohere\u2019s rerank model can be fine-tuned to further improve domain performance", "Cohere\u2019s powerful inference frameworks optimize throughput and reduce compute requirements", "Cohere\u2019s models can be accessed through a SaaS API, on cloud services (e.g. OCI, AWS SageMaker, Bedrock) and soon through private deployments (VPC and on-premise)", "Cohere\u2019s embedding performance ensures accurate reranking, even with noisy datasets", "Cohere\u2019s rerank model can be fine-tuned to further improve domain performance", "Cohere\u2019s powerful inference frameworks optimize throughput and reduce compute requirements", "Cohere\u2019s models can be accessed through a SaaS API, on cloud services (e.g. OCI, AWS SageMaker, Bedrock) and soon through private deployments (VPC and on-premise)", "Cohere docs", "Reranking", "Cohere docs", "Reranking Best Practices", "Get started with Cohere today!", "Reach out to us and let\u2019s discuss how to improve your search performance.", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/chat", "page_title": "Chat | Cohere", "headers": ["Build Conversational AI into your Apps with RAG", "Build Conversational AI into your Apps with RAG", "Retrieve information with connectors", "What\u2019s possible when Chat and RAG come together? ", "Why Chat with RAG?", "Conversation\u2028 is the new interface", "Reduce hallucinations with grounding, citations", "Keep your data private", "Try Chat with RAG in our Coral Showcase", "Try Chat with RAG in our Coral Showcase", "Chat resources"], "content": ["Try now", "Chat", "Chat with retrieval-augmented generation (RAG) integrates inputs, sources, and models to build more powerful product experiences. It\u2019s all powered by Command.\n", "Chat", "Chat with retrieval-augmented generation (RAG) integrates inputs, sources, and models to build more powerful product experiences. It\u2019s all powered by Command.\n", "Enterprise Datastores", "Point the model at internal datastores, allowing it to seamlessly (and securely) cite your proprietary data.", "The Internet", "Point the model at the web, allowing it to generate responses grounded in real-time information.", "Specific Documents", "Provide the model with manually selected documents, enabling grounded Q&A on particular content.", "Conversational Knowledge Assistants", "Customer Support", "Learning apps", "Chat understands the intent behind messages, remembers conversation history, and responds intelligently through multi-turn conversations. Chat responses are powered by Cohere's Command model.", "Reduce hallucinations and create trust between generated responses and users with citations to understand where responses are coming from. Command is trained to answer questions from additional sources.", "When privately deployed, the training data, input prompts, and output responses stay private and don't leave your secure environment.", "Simple APIs, powerful results", "No matter your level of experience with ML/AI, Cohere\u2019s Command model makes it easy to build chat interfaces in your applications.", "The Coral Showcase is our demo environment to preview Coral\u2019s latest enterprise chat capabilities. \n", "The Coral Showcase is our demo environment to preview Coral\u2019s latest enterprise chat capabilities. \n", "Explore our docs and articles, or get hands-on and build your own demo.", "Docs", "Learn how to integrate chat capabilities into your apps", "Cohere docs", "Retrieval Augmented Generation (RAG)", "Get started with Cohere today", "Contact us to discuss how chat can help transform your products", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]},
{"url": "https://cohere.com/coral-contact", "page_title": "Get in touch about Coral | Cohere", "headers": ["Learn more about Coral"], "content": ["Try now", "Coral is a knowledge assistant for enterprises \u2028to supercharge the productivity of their most strategic teams. Get started.\n", "Products", "Developers", "Company", "Contact", "\u00a9Cohere ", "2024"]}
]