[
{"url": "https://gretel.ai", "page_title": "Synthetic Data Generation Platform for Developers", "headers": ["Gretel", "The ", " platform for developers.", "We help developers unlock synthetic data.", "Generate accurate and safe synthetic data, on demand.", "Get started with synthetic data in less than five minutes.", "Deploy Gretel for enterprise use cases", "Using AI to Create Safe Synthetic Datasets for Genomics.", "Creating Synthetic Time Series Data for Global Financial Institutions.", "Run Gretel ", " ", "Implement privacy ", "Gretel Cloud", "...or in your environment.", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Train generative AI models that learn the statistical properties of your data.", "Validate your models and use cases with our quality and privacy scores.", "Generate as much data as you need, when you need it.", "Harness the power of generative AI in your enterprise with Gretel and Google Cloud.", "Unlock the power of multi-modal synthetic data in your enterprise with AWS.", "Unlock generative AI in Azure with custom AI models that are trained on safe, secure data.", "Using our cloud GPUs makes it radically more effortless for developers to train and generate synthetic data.", "Scale workloads automatically with no infrastructure to set up and manage.", "Invite team members to collaborate on cloud projects and share data across teams.", "Your data never leaves your environment. Runners can generate and transform locally, orchestrated by our APIs.", "Track progress and manage local workers from anywhere with the Gretel Console.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/faqs", "page_title": "Frequently Asked Questions - Gretel.ai", "headers": ["Frequently Asked Questions", "Commonly asked questions", "Gretel Cloud", "What types of Entities can Gretel identify?", "What is Gretel Cloud?", "Gretel-synthetics", "What is synthetic data?", "How does Gretel Synthetics create artificial data?", "What kinds of data can I send to Gretel Synthetics?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/faqs/gretel-cloud", "page_title": "Gretel Cloud FAQs - Gretel.ai", "headers": ["Gretel Cloud FAQs", "What is Gretel Cloud?", "What types of Entities can Gretel identify?", "How do I get started?", "What can I do with Gretel Cloud?", "What is a project in Gretel Cloud?", "What happens to the records I send?", "If you only keep 50,000 records, how do I look at more of my labeled records?", "Are there API limits?", "How do I get my data to Gretel Cloud?", "Where can I get help or support?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/security-reporting", "page_title": "Security reporting - Gretel.ai", "headers": ["Security Reporting", "Our Commitment", "Rules of Engagement", "Security Reports We Are Not Interested In", "How to Report Security Issues to Gretel", "What You Should Include in Your Report", "How Gretel Will Respond", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["\u200d", "Gretel was founded by veterans of the security community. We are passionate about ensuring that our interactions with security researchers are positive and respectful. We treat each report we receive with the utmost seriousness while we evaluate the potential impact that it could have for our customers across the various products & services we offer.", "During this process, Gretel will communicate as promptly as we\u2019re able until completion of our investigation and any necessary remediation. We thank you for your time & expertise to improve the security of our company and customers.", "To ensure a great experience with Gretel, we ask that bug reporters follow these simple rules of engagement to limit the potential that company and/or customer data may be at risk:", "If you are ever unclear on how far your testing should go, please reach out to Gretel (", ") to coordinate testing with us. We can often validate your suspicions in simple ways that can reduce the chance of harm occurring to our services & customers.", "While many of the security reports Gretel receives are of a useful nature, we do want to specify the types of reports that are not useful at this time. This list is not all inclusive, but it may help bug reporters prioritize their research. ", "If you have found a security issue or bug with something in Gretel's open source repositories:", "If you have found a security issue or bug in Gretel's services, tools, or infrastructure:", "Gretel will provide an initial acknowledgement of your security report within 48 hours under normal circumstances once our team receives it. We may follow up with additional questions to ensure we fully understand the report and its potential impact. Following our initial acknowledgement, Gretel will try to keep reporters aware of the progress on analysis, verification, and any required remediation steps along the way.", "If a reported issue has been validated and depending on system(s) impacted, Gretel may provide a public acknowledgement in relevant release notes, documentation, or blog posts that reference the issue. Gretel does not operate a bug bounty program at this time, but may choose to reward reporters of issues in some cases, at our discretion.", "Once a security report has been triaged, Gretel may take additional remediation steps. \u00a0These remediation steps may extend many weeks or months if the severity is low enough. In these cases, we will let you know that we plan to fix our issue, but will be unable to guarantee a timeline in those cases due to the nature of remediation prioritization."]},
{"url": "https://gretel.ai/privacy", "page_title": "Privacy Statement - Gretel.ai", "headers": ["Gretel Privacy Policy", "The short version", "What personal information Gretel collects", "Personal information users provide directly to Gretel", "Registration Information", "Payment Information", "Profile Information", "Personal information Gretel automatically collects from your use of the Services", "Usage Information", "Cookies and Similar Technologies Information", "Device Information", "Personal information we collect from third parties", "Social media platforms\u00a0", "What personal information Gretel does not collect", "How Gretel uses your personal information", "Our legal bases for processing personal information", "Contract Performance:", "Consent:", "Legitimate Interests:", "How we share the information we collect", "With your consent", "With service providers", "For security purposes", "For legal disclosure", "Retention", "Change in control or sale", "Aggregate, non-personally identifying information", "Other important information", "Data Project Content", "Our use of cookies and tracking", "What are cookies and other tracking technologies?", "How do we use Technologies?", "Third-Party Technologies list", "How Gretel secures your information", "Your privacy choices and rights\u00a0", "International data transfers\u00a0\u00a0", "Supplemental notice for California residents\u00a0", "\u201cSales\u201d of Personal Information under the CCPA", "Cross-Context Behavioral Advertising under the CCPA\u00a0", "Additional Privacy Rights for California Residents", "Supplemental notice for Nevada residents\u00a0", "Resolving complaints", "Changes to our Privacy Policy", "Contacting Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Thanks for entrusting Gretel Labs, Inc. (\u201c", "\u201d, \u201c", ",\u201d \u201c", "\u201d or \u201c", "\u201d) with your data, your projects, and your personal information. Holding on to your private information is a serious responsibility, and we want you to know how we\u2019re handling it. To make this Privacy Policy easier to read, our websites, products and services and other offerings are collectively called the \u201c", ".\u201d\u00a0", "As described below: We use your personal information as this Privacy Policy describes. No matter where you are, where you live, or what your citizenship is, we provide a high standard of privacy protection to all our users around the world, regardless of their country of origin or location.", "Of course, the short version and the Summary below don\u2019t tell you everything, so please read on for more details.", "We require some basic personal information at the time of account creation, such as your email address. When you initially register for Gretel, we ask you to connect with a valid GitHub or Gmail account and email address.", "If you sign on to a paid account with us, we collect your full name, address, and credit card information. Please note, Gretel does not process or store your credit card information, but our third-party payment processor does.", "You may choose to give us more information for your account profile, such as your full name, an avatar which may include a photograph, your biography, your location, your company, and a URL to a third-party website. This information may include personal information. Please note that your profile information may be visible to other users of our Services.", "If you have a paid account with us, we automatically collect certain information about your transactions on the Services, such as the date, time, and amount charged.", "We may automatically collect information regarding your use of our Services, such as the pages you view, the referring site, your IP address and session information, and the date and time of each request. This is information we collect from every visitor to our website, whether they have an account or not. This information may include personal information.", "As further described below, in the \u201c", "\u201d section, we may automatically collect information from cookies and similar Technologies (such as cookie ID and settings) to keep you logged in, to remember your preferences, and to identify you and your device.", "We may collect certain information about your device, such as its IP address, browser or client application information, language preference, operating system and application version, device type and ID, and device model and manufacturer. This information may include personal information.", "Gretel may collect personal information from third parties. For example, we collect name and email address data when you register via a GitHub or Gmail account. Gretel does not purchase personal information from third-party data brokers.", "Our Services may contain social media buttons or links, such as Discord, GitHub, Twitter, LinkedIn, YouTube, which might include widgets such as the \u201cshare this\u201d button or other interactive mini programs). These features may collect personal information such as your IP address and which page you are visiting on our Services, and may set a cookie to enable the feature to function properly. Your interactions with these platforms are governed by the privacy policy of the company providing it.", "We do not intentionally collect \u201cSensitive Personal Information\u201d, such as personal information revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person\u2019s sex life or sexual orientation. If you choose to store any Sensitive Personal Information on our servers, you are responsible for complying with any regulatory controls regarding that data.", "If you are a child under the age of 13, you may not have an account on Gretel. Our Service is not directed to children under 13\u00a0 (or other ages as required by local law), and Gretel does not knowingly collect personal information from or direct any of our content specifically to children under 13. If we learn or have reason to suspect that you are a user who is under the age of 13, we will have to delete any personal information we have collected from you, unless we have a legal obligation to keep it, and to close your account. We don\u2019t want to discourage you from learning to code, but those are the rules. Please see our Terms of Service for information about account termination. Different countries may have different minimum age limits, and if you are below the minimum age for providing consent for data collection in your country, you may not have an account on Gretel.", "If you are a parent or guardian and believe your child has uploaded personal information to Gretel without your consent, you may contact us as described in \u201c", "\u201d below.\u00a0", "We do not intentionally collect personal information that is stored in your data projects or other free-form content inputs. Any personal information within a user\u2019s data project is the responsibility of the data project owner.", "To the extent that our processing of your personal information is subject to certain international laws (including, but not limited to, the European Union\u2019s General Data Protection Regulation (GDPR)), Gretel is required to notify you about the legal basis on which we process personal information. Gretel processes personal information on the following legal bases:", "If you would like to request deletion of personal information we process on the basis of consent or if you object to our processing of personal information, you may contact us at ", " . You can also force deletion of all data yourself by deleting your account and data projects directly via the Services.", "We may share your personal information with third parties under one of the following circumstances:", "We share your personal information, if you consent, after letting you know what information will be shared, with whom, and why.\u00a0", "We share personal information with a limited number of service providers who process it on our behalf to provide or improve our Services. Our service providers perform payment processing, data hosting, customer support ticketing, network data transmission, security, and other similar services. Our service providers may process data outside of the United States or the European Union.", "If you are an authorized User (as defined in the ", ") of an organization that has purchased Gretel\u2019s Services, Gretel may share your username, usage information, and device information associated with with an owner and/or administrator of such organization who has agreed to the Gretel ", " or applicable customer agreements, to the extent that such information is provided only to investigate or respond to a security incident that affects or compromises the security of that particular organization.", "Gretel strives for transparency in complying with legal process and legal obligations. Unless prevented from doing so by law or court order, or in rare, exigent circumstances, we make a reasonable effort to notify users of any legally compelled or required disclosure of their information. Gretel may disclose personal information or other information we collect about you to law enforcement if required in response to a valid subpoena, court order, search warrant, a similar government order, or when we believe in good faith that disclosure is necessary to comply with our legal obligations, to protect our property or rights, or those of third parties or the public at large.", "We will retain your personal information for as long as your account is active or as long as needed to provide you the Services after which time it shall be deleted, subject to our right to retain and use such personal information necessary to comply with our legal obligations, resolve disputes, establish legal defenses, conduct audits, pursue legitimate business purposes, and to enforce our agreements.", "We may disclose or transfer your personal information if we are involved in a merger, sale, or acquisition of corporate entities or business units, financing due diligence, reorganization, bankruptcy, receivership, purchase or sale of assets, or transition of service to another provider. If any such change of ownership happens, we will ensure that it is under terms that preserve the confidentiality of personal information, and we will notify you on our Services or by email before any transfer of your personal information.\u00a0", "We share certain aggregated, non-personally identifying information with others about how our users, collectively, use Gretel, or how our users respond to our other offerings and new features. For example, we may compile statistics on the API activity across Gretel.", "Please note that some unrevised information may remain in our records after revision of such information or deletion of your account, or in cached and archived pages. Some information may remain viewable elsewhere to the extent that it was copied or stored by other users. We may use any aggregated data derived from or incorporating your personal information after you delete your information for usage activity, but not in a manner that would identify you personally.", "Gretel personnel do not access private data projects unless required for security purposes, to assist the data project owner with a support matter, to maintain the integrity of the Services, to comply with our legal obligations, or as otherwise described in the Terms of Service.\u00a0", "This part of the Privacy Policy explains what cookies, pixel tags, and other tracking technologies\u00a0 (collectively, \u201c", "\u201d) are and how we use them, the types of Technologies we use, i.e., the information we automatically collect through your use of our Services and using these Technologies, how that information is used, and how to manage the cookie settings.", "\u2022 ", ". Cookies are small text files that are used to store small pieces of information. They are stored on your device when the website is loaded on your browser. These cookies help us make the Services function properly, make it more secure, provide better user experience, and understand how the Gretel Services perform and to analyze what works and where it needs improvement.", "\u2022 ", ". A pixel tag (also known as a web beacon) is a piece of code embedded in our Services that collects information about engagement on our Services. The use of a pixel tag allows us to record, for example, that a user has visited a particular web page or clicked on a particular advertisement. We may also include web beacons in e-mails to understand whether messages have been opened, acted on, or forwarded.", "Gretel uses cookies to make interactions with our service easy and meaningful. Like most online services, our Service uses first-party and third-party Technologies for several purposes. First-party Technologies are mostly necessary for the Services to function the right way, and they do not collect any of your personal information.\u00a0", "The third-party Technologies used on our Services are mainly for understanding how the Services performs, how you interact with our Services, keeping our Services secure, providing advertisements that are relevant to you, and all in all providing you with a better and improved user experience and help speed up your future interactions with our Services.", "Our uses of these Technologies fall into the following general categories:", "In addition to this, different browsers provide different methods to block and delete the placement of Technologies used by websites on your device. You can change the settings as your browser or device permits to block/delete the Technologies. Listed below are the links to the support documents on how to manage and delete Technologies from the major web browsers. However, if you adjust your preferences, our Services may not work properly. Please note that cookie-based opt-outs are not effective on mobile applications. However, you may opt out of personalized advertisements on some mobile applications by following the instructions for ", ", ", ", and ", ".", "Chrome: ", "Safari: ", "Firefox: ", "Internet Explorer: ", "If you are using any other web browser, please visit your browser\u2019s official support documents.\u00a0", "The online advertising industry also provides websites from which you may opt out of receiving targeted ads from data partners and other advertising partners that participate in self-regulatory programs. You can access these and learn more about targeted advertising and consumer choice and privacy by visiting the ", ", ", ", ", ", and ", ".", "Gretel takes all measures reasonably necessary to protect personal information from unauthorized access, alteration, or destruction; maintain data accuracy; and help ensure the appropriate use of personal information.", "Gretel enforces a written information security program. Our program:", "By using our Services or providing personal information to us, you agree that we may communicate with you electronically regarding security, privacy, and administrative issues relating to your use of our Services. If we learn of a security system\u2019s breach impacting your personal information, we may attempt to notify you if required by law electronically by posting a notice on our Services, by mail, or by sending an email to you.", "Transmission of data on Gretel is encrypted using HTTPS (TLS). Our service is hosted within Amazon Web Services utilizing a high level of physical and network security, and all data stored at rest is encrypted.", "No method of transmission, or method of electronic storage, is 100% secure. Therefore, we cannot guarantee its absolute security. To the fullest extent permitted by applicable law, we do not accept liability for unauthorized access, use, disclosure, or loss of personal information.", "Please note you must separately opt out in each browser and on each device.\u00a0", ".\u00a0 In accordance with applicable law, you may have the right to:", "If you would like to exercise any of these rights, please contact us at ", ". We will process such requests in accordance with applicable laws.", "All information processed by us may be transferred, processed, and stored anywhere in the world, including, but not limited to, the United States or other countries, which may have data protection laws that are different from the laws where you live. We endeavor to safeguard your information consistent with the requirements of applicable laws.", "If we transfer personal information that originates in the European Economic Area, Switzerland, and/or the United Kingdom to a country that has not been found to provide an adequate level of protection under applicable data protection laws, one of the safeguards we may use to support such transfer is the ", ".\u00a0", "For more information about the safeguards we use for international transfers of your personal information, please contact us at ", "This Supplemental California Privacy Notice only applies to our processing of Personal Information that is subject to the California Consumer Privacy Act of 2018, as updated by the California Privacy Rights Act (once effective on January 1, 2023), together with its implementing regulations (collectively, the \u201c", "\u201d). The CCPA provides California residents with the right to know what categories of personal information Gretel has collected about them and whether Gretel disclosed that personal information for a business purpose (e.g., to a service provider) in the preceding 12 months. California residents can find this information below:", "The categories of sources from which we collect Personal Information and our business and commercial purposes for using Personal Information are set forth in \u201c", "\u201d and \u201c", "\u201d above, respectively.", "In the preceding twelve months, Gretel has not \u201csold\u201d any personal information (as defined by the CCPA), nor does Gretel have actual knowledge of any \u201csale\u201d of Personal Information of minors under 16 years of age.\u00a0", "California residents have the right to opt out of the \u201csharing\u201d of their Personal Information for \u201ccross-context behavioral advertising\u201d (as such terms are defined in the CCPA). Gretel may share your personal information for \u201ccross-context behavioral advertising\u201d when it uses advertising partners that set Technologies on your device for \u201cinterest-based\u201d or \u201cpersonalized advertising.\u201d More details can be found above in ", "You may opt-out of Gretel\u2019s sharing of your Personal Information for cross-context behavioral advertising by selecting \u201cDo Not Share My Personal Information\u201d in the cookie policy banner and by turning off all advertising-based cookies.", " California residents have the right not to receive discriminatory treatment by us for the exercise of their rights conferred by the CCPA.\u00a0\u00a0\u00a0\u00a0\u00a0", " Only you, or someone legally authorized to act on your behalf, may make a verifiable consumer request related to your Personal Information. You may also make a verifiable consumer request on behalf of your minor child. To designate an authorized agent, please contact us as set forth in \u201cContacting Gretel\u201d below.", ". To protect your privacy, we will take steps to reasonably verify your identity before fulfilling your request. These steps may involve asking you to provide sufficient information that allows us to reasonably verify you are the person about whom we collected Personal Information or an authorized representative, or to answer questions regarding your Account and use of our Service.", "If you are a California resident and would like to exercise any of your rights under the CCPA, please contact us as set forth in \u201cContacting Gretel\u201d below. We will process such requests in accordance with applicable laws.\u00a0\u00a0\u00a0\u00a0\u00a0", " If we create or receive de-identified information, we will not attempt to reidentify such information, except to comply with applicable law.", ". This Privacy Policy uses industry-standard technologies and was developed in line with the World Wide Web Consortium\u2019s Web Content Accessibility Guidelines, version 2.1. If you wish to print this policy, please do so from your web browser or by saving the page as a PDF.", " The California \u201cShine the Light\u201d law permits users who are California residents to request and obtain from us once a year, free of charge, a list of the third parties to whom we have disclosed their Personal Information (if any) for their direct marketing purposes in the prior calendar year, as well as the type of Personal Information disclosed to those parties.\u00a0\u00a0\u00a0\u00a0\u00a0", "If you are a resident of Nevada, you have the right to opt-out of the sale of certain personal information to third parties who intend to license or sell that personal information. You can exercise this right by contacting us at ", " with the subject line \u201cNevada Do Not Sell Request\u201d and providing us with your name and the email address associated with your account. Please note that we do not currently sell your personal information as sales are defined in Nevada Revised Statutes Chapter 603A. If you have any questions, please contact us as set forth in \u201c", "\u201d below.", "If you have concerns about the way Gretel is handling your User Personal Information, please let us know immediately. We want to help. You may contact us at ", " with the subject line \u201cPrivacy Concerns.\u201d We will respond promptly.", "Although most changes are likely to be minor, Gretel may change our Privacy Policy from time to time. If there are any material changes to this Privacy Policy, we will notify you as required by applicable law. For changes to this Privacy Policy that are not material changes or that do not affect your rights, we encourage users to check our Services and this Privacy Policy frequently. You understand and agree that you will be deemed to have accepted the updated Privacy Policy if you continue to use our Services after the new Privacy Policy takes effect.", "Questions regarding Gretel\u2019s Privacy Policy or information practices should be directed to:\u00a0", "Gretel Labs, Inc.\u00a0", "Legal/Privacy Department", "8910 University Center Lane, Suite 400", "San Diego, CA 92122"]},
{"url": "https://gretel.ai/acceptable-use", "page_title": "Acceptable use", "headers": ["Terms of Use", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Effective\u00a0October 20, 2022", "Please read these Terms of Use (the \u201c", "\u201d) and our Privacy Policy (", ") carefully because they govern your use of the website located at ", " (the \u201c", "\u201d) operated by Gretel Labs, Inc. (\u201c", "\u201d). \u00a0\u00a0", ". By using our Site, you agree to be bound by these Terms. If you don\u2019t agree to be bound by these Terms, do not use the Site. Unless you and Gretel.ai have entered into a separate written agreement, these Terms constitute the entire and exclusive agreement between you and Gretel.ai regarding the Site. These Terms apply exclusively to your access to and use of the Site and do not alter the terms and conditions of any other agreement you may have with Gretel.ai, including the Gretel.ai Subscription Services Agreement (", "). In the event of a conflict between these Terms and any separate written agreement entered into between you and Gretel.ai, the terms of such separate written agreement will prevail.", ". Please review our Privacy Policy, which also governs your use of the Site, for information on how we collect, use and share your information.", ". We may update these Terms from time to time in our sole discretion. If we do, we\u2019ll let you know by posting the updated Terms on the Site. If you continue to use the Site after we have posted updated Terms it means that you accept and agree to the changes. If you don\u2019t agree to be bound by the changes, you may not use the Site anymore.\u00a0", ". We may make content and other materials available through the Site that are subject to intellectual property rights. We retain all rights to that such content and materials. We grant you permission to copy, distribute and download the content and materials on the Site solely for personal, noncommercial use only, provided that you do not modify or create derivative works based upon the content and materials and that you retain all copyright and other proprietary notices contained in the content and materials. You may not, however, distribute, copy, reproduce, display, republish, download or transmit any content or material on the Site for public or commercial use without prior written approval of Gretel.ai.", ". If you provide Gretel.ai with suggestions, comments, feedback or the like with regard to the Site, the content or other materials on it or any of Gretel.ai\u2019s services (collectively, \u201c", "\u201d), you hereby grant Gretel.ai a worldwide, perpetual, irrevocable, transferable, sublicensable, royalty-free and fully-paid up license to use and exploit all Feedback in connection with Gretel.ai\u2019s business purposes, including, without limitation, the testing, development, maintenance and improvement of the Site and other products, services and technology of Gretel.ai.", ". Gretel.ai and its licensors exclusively own all right, title and interest in and to the Site and all content and materials provided on it, including all associated intellectual property rights. You acknowledge that the Site and all content and materials provided on it are protected by copyright, trademark, and other laws of the United States and foreign countries. You agree not to remove, alter or obscure any copyright, trademark, service mark or other proprietary rights notices incorporated into or accompanying the Site or any content or materials provided on it.", ". You agree not to do any of the following:", " The Site may allow you to access third-party websites or other resources. We provide access only as a convenience and are not responsible for the content, products or services on or available from those resources or links displayed on such websites. You acknowledge sole responsibility for and assume all risk arising from, your use of any third-party resources.", " THE SITE AND ALL CONTENT AND MATERIALS PROVIDED ON IT ARE PROVIDED \u201cAS IS,\u201d WITHOUT WARRANTY OF ANY KIND. WITHOUT LIMITING THE FOREGOING, WE EXPLICITLY DISCLAIM ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT AND NON-INFRINGEMENT, AND ANY WARRANTIES ARISING OUT OF COURSE OF DEALING OR USAGE OF TRADE. We make no warranty that the Site or any content or materials provided on it will meet your requirements or be available on an uninterrupted, secure, or error-free basis. We make no warranty regarding the quality, accuracy, timeliness, truthfulness, completeness or reliability of any content or materials on the Site.", " You will indemnify and hold Gretel.ai and its officers, directors, employees and agents, harmless from and against any claims, disputes, demands, liabilities, damages, losses, and costs and expenses, including, without limitation, reasonable legal and accounting fees arising out of or in any way connected with (a) your access to or use of the Site, or (b) your violation of these Terms.", "TO THE MAXIMUM EXTENT PERMITTED BY LAW, NEITHER GRETEL.AI NOR ITS SERVICE PROVIDERS INVOLVED IN CREATING, PRODUCING, OR DELIVERING THE SITE WILL BE LIABLE FOR ANY INCIDENTAL, SPECIAL, EXEMPLARY OR CONSEQUENTIAL DAMAGES, OR DAMAGES FOR LOST PROFITS, LOST REVENUES, LOST SAVINGS, LOST BUSINESS OPPORTUNITY, LOSS OF DATA OR GOODWILL, SERVICE INTERRUPTION, COMPUTER DAMAGE OR SYSTEM FAILURE OR THE COST OF SUBSTITUTE SERVICES OF ANY KIND ARISING OUT OF OR IN CONNECTION WITH THESE TERMS OR FROM THE USE OF OR INABILITY TO USE THE SITE OR ANY CONTENT OR OTHER MATERIALS ON IT, WHETHER BASED ON WARRANTY, CONTRACT, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER LEGAL THEORY, AND WHETHER OR NOT GRETEL.AI OR ITS SERVICE PROVIDERS HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGE, EVEN IF A LIMITED REMEDY SET FORTH HEREIN IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. TO THE MAXIMUM EXTENT PERMITTED BY LAW, WILL GRETEL.AI HAVE NO LIABILITY ARISING OUT OF OR IN CONNECTION WITH THE USE OF OR INABILITY TO USE THE SITE OR ANY CONTENT OR OTHER MATERIALS PROVIDED ON IT, AND IN NO EVENT WILL ITS LIABILITY EXCEED ONE HUNDRED DOLLARS ($100).\u00a0", " These Terms and any action related thereto will be governed by the laws of the State of California, without regard to its conflict of laws provisions. The parties expressly consent to personal and exclusive jurisdiction in the state and federal courts located in the City and County of San Francisco, California, and you and Gretel.ai each waive any objection to jurisdiction and venue in such courts.", " If any provision of these Terms is held invalid or unenforceable by an arbitrator or a court of competent jurisdiction, that provision will be enforced to the maximum extent permissible and the other provisions of these Terms will remain in full force and effect. Gretel.ai\u2019s failure to enforce any right or provision of these Terms will not be considered a waiver of such right or provision. The waiver of any such right or provision will be effective only if in writing and signed by a duly authorized representative of Gretel.ai. The exercise by Gretel.ai of any of its remedies under these Terms will be without prejudice to its other remedies under these Terms or otherwise.", "If you have any questions about these Terms, please contact Gretel.ai at ", "."]},
{"url": "https://gretel.ai/terms", "page_title": "Terms of Service - Gretel.ai", "headers": ["Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Effective October 20, 2022", "PLEASE READ THE FOLLOWING CAREFULLY AS IT AFFECTS YOUR LEGAL RIGHTS. THIS SUBSCRIPTION SERVICES AGREEMENT (\u201c", "\u201d, ALSO REFERRED TO ELSEWHERE AS OUR \u201c", "\u201d) GOVERNS YOUR USE OF THE SUBSCRIPTION SERVICES (AS DEFINED BELOW) PROVIDED BY GRETEL LABS, INC. (\u201c", "\u201d), INCLUDING BUT NOT LIMITED TO, ANY FREE, TRIAL, BETA OR PAID SUBSCRIPTION SERVICES OR SOFTWARE.", "The terms \u201cyou,\u201d \u201cyour,\u201d and \u201cCustomer\u201d are used interchangeably throughout the Agreement and have the same meaning. You represent to us that you are lawfully able to enter into contracts (e.g., you are not a minor). If you are entering into this Agreement on behalf of a company (such as\u00a0 your employer) or another legal entity (an \u201c", "\u201d), you are agreeing to this Agreement for that Entity and representing to Gretel.ai that you have the authority to bind such Entity to this Agreement, in which case the terms \u201cyou,\u201d \u201cyour,\u201d \"Customer\", or a related term herein will refer to such Entity. If you do not have such authority, or if you do not agree with this Agreement, you must not accept this Agreement and must not access or use the Subscription Services.", "\u201c", "\u201d means any person that you allow to access and use the Subscription Services, and may include your employees, contractors, service providers, and other third parties that use the Subscription Services in connection with your own business operations.", "\u201c", "\u201d means all information, data, content and other materials, in any form or medium, that is submitted, posted, collected, transmitted or otherwise provided by you or on your behalf through the Subscription Services or to Gretel.ai in connection with your use of the Subscription Services, but excluding, for clarity, Service Data and any other information, data, content or other materials owned or controlled by Gretel.ai and made available through or in connection with the Subscription Services.\u00a0", "\u201d means product and user documentation, training materials, specifications, minimum system configuration requirements, acceptable use policies, API and capacity limitations, compatible device and hardware list and other similar materials in hard copy or electronic form if and as provided by Gretel.ai to you (including any revised versions thereof) relating to the use of the Subscription Services, which may be updated from time to time.", "\u201c", "\u201d means the Subscription Services, the underlying software provided in conjunction with the Subscription Services, algorithms, interfaces, technology, databases, tools, know-how, processes and methods used to provide or deliver the Subscription Services, including Gretel.ai Software and APIs, all improvements, modifications or enhancements to, or derivative works of, the foregoing (regardless of inventorship or authorship).\u00a0", "\u201c", "\u201d means any individual, corporation, partnership, trust, limited liability company, association, governmental authority or other entity.", "\u201c", "\u201d means all data, models, observations, reports, analyses, statistics, databases and other information created, compiled, analyzed, generated or derived by Gretel.ai from server, network or traffic data generated by Gretel.ai in the course of providing the Subscription Services.", "\u201c", "\u201d means the Subscription Services plan you have ordered, including the terms, pricing, limits, volume or other measurement or conditions of permitted use by you and Authorized Users for the Subscription Service, and any applicable Gretel.ai Software or APIs, as set forth on Gretel.ai\u2019s pricing page (", ") or otherwise agreed or referenced in the applicable Order.", ".", "Conditioned on your compliance with this Agreement (including, without limitation, all payment obligations), Gretel.ai grants you a limited, non-exclusive, non-transferable and non-sublicensable right to access and use the Subscription Services specified in your Order in accordance with its Documentation during the applicable Subscription Term, solely for your internal business purposes and subject to the limits and conditions corresponding to your Subscription Tier. During the Subscription Term, Gretel.ai will use commercially reasonable efforts to provide you with basic customer support for the applicable Subscription Services through its standard support channels during its normal business hours as described on the Gretel.ai website (", "). Gretel only offers support via email, in-service communications and electronic messages and does not offer telephone support.", ". In connection with your use of the Subscription Services, Gretel.ai may provide certain Gretel.ai Software, as may be stated or referenced in an applicable Order, that is intended to or may optionally be installed and executed in your computing environment. Conditioned on your compliance with this Agreement, Gretel.ai grants you a limited, non-exclusive, non-transferable and non-sublicensable license to install and execute such Gretel.ai Software specified in your Order in accordance with its Documentation in a computing environment owned or controlled by you during the specified Subscription Term, solely for your internal business purposes and solely in connection with your use of the applicable Subscription Service and subject to the limits and conditions corresponding to your Subscription Tier. You will reproduce all of Gretel.ai\u2019s and its licensors\u2019 copyright notices and any other proprietary rights notices contained in the Gretel.ai Software (and not remove or alter any of the foregoing) in all copies that you make.", "Software. Gretel.ai may package or otherwise make the Subscription Services or associated Gretel.ai Software available with software or other material that is distributed as \u201cfree software,\u201d \u201copen source software\u201d or under similar licensing or distribution models, including the GNU General Public License, GNU Lesser General Public License, Mozilla Public License, BSD licenses, the MIT license, the Apache License and other such licenses, whether or not recognized or approved by the Free Software Foundation or the Open Source Initiative (\u201c", "\u201d), as noted in the Documentation. All such Open Source Software is licensed to you exclusively under the terms of their applicable licenses, including all warranty disclaimers and limitations on liability therein, and the restrictions set forth in this Agreement do not apply to such Open Source Software.", "Notwithstanding the foregoing, the licenses granted to you in this Section 3 does not include the right to, and you will not, and you will not permit any other individual or entity to, modify, combine, integrate or otherwise use the Subscription Services or any associated Gretel.ai Technology with Open Source Software or any other software or materials in such a manner that requires, or could require, the Subscription Services or any associated Gretel.ai Technology, in whole or part, to be (a) disclosed or distributed to third parties in source code form, (b) licensed to third parties for the purpose of making derivative works, or (c) redistributable by third parties at no charge, including under the terms of the GNU General Public License version 3, the GNU Affero General Public License version 3, the GNU Lesser General Public License version 3, or any prior or successor versions or equivalents of the foregoing.", ".", "Unless otherwise expressly permitted in writing by Gretel.ai, you will not at any time and will not permit any Person (including, without limitation, Authorized Users) to, directly or indirectly: (a) use the Gretel.ai Technology in any manner beyond the scope of rights expressly granted in this Agreement or Order, including any limits and conditions corresponding to your Subscription Tier; (b) reproduce, modify or create derivative works of the Gretel.ai Technology or Documentation, in whole or in part; (c) reverse engineer, disassemble, decompile, decode or otherwise attempt to derive or gain improper access to the Gretel.ai Technology, in whole or in part; (d) frame, mirror, sell, resell, distribute, sublicense or otherwise transfer or make available the Subscription Services or any associated Gretel.ai Technology or Documentation to any other Person, including by making any of the foregoing or their functionality available to any Person on a service bureau, time sharing, hosting or other computer services basis, including by providing a software-as-a-service, platform-as-a-service, infrastructure-as-a-service or other similar online service (a \u201c", "\u201d); (e) use the Subscription Services or any associated Gretel.ai Technology as part of a product or service, or to perform a service, that is offered to third parties, including by providing a XaaS Service; (f) use the Gretel.ai Technology or Documentation in any manner or for any purpose that infringes, misappropriates, or otherwise violates any intellectual property right or other right of any Person, or that violates any applicable law; (g) interfere with, or disrupt the integrity or performance of, the Gretel.ai Technology, or any data or content contained therein or transmitted thereby; (h) access or search the Gretel.ai Technology (or download any data or content contained therein or transmitted thereby) through the use of any engine, software, tool, agent, device or mechanism (including spiders, robots, crawlers or any other similar data mining tools) other than by using features of the Gretel.ai Technology expressly provided by Gretel.ai for such purposes; or (i) use the Gretel.ai Technology, Documentation or any other Gretel.ai Confidential Information for benchmarking or competitive analysis with respect to competitive or related products or services, or to develop, commercialize, license or sell any product, service or technology that could, directly or indirectly, compete with the Subscription Services.", ".", "You will not allow any Person other than Authorized Users to access or use the Subscription Services.\u00a0 You may permit Authorized Users to access and use the Subscription Services as authorized pursuant to this Agreement and applicable Orders, ", " that you ensure each Authorized User complies with all applicable terms and conditions of this Agreement. You are responsible for the acts or omissions of Authorized Users in connection with their access to and use of the Subscription Services.\u00a0 You will, and will require all Authorized Users to, use all reasonable means to secure user names, passwords and other access credentials (such as API tokens and OAuth credentials) created by or assigned to you (\u201c", "\u201d), hardware and software used to access the Subscription Services in accordance with customary security protocols, and will promptly notify Gretel.ai if you know or reasonably suspect that any Credentials have been compromised. You are solely responsible for maintaining the confidentiality of all Credentials and for all activities that occur with such Credentials.", ".\u00a0 Certain features and functionalities within the Subscription Services may allow you and your Authorized Users to interface or interact with, access and/or use compatible third-party services, products, technology and content (collectively, \u201c", "\u201d) through the Subscription Services.\u00a0 Gretel.ai does not provide any aspect of the Third-Party Services and is not responsible for any compatibility issues, errors or bugs in the Subscription Services or Third-Party Services caused in whole or in part by the Third-Party Services or any update or upgrade thereto. You are solely responsible for maintaining the Third-Party Services and obtaining any associated licenses and consents necessary for you to use the Third-Party Services in connection with the Subscription Services.", ". The Subscription Services, associated Gretel.ai Technology and Documentation were developed solely at private expense and are \u201ccommercial products\u201d, \u201ccommercial items\u201d, or \u201ccommercial computer software\u201d as defined in the Federal Acquisition Regulation 2.101 and other relevant government procurement regulations including agency supplements. Any use, duplication, or disclosure by or on behalf of the U.S. government is subject to restrictions as set forth in this Agreement as consistent with federal law and regulations. If these terms fail to meet the U.S. Government\u2019s needs or are inconsistent in any respect with federal law, you will immediately discontinue your use thereof.", ". You affirm that you are not named on, owned by, or acting on behalf of any U.S. government denied-party list and agree to comply fully with all relevant export control and sanctions laws and regulations of the United States and any other applicable jurisdictions ", "\u201c", "\u201d) to ensure that none of the Gretel.ai Technology, Documentation or Customer Data, nor any technical data related thereto is: (a)\u00a0used, exported or re-exported directly or indirectly in violation of Export Laws; or (b)\u00a0used for any purposes prohibited by the Export Laws, including, but not limited to, nuclear, chemical, or biological weapons proliferation, missile systems or technology, or restricted unmanned aerial vehicle applications. You will complete all undertakings required by Export Laws, including obtaining any necessary export license or other governmental approval.", ". Subject to the limited rights expressly granted hereunder, Gretel.ai reserves all rights, title and interest in and to the Gretel.ai Technology and Documentation, including all intellectual property rights therein. No rights are granted to you hereunder (whether by implication, estoppel, exhaustion or otherwise) other than as expressly set forth herein.\u00a0", ".\u00a0 From time to time you or your Authorized Users may provide Gretel.ai with suggestions, comments, feedback or the like with regard to the Subscription Services (collectively, \u201c", "\u201d).\u00a0 You hereby grant Gretel.ai a worldwide, perpetual, irrevocable, transferable, sublicensable, royalty-free and fully-paid up license to use and exploit all Feedback in connection with Gretel.ai\u2019s business purposes, including, without limitation, the testing, development, maintenance and improvement of the Subscription Services and other Gretel.ai Technology.", "Gretel.ai may offer free or trial versions of the Subscription Services (\u201c", "\u201d) from time to time. With respect to each such Free Service, Gretel.ai will make each such Free Service available to you free of charge as set forth in each applicable Order until the earlier of (a) the end of the Subscription Term for the Free Service (if applicable); (b) the start date of your Subscription Term for the Paid Services version of such Free Service; or (c) termination of the Free Service by Gretel.ai in its sole discretion.\u00a0Notwithstanding\u00a0", ", Gretel.ai will have no liability for any harm or damage arising out of or in connection with any Free Services.", ".", "From time to time, Gretel.ai may make non-production versions of the Subscription Services that are under development (\u201c", "\u201d) available to you under applicable Orders. You may access these Beta Services at your sole discretion. Beta Services are intended for testing purposes only, and may be subject to additional terms that will be presented to you at the time of sign-up. Gretel.ai is not obligated to provide you with support for the Beta Services or correct any bugs, defects, or errors in the Beta Services. Unless otherwise stated, any Subscription Term for Beta Services will terminate upon the earlier of 90 days from the testing start date or the date that a version of the Beta Services becomes generally available without the applicable Beta Services designation. Gretel.ai may discontinue, suspend, or remove Beta Services (including any Customer Data stored as part of the Beta Services) or your access thereto at any time in its sole discretion and may never make them generally available. In the event that a version of a Beta Service becomes generally available without the applicable Beta Service designation, you may be permitted to continue using the generally available Free Services or Paid Services, subject to additional Orders and terms as provided in the Agreement. You understand that any information you obtain regarding Beta Services is Gretel.ai\u2019s Confidential Information, and you agree not to disclose such information except as provided herein, and to only use such information in connection with your use of the Beta Services. Notwithstanding\u00a0", ", Gretel.ai will have no liability for any harm or damage arising out of or in connection with any Beta Services.", ".\u00a0", ". Subscription Services are provided to you on a subscription basis for the length of time specified for your Subscription Tier or otherwise stated in an applicable Order (\u201c", "\u201d). Unless otherwise specified, all of your subscriptions for Subscription Services will automatically renew for periods equal to your initial Subscription Term, and you will be charged at Gretel.ai\u2019s then-current rates unless you cancel your subscription for the Subscription Services through the Gretel.ai account dashboard prior to your next scheduled billing date.", ".", "In order to access those Subscription Services for which Gretel.ai requires a fee (\u201c", "\u201d), you will be required to provide Gretel.ai with your credit card information or other approved method of payment (\u201c", "\u201d). By providing a Payment Method, you are authorizing Gretel.ai to charge your Payment Method on a monthly, annual, or pay-as-you-go basis, or as otherwise applicable for the fees associated with the Paid Services that you sign up for or consume in accordance with each applicable Order. Gretel.ai will begin billing your Payment Method for the Paid Services on the day that you sign up for such Paid Services, regardless of whether you have fully configured the Paid Services as of that date.", "Some Subscription Services or features are billed based on your usage. A limited level or quantity of these Subscription Service or features may be included in your Subscription Tier for a limited term without additional charge. If you choose to purchase such Paid Services beyond the quantity included in your Subscription Tier, you will pay based on your actual usage in the preceding month. Monthly payment for these purchases will be charged on a periodic basis in arrears.", ". Any Payment Method that you provide Gretel.ai must be valid and kept current by you during the Subscription Term. By providing Gretel.ai with a Payment Method, you represent and warrant that you are authorized to use such Payment Method. If Gretel.ai cannot charge your selected Payment Method for any reason (such as expiration or insufficient funds), you remain responsible for any uncollected amounts, and Gretel.ai will attempt to charge the payment method again as you may update your Payment Method information. In accordance with local law, Gretel.ai may update information regarding your selected Payment Method if provided such information by your financial institution. If you fail to make any payment when due, late charges will accrue at the rate of 1.5% per month or, if lower, the highest rate permitted by applicable law and Gretel.ai may suspend the Subscription Services until all payments are made in full. You will reimburse Gretel.ai for all reasonable costs and expenses incurred (including reasonable attorneys\u2019 fees) in collecting any late payments or interest.", ".", "Gretel.ai reserves the right to change the fees it charges for the Subscription Services, at any time in its sole discretion, provided that Gretel.ai gives you at least 30 days\u2019 prior notice of such changes. Unless otherwise specified in such notice to you, any changes to such fees will take effect in the billing period immediately following Gretel.ai\u2019s notice to you.", ".", "FEES ARE NONREFUNDABLE. YOU WILL BE BILLED IN FULL FOR THE SUBSCRIPTION TERM IN WHICH YOU CANCEL AND NO REFUNDS WILL BE PROVIDED FOR THE UNUSED PORTION OF SUCH SUBSCRIPTION TERM. Gretel.ai may, in its sole discretion, provide a refund, discount, or credit (\u201c", "\u201d) to you in a specific instance, however the provision of Credits in a specific instance does not entitle you to Credits in the future for similar instances or obligate Gretel.ai to provide additional Credits.", ".", "Unless otherwise stated, Gretel.ai\u2019s charges do not include any taxes, levies, duties or similar governmental assessments, including value-added, sales, use or withholding taxes assessable by any local, state, provincial or foreign jurisdiction (collectively \u201c", "\u201d). You are responsible for paying Taxes, except those assessable against Gretel.ai as measured by its net income. Unless you provide evidence of an exemption, Gretel.ai will invoice and charge you for such Taxes if Gretel.ai believes it has a legal obligation to do so and you agree to pay such Taxes if so invoiced.", ".", ". Gretel.ai reserves the right to make modifications to this Agreement at any time in its sole discretion by posting a revised version on the Gretel.ai website or by otherwise notifying you in accordance with this Agreement. Gretel.ai will use commercially reasonable efforts to provide at least 30\u00a0days\u2019 advance notice, but in all events the modified terms will become effective upon the date specified in the posting or, if Gretel.ai notifies you by email, as stated in the email message. By continuing to use the Subscription Services after the effective date of any modifications to this Agreement, you agree to be bound by the modified terms. It is your responsibility to check the Gretel.ai website and your email regularly for modifications to this Agreement and to keep your email address current.\u00a0", ". Gretel.ai may change or discontinue any of the Subscription Services or their functionality, including any associated Gretel.ai Technology and Documentation, at any time in its sole discretion. Gretel.ai will use commercially reasonable efforts to provide at least 30\u00a0days\u2019 notice prior to materially degrading or discontinuing material functionality of, or entirely discontinuing, a Paid Service you are using.", ". Gretel.ai may suspend your or any Authorized User\u2019s right to access or use any portion or all of the Subscription Services immediately upon notice to you if Gretel.ai determines: (a) your or an Authorized User\u2019s use of the Subscription Services (i) poses a security risk to the Subscription Services or any third party, (ii) could adversely impact Gretel.ai\u2019s systems, the Subscription Services or the systems of any other Gretel.ai customers, or (iii) could subject Gretel.ai, its affiliates, or any third party to liability; (b) you are in breach of your payment obligations under Section 4; or (c) you have ceased to operate in the ordinary course, made an assignment for the benefit of creditors or similar disposition of your assets, or become the subject of any bankruptcy, reorganization, liquidation, dissolution or similar proceeding. If Gretel.ai suspends your right to access or use any portion or all of the Subscription Services: (a) you remain responsible for all fees you have incurred during or prior to the period of suspension; and (b) you will not be entitled to any refund or credit for any period of suspension.\u00a0", ".", ". You may cancel the Subscription Term for any or all Subscription Services at any time through the Gretel.ai account dashboard, effective at the end of their then-current Subscription Terms, and further terminate this Agreement upon the termination of all Subscription Terms by closing your account through the Gretel.ai account dashboard. Gretel.ai may terminate the Subscription Terms for any or all Subscription Services, and as applicable this Agreement and your account, for any reason by providing you at least 30 days\u2019 advance notice, effective (a) at the end of their then-current Subscription Terms in which the notice is given, or (b) at the end of the next renewal term if your Subscription Term renews prior to the end of such 30 day period.", ". Either party may terminate this Agreement, and the Subscription Terms for all Subscription Services, for cause if the other party is in material breach of this Agreement and the material breach remains uncured for a period of 30 days from receipt of notice by the other party. Gretel.ai may also terminate the Subscription Terms for any or all Subscription Services, and as applicable this Agreement and your account, immediately upon notice to you: (a) if Gretel.ai has the right to suspend your access to and use of any Subscription Services under Section 6; (b) if Gretel.ai\u2019s relationship with a third-party partner who provides software or other technology used to provide a Subscription Service expires, terminates or requires Gretel.ai to change the way Gretel.ai provide the software or other technology as part of the Services, or (c) in order to comply with applicable law or requests of governmental entities.", ". Upon termination of a Subscription Term, or as applicable this Agreement and your account: (a) the rights granted to you pursuant to Section 3 and applicable Orders will terminate and you will cease all use of applicable Gretel.ai Technology and Documentation, (b) you will immediately destroy or delete, or if instructed by Gretel.ai return, all applicable Gretel.ai Technology, Documentation and Confidential Information of Gretel.ai in your possession or control, including permanent removal (consistent with customary industry practice for data destruction) from any storage devices or other hosting environments. No termination will affect your obligation to pay all fees that may have become due or otherwise accrued through the effective date of termination, or entitle you to any refund or credit. Gretel.ai will allow you to retrieve Customer Data for the applicable Subscription Services for a period of 30 days following the termination date only if you have paid all amounts due under this Agreement. Sections 2, 3.9, 3.10, 4, 7.3, 8, 9, 11, 13 and 14, will survive termination of this Agreement.", ".", "As used herein, \u201c", "\u201d means any information that one party (the \u201c", "\u201d)", "provides to the other party (the \u201c", "\u201d) in connection with this Agreement, whether orally or in writing, that is designated as confidential or that reasonably should be considered to be confidential given the nature of the information and/or the circumstances of disclosure.", "For clarity, the Gretel.ai Technology and Documentation will be deemed Confidential Information of Gretel.ai. However, Confidential Information will not include any information or materials that: (a) were, at the date of disclosure, or have subsequently become, generally known or available to the public through no act or failure to act by the Receiving Party; (b) were rightfully known by the Receiving Party prior to receiving such information or materials from the Disclosing Party without a duty of non-disclosure; (c) are rightfully acquired by the Receiving Party from a third party who has the right to disclose such information or materials without breach of any confidentiality or non-use obligation to the Disclosing Party; or (d) are independently developed by or for the Receiving Party without use of or access to any Confidential Information of the Disclosing Party. To the extent the terms and conditions of this Agreement or any Order constitute Confidential Information of either or both parties, each may be disclosed on a confidential basis to the parties\u2019 advisors, attorneys, actual or bona fide potential acquirers, investors or other sources of funding (and their respective advisors and attorneys) for due diligence purposes.", "The Receiving Party will maintain the Disclosing Party\u2019s Confidential Information in strict confidence, and will not use the Confidential Information of the Disclosing Party except as necessary to perform its obligations or exercise its rights under this Agreement. The Receiving Party will not disclose or cause to be disclosed any Confidential Information of the Disclosing Party, except (a) to those employees, representatives, or contractors of the Receiving Party who have a bona fide need to know such Confidential Information to perform under this Agreement and who are bound by written agreements with use and nondisclosure restrictions at least as protective as those set forth in this Agreement, or (b) as such disclosure may be required by the order or requirement of a court, administrative agency or other governmental body, subject to the Receiving Party providing to the Disclosing Party reasonable written notice to allow the Disclosing Party to seek a protective order or otherwise contest the disclosure.\u00a0", "Each Party\u2019s obligations of non-disclosure with regard to Confidential Information will expire five years from the date first", "disclosed to the", "Receiving Party; ", ", however, with respect to any Confidential Information that constitutes a trade secret (as determined under applicable law), such obligations of non-disclosure will survive the termination or expiration of this Agreement for as long as such Confidential Information remains subject to trade secret protection under applicable law.", ".", "As between you and Gretel.ai and you, you will retain all right, title and interest in and to your Customer Data, including all intellectual property rights therein. Subject to the terms of this Agreement, you hereby grant Gretel.ai a non-exclusive, worldwide, royalty-free right and license to access, use, store, reproduce, display, perform, modify and otherwise process the Customer Data solely for the purpose of hosting, operating, improving and otherwise providing the Subscription Services and appliable Gretel.ai Technology during the appliable Subscription Term.", "You represent and warrant that (a) you have obtained and will obtain and continue to have, during all applicable Subscription Terms, all necessary rights, authority and licenses for the access, use and other processing of the Customer Data as contemplated by this Agreement, including any Personal Data (defined below) and (b) Gretel.ai\u2019s access, use and other processing of the Customer Data in accordance with this Agreement will not violate any applicable laws or regulations or cause a breach of any agreement or obligations between you and any third party.", "Gretel.ai retains all right, title, and interest in all Service Data, and will have the right to use Service Data for the purposes of providing, maintaining, developing, and improving the Subscription Services and appliable Gretel.ai Technology. Gretel.ai may monitor and inspect the traffic on the Gretel.ai network, including any related logs, as necessary to perform the Subscription Services. To the extent the Service Data includes any Personal Data, Gretel.ai will handle such Personal Data in compliance with applicable data protection laws.", ".", ".", "If Customer Data includes the personal data of European data subjects as those terms are defined by EU and UK Data Protection Laws (collectively, \u201c", "\u201d), then Gretel.ai is a data processor or sub-processor, as applicable, and Gretel.ai will handle such Personal Data in compliance with Gretel.ai\u2019s Data Processing Addendum (\u201c", "\u201d), as may be updated from time to time, which is hereby incorporated by reference into this Agreement (", "). \u201cEU and UK Data Protection Laws\u201d means all Laws and regulations of the European Union, the European Economic Area, their member states, Switzerland, and the United Kingdom, applicable to the processing of Personal Data including (where applicable), the Swiss Federal Act on Data Protection, the UK Data Protection Act and the General Data Protection Regulation (Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of Personal Data and on the free movement of such data).", ".", "Gretel.ai implements security procedures to help protect Customer Data from security threats. However, you understand that your use of the Subscription Services necessarily involves transmission of Customer Data over networks that are not owned, operated or controlled by it, and Gretel.ai is not responsible for any of Customer Data that is lost, altered, intercepted or stored across such networks. Gretel.ai cannot guarantee that its security procedures will be error-free, that transmissions of Customer Data will always be secure or that unauthorized third parties will never be able to defeat Gretel.ai\u2019s security measures or those of its third party service providers.", ".", ".\u00a0 Subject to the exclusions set forth herein, Gretel.ai will defend you against any claim, suit or proceeding brought by a third party (\u201c", "\u201d) alleging that your Use of the Paid Services infringes or misappropriates such third party\u2019s intellectual property rights, and will indemnify and hold you harmless against any damages and costs awarded against you or agreed in settlement by Gretel.ai (including reasonable attorneys\u2019 fees) resulting from such Claim.\u00a0", ". Gretel.ai\u2019s obligations under Section 11.1 will not apply if the underlying third-party Claim arises from or as a result of: (a) your breach of this Agreement, negligence, willful misconduct or fraud; (b) any Customer Data; (c) your failure to use any enhancements, modifications, or updates to the Subscription Services that have been provided by Gretel.ai; (d) modifications to the Subscription Services by anyone other than Gretel.ai; or (e) combinations of the Subscription Services with hardware, software, data or other materials not provided by Gretel.ai.\u00a0", ". If Gretel.ai reasonably believes the Paid Services (or any associated component of Gretel.ai Technology) could infringe any third party\u2019s intellectual property rights, Gretel.ai may, at its sole option and expense use commercially reasonable efforts to: (a) modify or replace the Paid Services, or any component or part thereof, to make it non-infringing; or (b) procure the right for you to continue their use.\u00a0 If Gretel.ai determines that neither alternative is commercially practicable, Gretel.ai may terminate this Agreement, in its entirety or with respect to the affected Paid Services, by providing written notice to you.\u00a0 In the event of any such termination, Gretel.ai will refund to you a pro-rata portion of the fees that you paid for the unexpired portion of the Subscription Term. The rights and remedies set forth in this Section 11 will constitute your sole and exclusive remedy for any infringement or misappropriation of intellectual property rights in connection with the Paid Services.\u00a0", ". You will defend Gretel.ai against Claims arising from: (a) the use of any Customer Data as contemplated in this Agreement, including, without limitation, any Claim that the any Claim that the use, provision, transmission, display, storage or other processing of Customer Data (i) infringes, misappropriates or otherwise violates any third party\u2019s intellectual property rights or privacy or other rights or (ii) violates any applicable law, rule or regulation; or (b) use of the Subscription Services, or any associated Gretel.ai Technology, by you or your Authorized Users in a manner that is not in accordance with this Agreement, applicable Orders or the Documentation, including, without limitation, any breach of the limitations, restrictions and obligations in Section\u00a03; and in each case, you will indemnify and hold harmless Gretel.ai against any damages and costs awarded against Gretel.ai or agreed in settlement (including reasonable attorneys\u2019 fees) resulting from such Claim.", ". The party seeking defense and indemnity (the \u201c", "\u201d) will promptly notify the other party (the \u201c", "\u201d) of the Claim for which indemnity is being sought, and will reasonably cooperate with the Indemnifying Party in the defense and/or settlement thereof. The Indemnifying Party will have the sole right to conduct the defense of any Claim for which the Indemnifying Party is responsible hereunder (", " that the Indemnifying Party may not settle any Claim without the Indemnified Party\u2019s prior written approval unless the settlement is for a monetary amount to be paid by the Indemnifying Party, unconditionally releases the Indemnified Party from all liability without prejudice, does not require any admission by the Indemnified Party, and does not place restrictions upon the Indemnified Party\u2019s business, products or services). The Indemnified Party may participate in the defense or settlement of any such Claim at its own expense and with its own choice of counsel or, if the Indemnifying Party refuses to fulfill its obligation of defense, the Indemnified Party may defend itself and seek reimbursement from the Indemnifying Party. Notwithstanding the foregoing, Gretel.ai reserves the right, at its own expense, to assume the exclusive defense and control of any Claim otherwise subject to defense and indemnification by you (and without limiting your obligations with respect to such Claim), and in such case, you agree to cooperate with Gretel.ai\u2019s defense of such Claim at your own expense.", ". THE SUBSCRIPTION SERVICES, INCLUDING ALL ASSOCIATED GRETEL.AI TECHNOLOGY AND DOCUMENTATION, ARE PROVIDED \u201cAS IS,\u201d WITHOUT WARRANTY OF ANY KIND. WITHOUT LIMITING THE FOREGOING, GRETEL.AI EXPLICITLY DISCLAIMS ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT AND NON-INFRINGEMENT, AND ANY WARRANTIES ARISING OUT OF COURSE OF DEALING OR USAGE OF TRADE. Gretel.ai makes no warranty that the Subscription Services will meet your requirements or be available on an uninterrupted, secure, or error-free basis or as to the quality, accuracy, timeliness, completeness or reliability of any information, content or results derived from the Subscription Services.", ". TO THE MAXIMUM EXTENT PERMITTED BY LAW, GRETEL.AI WILL NOT BE LIABLE FOR ANY INCIDENTAL, SPECIAL, EXEMPLARY OR CONSEQUENTIAL DAMAGES, OR DAMAGES FOR LOST PROFITS, LOST REVENUES, LOST SAVINGS, LOST BUSINESS OPPORTUNITY, LOSS OF DATA OR GOODWILL, SERVICE INTERRUPTION, COMPUTER DAMAGE OR SYSTEM FAILURE OR THE COST OF SUBSTITUTE SERVICES OF ANY KIND ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT, INCLUDING FROM THE USE OF OR INABILITY TO USE THE SUBSCRIPTION SERVICES OR ANY ASSOCIATED GRETEL.AI TECHNOLOGY OR DOCUMENTATION, WHETHER BASED ON WARRANTY, CONTRACT, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER LEGAL THEORY, AND WHETHER OR NOT GRETEL.AI HAS BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGE, EVEN IF A LIMITED REMEDY SET FORTH HEREIN IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE.\u00a0", "TO THE MAXIMUM EXTENT PERMITTED BY LAW, GRETEL.AI\u2019S TOTAL LIABILITY ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT, INCLUDING FROM THE USE OF OR INABILITY TO USE THE SUBSCRIPTION SERVICES OR ANY ASSOCIATED GRETEL.AI TECHNOLOGY OR DOCUMENTATION, WILL NOT EXCEED THE AMOUNTS YOU HAVE PAID TO GRETEL.AI FOR THE AFFECTED SUBSCRIPTION TERM. IF YOU HAVE NOT HAD ANY PAYMENT OBLIGATIONS TO GRETEL.AI, INCLUDING FOR FREE SERVICES OR BETA SERVICES, IN NO EVENT WILL GRETEL.AI\u2019S TOTAL LIABILITY EXCEED ONE HUNDRED DOLLARS ($100).", "THE EXCLUSIONS AND LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN GRETEL.AI AND YOU.", ". This Agreement incorporates Gretel.ai\u2019s Privacy Policy (", ") by reference and constitutes the entire and exclusive understanding and agreement between you and Gretel.ai regarding your use of and access to the Service. You may not assign or transfer this Agreement or your rights hereunder, in whole or in part, by operation of law or otherwise, without Gretel.ai\u2019s prior written consent. Gretel.ai may assign this Agreement at any time without notice. The failure to require performance of any provision will not affect Gretel.ai\u2019s right to require performance at any time thereafter, nor will a waiver of any breach or default of this Agreement or any provision of this Agreement constitute a waiver of any subsequent breach or default or a waiver of the provision itself. Use of section headers in this Agreement is for convenience only and will not have any impact on the interpretation of particular provisions. In the event that any part of this Agreement is held to be invalid or unenforceable, the unenforceable part will be given effect to the greatest extent possible and the remaining parts will remain in full force and effect.", ". This Agreement will be governed by the laws of the State of California without regard to conflict of law principles. To the extent that any lawsuit or court proceeding is permitted hereunder, you and Gretel.ai agree to submit to the personal and exclusive jurisdiction of the state and federal courts located within San Francisco County, California for the purpose of litigating all such disputes.", ". You agree to allow Gretel.ai to identify you as a customer, to use your website\u2019s name in connection with proposals to prospective customers, to hyperlink to your website\u2019s home page, to display your logo on the Gretel.ai website, and to otherwise refer to you in print or electronic form for marketing or reference purposes. If you do not wish for Gretel.ai to use your name or logo in any of the preceding ways, please contact ", ".", "The Subscription Services are offered by Gretel.ai, Inc., whose address for notice is PO\u00a0Box 70097, Sunnyvale, California 94086. You may contact Gretel.ai by sending correspondence to the foregoing address or by emailing Gretel.ai at\u00a0", "; ", ", however, to give Gretel.ai legal notice under this Agreement, you must deliver such notices to Gretel.ai by personal delivery, overnight courier or registered or certified mail to the mailing address listed above. Gretel.ai may update the address for such notices by posting a notice on the Gretel.ai website. Notices provided by personal delivery will be effective immediately. Notices provided by overnight courier will be effective one business day after they are sent. Notices provided by registered or certified mail will be effective three business days after they are sent.", "Gretel.ai may provide any notice to you under this Agreement by: (a) posting a notice on the Gretel.ai website; or (b) sending a message to the email address then associated with your account. Notices Gretel.ai provides by posting on the Gretel.ai website will be effective upon posting and notices Gretel.ai provides by email will be effective when Gretel.ai sends the email. It is your responsibility to check the Gretel.ai website and your email regularly for notices and to keep your email address current\u00a0 You will be deemed to have received any email sent to the email address then associated with your account when Gretel.ai sends the email, whether or not you actually receive the email.", "You agree that any notices, agreements, disclosures, or other communications that Gretel.ai sends to you electronically will satisfy any legal communication requirements, including that such communications be in writing."]},
{"url": "https://gretel.ai/code-of-conduct", "page_title": "Code of Conduct", "headers": ["Gretel Code of Conduct", "Our Pledge", "Our Standards", "Enforcement Responsibilities", "Scope", "Enforcement", "Enforcement Guidelines", "1. Correction", "2. Warning", "3. Temporary Ban", "4. Permanent Ban", "Attribution", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["\u200d", "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.", "We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.", "Examples of behavior that contributes to a positive environment for our community include:", "Examples of unacceptable behavior include:", "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.", "Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.", "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.", "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at ", ". All complaints will be reviewed and investigated promptly and fairly.", "All community leaders are obligated to respect the privacy and security of the reporter of any incident.", "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:", ": Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.", ": A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.", ": A violation through a single incident or series of actions.", ": A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.", ": A serious violation of community standards, including sustained inappropriate behavior.", ": A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.", ": Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.", ": A permanent ban from any sort of public interaction within the community.", "This Code of Conduct is adapted from the ", ", version 2.1, available at ", ".", "Community Impact Guidelines were inspired by ", ".", "For answers to common questions about this code of conduct, see the FAQ at ", ". Translations are available at ", "."]},
{"url": "https://gretel.ai/faqs/gretel-synthetics", "page_title": "Gretel Synthetics FAQs - Gretel.ai", "headers": ["Gretel Synthetics FAQs", "What is synthetic data?", "How does Gretel Synthetics create artificial data?", "Is there an architecture diagram?", "What kinds of data can I send to Gretel Synthetics?", "What are the outputs from Gretel Synthetics?", "Can I run Gretel Synthetics on premises?", "What are gretel-synthetics premium features?", "Do I still need to de-identify sensitive data when using gretel-synthetics?", "What kinds of privacy protections can Gretel Synthetics help with?", "How is Gretel-synthetics differential privacy different from traditional implementations?", "How is synthetic data different from the original source data it was trained on?", "How many lines of input data do I need to train a synthetic model?", "How many columns of training data can I have?", "How many epochs should I train my model with?", "Does training a synthetic model require a GPU?", "What is differential privacy?", "How does Gretel-synthetics leverage differential privacy?", "How does Gretel-synthetics implement differential privacy?", "If my model trained in batches using differential privacy, what is my final epsilon (privacy guarantee)?", "What are good epsilon (\u03b5) and delta (\u03b4) values in differential privacy?", "How is Stochastic Gradient Descent (SGD) modified to be differentially private?", "What does RDP order mean?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/partnerships/gretel-on-aws", "page_title": "Gretel on AWS", "headers": ["Gretel and AWS", "Unlock the power of multi-modal synthetic data in your enterprise ", "The world's leading enterprises are achieving more with Gretel and AWS", "Use Gretel on AWS Today", "Gretel is now available in the AWS Marketplace", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": [" AI training with synthetic tabular, text, image, time-series, and relational data.", " safe versions of sensitive data for training and testing.", " data privacy requirements, including GDPR\u00a0and CCPA.", "customized scenarios for testing using conditional data generation.", " the development of AI/ML, R&D, and applications.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/why-gretel", "page_title": "Why Gretel", "headers": ["Access accurate and safe synthetic data, on-demand.", "Data synthesized with Gretel is really accurate.", "Provably private to mitigate GDPR, CCPA, and HIPAA\u00a0risks.", "Unlock exponential data scale with our growing community.", "Multiple models to support your synthetic data needs.", "Synthesize tabular data, think rows and columns.", "Synthesize unstructured text, think human language and chatbots.", "Synthesize time-series data, think sensors and financial data.", "Synthesize related tables, think large databases or multiple files.", "Synthesize images, think anything from AI-art to\u00a0complex real-world scenarios.", "Get started with synthetic data in less than five minutes.", "Deploy Gretel for enterprise use cases", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Harness the power of generative AI in your enterprise with Gretel and Google Cloud.", "Unlock the power of multi-modal synthetic data in your enterprise with AWS.", "Unlock generative AI in Azure with custom AI models that are trained on safe, secure data.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/license", "page_title": "Gretel.ai Licenses", "headers": ["License", "\u200d", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["A source available license gives access to source code, but places restrictions on its use. The Gretel.ai SAL allows largely unrestricted use of some of our proprietary software at no charge for internal business purposes and for non-commercial academic, research and other personal use. You may also make modifications and derivatives, but you may not distribute the software (or your modifications or derivatives) in any form to third parties or use them to provide a service to third parties (e.g., by providing a hosted service or product that uses the software).", "\u200d", "Our software delivers substantial value to our customers but requires significant development and support resources. This change to the licensing model for some of our software, which was previously provided as open source software under the Apache 2.0 license, allows us to continue making source code generally available to the public for all features while selectively charging for features based on certain use cases.", "In all cases, (a) the Gretel.ai SAL will only apply prospectively and the changes here do not affect prior versions of software that were licensed under the Apache 2.0 license and (b) any third party open source components remain licensed under their original license.", "\u200d", "When the Gretel.ai Source Available LICENSE file appears at the root of a repository, the SAL applies to all source code within the repository. A note in the README.md often identifies the use of the SAL and links to this FAQ.", "When the Gretel.ai Source Available LICENSE file appears in a specific directory, the SAL applies to all source code within that directory.", "For additional clarity, additional notices may appear on the relevant GitHub repository pages.", "\u200d", "Initially, the Gretel.ai SAL will be applied to ", ", starting with version 0.19.0, and ", ", starting with version 0.3.0. Currently, Gretel.ai expects to keep other code, such as the ", " and ", " under open source licenses, typically the Apache 2.0 license. If Gretel.ai applies the SAL to new projects or other existing projects, they will be identified as such and where applicable changes noted in the README.md file of the GitHub repository.", "\u200d", "Yes. \u00a0Gretel.ai welcomes contributions. As with all Gretel.ai repositories, you will need to sign the Gretel.ai CLA. Contributions will not be accepted without a signed CLA. Please see ", " for more information.", "\u200d", "No. The Gretel.ai SAL applies to modifications and derivatives. Gretel.ai wants to make source code available for these projects to give you freedom to experiment and implement them in a way that works for you, but not to commercialize them. If you need to negotiate a different license, please ask us.", "\u200d", "At a high level, \"internal business purposes\" means the software can be used for your own business needs. If you want to generate synthetic data based on your data, including customer data (with permission, of course), to internally train a machine learning model used in your product, that is an acceptable purpose. Similarly, let's say you need to analyze data for personal information (and/or anonymize it) to comply with your privacy/security obligations. You may use the software for those purposes. What you cannot do, as detailed in the restrictions to the SAL, is use the software (including as modified by you) to give or sell to others. As such, you cannot use the software to create / transform / classify data to be used by others. Along the same lines, you cannot generate data to train someone else's machine learning model for them.", "\u200d", "No. This FAQ is for informational purposes only. The Gretel.ai SAL stands on its own, and this FAQ does not affect its meaning.", "\u200d", "The full text of the SAL can be found ", ". You will also find it reproduced in the license files for applicable software on Gretel.ai\u2019s GitHub repositories.", "\u200d", "If you have any questions about the Gretel.ai SAL, please contact Gretel.ai at ", ".", "\u200d", "A CLA is a legal document that outlines the rights granted and retained with respect to a person\u2019s or entity\u2019s contributions. It is common practice for open source projects to require CLAs before an individual or entity can contribute code to a project.", "\u200d", "Gretel.ai requires a signed CLA before contributions are made to Gretel.ai projects to make sure that Gretel.ai and other licensees and users of its open source and other software, products and services are legally entitled to use your contributed code and that you had the right to make the contribution.", "\u200d", "Yes. Under the CLA, you are only granting Gretel.ai a non-exclusive license under your copyrights and patent rights in your contributions. ", "This means you retain ownership of your intellectual property rights in your contributions, including copyright and patent rights, and have the same rights to use, distribute or license your contributions that you would have had without entering into the CLA. This includes making your contributions available to other projects or organizations as you wish.", "\u200d", "No. Once you have provided a contribution, you cannot withdraw permission for its use. Gretel.ai\u2019s open source and other software, products and services are intended to be used and distributed as widely as possible. To do this with confidence, Gretel.ai needs to ensure the continuing existence and availability of the code.", "\u200d", "Yes, but there are some considerations. By signing our CLA, you represent that all the code you contribute is your original creation. If you wish to submit code that is not your original creation, you must first get written permission from the owner of that code to submit it to Gretel.ai under the CLA. Submission of such third-party code must be separate from any contribution and follow the requirements set forth in Section 8 of the CLA.", "As with all submissions, Gretel.ai is not obligated to accept a contribution and for third-party code may require the owner of the code to make their own submission under the CLA.", "\u200d", "Code developed as part of your employment may not belong to you. It may be owned by your employer. If you have signed the CLA as an individual, but your employer has not signed one, be very careful about submitting code you have developed. Gretel.ai cannot accept code that belongs to your employer unless they have signed the CLA. Similarly, if you developed code as part of a consulting engagement, make sure you and the person or entity that hired you understand who owns the rights to the code. The CLA is a legal declaration by you that you have the right to grant the specified licenses for your contributions. Only submit code if you are entitled to make those license grants.", "\u200d", "Generally, no. The CLA covers all Gretel.ai projects. However, there may be circumstances in which Gretel.ai may ask you to sign the CLA again, such as if Gretel.ai is not able to match up the identifying information in the CLA with your GitHub handle or email address when you make a submission.", "\u200d", "No. This FAQ is for informational purposes only. The Gretel.ai Contributor License Agreement stands on its own, and this FAQ does not affect its meaning.", "\u200d", "The full text of the CLA can be found ", ".", "\u200d", "If you have any questions about the CLA, please contact Gretel.ai at ", "."]},
{"url": "https://gretel.ai/media", "page_title": "Videos, podcasts, news and more - Gretel.ai", "headers": ["Resources", "In the news", "Financial Times: Why computer-made data is being used to train AI models", "Gretel Partners With Google Cloud to Harness the Power of Synthetic Data and Accelerate Adoption of Safer Generative AI in the Enterprise", "Forbes: Synthetic Data Is About To Transform AI", "Gretel.ai + Illumina - Using AI to create safe, synthetic datasets for genomics", "Gretel AI raises $50M for a platform that lets engineers build and use synthetic data sets to ensure the privacy of their actual data.", "AI 50: America\u2019s Most Promising Artificial Intelligence Companies", " Gretel announces $12M Series A to make it easier to anonymize data", "How synthetic data could save AI", "A group of ex-NSA and Amazon engineers are building a \u2018GitHub for data\u2019", "Recent videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Developer Workshop: Synthetic Text Generation with Gretel GPT", "Deep Dive on Synthetic Time-Series Data with Gretel.ai", "Conditional Text Generation with Gretel GPT", "Recent podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Helpful resources to start making your job easier."]},
{"url": "https://gretel.ai/tabular-llm", "page_title": "Tabular LLM early access \u2014 Gretel.ai", "headers": ["Gretel's Tabular LLM", "Why try Gretel\u2019s Tabular LLM?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Quickly generate highly realistic data for demos, testing, and pre-production environments. Create domain-specific data for any region, in any language. ", "Have incomplete data? Edit and augment existing datasets, and fill in gaps where data is missing. Use both SQL and natural language for prompting.", " Generate correlated datasets without needing any training data.", " Automatically add columns and records to existing data.", " Replace nulls and missing fields with realistic values.", "Use a SQL schema or query to create an entire dataset with the correct datatypes.", " Build datasets for different regions, including names, email domains, addresses, and company names.", " Generate millions of records and integrate them into your MLOps pipelines."]},
{"url": "https://gretel.ai/partnerships/gretel-plus-google-cloud", "page_title": "Gretel.ai + Google Cloud", "headers": ["Gretel and Google Cloud", "Harness the power of generative AI in your enterprise with Gretel and Google Cloud ", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Overcome data limitations, deploy models more quickly, and keep them deployed longer using synthetic data.", " your data with models that have been trained on millions of images and billions of words.", " synthetic versions of sensitive data with real world accuracy for training and testing.", " privacy and data augmentation requirements using synthetic data.", "customized scenarios for testing using conditional data generation.", " ML R&D and operationalization.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/careers", "page_title": "Come work with us!  - Gretel.ai", "headers": ["Work at Gretel", "Open Roles", "Department Title", "Department Title", "Don't see your role?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We\u2019re a highly collaborative remote company with employees across the US & Canada. Our innovative and transparent culture offers employees the autonomy, tools, and trust to act like owners so that we can focus on our mission - building the best developer platform for synthetic data.", "We\u2019re proud to offer benefits that help you be your best. Access a range of benefits, resources and expert guidance to help you prioritize your well-being, so you can thrive with a healthy body and mind.", "We\u2019re always looking for talented people to join our team. Send your resume to ", " and we will contact you if we feel you would be a good fit for any future roles. "]},
{"url": "https://gretel.ai/media/videos", "page_title": "Gretel Videos - Watch, learn, explore.", "headers": ["Resources", "Presentations &\u00a0workshops", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Developer Workshop: Synthetic Text Generation with Gretel GPT", "Deep Dive on Synthetic Time-Series Data with Gretel.ai", "ODSC East 2022 Workshop: Open-source Tools for Generating Synthetic Data on Demand", "NVIDIA GTC Spring 2022 Workshop: Outperforming Real-World Data With Synthetic Data", "Machine Learning Frameworks - Weights & Biases", "Generating Synthetic Data for Healthcare & Life Sciences", "Build a synthetic data pipeline using Gretel and Apache Airflow", "Dataset Joinability at Scale with HyperLogLog", "Tutorials", "Conditional Text Generation with Gretel GPT", "How to create differentially private synthetic data", "Setting up your Gretel environment (4/4) - Generate synthetic data locally from the SDK", "Setting up your Gretel environment (3/4) - Generate synthetic data locally from the CLI", "Setting up your Gretel environment (2/4) - How to set up the Gretel SDK", "Setting up your Gretel environment (1/4) - How to set up a VM for deep learning", "Use AI to Create Synthetic Data from a DataFrame or CSV", "Classify PII with the Gretel CLI", "Redact PII from data with the Gretel CLI", "Create Synthetic Data with the Gretel CLI", "Generate Synthetic Data in 60 seconds", "How to generate synthetic data with Gretel", "Label PII in Elasticsearch with Python and Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Helpful resources to start making your job easier."]},
{"url": "https://gretel.ai/contact", "page_title": "Contact us - Gretel.ai", "headers": ["Get in touch", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Need technical support? Please contact us at ", "Gretel Labs, Inc. ", "8910 University Center Lane, Suite 400 ", "San Diego, CA 92122"]},
{"url": "https://gretel.ai/media/news", "page_title": "News - Gretel.ai", "headers": ["News", "Resources", "Financial Times: Why computer-made data is being used to train AI models", "Gretel Partners With Google Cloud to Harness the Power of Synthetic Data and Accelerate Adoption of Safer Generative AI in the Enterprise", "Forbes: Synthetic Data Is About To Transform AI", "Gretel.ai + Illumina - Using AI to create safe, synthetic datasets for genomics", "Gretel AI raises $50M for a platform that lets engineers build and use synthetic data sets to ensure the privacy of their actual data.", "AI 50: America\u2019s Most Promising Artificial Intelligence Companies", " Gretel announces $12M Series A to make it easier to anonymize data", "How synthetic data could save AI", "A group of ex-NSA and Amazon engineers are building a \u2018GitHub for data\u2019", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Helpful resources to start making your job easier."]},
{"url": "https://gretel.ai/about", "page_title": "About Us - Gretel.ai", "headers": ["About Gretel", "Our Team", "Investors", "We help developers build with data, together.", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Privacy is a problem ", " ", " ", " not compliance. \u00a0We obsessively seek out elegant, composable abstractions that enable scalable, ", "We think the best ideas come from the blending of diverse perspectives. Our team is comprised of the best minds in their fields - and we believe that adding perspectives will make our solutions much stronger. We hire people who are superb at what they do, drawn to the cool edges where fields touch, and like to laugh. We are deeply collaborative, apolitical, and mission-oriented.", "Work from anywhere. The best and most creative thinkers are all over the planet, and our team has years of experience working in global teams.", "Interested? "]},
{"url": "https://gretel.ai/pricing", "page_title": "Pricing - Gretel.ai", "headers": ["Pricing", "Simple pricing that scales with you", "Synthetic data for everyone", "Upgrade to power your most critical use cases", "Team", "Enterprise", "Compare plans", "Cost and fees", "Usage", "Features", "Support", "Frequently asked questions", "What is a credit?", "How are credits calculated?", "Do credits roll over?", "How can I view my monthly usage?", "What if I go over my developer-tier free monthly credits?", "How much data can an Gretel worker instance process per credit?", "How many Gretel worker instances can I run concurrently?", "What connectors does Gretel offer?", "How can I control my total monthly spend? ", "\u200d", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Start with the fully-featured free tier, then pay for only what you use as you grow.", "A credit is a simple unit measuring API call duration for cloud or local compute. 1 credit is equal to 5 minutes of cloud or local API duration.", "Credit usage is based on the duration from the time you issue an API call until the point your tasks terminate, rounded up to the nearest minute. Each minute of API duration bills as 1/5th of a credit for cloud or local compute.", "The free credits included in your subscription are reset each month and do not roll over.", "Customers can view their API usage in the Console, including credits used and remaining, at daily granularity. Gretel will automatically email customers when they have used 75% and 100% of their monthly subscription credits. ", "If a customer has not entered credit card information, their service use will be suspended after the free monthly credits have been used. Once a credit card is provided, usage will be metered and the customer will be charged for the total number of credits consumed at the end of a month.", "This answer varies based on the complexity and type of data. On a typical CSV dataset containing 156 characters per record and a mix of numeric and categorical data, a Gretel cloud instance is able to synthesize approximately 6k records per credit, including training. A Gretel cloud instance can transform approximately 133k records per credit, or classify 133k records when using a standard policy to redact or label personal data types.", "Gretel\u2019s cloud and local on-premises workers are built to scale linearly, so you can scale concurrent containers to meet your needs. By default, each Developer account is limited to 2 concurrent workers, and Team accounts are limited to 10 concurrent workers. These limits are applied across the entire account and are not per user.\u00a0", "We currently have object storage connectors for AWS (S3), GCP (Google Cloud Storage), and Azure (Blob Storage). We are also looking to build connectors for databases and other data sources. If there are any that would be particularly helpful to you, please reach out to ", " and let us know!", "To protect our customers against unexpected charges, we set a soft limit of 100 additional credits for Developer accounts. If you would like to have that limit increased, please email us at ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/partnerships/gretel-on-azure", "page_title": "Gretel on Azure", "headers": ["Gretel on Azure", "Generate safe synthetic data on demand with Gretel and Azure", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": [" your data with models that have been trained on millions of images and billions of words.", " synthetic versions of sensitive data with real world accuracy for training and testing.", " privacy and data augmentation requirements using synthetic data.", "customized scenarios for testing using conditional data generation.", " ML R&D and operationalization.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/media/podcasts", "page_title": "Podcasts - Gretel.ai", "headers": ["Podcasts", "Resources", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Helpful resources to start making your job easier."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/where-can-i-get-help-or-support", "page_title": "Where can I get help or support?", "headers": ["Where can I get help or support?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["For questions about Gretel Console, our APIs, or synthetic data, please reach out in the ", ". We have a #support channel there, and would love to help you!"]},
{"url": "https://gretel.ai/gretel-cloud-faqs/are-there-api-limits", "page_title": "Are there API limits?", "headers": ["Are there API limits?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Yes. For the Beta, ingest APIs will be throttled at 300 records per 5 minute window, on a rolling basis. You may burst up to 300 API calls, then will be throttled after that. \u00a0This throttling is designed to both protect our service and help guarantee fair access and testing for all users. These rates may change based on usage and will be communicated if changed."]},
{"url": "https://gretel.ai/podcasts/cooking-up-synthetic-data-with-gretel", "page_title": "Podcast - Cooking up synthetic data with Gretel", "headers": ["Cooking up synthetic data with Gretel", "Transcript", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": [" Welcome to another episode of Practical AI. This is Daniel Whitenack, I am a data scientist with SIL International, and I'm joined as always by my co-host, Chris Benson, who is a principal emerging technology strategist at Lockheed Martin. How are you doing, Chris?", " I am doing very well. How's it going today, Daniel?", " It's going good, it's a nice, cold day here in Indiana... We'll have a few more of those before this is all done, so...", " Probably so... It's not quite as cold down here in the sunny South... Although it's not sunny today actually, so...", " Yeah... And we had a new project funded this year related to some AI work for local languages, and trying to get all of that spun up, and infrastructure in place, and community website up for some shared AI tasks, and that sort of thing... It's a lot of structuring and setup right now for me, I would say. I don't know what big things you've got going, but yeah, it seems like that time of year for me.", " Absolutely. From my standpoint, I've just been enjoying getting out there this weekend; I flew around a little bit. I had my first night flight... Pleasant to get back to work here this week, and it's good.", " Yeah, Chris is getting his pilot's license, for any of those out there that are wondering what he's talking about... But yeah, that'll be exciting.", " I didn't run into anything last night. It was a good story.", " Yeah, I'm close by Purdue University, and we have an airport at the university, so I told Chris as soon as he gets his license, he can fly up here and then we can do our recordings in-person, which will be nice.", " There you go.", " Today I'm really excited by the topic that we have going. Chris, a lot of times in our past conversations we've made reference to synthetic data, or augmented data, or data augmentation methods... We've also talked in various forms about privacy... But I don't think we've really had an episode that has combined those in the great way that we're about to... So today we have with us John Myers, who is CTO and co-founder at Gretel. Welcome, John!", " Hey, good morning. Happy to be here.", " Yeah. First, before we dive into all of that good data-related stuff and practical goodness, could you just give us a little bit of an idea about your background and how you ended up working with Gretel?", " Yeah, absolutely. I think the way I ended up at Gretel is somewhat accidentally, which I think a lot of folks who ended up in this field ended up that way... So my background is computer science by education, and then I did about 14 years in the Air Force. When I joined the Air Force, I came in as a communications and informations officer, which is kind of a fancy word for network IT leader. I did a couple years working in space launch communications out in California, the Vandenberg Air Force Base... And then I got an interesting application to go to the National Security Agency and do some really cool, hands-on engineering and development work.", "It sounded really awesome. At that point, my knowledge of the NSA was basically having seen Enemy of the State with Will Smith, and I was like \"That sounds...\" [laughter]", " Wait, you mean it's not like that?", " I wouldn't say it is a documentary of sorts... I was like \"Well, it sounds really cool.\"", " Loosely based on reality.", " Yeah, loosely based on reality. I didn't meet Will Smith or anything, so that was a bummer, but... It was over in Maryland. I'm from Philly originally. I was like \"It's close to home. I can get out there and just experience something new.\" When I got there, I got immersed into the intelligence community, and working at NSA, and I still wore the Air Force uniform, but I was kind of in an offshoot program there; I got to work on really cool stuff, working on low-level operating system engineering, building exploits, stuff like that.", "Then I kind of pivoted into doing big data analysis, which was a kind of up-and-coming field at that time. Then I left there, did one more stint with the Air Force, doing a different set of things out of Las Vegas, and at that point I was at a critical point of if I wanted to stay in full-time or do something else. At that point I was so hooked on building... I just wanted to build, and engineer, and building engineering teams, that I decided to leave active duty; I joined the reserves, and then when I got out, I did the complete opposite thing you can do when you're part of a 300,000-person organization, and I have launched a startup in cyber-security with three other people.", "[00:08:21.01] Then we did an enterprise security startup, and we did that for three years (it was called Efflux Systems), and then we were acquired by a company called NetScout, that kind of is one of the leaders in network performance monitoring, and wanted to utilize some of what we had in cyber-security for some of their upcoming products.", "When I got there, I was a principal architect, and among a lot of projects I worked on, I worked a lot on a lot of their cloud infrastructure. They build capabilities that also help enterprises and service providers detect and stop DDOS attacks. A lot of those devices collect a ton of telemetry from the customer environment and send it up to a cloud repository where it's securely kept... And we wanted to look at how we can do analysis of that data. And you start [unintelligible 00:09:12.15] through that data and you start to realize there's a lot of sensitive information in this data, and we should probably pre-process it, so we can kind of work on it safely. I think I spent a lot of time doing that; probably more time than I wanted. That was kind of like this pivotal transition point where I got into doing engineering to enable me to do engineering.", "At the same time, back of my head it's kind of like one of those Shark Tank pitches; I'm like \"Surely, I'm not the only person, but this problem - what can we do about it?\" with some other close colleagues of mine, as we started talking about these stories and we all kind of shared very similar, but different pain points; we kind of orbited around this idea of \"What if we could just make data anonymization, and being able to make data safe to use, kind of just like a general-purpose thing that engineers everywhere can use?\" and yadda-yadda-yadda. We launched Gretel.", "So we launched Gretel in the fall of 2019, with myself and our CEO and my other co-founder, Alex Watson, who has a very heavy machine learning and data science background, and was previously the general manager at AWS Macie, which is another product that is very successful at AWS, for detecting sensitive information in S3 buckets.", "So he has a whole different slew of stories - some will call them nightmares - about data anonymization. And yeah, we've been doing it ever since. And really, our mission is to make data anonymization and creating safe data generally available to engineers everywhere, not just the resourced organizations like the Facebooks and the Googles, who have massive resources to experiment with all the techniques to do it.", " I am kind of struck -- I was thinking while you were talking, especially with your background in Air Force and at the National Security Agency and other places... I was sort of remembering back to our conversations with the founders of Immuta, which is another company that does -- I guess they're more focused on the combination of law and data and governance and all of that sort of thing... But it seems like there's this really strong - whatever it is about people coming from that sort of background; they had a sort of similar background, it sounded like. It really creates some deep thinking around these problems of data anonymization, privacy, governance... I don't know, John, what's your perspective from that side? How do you think your background with these sorts of agencies or the military has shaped how you think about data maybe differently than maybe someone like me, who's just always sort of -- started in startups and just sort of got what data I could, and have used it, and... Yeah.", " And Daniel, that was exactly what I was gonna ask next, too. Just so that you know.", " Yeah. Well, also - Chris has some experience in that world as well.", " [00:12:02.23] Gotcha. Yeah, so I think one of the things that I learned a lot about doing intelligence work in the military and working with data is that I learned a lot about the chain of custody of data. And a lot of times when I meet folks that are like yourself, a data scientist, or they're in the analytics space, a lot of times they are kind of just given data and given some task, and say \"Here, go make magic happen.\" And I don't know how often the chain of custody or how that data was actually generated is thought about, but for me, I always think about \"Where was that data born? At the moment that data was collected, something happened and it was written into a database.\" I think that way a lot.", "My other co-founders also have a background in the intelligence community, so it was something we were all aware of. So when we started talking about Gretel, really we wanted to make consumers safer, to make their personal data not used as the way it is today... Because often when you think about big companies like Google, Facebook - they build products, but they also look at their users as a product.", "So we kind of backed into saying \"What if we can enable engineers to make the data safe at the moment that it's created? So right at that inception of the data.\" That's something that we are just really aware of, of where we came from, because that chain of custody of the data is so important.", "And it's not as much a governance thing, versus more of an engineering problem... Because as a data engineer, when I'm writing my data into my production database, can I at the same time create a safe version of that data and write it into a staging database that anyone can access, but privacy guarantees, so I don't have to go through this whole repetitive process of snapshotting my production database, combing over it, writing some bespoke script to sanitize it; can't we just make it part of the entire pipeline, at the point where the data is created?", " I'd like to go back for one second... When you got to that moment where you realized \"Am I the only one that's dealing with this issue?\" and you kind of had maybe an a-ha moment or something there, where you realized that - what got you to that? I'm curious about that moment of recognition, because I think other engineers and other data scientists wonder \"Are they gonna have something similar as they're out there creating?\" What was it that made you suddenly realize \"This is something that I'm recognizing not only impacts me, but probably impacts the broader community\", as well as \"I have something to contribute toward that solution\"? And as part of that, was any of the background -- you know, we talked about your intelligence background there... Did any of that contribute to that moment and that recognition? If you hadn't had any of those experiences, might you have missed that altogether?", " I think there's two big things to answer that, and I'll start with the former. When I had the a-ha moment, I had a small team at my previous company, and we were kind of analyzing the data, and we realized that we needed right ways to detect the sensitive information that's in it... And the sensitive information is an information -- the fact of the sensitive information is it's like names, company names, email addresses, IP addresses, it was things that can identify our customers...", " PPI...", " PPI, yeah... So we were like \"Okay, let's just write some detectors for it. We can use a lot of regex'es, we can write custom rules\", and then we were like \"Okay, now we need a way to write a rule really quickly. Okay, now we need a framework to put the rule into.\" And I was like \"I can't be the only person who's trying to figure out if an email address slipped into a data stream.\"", "Some communities have really specific data structures, that are really specific to them, like in healthcare, and stuff. This was just things that our PPI, and to a degree, PII that identify organizations and people... And there are generic ways to do that, where like \"Can you just bring a regular expression to the table, and just like some framework kicks in and can scale for you?\" That was one of the things I turned at; what we were doing was fairly a repeatable process, and we assumed it was a repeatable process in many industries.", "[00:15:54.20] And the second part of that was where to apply that detection and where to apply whatever type of transformation or synthesis we wanna do. It was kind of a no-brainer that you wanna do it as close to the source, where the source of the data is itself. We're talking about systems that can collect private information... Like, can you do it on the system before you even think about transmitting it to the cloud, so there's no risk there? Or can you do it on the edge, as people would say these days? That's not even a question for me, based off of our backgrounds and being so in tune with data custody.", " So I'm curious, as you've built out this set of products, which - we'll definitely get into the details of those in a bit, and talk a lot more about the practicalities of synthetic data, and all that... But you kind of mentioned that this was like doing engineering so that you could do engineering...", " Yeah.", " As you've engaged with various companies that are using your product, has that story been getting -- they sort of immediately understand what you're after? Because I remember when I was first getting into data science there wasn't a lot of talk about this sort of rigor and the way we were treating data... And probably people might have seen something like this as maybe a little bit burdensome, like something they have to do before they actually get into the work that they really wanna do... But how have people been feeling that need in the industry, and been accepting this sort of solution, from your perspective?", " Yeah, I think it's been received really well, and it's kind of a classic build vs. buy problem, and a lot of folks are just looking to buy... But what they don't wanna buy is some type of really difficult to install appliance or virtual appliance that kind of breaks their workflow... So the way that we're targeting this is making it so it is our end user, our developers that can easily integrate it into what they're doing already... So making just another API call in their stack of what they're executing on, versus saying \"Yeah, sure, we can do this, but we have to kind of common-install a virtual plant\", and you have to re-route your entire data pipeline through it.", "So as soon as we kind of explain that, it works right into their existing infrastructure, and we take care of the scale for them, where they can bring their predictors or bring what they wanna actually detect on to the table. They'd much rather just buy versus build it, because it eats up a ton of cycles for them to build this thing. It's not a build once and deploy type of thing either; it's not like they're building a framework that they can deploy once. It requires care and feeding, because you're constantly adjusting what type of information you're processing and what types of things you wanna anonymize on... So we can kind of go on that journey with you and enable you to [unintelligible 00:18:39.15] a lot faster.", " [00:18:41.11]", " I'm kind of curious - I know in the beginning of the conversation, when Daniel was introducing you, John, he talked about synthetic data... Could you start off by telling us what is synthetic data and kind of give us a little bit of a background before we dive into the specifics of what Gretel does and how it gets there? Give us the terms that we need to know to be able to follow.", " Sure. So we were at a happy hour, [unintelligible 00:21:19.06] give you that level of definition...", " Perfect. [laughter] Our podcast is always the happiest of hours in our listeners' week, I'm sure.", " That was funny.", " So I would say synthetic data is the ability to generate data that is almost indistinguishably recognizable from some type of source dataset. And it has all the granular elements of the original dataset that you would want; however, if you combine those granular elements, you don't have a one for one matching to a record in the source dataset.", "It heavily relies on machine learning and artificial intelligence to learn the semantics of the source dataset, and at that point, once you learn those semantics and that model is built, you could just continue to generate records that in aggregate tell the same story as the source data, which is kind of like one of the key elements that we always like to talk about - you could still run the same types of aggregate queries and get the same story. It's not about just being able to use the individual records that you synthesize.", "Now, we'll say, there are use cases for using those individual records, like if you have a development environment and you're building a system and you wanna look and see how the records fit into your layouts and stuff, but for the most part, the idea is to be able to use those records in some type of set of aggregate feature.", " There's a whole lot of jargon, of course, in our industry, and you've already mentioned as well anonymizing data. How does synthetic data complement anonymization techniques, or maybe it's an alternative to it? How do those two things fit together in terms of anonymization and synthetic data?", " Yeah. I would say it all starts with the core use case, but it could either be a complement, it could be totally separate, or they could support each other. So we have two large buckets that we focus on at Gretel, and one of them is being able to detect PII, detect PPI, and then apply different transformation techniques to the data in place, so that your data is essentially the same, but there's typical redactions, or character replacements, or whatever. And that falls in line with a lot of the existing solutions that are out there, that fall under kind of like a data loss prevention capability... And you'll see a lot of the cloud providers, like Azure, AWS, Google - they all have a DLP set of APIs you can apply... Except that usually requires to be bought into their ecosystem and already have your data sitting there. In our mind, that's table stakes, just to even have a conversation about privacy. We offer a set of APIs that allow you to detect and do those typical transforms.", "[00:24:01.21] And synthetic data for us a way to take the dataset, build a model, and let the model generate new records that you can just accumulate and use however... And it doesn't necessarily require you to funnel each record through a certain type of detector and look for PII, because we're just gonna learn the semantics of the entire dataset and generate new records... But those records should not be the original records that you had. And they play hand in hand.", "For one example, let's say you have really sensitive PII, let's say social security numbers in the source dataset - if you can detect that a certain column are social security numbers, we might go ahead and recommend that you generate new randomized social security numbers, which is very deterministic... And then you can have that new column in that dataset, then send it into our synthetic capability, and that will just help guarantee that we don't memorize any of the tokens or replay any of those social security numbers... Because that is always a risk with synthetic data - you might memorize and replace some secrets. And that's where that whole field of differential privacy is coming in to address that situation as well.", " So the synthetic data that's being generated - does it always start with being a replacement for the actual PII that you're contending with at the time? Is it always kind of starting as a replacement factor, or is there ever a use case where you're generating maybe -- what if you're starting with no data, and you wanted to generate it entirely synthetic, just because you don't have something to start with. Is that within that, or would that be a separate type of use case, a separate product?", " I would say that the synthetic data generation is not just based on doing anonymization, because you can kind of do that type of anonymization without the underlying need for machine learning and AI.", " I see.", " I think the issue that comes up is that you have a lot of different attacks, like [unintelligible 00:25:51.04] attacks, that are completely plausible and possible on data that's been just anonymized in place. So just because you're anonymizing names and addresses and phone numbers and email addresses - well, let's say just for argument's sake you have a bag of customer data and you have a bunch of records... You know, I like in Baltimore, and let's say I'm your only customer who is a male in his mid-thirties in Baltimore; even if you take all my personal information out, you might be able to join the fact that you have a customer like me in Baltimore and I'm the only one - well, now you've re-identified me.", "So with synthetic data it's \"How can we actually generate a lot of those other risky fields that are really risky in aggregate?\" So you look at categorical fields like ages, genders, locations... How do you actually generate those records, so that they can't be recombined to really identify someone, but they're still useful when you wanna look up the average amount of revenue you get from people in Baltimore, or some type of aggregate question like that.", "And then on the second question, for us to generate synthetic data you do need some type of training input to learn the underlying semantics. And then once you have that model, you can generate any number of records. it doesn't have to be a one-to-one. If I have 5,000 training records, you can generate five synthetic records, you can generate 20,000. But once you learn that semantics and the fact that you can generate any number, you can do a lot of interesting things. You can do enforcement on what you're generating.", "Let's say I wanna generate records, but I only wanna accept records that are of a certain category, a certain age or a gender group. You can use that to synthesize new records that help balance a dataset that might be otherwise biased, and not have enough samples of something that you're trying to predict on, for example. Once you have that core model built, you can kind of generate records to meet a lot of those needs.", " We've mostly been talking about use cases around private data, and privacy aspects, but is this synthetic data generation capability - does it also help people who are working in data-scarce scenarios, or imbalanced dataset scenarios? Let's say that we don't have any personally-identifying data in our dataset, at least to our knowledge we're not dealing with that issue... But we do either have an imbalance dataset, or maybe we're just working in a sort of data-scarce domain, where we do have some data... Like you say, maybe we have 5,000 records, but we really need 25,000 records for our model... Is it viable to use synthetic data in that type of scenario?", " [00:28:31.27] Yes, and that is actually one of the core use cases that we have experienced, where there might be already a situation where data is deemed safe to use. I'll use fraud as an example, because frauds are a really good one, where you have so many records that are not fraud. You're usually trying to predict the opposite, so the field you're trying to predict is an actual fraudulent event, but you might have just not enough records of that fraudulent event.", "What we're able to do is kind of guide you through how to synthesize more records that fit into the fraud category, so that when you go and you build your actual machine learning algorithm, there's enough of those fraudulent records there that it could actually create a proper decision boundary, or whatever, so you have a net better model at the end.", " I was just wondering, like -- because I'm familiar with imbalanced datasets, where you do some type of maybe interpolation to get some extra points, or something like that... But you're talking about really training a neural network model or some type of machine learning model to actually generate this data. So maybe this is too much of a simplification; I'm sure your methods are pretty advanced... But is the basic idea that you would have some of the fields of a record be input to the model during inference, and the it's trying to predict another of the fields of a record? Similar to predicting the next word after a sequence of other words, or something like that. Is that some of the basic idea, or can you give us any intuition in terms of how the data is set up and how that works?", " Yeah, that's actually very close... So our core engine for training and generating synthetic data is completely open source; it's on GitHub under GretelAI... And our initial implementation with the open source package - it's kind of a wrapper framework, and you can have a bunch of pluggable backends... So right now, our first pluggable backend is an LSTM on TensorFlow. It is kind of a sequential model, which is a lot different than a lot of the other techniques that are out there... And what we do is we have the ability to focus on text input.", "Then in the open source package we also have another module that kind of wraps that entire thing inside of a data frame... And then we infer different field delimiters, and essentially we're able to reconstruct those records as a sequence, and then exactly [unintelligible 00:30:56.15] We can do it one of two ways. One, you can just say \"Just keep generating records with no input\", or you could specify what we call a seed, where it's like \"Okay, I only want you to create records that start with this age group, this gender\", and then it'll complete those records. That allows you to more efficiently increase a record of a certain type based on what your requirements are.", "Then what we actually as the product - we have a bunch of different layers that work on top of that to do data validation. We have separate models that we build to learn and enforce the semantics of individual fields to make sure that when records are generated, they still fit within the constraints that you had before, whether it's the right character sequences, the right structure of the fields if you have date times, making sure that categorical fields are always recreated... We don't wanna invent a new state. So if there's 50 states that are in a certain column, we'll make sure we're only generating valid states. Those are things that we provide inside of the product. But the open source package lets you kind of just jump right in to build and train on structured or unstructured data.", " [00:32:04.13]", " John, I'm curious if some types of data are easier to synthesize than other types of data. You mentioned dates, categorical variables, categories, labels, numbers... But then we also have things like audio, and imagery, and other things like that. What's the sort of current state of the art in terms of synthesized data, and what data types or domains of data maybe are the bread and butter right now, and maybe which ones have some challenges in terms of synthesizing data in certain scenarios?", " Yeah, so right now what Gretel ships is really focused around structured and unstructured text. So I think about records from a database, or any type of text input. Audio and video and imagery is next that we will probably see in a future iteration of the product, and it's something that we're working on now... A lot of the state of the art around that is in our wheelhouse now, because we were able to just back into our customer problems via structured records. Right now, we kind of have to pick our battles, and right now that's the main one that we're focused on, is being able to enable people to synthesize new versions of database tables, or static datasets, so they can more safely share them.", " I'm curious - we've talked a little bit about the product side of things, and also you've made reference to the open source as well... Could you differentiate a little bit between what each side of that has to offer, to kind of give people a framework in their head about what they would go to for each, and where do they maybe step up from open source to your paid products/services, that kind of thing?", " Yeah, absolutely. Right now the open source packages - and there's two of them; one of them allows you to get started with synthetic data, and the other one allows you to get started with our traditional transformers, to kind of mutate data in place. Those are Python libraries that are available to anyone, licensed under Apache 2.0. Obviously, you go in using those knowing that it's Python, and that already is kind of a qualification for a lot of our customers... Which isn't a problem; we have a ton of researchers and data scientists that live and breathe in Jupyter Notebooks, so they're able to plug that right in.", "Last August we launched a beta of more of our premium features, and that beta basically allows you to use our cloud service to test out our labeling capabilities. Then what we ended up doing was packaging up a lot of our premium capabilities, which include automatic data validation, it does a lot of analysis to make sure that correlations across all your data are held, distributions across your data are held properly... We also released those available as an SDK that you can download through our authenticated API.", "[00:35:52.13] We had a great about several months of going through that beta, getting a ton of feedback from users, and then what we walked away from that knowing is that what we really wanna do is make this available to engineers everywhere. And engineers everywhere can't necessarily just download a Python SDK and incorporate it in their pipeline if, let's say, your entire backend is written in Java. And so how do we drastically simplify what these premium SDKs do? So what we're building now is the ability to launch Gretel services as kind of containerized capabilities that are backed by REST APIs. So now you can interact with our services purely through a REST API, which is completely language-agnostic. Every engineer at some point has gone through the process of making API calls through remote service, and so now that is kind of the qualification factor.", "We wouldn't have learned everything that we learned if we didn't have that granular-level capability out there through the beta... So now the entry point will kind of either be you can run Gretel services in your environment, we're also building a hosted service where we can run and scale these capabilities for you... But it should be as easy as taking your dataset, or taking some records, a lightweight configuration, pushing it to an endpoint, and that endpoint will then trigger a whole bunch of backend work to learn, build a model, or generate data for you. And at that point we really just wanna be a bump in the line in your entire data workflow, to be able to call into these API. So that's what we're working on now, is just really simplify that down.", " Based on what you've seen with your current users and customers -- like, if I'm a data scientist working on a new project, getting into some new data, do you have any recommendations in terms of workflow with your tools? Like, you know, when I get the data, I'm profiling it, I'm doing some exploratory analysis, where and when should I be thinking about fitting in some of these REST calls, or Python SDK elements into my workflow, so that I can make sure that I'm dealing with maybe both sides of things, anonymity and creating synthetic examples? Maybe more specifically my question would be are you seeing that done in a workflow upfront, and they do this on data, and then they use that data moving forward always? Or are you seeing this as sort of an ongoing part of people's workflow?", " I would say that upfront is not the usual case that we recommend, and we recommend that there's usually a little bit of data cleaning you wanna do; not down to the granularity of doing a ton of all the exact feature engineering you would do to build a model, but at a minimum -- and we have blueprints that help folks go through this process as well... You wanna identify, for example, columns that you probably don't need to worry about synthesizing, because they're not something that your model is gonna grab onto.", "So if you have records that have maybe people names in them, typically those people names aren't gonna be correlated to a lot of your continuous variables and your other variables in the dataset, and if you can drop those columns first, you're gonna save a lot of time on being able to train a synthetic model for that.", "Other examples would be, you know, there's a lot of datasets we get from customers that are highly dimensional - several hundred columns - and they're trying to train a model, maybe like [unintelligible 00:39:08.03] model on that to predict something, and a lot of times what we recommend is \"Look, if you can kind of train your model first, and then you identify what the algorithm deems are the most valuable columns, just drop a lot of the other columns\", because then you're gonna get way better performance out of maintaining the correlations on different subsets of the dataset.", "So we do recommend at that point - really right before you would actually think about actually training your model - once your data is pretty much in that good state, around that ballpark... But it completely varies based off of use case. We have some customers that the first stop is coming to Gretel, because they wanna immediately [unintelligible 00:39:45.25] PII that they can remove. So I'd say it definitely varies.", " [00:39:51.24] Yeah, and I've found that Gretel's Blueprints repo, which seems pretty interesting. I see a bunch of these examples [unintelligible 00:39:58.17] create synthetic data from a CSV or a data frame... All sorts of examples. So if our listeners are interested in that, it looks like there's some notebooks and things in there that they can look at. We'll link that in our show notes for sure for people to take a look at.", "Maybe one thing to kind of start us thinking about things into the future - where do you see the current challenges that are unsolved right now in terms of privacy, and maybe data augmentation, or synthetic data? What are some of those problems out there that you still see as open problems that need to be addressed?", " Yeah, I'd say there's a couple of problems, and they're somewhat related. One of them is it's still a very nascent field, and there's a lot of tools out there, and there's no magic bullet. There's no way just to magically take a dataset and create a version of it that is perfect and doesn't violate privacy. There's always going to be a trade-off between utility and privacy, and helping out people understand that I think is gonna be a really big challenge.", "There's a ton of great research out there into how to do that trade-off between utility and privacy, and that's one of the things that we wanna figure out, is how to make that more obvious to engineers when they wanna anonymize data or make data safe to share; it's like all these knobs you can tune. Ideally, you don't wanna go to a software engineer who's maybe a full-stack engineer and they have access to a production table and they want to make a safe version of that data - you don't wanna ask them to tune a bunch of hyperparameters for a TensorFlow LSTM, because they're gonna be like \"Whoa, I don't know what's going on here.\"", "But you might wanna ask them to say like \"Look, what is the trade-off in utility and privacy that we should have here? Are you sharing this externally, are you sharing this internally?\" Ask them what those levels are, and then how can we infer what all those really nitty-gritty knobs are that need to be turned for the underlying model that needs to be built...", "Which kind of segues into the second problem I see - making these tools generally available to software engineers everywhere is gonna be a massive challenge. You can't ask every engineer to download a Python SDK and have a crash course in machine learning to ask them to build a safe version of their dataset... So how do we kind of bundle and package these capabilities in a way that engineers everywhere wanna use as part of their day-to-day workflow? If you look at companies that made things dead simple - Stripe made payments less scary, because they have a ton of language bindings, it's really easy to integrate into your app; it's just another API call that you make and you don't think about it, and they're doing all this heavy-lifting of processing payments, which is a very complex thing... How do we kind of generalize down to that level? And that's definitely one of the big visions and missions that we have here at Gretel.", " [00:42:55.20] I'm kind of curious - as you're describing that, and going back to the beginning of that second challenge that you're looking at in terms of... It really strikes me the scale of what needs to happen here... So kind of beyond the specific challenges that maybe need to be solved, and that maybe Gretel wants to address, the scale of this is definitely holding a lot of engineers back, that are contending with this and can't get where they wanna go... And if you're looking out over the next few years at where this has to go as an industry, and the need to broadly, at scale, be able to increase productivity in AI/ML in general, and this being such a core tenet of that - where do you see the industry going with that? What needs to happen in the large to enable ten times, a hundred times as many engineers to be able to overcome these problems and get productive with the problems they're trying to solve? You really got me thinking as you were answering those last two about how to get there from here... How do you get there?", " That is a great question. In my mind - and this is something that we even do inside of Gretel - I think one of the key things that has to get us there is that we just have more of a free form exchange of (I guess) ideas and talent among different types of developers and engineers that are out there. When you look at a lot of organizations, there's still always a lot of segregation between your platform engineers, and your software engineers, and your data engineers, and you have your machine learning engineers, and your data scientists... And really, I think everyone needs to be able to do a little bit of everything, and it's like, how do you build toolsets that allow a software engineer to easily take a look at the data - even though it's using some complex machine learning capabilities - without having to go and request a machine learning engineer to spend tons of time doing it, when that MLE should be maybe researching other parts that are more vital to what the core mission is of that organization.", "You see that there's been a lot of acceleration in micro-frameworks that are building REST APIs, and that is a really good example for how that allowed a lot of people to operationalize things. Even as a data scientist, you could fire up model training and predictions and back it with a REST API and make it generally available... Like, what's the machine learning version of that micro-framework for a REST API that allows software engineers to quickly take use of all the capabilities that are out there, that are up and coming with synthetic data?", "At our company now we have a complete blend of backgrounds; we don't want the whole sequential motion of like \"Well, this person builds the model, and then hands the model over to this person, who builds this...\" We just want everyone to be able to plug in and build... So how do organizations move to that methodology.", " Tearing down the walls there, so to speak, of those distinctions.", " Yeah, for sure. Well, John, I am super-excited about what Gretel is doing, and I really appreciate your detailed description of why these things are important, and how you're solving some of these problems. I think it's really important. We'll make sure and link, like I mentioned, some of these links that we talked about in our show notes for people to check out. Please go and check these out, try to generate some synthetic data with their tools and check out their platform.", "Thank you so much, John. I appreciate you taking time to talk to us.", " Awesome. It's been a pleasure to be on the show. Thanks for having me.", "\u200d", "\u200d"]},
{"url": "https://gretel.ai/podcasts/diving-deep-into-synthetic-data-with-alex-watson-of-gretel-ai", "page_title": "Podcast - Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "headers": ["Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Links", "About the Podcast", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Alex Watson is the co-founder and CEO of ", ", a startup that offers APIs for creating anonymized and synthetic datasets. Previously he was the founder of Harvest.ai, whose product Macie, an analytics platform protecting against data breaches, was acquired by AWS.", "Helping you bring ML out of the lab and into products that people love.", "Follow Charlie on Twitter: ", "Subscribe to ML Engineered: ", "Comments? Questions? Submit them here: ", "Take the Giving What We Can Pledge: "]},
{"url": "https://discord.com/invite/WD7AaJmu4Z", "page_title": "Synthetic Data Community", "headers": [], "content": []},
{"url": "https://console.gretel.ai/", "page_title": "Gretel Console", "headers": [], "content": []},
{"url": "https://gretel.ai/gretel-cloud-faqs/how-do-i-get-my-data-to-gretel-cloud", "page_title": "How do I get my data to Gretel Cloud?", "headers": ["How do I get my data to Gretel Cloud?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Data can be sent to Gretel Cloud via our ", ". \u00a0If you have ever used technologies like ElasticSearch, then using the Records endpoints should be very familiar to you. Additionally, our Gretel Console allows the upload of CSVs (max of 5000 records per file) and our ", "provides bindings to work with our REST API."]},
{"url": "https://gretel.ai/products", "page_title": "Smart, Safe, Synthetic Data On Demand - Gretel.ai", "headers": ["Smart, Safe, Synthetic ", "Data On Demand", "Gretel\u2019s APIs Have You Covered", "Produce Scalable Privacy for your Data with ", "The Gretel\u00a0Synthetics Platform", "Generate unlimited ", "Perform privacy preserving transformations on datasets at scale.", "Identify PII with advanced NLP detection.", "Gretel APIs in Action", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/videos/label-pii-in-elasticsearch-with-python-and-gretel", "page_title": "Label PII in Elasticsearch with Python and Gretel - Video", "headers": ["Label PII in Elasticsearch with Python and Gretel", "Video description", "Read the blog post", "Load NER data into Elasticsearch", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The Gretel Console provides several views and tools to help explore your data. Another option that may work well with common existing workflows is to load the data into Elasticsearch and use Kibana for exploration and reporting.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/gretel-joins-microsoft-azure-marketplace-and-microsoft-for-startups-pegasus-program", "page_title": "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "headers": ["Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today, we are thrilled to announce the availability of Gretel's synthetic data generation platform and privacy-preserving technologies on ", ", offering multimodal solutions for creating synthetic text, tabular, time series, and image data. This expansion not only signifies a new chapter of safe data accessibility for Azure users but also reflects our shared commitment to fostering innovation in the realm of data privacy and responsible AI. Now, Azure users can leverage Gretel's tools to safely train custom AI models on proprietary data, meet stringent data privacy compliance mandates, unlock new avenues for secure data collaboration, and expedite their market entry with new products and services\u2014all while capitalizing on existing Azure credits and commitments.\u00a0", "Our inclusion in Microsoft for Startups Pegasus Program further cements our alliance, aiming to accelerate the adoption of responsible AI practices across industries. This collaboration amplifies our mission to facilitate the creation of high-quality, domain-specific synthetic data, ensuring privacy compliance while unlocking the full potential of AI and machine learning applications. Together, we are setting new benchmarks in ensuring data privacy, security, and governance in AI.", "As enterprises accelerate their journey towards embedding large language models (LLMs) workflows and scaling generative AI applications and \u201cfrontier AI\u201d models, the quest for safe, accurate, and accessible data has become paramount. Yet, the landscape of data access and management often presents a quagmire of privacy and security challenges. Gretel's synthetic data generation platform offers a secure path to safe data accessibility and enhanced AI capabilities.", "The integration of synthetic data generation into AI and data governance frameworks is reshaping strategies across enterprises, governments, and research communities.\u00a0", " \u2014 ", "Azure customers can use their existing Azure credits and commitments to build with Gretel. To learn more about these products, services, and solutions available check out ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/create-synthetic-data-with-the-gretel-cli", "page_title": "Create Synthetic Data with the Gretel CLI - Video", "headers": ["Create Synthetic Data with the Gretel CLI", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this tutorial, Alex shows you how to create synthetic data with the Gretel CLI.", "Alex (00:04):", "Hey, I'm Alex. I'm a co-founder here at Gretel and today I'm going to walk through using the Gretel CLI and APIs to generate synthetic data. Let's jump on in. First thing you'll want to do is to go ahead and log into the Gretel console. We use this to generate an API key, which we will connect to the CLI so it knows how to interface with our service. Next step is to go ahead and install the Gretel CLI, simple PIP command. It requires Python 3 or greater. That's setup, from there you want to go ahead and make sure it's setup correctly, just typing gretel ought to show you an overview of the different commands that you can execute. Running gretel configure will allow us to go ahead and set up the defaults. Go ahead and accept the defaults we're going to be training our model in the cloud. You also have the option of training a model locally where your data never leaves your environment if you wish. I'm going to go ahead and grab our API key and paste that in here.", "Alex (01:06):", "And it looks like we are good to go. From there let's go ahead and jump over to the docs and we'll follow along with the docs in the example for creating synthetic data. CLI tutorials create synthetic data. From here the first thing we'll do is create a project, so we're going to create a default project, which is the landing place for our models and the data we create within the Gretel SaaS Service. Take a look at this command here, gretel projects create. We're giving it a display name called healthcare, which is meant to just kind of give us a description for the type of data set we're working on. We're also setting that as the default. So you don't have to specify that in the next commands. That was created correctly.", "Alex (01:52):", "Next thing we're going to do. You can't actually just call Gretel directly on this file that's located in the cloud, but I wanted to pull it down and take a look and take a look at the data before I started training. So we use wget, we'll pull down this example, EHR data that we're going to create a synthetic version of. We went and downloaded that from S3, let's take a look. What we see here are about 20 different columns of data, got nice mixed integer, categorical, free taxed numeric data so it's a good challenge for any synthetic data system. And our challenge here is to train a neural network essentially on this text, in this, essentially we're training the language model that learns to recreate this text. And we look at whether it's capable of learning the intext and correlations that exist in the input data and recreating that into a new artificial synthetic dataset.", "Alex (02:46):", "Okay, so we've got that downloaded. The next step here is we are going to go ahead and train a synthetic model on our dataset. Let's take a look at this command. You can go ahead and paste this in. What we're running here are gretel models create, so essentially you're telling it to train a new model within our project. We're running in the cloud, we're using a configuration here. We're specifying configuration set. You can use the default set. We're using one here called high field count, which works well for this data set as it had about 20 fields in it. The input data is hospital EHR data, the output is to the local file system. And then we're storing the model ID to a file that we can use subsequently for calls to generate records.", "Alex (03:31):", "We mentioned here that the high field counts common config is the one that we're going with. You can browse different configurations or even make your own here. So options would be to view any one of these configurations or make small changes to it, fine tune them, or create your own. One of the popular ones we see often with our customers is training with differential privacy. So here you can see an example of a YAML Config, turning on differential privacy, and specifying the default primers we use to instruct training. From here we're ready to start training so let's go ahead and kick this off.", "Alex (04:04):", "Preparing the model, it's uploading the data source that we have to our project in Gretel. It's creating the project, queuing model creation. So what it's doing now is looking for a worker which will pick up this project. A worker is a GPU-enabled box that will essentially start training our neural network for us within the Gretel cloud. The model has completed training, it went for the full 100 epochs without using early stopping, so it was continually proofing on the validation set. You can see here a quick look at it shows that the accuracy is quite good. 0.8 on accuracy and validation loss, typically anything under one you've got a pretty solid model, so we could expect this to perform pretty well. The next step here it's going to start generating records. So here we see each different batch the number of records here, 365 generated in the first batch, five failed validation.", "Alex (04:58):", "So we train a set of expert validators that look at the training data, and they look to make sure that anything created by the neural network kind of matches the semantics and the types of data that we saw in the training data. If it fails, it fails to record, which really high confidence in the synthetic data that's been created. So here it completed the 5,000 records we asked it to create and went ahead and generated a report. And go ahead and take a look at that report now and try to get an idea of how good of a job or how synthetic is my synthetic data.", "Alex (05:27):", "And open up the report. Here we can see our model got an excellent quality score. So really what this report allows us to do is deep dive on the data. See how many of the original training lines were duplicated for example, get some idea around privacy, as well as the insight from distribution of the original data. What I like to look for, I like to typically start with looking at correlations in the data, and here we can see that our neural network appeared to have learned and recreated the correlations and the original data set very well. If this is something you're not seeing enough of, I would recommend adding additional records, which helps quite a bit. Sometimes in increasing the complexity of the neural network will help as well.", "Alex (06:16):", "Second graph I really like to look at is the PCA. What PCA does is it compresses really highly dimensional data. You do the 20 different columns we have into a two dimensional plane. And what I like to do is really just kind of compare the shape of the outfit sets here and make sure that they're similar, right? So this shows that we didn't overfit and kind of focus on a few, learning a few models, rather we would like to see kind of a uniform distribution across both different data sets. From there you have the ability to dive in on a field by field level. And look at the example here, available rooms in the hospital or admission deposit. And we're going to look at here is the distributions of elements we're seeing in the synthetic dataset versus the training set that it was based upon. So this really gives you a high confidence view before you start to use your model to generate large amounts of data about the quality of that model, and how well it would work for your use case, whether you're doing downstream ML, or you're seeking to balance dataset or a creative reproduction environment. So you're happy with that, let's go ahead and take a look at the data proofing we did.", "Alex (07:21):", "So from here we see our data preview gzip. And what we have here looks like a really nice synthetic data set. That's created, we generated 5,000 records. Next we'll go back to our documentation and we'll generate some more data. So now we'll use a Gretel records generate command. We'll also point it at that model_data.json that we created earlier. So go ahead and look at this command again, this time we're using Gretel records generate, we're passing for the model ID, we're passing it the file that we stored the model ID to. Running it in cloud, it's going to generate 5,000 records. You're going to see the task being sent to the cloud, cloud worker being identified. Now you have it down generated additional 5,000 records, the model can generate an unlimited amount of data. For next steps going through this tutorial, I would suggest fine tuning the configuration that we used earlier to see how you can get the maximum synthetic data quality score for your use case.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/how-to-generate-synthetic-data-with-gretel", "page_title": "How to generate synthetic data with Gretel - Video", "headers": ["How to generate synthetic data with Gretel", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this tutorial, Alex teaches you how to generate synthetic data with Gretel.", "Alex (00:06):", "Go ahead and log into the console using a Google or a GitHub address. From here, we can either download an API key and the console to get started, or we can use the console workflows generate synthetic data. I like the console workflows is a good way to kind of dip your toe in with synthetic data or see how a different data set will perform. From here, you have the option of just dragging in a sample CSV to make things a little bit easier to visualize here. I'm going to go ahead and load a CSV from my own computer.", "Alex (00:38):", "Let's go ahead and take a look at this. So, we'll open this up in Numbers. Take a look. So, you see some pretty nice mixed categorical numerical data, things like that. This is a reference data set that was released by the U.S. Census that's used commonly for machine learning tasks. So, it's a good data set to start with. We're going to see if we can create a synthetic version of this dataset, looks just like it has the same dimensions, same insights, same look, and feel, but... In which case, none of the records that we're looking at here have actually been duplicated.", "Alex (01:10):", "So from here, we'll go ahead and choose file. We'll load that same adult income data file, that we were looking at a second ago. I'm going to go ahead and upload this to Gretel. One of the neat things about Gretel is you have the option to pick where your data sets run. Using the CLI, you can choose to run locally on a worker where your data never leaves your environment, or you can go ahead and have the Gretel cloud compute this synthetic data for you.", "Alex (01:35):", "We've uploaded our data set here, let's chose generate synthetic data. Go ahead and click continue. So, here we can see, as we saw a second ago, 15,000, 14,000 records in 15 fields. From here, we can go ahead and click continue. It's that easy to go ahead, Gretel trained model and generated data set for us.", "Alex (01:54):", "What I'm going to do is go ahead and customize this, because I know this is a workflow that a lot of data scientists like to have a little bit of control over. There are a ton of different fields that can be experimented with in the Gretel of synthetic data model. So, you can go here to our docs and click on synthetics and learn all about the different options.", "Alex (02:13):", "What I'll do here is I'm going to go ahead and browse templates on GitHub, and we're going to downloaded and modify our own configuration file. So, click browse. This takes you right to your GitHub, and here you can see a set of different templates that we have for different use cases with synthetic data. Whether you're working with complex or free texts, if you're training on tweets or chat records or things like that, that have raw text in it. If you want to train with differential privacy, each one of these are different configurations that we can use.", "Alex (02:41):", "I'll start with the default and just make a quick modification to it. Here, we can see the default configuration settings. Let me go ahead and grab these and copy them. Once again, if you're just getting started for the first time here, you can just click use the recommended settings, which uses default and get started. I'm going to go ahead from here, open up a terminal. I cut and paste the records in there. And I'm going to make one change to this setting. I'm going to have it just run for 50 epochs.", "Alex (03:16):", "One of the things about Gretel, it uses early stopping. So the number of epochs, which is the amount of times that we train the neural network on the data before the model gets finalized, is automatically set. When it starts/stops essentially gaining loss, it will go ahead and terminate the model for you. So, this setting, you should never have to use it's just a good example.", "Alex (03:39):", "We'll go back, and we'll choose our file now. So, we just created a default.yaml. We'll go ahead and select that. We'll continue. What's happening, now? It's loading the example data set that we uploaded here. We can see a sample of that dataset. It's going to take that. It's going to take a look at our training configuration that we just uploaded. We can see all the different settings inside of here. So, 50 epochs as we configured. Other settings here, that you can change based upon your use case. We've got a lot of documentation on that as well. From most datasets though, you should be good to go with one of her default configurations.", "Alex (04:17):", "What's it's doing right now is it's going ahead and reaching out to the Kubernetes cluster on Gretel's back end here, and it's starting a worker. Big difference between beta-2 and beta-1 that we've noticed here, is moving from requiring you to run your own model or a backend using TensorFlow and a GPU for acceleration, to running this automatically on demand on behalf of a training job when it comes in. So, we can see we are going ahead and starting the process. It's found a worker. It's beginning ML model training, and it's going ahead and train the model now. This will take a few minutes.", "Alex (04:52):", "I can now see that the model completed training. So, it did 49 EPoX or passes over the data. As you can see here, the accuracy was increasing and loss was decreasing, as well as the validation set, which we use to prevent model over fitting. Accuracy increasing. Loss decreasing. Accuracy here about 90%. Loss here at around 0.40. Anecdotally, anything less than one for your loss, it's usually an indication that you have a good model. The model's learned your data and will be able to recreate it pretty well.", "Alex (05:52):", "Next, what's happening is the model here is generating a sample set of 5,000 records. We use this to essentially validate the models, learn the model well, the synthetic data set that it's trained on. And we'll use that to create a report that compares the training data that was originally trained on to the synthetic data you generate, and make sure that the model is able to learn and maintain the different correlations and insights that existed in the original data.", "Alex (06:16):", "The output of this process is a neural network model that can be used to generate as much data as you want, and we'll walk through doing that as well. Here, we can see that it generated 5,000 records. 124 of those records does not pass validation. So, in addition to the neural network, which learns the structure of your data, it does a set of validation checks, essentially making sure that each value that it sees in a column makes sense based on what it saw on the input data.", "Alex (06:44):", "After that it uses the 5,000 records that we generated to create a synthetic data quality report. This is answering the question of how synthetic is my synthetic data and will it work well for my use cases. Here you can see we have an excellent model. What that means for us kind of anecdotally, or for many customers, is that this model can be used to create synthetic data sets that could be used to train in the downstream machine learning task, or it could be shared with data scientists across your organization. Things where people are looking not only for the size and the shape of the data to look accurate, but they want to have the same insights in the data as the original source data.", "Alex (07:20):", "Here, we see a nice kind of high level overview. You can see the distribution, stabilities, everything lining up and looking good. Also, if you want to dive in and go a level deeper, you can download the synthetic data report and get a deep dive on our data, which we'll go ahead and do now.", "Alex (07:38):", "So, I'll go ahead and open up this synthetic data report for our model. And this is where from a data science perspective or from running a machine learning pipeline, I can really dive in and say, does my data set have the same types of correlations, the synthetic data versus the original training data here. And you can see visually that it appeared to have learned and recreated these correlations very well.", "Alex (07:58):", "Another one that I really liked to look at is the PCA. So the principal component analysis. Essentially what it does is takes highly dimensional data, compresses it down into a 2-D plane where you can look at the distribution of elements across both data sets. And what you look for are similar sizes and shapes between the data that would indicate, not only did it learn the data well, but it recreated even down to the different types of outliers that exist in the original data. It recreated them well.", "Alex (08:10):", "From here, we can do a really per field view, hours per week. And what we're doing here is comparing the synthetic data model, put it outputted towards the original data that was observed. So, essentially what we're doing here is throwing the kitchen sink from a statistical perspective at the synthetic data set we created, comparing it to the original data set, and giving you a high sense of confidence that your model that's been created is going to work well for those use cases you have around, creating mock data or a pre-production environment or sharing with partners.", "Alex (08:10):", "So, our data has been created. We can go ahead and view those initial records as well. So here we're going to go right. It's going to take us to our project view, and we'll look at the model. So when the US adult income data set, looking at models, let's say that we've got a data set that's been created. I'm going to go ahead and generate another 5,000 records. It will be a 80 pound valid data set. You can also download the synthetic data set that we just created, along with our model. That's preloaded 5,000 records.", "Alex (08:10):", "Initiating a new job. We'll spin up a worker, identify a worker, and start creating dataset using this model that we've created where you can look at the distribution of elements across both data sets. And what you'd look for are similar sizes and shapes between the data that would indicate, not only did it learn the data well, but even down to the different types of outliers that existed in the original data, it recreated them well.", "Alex (08:25):", "From here, we can do like a really per field view hours per week. And what we're doing here is comparing the synthetic data model, put it outputted towards the original data that was observed. So, essentially what we're doing here is throwing the kitchen sink from a statistical perspective at the synthetic data set we created, comparing it to the original data side, and giving you a high sense of confidence that your model that's been created is going to work well for those use cases you have around creating mock data or a pre-production environment or sharing with partners.", "Alex (08:57):", "So, our data has been created. We can go ahead and view those initial records as well. So here we're going to go right. It's going to take us to our project view and we'll look at the model. So when the US adult income data set, looking at models, let's say that we've got a data set that's been created. I'm going to go ahead and generate another 5,000 records. That will be a valid data set. You can also download the synthetic data set that we just created along with our model. That's pre-loaded 5,000 records.", "Alex (09:27):", "I'm initiating a new job. We'll spin up a worker, identify a worker, and start creating dataset using this model that we've created. Looks like the data sets completed. We can go ahead and download that data set. Take a look at our newly created synthetic dataset. We'll open it up numbers as well. You can see, we have a nice looking data set here that very closely matches the original data that was training.", "Alex (11:32):", "On the model view, you can have as many models as you want to per project. And go ahead and look at the data source that was originally uploaded along with the model. You can also view the model configuration that was used to train that model originally. Like within here, I can pick config. You can Also invite new members to our project. So, you can invite via email address. This will invite other people to collaborate and be able to use your model to generate data.", "Alex (11:59):", "When you're done, if you choose to delete your data, you can simply click here and choose nuke my project and delete the data all the way through. If you hold down the button, it'll go ahead and delete your data.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/license/contributor-license-agreement", "page_title": "Contributor License Agreement", "headers": ["Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": [" ", "In order to clarify the intellectual property license granted with Contributions (as defined below) from any person or entity, Gretel Labs, Inc. (\u201c", "\u201d) must have a Contributor License Agreement (\u201c", "\u201d) on file that has been signed by each Contributor (as defined below), indicating agreement to the license terms below. This CLA is for Your protection as a Contributor as well as the protection of Gretel.ai and its users; it does not change Your rights to use Your own Contributions for any other purpose.", " This CLA allows either an individual or an entity (a \"", "\") to submit Contributions to Gretel.ai, to authorize Contributions submitted by its employees or agents to Gretel.ai (in the case of a Corporation), and to grant copyright and patent licenses thereto.", " Please read this document carefully and provide the Contributor information requested below before signing and keep a copy for Your records. If You have questions about this CLA, please contact Gretel.ai at ", ".", " By signing below, You accept and agree to the following terms and conditions for Your present and future Contributions submitted to Gretel.ai. Except for the license granted herein to Gretel.ai and recipients of software distributed by Gretel.ai, You reserve all right, title, and interest in and to Your Contributions.", "\u200d", "\"", "\" (or \"", "\") shall mean you as an individual (if you are signing this CLA on your own behalf) or the Corporation (if the person signing this CLA is acting on behalf of the Corporation) that is making this CLA with Gretel.ai. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor and this CLA shall apply to Contributions by all such entities. For the purposes of this definition, \"", "\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.", "\u200d", "\"", "\" shall mean the code, documentation or other original work of authorship, including any modifications or additions to an existing work, that is intentionally submitted by You to Gretel.ai for inclusion in, or documentation of, any software or other products owned or managed by Gretel.ai (the \"", "\"). For the purposes of this definition, \"", "\" means any form of electronic, verbal, or written communication sent to Gretel.ai or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Gretel.ai for the purpose of discussing and improving the Work, but excluding any communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"", "\u200d", " ", "Subject to the terms and conditions of this CLA, You hereby grant to Gretel.ai and to recipients of software distributed by or on behalf of Gretel.ai a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works. If moral rights apply to the Contribution, to the maximum extent permitted by law, You waive and agree not to assert such moral rights against Us or our successors in interest, or any of our licensees, either direct or indirect.", "\u200d", " ", "Subject to the terms and conditions of this CLA, You hereby grant to Gretel.ai and to recipients of software distributed by or on behalf of Gretel.ai a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) was submitted.", "\u200d", "You represent that You have sufficient rights and are legally entitled to grant the above licenses with respect to each Contribution. If this CLA is being made on behalf of the Corporation, the individual signing this CLA represents that he or she is authorized to do so on behalf of the Corporation to enter into this CLA, and the Corporation represents further that each employee or agent of the Corporation designated by it is authorized to submit Contributions on behalf of the Corporation. If You are making the Contribution as an individual, and if Your present or past employer(s) or other entities for whom you have performed work has rights to intellectual property that You create that includes Your Contributions, You represent that You have received permission to make Contributions on behalf of that employer or such other entity, that Your employer or such other entity has waived such rights for Your Contributions to Gretel.ai , or that Your employer or such other entity has executed a separate CLA with Gretel.ai.", "\u200d", " You represent that each of Your Contributions is Your original creation (see section 8 for submissions that are not Your original creation).", "\u200d", " Based on the grant of the rights in sections 2 and 3, You acknowledge and agree that we may license Your Contributions under any license, including copyleft, permissive, commercial, or proprietary license.", "\u200d", " You are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. Unless required by applicable law or agreed to in writing, You provide Your Contributions on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.", "\u200d", " Should You wish to submit work that is not Your original creation, You may submit it to Gretel.ai separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\".", "\u200d", " You agree to notify Gretel.ai of any facts or circumstances of which You become aware that would make these representations inaccurate in any respect.", "\u200d", " You acknowledge and agree that Your Contributions and information about Your Contributions may be maintained indefinitely and disclosed publicly, including Your name and other personal information that You submit with Your Contributions. Subject to the foregoing, the information You provide in this CLA will be maintained in accordance with Gretel.ai\u2019s Privacy Statement (", "). Questions regarding Gretel's Privacy Statement or information practices should be directed to ", ".", "\u200d", "This CLA is the entire agreement between the parties, and supersedes any and all prior agreements, understandings or communications, written or oral, between the parties relating to the subject matter hereof and may be assigned by Gretel.ai without Your consent. This CLA and any action related thereto will be governed by the laws of California, USA without regard to its conflict of laws provisions. Exclusive jurisdiction and venue for actions related to this CLA will be a court of competent jurisdiction in City and County of San Francisco, California and both parties consent to the jurisdiction of such courts with respect to any such actions. If this CLA is modified or updated by Gretel.ai, Gretel.ai will notify You of such updates on Gretel.ai\u2019s GitHub portal or will notify You via Your registered email address or via Your GitHub account (if applicable). If You do not reject the update and send Your rejection to Gretel.ai within five business days after Gretel.ai notifies You of the change to the CLA then You will be deemed to have accepted the changed terms to the CLA. The parties agree this CLA may be executed and delivered by electronic signatures and that the signatures appearing on this CLA are the same as handwritten signatures for the purposes of validity, enforceability and admissibility.", "\u200d", " ", "Contributor Information:", " ", "Name of Individual or Corporation: ________________________________________________________________", "Mailing address: _______________________________________________________________________________", "Country: _____________________________________________________________________________________", "Telephone: ___________________________________________________________________________________", "E-Mail: _______________________________________________________________________________________", " ", "Accepted and Agreed by Contributor:", "Signature: __________________________________ ", "Name: _____________________________________", "Date: ______________________________________", " "]},
{"url": "https://gretel.ai/videos/generate-synthetic-data-in-60-seconds", "page_title": "Generate Synthetic Data in 60 seconds - Video", "headers": ["Generate Synthetic Data in 60 seconds", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Alex introduces you to the Gretel Console in 60 seconds.", "Alex (00:00):", "We're going to walk through an example of creating synthetic data with Gretel in 60 seconds. We'll go ahead and name our project, and drag our sample CSV directly into the upload files, or pick your own CSV. Choose Generate Synthetic Data. Use the recommended configuration. You can see that the URL is pointing to you right now. Click Train. We can see that the model finished training after 39 passes over the data. The loss of being at about 0.44, and the accuracy being at about 90%, indicating we have a pretty good fit with the model. It went ahead and generated 5,000 records, and then generated a report. This report tells us how good of a job the model did learning the distribution of the insights that the original data. 94 indicates that we have an excellent report here or synthetic model. Go ahead and click on View Records. We can go ahead from here and download the example of 5,000 records that were created.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/redact-pii-from-data-with-the-gretel-cli", "page_title": "Redact PII from data with the Gretel CLI - Video", "headers": ["Redact PII from data with the Gretel CLI", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this tutorial, Alex walks you through how to redact PII (personally identifiable information) from a dataset using the Gretel CLI.", "Alex (00:04):", "Hey, I'm Alex. I'm a co-founder at Gretel. And today we are going to use the Gretel CLI to walk through identifying and transforming or redacting sensitive data. Let's go ahead and jump on in.", "Alex (00:18):", "First thing I want to do is to go ahead and log into the Gretel console. From here, you can generate an API key that we can use to connect the CLI that's running on our endpoint or our remote machine here to the Gretel service. Go ahead and create that.", "Alex (00:34):", "Next thing you want to do from your terminal window, if you haven't done that already, is go ahead and install the Gretel client. pip install gretel-client. Next thing I want to do is run Gretel configure. Very similar to the AWS CLI we can go ahead and accept most of the defaults here. We're going to go ahead and build in the cloud. You have the ability to deploy as a container and run locally, if you wish.", "Alex (01:02):", "We'll go ahead and grab our API key here one more time. Paste it in. Select default project. Looking good.", "Alex (01:16):", "From here we'll go ahead and follow the documentation on the CLI tutorial for redacting sensitive PII. Okay. And I start to restore things in. We're going to create a configuration file as our first step that tells our APIs how we wish to train the model.", "Alex (01:47):", "We're going to name this redact_pii.yaml, And go ahead and take a look through this. So we are specifying a model creation, a transform type. What that means is that we're going to be mutating the record. So not only labeling data, but transforming the labels that we found, giving it a quick descriptive name called remove PII. What we're doing here is setting a set of rules. So we're searching for person name, credit card number, and so on, including a custom regular expression we find down here at the bottom. Whenever any of these are found, we're going to transform that data. When possible, we used a fake data generator. So we'll replace a name with a name or an address with an address. If no fake name generator is possible, we will simply redact that data with the character. This is a big use case for developers that are training machine learning models on customer chat records or things like that, where you want to maintain the semantics of the data, but you don't want to use any actual customer-sensitive information.", "Alex (02:43):", "Let's go ahead and save this. Next thing it's going to ask us to do is to create a pii.csv. So we can see this. We'll just copy this into our new file. Here we see ID names, emails, phone numbers, and so on. So we're going to really measure our ability to detect and to transform this data. And the tutorial, the next step here is we're going to use the Gretel CLI to create a new project. You can think of a project as a holding place for your models and your data that are stored in the Gretel cloud. Let's go ahead and copy this in. Then we're going to create one called redact PII. We're going to set this is as our default project.", "Alex (03:30):", "Okay. Since it's our default project, we no longer need to specify it. So we will start the first process here, which is to create a Gretel model here based on training on our input data. So I'll go ahead and copy this in creating a model. We're using our redact_pii.yaml. It's going to examine our input file here and essentially create a transformation model you can use for subsequent transformations of similar data types. We're going to use the cloud. We're going to output the Model ID to something called model_id.json which makes it really simple for us to reference it later. Let's go ahead and kick that off.", "Alex (04:07):", "So from here, it's uploading our data to the Gretel cloud. If you choose to run a local runner and you have Docker installed, it will spin up a local container and run against that. Searching for cloud worker. It's beginning to model training. It should happen pretty quickly with such a small data set. Okay it's done. Take a look at what's been created in our environments. We have a model_id.json. Now that we've trained the model, we can essentially pass into any amount of data that we want to, to the model and have it both labeled and then transform the data.", "Alex (04:59):", "So it's going to take a look this. Now we're going to use the Gretel records command, going to tell us to transform the data using Model ID, model_data.json, So we don't have to memorize the ID here. Using input data pii.csv and telling it to output locally.", "Alex (05:42):", "Okay. Project is done let's go ahead and look at the data. It downloaded the results here are transformed data set to something called data.gz. So we're going to have some commands here to make it easy to visualize the results. And here we can see our transformed data set looks significantly different than the original data here. So we see fake IDs being put in here. We see email addresses, fake phone numbers, fake visas, fake social security numbers.", "Alex (06:19):", "We did not have a faker for the last column here, this user ID. So essentially the transformed results from this pii.csv have been turned into X's since we didn't have a fake data generator for that. If you wish to change this, one of the things you want to experiment with is using a fake data generator for all or using redact with all. So you could simply change your redact_pii.csv Here, and instead of using faker, you could do redact for all.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/improving-massively-imbalanced-datasets-in-machine-learning-with-synthetic-data", "page_title": "Improving massively imbalanced datasets in machine learning with synthetic data", "headers": ["Improving massively imbalanced datasets in machine learning with synthetic data", "Our imbalanced dataset", "The metric trap", "Augmenting fraud examples with synthetic data", "Synthetic Minority Oversampling Technique", "Gretel synthetics with concepts from SMOTE", "Examining our synthetic dataset", "Boosting our training dataset with synthetic data", "Final remarks", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Handling imbalanced datasets in machine learning is a difficult challenge, and can include topics such as payment fraud, diagnosing cancer or disease, and even cyber security attacks. What all of these have in common are that only a very small percentage of the overall transactions are actually fraud, and those are the ones that we really care about detecting. In this post, we will boost accuracy on a ", " by training a generative synthetic data model to create additional fraudulent records. Uniquely, this model will incorporate features from both fraudulent records and their nearest neighbors, which are labeled as non-fraudulent but are close enough to the fraudulent records to be a little \u201cshady\u201d.", "For this post, we selected the popular \u201c", "\u201d dataset on Kaggle. This dataset contains labeled transactions from European credit card holders in September 2013. To protect user identities, the dataset uses dimensionality reduction of sensitive features into 27 floating point columns (V1\u201327) and a Time column (the number of seconds elapsed between this transaction and the first in the dataset). For this post, we will work with the first 10k records in the Credit Card fraud dataset- click below to generate the graphs below in Google Colaboratory.", "Let\u2019s see what kind of performance we can get detecting fraudulent records with a cutting edge ML classifier. We will start by dividing our dataset into a Train and Test set.", "Wow, 99.75% detection. That\u2019s awesome, right?! Maybe- looking at overall model accuracy just shows how well the model performed across the entire set, but not how well we did on detecting fraudulent records. To see how well we really performed, print a confusion matrix and accuracy report.", "In this section we will focus on how we can improve model performance and generalization for the Fraud records, by using ", " to generate additional examples of fraudulent records. Let\u2019s start with what we want to accomplish- our goal is to generate additional samples of fraudulent records that will help our classifier generalize and better detect the fraudulent records in our test set.", "A popular technique in the data science community to achieve this is called SMOTE (", "ynthetic ", "inority ", "versampling ", "chnique), described by Nitesh Chawla et al. in their 2002 ", ". SMOTE works by selecting examples from the minority class, finding their nearest neighbors in the minority class, and effectively interpolating new points between them. SMOTE does not have the ability to incorporate data from records outside of the minority class, which in our examples may include useful information- including fraudulent-like or mislabeled records.", "Having only 31 examples of fraudulent data in our training set presents a unique challenge for generalization, as ", " utilizes deep learning techniques to learn and generate new samples, which traditionally require a lot of data to converge. Open the notebook below to generate your own synthetic fraud dataset for free with Google Colab.", "By borrowing SMOTE\u2019s approach of finding the nearest neighbors to to the fraudulent set and incorporating a few of the nearest neighbors from the majority class, we have the opportunity both to expand our training set examples, and to incorporate in some learnings from our fraudulent-like (", ") records. This approach will not require any changes to ", ", we\u2019re just intelligently picking the dataset from the fraudulent + nearest positive neighbor (shady) records. Let\u2019s get started!", "To build our synthetic model, will use Gretel\u2019s new ", " defaults with a few parameters set below to optimize results:", "Now let\u2019s take a look at our synthetic data and see if we can visually confirm that our synthetic records are representative of the fraudulent records that they were trained on. Our dataset has 30 dimensions, so we will use a dimensionality reduction technique from data science called Principal Component Analysis (PCA) to visualize the data in 2D and 3D.", "Below we can see our training, synthetic, and test datasets compressed down to two dimensions. Visually, it looks like the 883 new fraudulent synthetic records may be very helpful to the classifier, in addition to the 31 original training examples. We added the 7 test-set positive examples (where our default model mis-classifies 3/7, and we\u2019re hoping that the augmented synthetic data will help boost detection.", "From what we can see in our graph, it appears our synthetically generated fraudulent examples may be really useful! Note what appear to be a false positive examples near the Training Negative set. If you see a lot of these examples, try reducing the NEAREST_NEIGHBOR_COUNT from 5 to 3 for better results. Let\u2019s visualize the same PCA visualization 3 dimensions.", "Looking at the datasets above, it appears that boosting our minority set of fraudulent records with synthetic data may help significantly with model performance. Let\u2019s try it!", "Now we reload the train and test datasets, but this time augment our existing training data with the newly generated synthetic records.", "Train XGBoost on the augmented dataset, run the model against the test dataset and examine the confusion matrix.", "As we have seen, it is a hard challenge to train machine learning models to accurately detect extreme minority classes. But, synthetic data creates a way to boost accuracy and potentially improve models ability to generalize to new datasets- and can uniquely incorporate features and correlations from the entire dataset into synthetic fraud examples.", "For next steps, try running the notebooks above on your own data. Want to learn more about synthetic data? Check out Towards Data Science articles mentioning Gretel-Synthetics ", " and ", ".", "At ", " we are super excited about the possibility of using synthetic data to augment training sets to create ML and AI models that generalize better against unknown data and with reduced algorithmic biases. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Follow us to keep up on the latest trends with synthetic data!", "Interested in training on your own dataset? ", " is free and open source, and you can start experimenting in seconds via ", ". If you like gretel-synthetics give us a \u2b50 on ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/deep-dive-on-generating-synthetic-data-for-healthcare", "page_title": "Deep dive on generating synthetic data for Healthcare", "headers": ["Deep dive on generating synthetic data for Healthcare", "Generating synthetic EHR data", "Easy: Similarity of value distributions per column", "Hard: Maintaining insights across fields", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Sharing data safely is one of the biggest challenges in the healthcare industry today. For hospitals and health organizations, being able to compare and contrast new patient data with other medical organizations in their area and across the world can help doctors quickly diagnose patients and provide the best treatment possible.", "In this post, we will deep dive on training Gretel\u2019s open-source, ", " to generate ", " (EHR) that protect individual privacy, while capturing the key statistical insights from the original source data. For our example dataset we\u2019ll be working with information exported from ", ", a top EHR system used by hospitals.", "Our dataset contains 27,963 de-identified emergency room discharge summaries from an ER over the course of six months. There are 22 columns of data in CSV format, including date, arrival time, demographics, chief complaint and primary diagnosis (free text). Check out our ", " to try generating synthetic EHR records with your own dataset. Below is the configuration we\u2019re using for this post.", "From the training results above, we can see that even with differential privacy enabled, our model was able to quickly learn the semantics of the training set, getting to 70%+ accuracy within 10 epochs of training. For the remainder of this post, we\u2019ll focus on the question of \u201cHow useful is my synthetic dataset\u201d? We\u2019ll do this by exploring the similarities and differences between our artificially generated synthetic dataset (with guarantees that no actual health records were memorized or replayed), vs. the original source dataset.", "The simplest, most naive approach to compare our original and synthetic data is to compare the distributions of each column. Ideally, the distributions for both categorical and numerical datatypes should match. Let\u2019s look at this for an example field Patient.Class .", "\u200d", "At first glance the field distributions for our example field Patient.Classlook pretty close, with the synthetic data model having a slightly higher propensity to generate \u201cInpatient\u201d records than \u201cEmergency\u201d records vs. the source data. Let\u2019s try something harder, like recreating the times that patients check in to the ER.", "How do we look at this quantitatively? In statistics, we can leverage the ", " (also known as KL Divergence or relative entropy), which is a measure of how one probability distribution is different than a second. Fields with a high KL divergence might not be ideal for synthetic data, as they might (for example) be highly random to begin with.", "With synthetic data what we really want to capture are the statistical insights, or relationships between fields. For a lot of synthetic data use cases, such as data sharing or training machine learning models, maintaining the \u201cinsights\u201d in a dataset is even more important than the per-column distribution above.", "We can express these relationships and insights more quantitatively as the ", " between field values. In human language, the correlation is the measure of the relationship between two values. And this correlation can be positive or negative. For example:", "Formalizing this mathematically, we can use correlations such as Pearson\u2019s correlation coefficient, which results in a value in the range [-1, 1].", "\u200d", "Below is a correlation matrix across the fields in our original training set. In building a synthetic dataset, we seek to learn and replay these correlations as much as possible.", "\u200d", "Running correlations against our original training dataset, the Gretel-generated synthetic dataset, and a \u201cNaive\u201d implementation of synthetic data, we can quantify what insights from the original data have been retained by our model.", "A naive approach to creating a synthetic dataset would be to model the value distributions per column, and then shuffle their distributions, with the goal of enabling per-column statistics but with some privacy such as prevent re-identification attacks. This approach has limited utility, as the relationships between fields get lost.", "\u200d", "Utilizing ", "during training, Gretel synthetics can re-create many of the same distributions and insights from the source data, while guaranteeing that individual health records are not memorized or replayed.", "In the matrix below, we can visually see the similarities between the correlation matrices for the original and synthetic datasets. To characterize overall model quality, we can calculate the Root Mean Squared Error (RMSE) For our EHR dataset, the squared error is only 0.093 even while using aggressive differential privacy guarantees (\u03b5 = 0.606 and delta = 1.710e-07) to protect user privacy!", "\u200d", "We\u2019re super excited about the possibility of synthetic datasets to enable health organizations to safely share data with differential privacy guarantees. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Follow us to keep up on the latest trends with synthetic data!", "Interested in training on your own dataset? ", " is free and open source, and you can start experimenting in seconds via ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/use-ai-to-create-synthetic-data-from-a-dataframe-or-csv", "page_title": "Use AI to Create Synthetic Data from a DataFrame or CSV - Video", "headers": ["Use AI to Create Synthetic Data from a DataFrame or CSV", "Video description", "Read the blog post", "Walkthrough: Create Synthetic Data from any DataFrame or CSV", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this tutorial, Alex walks you through how to use AI to create synthetic data from a DataFrame or CSV using Gretel.", "Today, we're going to walk through an end-to-end example of generating synthetic data using Gretel.ai services. First, we'll go ahead and sign in. You can go to console.gretel.cloud, and you can sign in with a developer ID or GitHub address there. From here once we're signed in, we'll go ahead and select new project. Select blank project. You can name your project if you want to.", "Once you've created your new project, we'll go to transform. From here, we have a set of Python Notebooks that help you address popular use cases that we see around using Gretel.ai. For example, today we're going to launch a notebook called Create Synthetic Data from a CSV or DataFrame. This opens us up into the Google Colab environment, which is a hosted notebook environment from Google. It's notable because it gives you free GPU access, which is really nice for training machine learning models. You also have the option if you wish of going ahead and downloading the notebook, or just downloading the raw Python to run in your own environment.", "Once again, heavily recommended you run along with the GPU. We'll go ahead and click run all, and we'll go ahead and walk through what's happening here as we train a model and generate synthetic data. What's happening first is the notebook environment here is installing our open source packages. You can see the Gretel.ai client, the Gretel.ai synthetic data library and pandas are all being installed.", "Next, it's going to ask us for an API key to authenticate for some of our premium services. We go back to the Gretel.ai console, choose integration, show API key, grab the API key, and we'll go back to the console here. To put an API key, we'll go ahead and download those premium packages into our environment. It will be accessing the SDKs for the rest of this exercise. The next step here is creating our CSV dataset. The ideal parameters to pass to our synthetic data library to create an artificial dataset are CSV, JSON, or pandas dataframe.", "Here, we're going to go ahead and load a dataframe. We're using a popular healthcare analytics dataset from Kaggle. You can see here we're going to pull back 10,000 records. Read that using pandas into a CSV. Or sorry, into a pandas dataframe, that will pass to our synthetic data training library. Here, you can see it downloaded the CSV, loaded it into a dataframe, and we can take a look at it here. We have 10,000 rows, 18 columns. Really varied kind of data. So, this is a good example to use when training our synthetic model.", "You have some integer data, some categorical data, floating points, dates, age ranges, things like that. All of these get learned. And really, our goal going through this exercise is to create another dataset that has a very similar distribution to this original dataset, but has none of the duplicated records. You get really kind of nice privacy benefits using this. Going through here, we see a configuration template. Tons of options here. This is really minimized. This is the bare minimum that you need to auto-generate a synthetic model.", "If you want to go ahead and customize this yourself for your particular use case, you can simply go to our documentation. If you go to our open source library for synthetics, there's a link that you can grab right there. Let's go ahead and take a look at documentation for configuration. We are using a TensorFlow backend here, because it's the default model and it supports differential privacy-based training. We'll go ahead and look at the TensorFlow config. This shows us the options we have of setting here where we can configure anywhere from the number of epochs to the learning rate to the complexity of the RNN that's being used on the backend, and whether we'd like to use TensorFlow differential privacy or not.", "In this mode, we're going to run without different privacy. Just use the default settings to generate a synthetic model. Here, we defined a vocabulary size, passed our tokenizer of 20,000, and we can see it going ahead and starting to build our model. We define our model here. We pass in a few parameters here. We're going to tell it to build a validator. What is a validator? A validator is code that runs. It's part of our premium package that takes a look at different columns across columns inside of your data. It looks at the ranges that are expected of the columns, whether it's categorical or floating, and making sure that the neural network outputs ranges that are seen based on what we saw the training data. Really, it gives you confidence that the data you're creating matches the shape and the distribution of your original data.", "We have early stopping configured here to prevent overfitting from happening, but you see it here training the model. See the loss dropping pretty significantly. In my experience, anywhere less than typically 0.7, 0.8, you start to get pretty good quality synthetic data. You can see it very quickly converged on a good solution here. We don't want this to overfit. We also want it to run really quickly. So, what I'm going to do here is go ahead and stop this training after 20 epochs, and we'll go right over to generating data. Let's let it finish up or 20 epochs. Ought to be a good enough solution to get reasonable quality data. Let's stop there. Okay.", "I went ahead and stopped that. Here, we're going to tell it to generate ... let's just have it generate a thousand rows. This happens really quickly, and we'll continue running. Here, you see two different fields. It's essentially loading the model right now, and we've told it to generate a thousand records for us. You see two different counters here happening at the bottom. Valid records are records that are being generated by a neural network that are passing validation. They're passing those inferred parameters that we learned looking at the training set. You can see it here very quickly building out those thousand records.", "The next thing we'll do is preview that and take a look. Does this data look sane? Does it look like it matches our input? Here, you can see ... at least to initial inspection, it looks very good. Here you see, once again, we created a thousand rows. If we ask it to create 18 columns, we can see the columns even down to the age bracket seemed to very closely match what we had in our original data. To get that additional set of confidence ... so, you want to understand the distribution of data, and did our model actually learn those really cool correlations and things like that.", "The next thing we do is generate a detailed report. As you can see here, this detailed report shows you distributions across every column between both the synthetic and the training datasets. And then also some really cool high level statistics. Really starting off with how many of the original training lines were duplicated in synthetic dataset. Here, we don't see any. And then we have a bunch of distance metrics that we can walk through here as well, looking at the distance metrics and the number of unique values, and distribution between the original dataset and the synthetic dataset we created.", "One of my favorite ones to look at right here is the correlation graph. This is a column by column comparison of the correlations that existed in the original dataset, and how will they be replicate in the synthetic dataset. Here, this looks a little bit like a Minecraft sword. Essentially what we're trying to do is make sure that the correlations we see existing in this first dataset, for example, the number of stay ... it appears to be highly correlated with the number of visitors that you have, which is interesting. You want to make sure that gets replicated in the synthetic dataset.", "The view below that is very similar here. It's just a subtraction of these two. Anywhere we see a color change here would indicate that there was a correlation that existed in one of the datasets that did not exist at the other. You can give us an indication if there's anything we need to dive into. Once again, we don't really see anything here yet. Once again, we can go through and look at the individual distributions, and see how well they match up. We only generate a very limited amount of data. We generated a thousand rows versus 10,000 rows of training data. So, you can't expect it to have perfect stuff here, but the correlation is actually in the distributions between these different categorical fields. And numeric fields actually appear to be very good, giving us an indication that we've got a healthy synthetic model we can use to generate data for our use cases.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/how-to-set-up-a-box-for-deep-learning-with-gretel", "page_title": "Setting up your Gretel environment (1/4) - How to set up a VM for deep learning - Video", "headers": ["Setting up your Gretel environment (1/4) - How to set up a VM for deep learning", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In part 1 of this video series on setting up your Gretel environment, Alex walks through how to set up a VM and get started with deep learning and Gretel.", "(00:05): Hey, this is Alex from Gretel. Today, we're going to walk through a complete end-to-end example of setting up a deep learning box for training synthetic models inside your own environment. Best place to start here is with the Gretel doc sites. We just go to docs.gretel.ai. We're going to follow the instructions in the environment setup guide.", "(00:23): So first thing you'll want to do is set up a machine somewhere that's connected to an NVIDIA or deep learning compatible GPU. Options are GCP, Azure, AWS. Your choice. Today, we'll be setting up a box that I configured running a base Ubuntu 18.04 instance inside GCP. Go ahead and SSH in. I'll see if I can make the font here a little bigger.", "(00:50): Great. We have our base instance here. The first thing we want to do is set up Docker. So we'll use Docker to run our deep learning containers in the background. This script here will go ahead and set up Docker to run on our instance.", "(01:32): Great, Docker's up and running. Next thing we'll do is make sure we have the user added to the group, set up correctly. And we'll make sure that the user has correct privileges to run. To confirm whether Docker's running next, we're going to run Docker PS. In Ubuntu you need a run sudo first. So, sudo Docker. Here, we can see that Docker service is running correctly, even though we don't have any containers running at the moment.", "(02:06): Ready for the next step. So since this is a little different, we're configuring our deep learning box, we're going to skip past the API setup instructions for now. And we're going to go down to the GPU plus Docker configuration. So here we're going to add the extra drivers necessary for Docker to be able to use the machine's GPU. We've made a simple setup script here, and this is going to install TensorFlow. This will install the correct appropriate matching NVIDIA driver versions in CUDA as well. This process takes a while, so go ahead and have a cup of coffee and we'll come on back.", "(02:41): Looks like the driver installation completed. What we see it doing now is downloading an image from Docker that verifies the ability to interact with the GPU. So the driver's been set up correctly. You can see running the NVIDIA SMI command here inside of the container shows us it does detect and able to talk to the Tesla T4 GPU we have. You can also run the same command from your local instance. So, they're the same, just run NVIDIA-SMI. And now we can be confident that the drivers are set up and running correctly. Jump into the next video on an end-to-end training on a local box here using the Gretel SDK.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/setting-up-your-gretel-environment-4-4-generate-synthetic-data-locally-from-the-sdk", "page_title": "Setting up your Gretel environment (4/4) - Generate synthetic data locally from the SDK - Video", "headers": ["Setting up your Gretel environment (4/4) - Generate synthetic data locally from the SDK", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In part 4 of this video series on setting up your Gretel environment, Alex walks through the steps to generate synthetic data locally from the Gretel SDK.", "(00:05): Hey, in this example, we're going to use Gretel's SDK to train a local model on a box that could be located on your workstation or in the cloud, connected to a GPU. First step, is to go ahead and log into the box here. We need to install... Preferred way to interact with these different models is to use the Jupyter Notebooks. So, we're going to go ahead and install Jupyter. We're going to tunnel through to this instance that sits in the cloud and then we'll use that to train a synthetic model. So, we've got a directory set up with our example input data here.", "(00:30): We've just used in the previous video to train a model using the Gretel CLI. Now we're going to go ahead and set up Jupyter. So, we'll go ahead and pick Install -U Jupyter. That's up and running. We need to set up a way to tunnel into this box. So, we're going to exit out and I'm going to use a kind of a handy shortcut on a Mac or a Lennox box that you can use to automatically create a tunnel into the cloud. So, we'll go ahead and open vi HOME/.SSH/config. ", "(01:00): What you can see here is for my cloud box here I've added a special command here, which is local forward. What that's going to do is going to take the remote box port 8888, which is where Jupyter run by default and forward it to my local machine, my Mac, in this case on port 8899. So, as soon as I tunnel in there now, it will allow that tunnel to exist.", "(01:30): Great. So, let's go to our synthetic data directory and launch our Jupyter Notebook. I've fired up the notebook here by default that uses an authentication token, which you want to grab. This port should now be accessible from my local instance. So, we'll connect at 127.0, web host 8899. It's going to ask for a token, so we'll use the token that was generated when Jupyter first started.", "(02:00): And we are in. So, next step here is we want to upload this example notebook for local synthetics training. I got a couple ways that you could do that. Simplest way, I think, either going through GitHub or you can just go to Colab and save the notebook. So, we'll go ahead and do that here. I'll open up the local synthetics notebook in Colab. We don't want to be running the Colab service, so we will go ahead and just download this file and we'll upload it to our Jupyter Notebook server.", "(02:30): Great. So, go ahead. Connect here. Open up this notebook and we'll take a quick look at what's happening. First, we have some dependencies that we need to install. So, I'm going to go ahead and install these guys. Since we're using a plain Python environment, we probably don't have pandas or a PMO installed yet. Go ahead and do our package installs.", "(03:02): Going through here, we can see what's happening. It's loading from a remote data source. So, it's loading our US adult income data set from S3. You can point this to any URL or file inside of your local environment. It's going to export that to a training CSV. It allows you to make some changes to it if you wish, some basic configuration options that we can modify here as well. It's going to create a project and it's going to go ahead and train a synthetic model on that project by submitting it to Docker and, in which case, we'll be able to grab the synthetic data or the model or the report or any of those types of artifacts from the model as soon as it's done.", "(03:40): So, now we'll go ahead and just do Restart and Run All and this should run through the entire notebook. Here you can see a preview of the data set, so this time we'll be synthesizing the US adult income kind of classic machine learning data set that's used to predict US adult income based on US census data.", "(04:00): Moving down a little bit here, we see it loading our configuration. So, we're loading it from a configuration template that sits in the Gretel cloud and go ahead and take a look at that if you like. The Gretel blueprints repository inside GitHub. Here you can see we're making a few configuration changes. We're telling it only to train for 50 epochs and the name of the data source we want to use is a local file called training_data.csv. I can see it starting up here. ", "(04:30): Validating the configuration is correct. Submitting the model to our local box, which is sitting here in the cloud or GCP for training. Here, we can see it's instantiated a local worker. So, this is training on a remote box currently. It's going to do 50 passes roughly over the data and it'll use that to generate a new dataset. The grand privacy filters on it. I didn't feel a need to remove any data. ", "(05:00): So, here we see a total of 5,000 records generated. It's saving the model, so we can generate an arbitrary amount of new data and also downloading the 5,000 sample records that we just created. So, here is a first look at our sample synthetic CSV. Here, we can look at the report. This doesn't always render correctly inside a Jupyter Notebook, but you can download the HTML or look at that on your local instance to be able to see everything but, from the overview here, we can see that the score was actually very good. ", "(05:35): And we're going to call the model again, just as an example of how to generate more data with a model that's already been trained telling it to generate an additional 100 records. It looks like that. It is just firing up the generator right now and will finish in a few seconds. And there we have it, an additional 100 records and we have trained and generated data on a local box.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/setting-up-the-gretel-sdk", "page_title": "Setting up your Gretel environment (2/4) - How to set up the Gretel SDK - Video", "headers": ["Setting up your Gretel environment (2/4) - How to set up the Gretel SDK", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In part 2 of this video series on setting up your Gretel environment, Alex walks through using a local box and then configures the Gretel SDK to create a synthetic model.", "(00:02): In our last video, we set up a box to run in GCP that is configured for deep learning with Gretel and training models using Gretel's synthetics. In this video, we're going to walk through using the local boxes, it could be something running in GCP, Azure, AWS, or even your local desktop, that's configured to work with the GPU. We will go ahead through setting up the Gretel SDK and configuring it to train a synthetic model. Picking right up where we left off, the first thing we're going to do, we will come back to generating an API key later. The first step for us is to go ahead and set up the CLI. We started with a base instance of Ubuntu and one of the surprises about it is it really ships with a really old version of Python. So let's go ahead and check out and see what we're running here.", "(00:48): Let's see what we have here. It's called Python 3. So here we see a Python 3.6, which is too old to work with the Gretel client, so we're gonna go ahead and upgrade Python first. We have a handy script here that we can run, so go ahead and open this up. Might make some quick changes to the script and we will go ahead and run it. We'll grab the script; copy it in my environment. We'll call it \"set up\". Yeah, I set up Python. Gonna make a quick change here. So here, what we see it doing is we are adding the app repository for Python. We're setting up Python 3-dev -e as the install. Since we don't have a Python binary here, we're going to update this to be Python 3, just like we ran in the previous command. And we're going to point Python 3.8 at Python 3. Go ahead and exit out using VI or your favorite editor. And we'll go ahead and run this script. Here, you can see it connecting to the repositories, downloading the base image for Python 3 and upgrading us to 3.8.", "(02:18): Jump back in and we are ready to start installing our Python packages. So let's go ahead and use PIP, and we're gonna install the Gretel client. And we are installed. Next step, following the instructions here is to run Gretel configure.", "(02:45): We're going to connect to the cloud end point. This just sends usage information back to the Gretel cloud. So API that Gretel.cloud is correct. We're going to use a local runner here since we're going to be using our local box to do the compute, instead of asking the cloud service to do this for us. So type \"local\". Next, it's going to ask for API key. So if you haven't done this already, we're going to go log into the Gretel console and generate an API key Simplest way to get there is to connect to Gretel.ai, click sign in, connect with your favorite service provider. Go over to API key, and we'll export our API key to use here. So copy that to our clipboard. We'll go back to our command line interface here and paste that in. Since we don't have a project yet, we're going to create \"none\" for the default project. So we were mostly set up. Now we're going to create a default project. A project is a space for your models to be stored or essentially for you to enable collaboration with other people and share datasets. We'll go ahead and create a default local project here.", "(04:02): There's a command line here to tell us what to do. I'm going to create a new project and name it. We'll make it our default. Great. We are all set up to start training a model using Gretel.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/podcasts/protecting-data-privacy-within-databases", "page_title": "Podcast - Protecting Data Privacy Within Databases", "headers": ["Protecting Data Privacy Within Databases", "Respecting individual privacy while achieving reliable databases.", "Let\u2019s consider two use cases.", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We all value privacy \u2013 at least to some extent. But some of us want to be famous, and all of us want to connect with friends and acquaintances. We like the convenience from technology that requires our personal information to operate. So we share our personal details in many ways, and our data flows like water down a stream into lakes and oceans, some of which we\u2019d prefer to avoid. And our information becomes a piece of society\u2019s knowledge base. Databases like the U.S. Census have essential purposes, but they\u2019re only reliable and complete if we are comfortable sharing our data. How to respect individual privacy and achieve reliable databases? That\u2019s a challenge!", "In this podcast, Alex Watson, co-founder and CEO of ", ", explains two essential phrases to understand how this can be done. Alex founded a security startup called Harvest.ai, which was acquired by Amazon Web Services in 2016 when he became AWS General Manager and it launched its first customer-facing security offering. Gretel.ai is an early-stage startup that offers tools to help developers safely share and collaborate with sensitive data in real-time.", "Alex explains that privacy is a problem rooted in code, not in compliance. By ", ", the personal data of an individual is separated from the underlying data so that the database where the information is needed comes to it without identifying the individual. The essential information is shared without allowing someone to know which individual\u2019s information it is. While nothing is hack-proof, auto-anonymization eliminates the link between an individual and data about that individual as it moves to another user. Personal privacy is preserved in the transmission and further use.", "The other key phrase to understand is ", ". Data Privacy Detective Podcast 55 offers an introduction to the topic. This phrase means that information within a database has been changed to eliminate the ability to trace back the data to a particular individual. The information is private and individual to a person, but as pieces of data are shared for a purpose, they are not traceable to a specific person. The database user only needs the provided information, not the identity of individuals who contributed to each piece.", "Uber has scooters in some cities available for short-term use. The user provides information that lets Uber bill for the usage (Uber obviously needs to know who is using and paying). When this service was launched, it allowed third parties to determine who was using each scooter and where the person was or had been. By using AI to separate the information about a scooter\u2019s location from a particular user\u2019s identity, Uber could share the information with a city for traffic and other urban planning purposes, while assuring users that their personal identity was not being shared with third parties that had no reason (or business) knowing the user\u2019s personal identity. ", "A second example concerns health data. A medical database was used to develop recommendations about heart disease. But when the database was analyzed to ensure that it was reliably useful, it was found that 68% of the individuals within the database were male. This wrongly skewed the database against females, who have different heart disease risks and profiles from men. A database weighted two thirds to one sex was not properly useful to provide gender-neutral conclusions. In this case, an individual\u2019s identity was not essential to compile a database of equally representative individuals by gender. "]},
{"url": "https://gretel.ai/blog/gretel-is-now-available-to-all-aws-marketplace-customers", "page_title": "Gretel is now available in the AWS Marketplace", "headers": ["Gretel is now available in the AWS Marketplace", "Connect with Gretel", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today, we\u2019re excited to announce that our synthetic data generation tools are now available in the ", ", including APIs to create synthetic text, tabular, time series, and image data. For AWS customers, this means they can now use Gretel to safely train custom AI\u00a0models on their proprietary data, meet data privacy compliance requirements, unlock secure data access and collaborative opportunities, and accelerate their time to market with new products and services. Best yet, they can do all this while drawing down on existing AWS credits and commitments. You can find Gretel in the AWS Marketplace ", ".", "As businesses move quickly to incorporate large language models (LLMs) into their services and scale generative AI applications, the demand for data that is safe, accurate, and timely has never been higher. However, getting and managing access to data is fraught with privacy and security risks. Beyond these concerns, oftentimes the data that enterprise teams need is in short supply, has massive class imbalances, or simply is unavailable. Generative AI has enormous capabilities and potential, but to capture its true value, you need to leverage and safely integrate ", " than just the data that is currently available in the public domain. ", "Gretel's comprehensive solution combines privacy-enhancing technologies with cutting-edge generative models to enable anyone to create high-quality, domain-specific synthetic data on demand. We believe this approach is necessary for teams to safely scale AI applications and are excited to show the community the benefits and opportunities that come with adopting it.\u00a0", "Every day, organizations are discovering new ways to incorporate synthetic data generation into their AI and data governance strategies. While the value varies by use case, Gretel\u2019s work with governments, global brands, and startups across industries, has shown that synthetic data is helping teams everywhere:\u00a0", "If you have any questions about Gretel\u2019s platform or would like to learn more about how synthetic data can help your business, ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/dataset-joinability-at-scale-with-hyperloglog", "page_title": "Dataset Joinability at Scale with HyperLogLog - Video", "headers": ["Dataset Joinability at Scale with HyperLogLog", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["John Myers, CTO at Gretel.ai presents a talk on our streaming implementation of Google's K-HyperLogLog (KHLL) research paper at RedisConf 2020.", "John Myers (00:07):", "Hi everyone. My name is John Myers. I'm the CTO and co-founder of a seed-stage startup called Gretel. And today, I have the privilege to talk to you guys about some of the analysis we're doing using Redis to determine joinability between data sets using HyperLogLog. So to start, I'll go over a quick overview of what Gretel is actually building, some of our customer challenges and requirements, talk about some of the existing research already in this field, and then walk through what the basics are of containment and some experiments we ran, and then finally talk about our implementation of how we are determining joinability between data sets at scale.", "John Myers (00:53):", "So at a high level, Gretel is a service that is empowering developers to quickly build derivative data sets through privacy analysis, data transformations, and generative modeling that allows people to generate synthetic data from existing data. So the whole purpose is that this derivative data set becomes safe to share without actually infringing on individual privacy. For the purposes of this discussion, we're going to focus more on the privacy analysis side, where we can use joinability between data sets to determine if a data set is actually safe to share, to make sure that information can't be re-identified back to its original source through a second party, third party joins.", "John Myers (01:37):", "So some of our customer challenges summarized is, \"Can you help us determine what overlaps we have between our siloed data?\" So one of the biggest challenges we see in very large organizations or between B2B scenarios is that we have data silos where you have a lot of overlapping data, but you can't really see what the other team has without giving access to the data. So it becomes this all-or-one situation, or maybe I should say all-or-none. So you either get access to the data or you get no access to the data, and it's really hard to determine how you can mutate or transform the data to make it possible to share.", "John Myers (02:20):", "So some of the use cases we have is customers want to know what analytics are in a realm of possible across data sets. So this would be looking to determine if two data sets can be joined so that net new insights can be generated between that data. Additionally, they want to verify if there's any re-identification risk in a data set before it's shared or released. So this would be making sure that if you were going to release a data set and it turns out that some of the columns or elements can be joined with a publicly available data set or another data set that another company or organization has, you want to mitigate that risk before you even release it.", "John Myers (02:59):", "So what we're talking about is joinability and containment, and I often use the two terms interchangeably, but really joinability in my mind is a yes or no question. Can data be joined? Yes or no? Containment is the actual measurement of how much of the data can be joined together. So really containment is the percentage overlap between data set A and data set B and vice versa. One of the big use cases we have is being able to do transitive join analysis between data sets that are more than one degree separated. So in this case, if I can show that data set A could be joined with data set B and data set B can be joined with data set C, this could reduce to showing that A and C are joinable when they don't actually have any overlapping data to begin with.", "John Myers (03:51):", "Some of the challenges here is that we need to operate on streaming data. So a lot of our customers are working with event driven architectures. They're dealing with transactions. They're dealing with unbounded data sets. And finally, they want to be able to determine joinability without accessing the raw data. So how can I determine that we have the same cards in our hands without actually having to show each other our hands?", "John Myers (04:18):", "So we'll talk a little bit about some existing research in the field. So Google was kind of the basis for this, and they have an excellent paper that discusses two major approaches to privacy analysis. And the paper is based around this notion of a KHyperLogLog, which is a new data structure that they proposed for doing two things - one, for determining joinability between data sets, and then second, to do re-identification analysis. So when we talk about re-identification analysis, we talk about taking, let's say, a sensitive field in a data set that could be the primary key. It could be a unique identifier. It could be a specific piece of PII to a customer, and then determining what combinations of other fields exist that are just as unique.", "John Myers (05:14):", "The proof of concept code for both the joinability/containment and the re-identification analysis is in a GitHub repo under Google, and they use BigQuery to demonstrate this capability. Additionally, Neustar has a blog about using Redis to determine intersections between HyperLogLogs, which are just determining set intersections at scale. This is a fundamental piece of the containment analysis. Unfortunately, the blog is no longer available. When I go there, it seems to have like a WordPress error. On the other hand, I was able to find a reference to that blog, which gives a good high level summary of what they discovered.", "John Myers (05:59):", "So what is a HyperLogLog? HyperLogLog is a data structure in Redis that provides estimated cardinalities. And for our purposes, we are going to focus on the joinability side of things. So we are going to focus on determining joinability between two data sets purely based off of the cardinalities of those sets. So Redis HyperLogLog can count the number of unique elements in a set. It's a probabilistic data structure. So it's not as precise as doing a pure Redis set operation. However, it has a huge efficiency in memory usage. So it uses a constant amount of memory, approximately 12 kilobytes and it is extremely quick at adding elements to a set and computing the estimated cardinality of a set. There is a standard error of 0.81% because of its probabilistic nature. I won't go too much into the underlying implementation, but there is an excellent blog out there from a Redis developer and it outlines the exact architecture and how it works. And finally, Redis works really well for event-driven and streaming scenarios because we can accumulate data into these HyperLogLogs without having to do massive table scans.", "John Myers (07:27):", "So next, we'll get into containment basics. And so before I move on from here, I will point out that all of the code snippets you see and the experiment that I run here, the code is all available on GitHub. So anyone can pull it down and modify it and mess with it themselves and tailor it to the use case or whatever you'd like to do with it. So what is containment? Containment is an asymmetric calculation of what percent of values overlap between sets A and sets B. And when I say asymmetric, it's because it's two way. So you can actually calculate the containment of set A inside of set B and set B in set A. This is a little different than other techniques like at your card index, where that gives you a one value for any pairwise tuple of sets. So we'll write this as \"con(A, B)\" and \"con(B, A)\" for our formula, and we'll let the cardinality of a field be defined as card of F.", "John Myers (08:41):", "So the containment of A inside of B is defined as the cardinality of the intersection between the two sets divided by the cardinality of A. Now if we were saying the containment of B inside of A, our numerator would be the same, but our denominator would be the cardinality of B. This yields a score between 0 and 1 that tells you what percentage of A is actually inside of B. And to determine the intersection, we can actually use the inclusion-exclusion principle, which relies on the union of the two data sets. So in this case, we take the absolute value of the cardinality of A plus the cardinality of B, and then we subtract the cardinality of the union. As we'll see later, we were able to easily compute the cardinality of the union using our built-in Redis data structures.", "John Myers (09:40):", "And then in the GitHub repo, we'll see that there is a sample code. It's an Etsy directory, and it's called basics.py, I think, and this just runs a very simple simulation. So what we do is we use UUIDs as our set items, and then we generate 500 identical UUIDs that will be between sets A and sets B. Set A contains 1,000 items and set B contains 1,500 items. And there are 500 distinct items that overlap between the two of them. So when we run this calculation and we were able to add each item to the HyperLogLog, which is named \"f_a\" and \"f_b\" respectively, we can run inclusion-exclusion principle, determine the containability score. And on the bottom left here, we see that the containment of A and B is approximately 50%. Right? So that is saying 50% or, in this case, 49% of the UUIDs inside of set A are found inside of set B, which gives us very close to what our actual truth is. Now on the flip side, we look at the containment of B inside of A. Now because of B has more items, it has 1,500 and only 500 are shared, we see that there's approximately a 33% overlap. So very simple way to implement, very quick and very efficient on memory.", "John Myers (11:15):", "So one of the two experiment about the standard error that I mentioned before. So in this situation, we do 100,000 iterations of creating two sets that have zero overlap. And the reason why we do this is we want to see what this actual difference is. So for field A, we generate a random 10-digit string, and for field B, we generate a random 3-digit string. The estimated cardinality of A comes in just under 100,000, and the estimated cardinality of B comes in at 997. Now, obviously because we have a limit on 3-digit string, there's only so many combinations of the 3-digit string that we could have, which lowers our cardinality for set B no matter how many combinations of that we create. So really, we're comparing a set of a hundred thousand items to a hundred items, which is approximately a hundred times larger than set B.", "John Myers (12:27):", "So in this case, when I ran the simulation one time, my estimated intersection came in at 159, which gives a containment score of 16%, which we know the actual answer is 0%. So what we learned from this is that the order of magnitude between the two set cardinalities could really let that standard error rate throw a false positive. So that's just something we have to keep in mind for our implementation. In the Neustar blog, they suggested using 20 as kind of the multiplier. So if your cardinality between sets A and B is 20x, meaning one of the sets is 20 times or more larger than the other one in terms of unique items, then you should be very cautious about trusting the results.", "John Myers (13:23):", "So I wanted to kind of dive a little deeper and give more of a real-world scenario here. So in this situation, there is a script in the repo that generates a bunch of dummy credit card data. And I chose this scenario because I think it's a scenario that lots of people are familiar with. A lot of people have been subject to credit card fraud, and it's a scenario where there are a lot of companies in the security space that are in the business of determining what is the order of magnitude of damage that happens when there's a credit card leak. Right? So these companies are able to kind of procure the leaks of data from the dark web or wherever. And then they're able to kind of inform their customers on what types of things are being leaked, so banks and other financial institutions can take action.", "John Myers (14:14):", "So I kind of wanted to replay that scenario and say like, \"Hey, how could we, in theory, demonstrate this without actually having to share real numbers with each other?\" Another solution to do this would be, well, we can just hash everything, right? We could say, let's just hash every credit card that the bank has and hash every credit card that is leaked and then compare them, which works, but you're still exhausting a lot of space to do that computationally. Right? It's still a one-to-one scenario. So how can we do this in a singular footprint? So what we do is we generate two million records and we say they belong to the bank. Very simple schema. We have the type of credit card. We make a fake name for the credit card, the credit card number itself, an expiration date, and a CVV code.", "John Myers (15:04):", "And then we generate some cards that belong to the thief. So in this situation, the thief steals 50,000 records, and they have a much smaller footprint in headers where they only have the credit card type, the credit card number and the CVV. When I used this data set, I generated it with a Python library called Figure. And so what you see is the output from baker, where it actually tells you the name of the card and the number of digits. And I left that as is because what we really care about are the credit card numbers.", "John Myers (15:40):", "So the question we want to answer is how vulnerable is the bank relative to the entire stash of stolen credit cards? So before we go on, I'll tell you that the dramatic irony of the situation is the generator created 25,000 records that overlap between the two. So 25,000 of the thief's records belong to the bank. On one note here is I got a little aggressive and this is actually a 40x difference in set size. So 40 times 50,000 is 2 million. And so this is more than double outside of the recommendation from Neustar, and I did this on purpose so we can see how things react regardless. And then before I go on, inside of the GitHub repo, there's another script that will load this dummy data into Redis using the redis-cli bulk-pipe technique. And then there is a containment script that will actually go and compute all the containments and kick out a lightweight data structure.", "John Myers (16:54):", "So how I implemented this inside of the scenario is once we load everything inside of Redis, each column gets its own HyperLogLog. So we have eight total HyperLogLogs that get created in this scenario. There are five headers for the bank, three headers for the thief. That gives us our eight, and we will compute containment across 2-tuples. So we'll take every combination of header and then create a containment score for that. This becomes an 8 choose 2 problem, so that gives us 28 total combinations of headers between the bank and the thief. Now I am actually comparing fields in the same table. You don't necessarily have to do that. It totally depends on your use case. I will say that for a lot of streaming scenarios, I have found that you don't necessarily have a clean table schema, and then a lot of times when you're streaming data, you'll have a data structure that streaming and then you'll have actual overlap between values in the data set, whether it's a duplicate value or it's a value from one field that is contained in another field, but with some other type of metadata around it.", "John Myers (18:17):", "Okay. So getting into our scenario results, the highest score is a 100% containment between the thief's CVV numbers and the bank's CVV numbers. So 4,300 and change from the thief and 11,000 and change from the bank. This isn't surprising because we're going to exhaust the entire key space for the possible CVVs pretty quickly. So it's reasonable to say that this is extremely accurate, and we also don't really necessarily care. So this is the meat that we're looking for right here. So we have 48% of the thief's credit card numbers, which comes in at an estimate of 50,275, where the real number was 50,000, are contained inside of the bank's credit card numbers, which comes in at an estimate of 1,991,000 and change. So this is the target that we're looking for. We know that the real answer is 50%. We commented 48% given the difference in the cardinality sizes here. Again, the cardinality sizes are about 40 times bigger for the bank than it is the thief. This is still pretty good.", "John Myers (19:29):", "As we move down the list, again, we see the CVV score pop back up. So the bank's CVVs are 39% contained inside of the thief's, which again, you would expect that because you would expect that a subset of the bank CVVs could be arbitrarily generated by the thief. And then finally, we got a containment score here showing that 19% of the thief's CVVs are contained in the bank's credit card numbers. So this is that wildly different cardinality size kicking in the gear here. So now we're looking at a cardinality of 4,000 compared with about 2 million. So this is where you're going to see a false positive come into play, but on the other hand, it gives you a really low containment score and this is way more than 40 times the size of the other set when we're comparing 2 million to 4,000. So you can reasonably say, this is probably a false positive and we won't even pay attention to it.", "John Myers (20:38):", "So some implementation considerations for real world based off of this experiment. So field context, super important when you're comparing data. And what I mean by field context is, can you derive some type of information about the field other than the value itself? So pre-processing and normalization of the field to extract any type of type or entity would be really useful here. And one way that we do that is we use a different varying techniques of natural language processing, regular expressions pattern matching, and custom extractions to identify contexts about a field before we make a determination to build a HyperLogLog for it. So can we determine that a field is containing phone numbers or addresses or zip codes or credit cards, names, other types of technical metadata? If we can extract it and label it, it gives us some context we can grab on to before we start doing comparisons for joinability.", "John Myers (21:43):", "And this is really just to have some notion of fields containing similar things. So we want to make sure in general that we're comparing credit cards to credit cards or locations to locations, PII to PII, et cetera. On the location front, I will say it's an extremely interesting opportunity here to do geocoding, reverse geocoding, to really canonicalize locations together. So if you can move locations into a common format, something like latitude and longitude, and then compare them for joinability, that has a huge opportunity there. Right? So addresses can be converted down to lat longs with a certain level of precision. And then if you already have lat longs coming in, you could normalize those to a certain degree of precision, maybe three or four decimal places, and then you can do containability there.", "John Myers (22:34):", "And then finally, you want to minimize comparing every possible field. So again, you have an N choose M problem if you choose to compare every field with itself regardless of the source table or source data source. So that scales very quickly. It becomes exponential. So if you can minimize which field you want to compare with each other, you're actually going to be better off. And while the PFCOUNT command is extremely fast, when you do PFCOUNT with multiple fields to actually get the union between two fields, you incur a little bit of a penalty because Redis has to actually create a temporary merged HyperLogLog. So that can be exhaustive over time.", "John Myers (23:16):", "And then finally, as we saw before, you want to be cautious about fields with much different cardinalities, And as I mentioned before, Neustar recommend a 20x. We were able to demonstrate that it still works pretty well at 40x. I think your mileage may vary depending on your exact use cases and the types of data you're working with. One option here is that you don't necessarily have to throw the score away, but you could consider flagging field 2-tuples with some type of flag giving a warning that, \"Hey, this score is computed from two sets that have a 50x difference in cardinality size. So you might want to take it with a grain of salt.\"", "John Myers (24:05):", "And then generally, we're looking for containment scores above or equal to 40%, and I'm saying that anecdotally that's based off of the work that we've done and the type of data we're processing. It's really important to kind of set your expectations early, and our expectation is that we're looking for massive overlaps in data sets. We're not looking for needles in the haystack. So if you have two ginormous data sets and you want to just see if a handful of records of one exists in the other, this is not the solution for you. And finally, HyperLogLogs are exportable as byte strings. So under the hood, you can look at a HyperLogLog in Redis because at the end of the day, you can actually set and get it, and you can actually export them as byte strings, which is super useful and you'll see why.", "John Myers (24:55):", "So next, we'll get into some high level implementation details of how we're enabling this. So first, we extract all possible context for a field, and we do that based off of the contained values and we also can try to exploit the header names. So we've done a lot of work in building ways to model header names to give context as to what the actual value is. And so we've collected data from millions and millions and millions of database tables that has been made available by Google and we can kind of build a corpus of header substrings and fasttext vectors to do analysis, to try to say, \"Hey, based off of the header name and the values down that header, we can reasonably say that this is an address or reasonably say that this is a first and last name.\" And then we attach that context as metadata to the field.", "John Myers (25:52):", "Normalization and encoding, you can get a lot more coverage this way. So if you're able to post-process an extraction, for instance, let's say we're looking at phone numbers, then you could actually strip out a lot of the formatting metadata in a phone number. Right? You don't need the parentheses and the dashes and the periods. You can just extract out the 10-digit number, the 11, 12-digit number, depending on what localization you're looking at, and then use that as your value going into the HyperLogLog data structure. And then finally, the high level abstract data structure that we are tracking for joinability becomes a 3-tuple, and I'll kind of walk through how we compute that abstract 3-tuple here.", "John Myers (26:34):", "So what we do, we have this notion of a project. The project is a stream, just like a repo in GitHub. And so what we do is we maintain a HyperLogLog per project, per field, per context, and this is formatted by the field name and the context itself. So here's an example of a key name for Redis and adding a credit card number to it. So our field name is prefixed with \"cont\" for containment and \"hll,\" and that is just kind of a prefix you might use to kind of differentiate the keys from other keys you have in the database. And then here, we're combining the project name with the name of the field, which in this case is number. And then the context about the field, we've determined that as a credit card, and here, we're adding a fake value to this supposed HyperLogLog.", "John Myers (27:31):", "Finally, one thing we can do is then we can actually use a sorted set to track the pairs of field names and context to something like a last seen epoch, and this becomes useful when we want to go compute the containment signatures or the values is that we can determine which values to use based off of which ones have changed over time. So in this case, we can do a ZADD to a sorted set data structure where, again, it's prefixed here just to kind of namespace it. And then we can have a sorted set per project. And then the values in the sorted set are the names of the field and the context pairs with something like a last seen epoch. And so this makes it pretty straight forward to iterate over the sorted set to generate the actual HyperLogLog key names we'll use to make our computations. So our ideal final 3-tuple here is that we get a field name, some type of context, and then the actual HyperLogLog bytes.", "John Myers (28:42):", "So when we move into what we can do with this is we can actually generate a serialized signature. So in this case, I've generated two signatures, and when you run the sample code on GitHub, it does this for you and writes them in, excuse me, write to them out as a JSON file. And so in this case, since the field names and the context are the same, the field and context fields match up between the thief and the bank. And then what I've done is simply take the HyperLogLog byte string and write it to a base64-encoded string before we write it to disk. And if you take this and you reverse the process and you put it back into Redis and run the same computation will actually kick out the same exact score that we saw from earlier. So we'll see that it's approximately 48% overlap.", "John Myers (29:33):", "So again, these byte strings increase in size of course when you base64-encode them. They'll be a little higher than the 12 kilobytes that they are inside of Redis, but they'll still be generally consistent. It doesn't matter how many values you've added to the HyperLogLog. You always get that consistent byte string. And the most important thing is these signatures don't contain any of the personal information. So we essentially have a way to now determine if two data sets have a high degree of overlap in data without ever having to compare the raw data with itself.", "John Myers (30:18):", "So let's get into kind of how we execute this architecturally. So what we have is an inbound API that's very similar to an Elasticsearch API where you can post JSON records. And then what we do is we drop those records into a queue, and then we have a series of services that we kind of abstractly call our context extractors. This does a lot of our natural language processing, our regular expression processing, week or month custom patterns provided by customers that allow them to build patterns or regexes or extractions to tag types of data based off of what's in the raw content. And then we can label a field with all the different types of contexts we've seen in that field. This routes down into what we call our service queue. And then we have a bunch of downstream services that do anonymization and transformation and synthetic data generation, but for our purposes, we're more worried about how we generate these HyperLogLog containment signatures.", "John Myers (31:22):", "So when we do the context extraction, what we're doing is actually, in addition to tagging the fields with all the different contexts we've seen, we will actually generate the Redis write commands that we need to insert this data into our HyperLogLogs. Oops. So what we do is we actually buffer these up inside of Redis pipelines, and then we can take that command stack, and then we serialize those commands and we write them onto a queue. And then we have a series of microservices that just pull from that queue and actually do the Redis writes. Now, the reason why we do this is because the most expensive part of our entire pipeline is this context extraction. So that could scale to have many, many, many workers, and we don't want each one of those workers creating independent connections to Redis.", "John Myers (32:19):", "So what we do is we buffer up all the commands in a queue, and then we can control exactly how many Redis writers pull out of that queue and write to Redis. This is a little bit of a flipped use case in the sense that this is way more write-intensive than it is read-intensive. So it's important that we can buffer those writes. Obviously, we can mitigate this further by using Redis clustering, where we're kind of spreading out our keys across multiple shards, but for right now, we're actually still just using a single master writer endpoint, and just we're able to buffer the writes well enough so that we still have good performance.", "John Myers (32:58):", "So then we can have a routine that comes along every so often and pulls out all the HyperLogLog structures that have recently changed. We can generate those signatures and then we can keep them into a signature store. And so then what you're able to do is search that signature store for projects that have similar contexts to the ones that you're collecting, and it becomes very trivial to take any two signatures and then load them into an ephemeral or very volatile version of instance of Redis running, so you can compute the score and then flush out the keys or whatever. Right? So it's kind of just like a TrueCache version of Redis just to compute the scores very quickly and send them back out.", "John Myers (33:47):", "So in summary, we have found that HyperLogLogs have made containment scoring pretty painless and have satisfied a lot of our requirements to be able to help customers do analysis across siloed data without ever actually having to show the raw data to the two interested parties. One of the big things that we realized, again, is that having some context around the two HyperLogLogs we were comparing is really important. Fortunately, we had a requirement to do labeling and content extraction in our data separately. So we were able to leverage that to add additional context to our entire process of doing joinability. Normalization/Encoding is super helpful. So if you're able to post-process the data so it's in a standard format before you put it into a HyperLogLog, that's going to give you dividends down the road, for sure.", "John Myers (34:50):", "And of course, this is ideal for event-driven architectures. Since we don't have to do retroactive table scans to build these models because we're dealing with streaming data, we're able to run a very typical queuing architecture to buffer up these writes into Redis and then have periodic read out to build these containment signatures. Well, that's it for me. I hope this has been useful. For those of you watching, for those of you who are going to tune in or have tuned in already, I really appreciate it. I hope everyone's staying safe and feel free to reach out to me or my team. Check out our work on GitHub. We'd love to hear any feedback, any use cases around this. And Redis kicks ass and I continue to love to use it.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://console.gretel.ai/login/", "page_title": "Gretel Console", "headers": [], "content": []},
{"url": "https://gretel.ai/podcasts/al-and-alex-watson-discuss-gretel-security-and-privacy-issues-around-synthetic-data", "page_title": "Podcast - Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "headers": ["Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Abstract", "Show Notes", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Hosted by Al Martin, VP, IBM Expert Services Delivery, Making Data Simple provides the latest thinking on big data, A.I., and the implications for the enterprise from a range of experts.", "This week on Making Data Simple, we have Alex Watson. Alex was previously a GM at AWS and is currently a Co-Founder at Gretel.ai. Gretel is a privacy startup that enables developers, researchers, and scientists to quickly create safe versions of data for use in pre-production environments and machine learning workloads, which are shareable across teams and organizations. These tools address head-on the massive data privacy bottleneck--which has stifled innovation across multiple industries for years\u2014by equipping builders everywhere with the ability to create quality datasets that scale.", "In short, synthetic data levels the playing field for everyone. This democratization of data will foster competition, scientific discoveries, and the inventions that will drive the next revolution of our data economy.\u00a0", "The company recently closed their series-A funding, led by Greylock, for another $12 million and brought Jason Warner, the current CTO for GitHub, on as an investor. Gretel also launched its latest public beta, Beta2, which offers privacy engineering as a service for everyone, not just developers.", "\u200d", "2:03 \u2013 Alex\u2019s background", "4:36 \u2013 What time frame was Harvest AI?", "7:14 \u2013 How does NLP play into Harvest AI?", "10:50 \u2013 How can we not have enough knowledge?", "14:08 \u2013 Does the tech exist today for security?", "18:14 \u2013 Privacy issues", "20:42 \u2013 What does Gretel stand for?", "27:42 \u2013 Do you increase the opportunity for bias?", "31:18 \u2013 Where is the sweet spot for Gretel?", "33:30 \u2013 When do synthetic not work?", "37:42 \u2013 What is practical privacy?"]},
{"url": "https://gretel.ai/blog/differentially-private-synthetic-text-generation-at-scale-part-1", "page_title": "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "headers": ["Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "Industry Use Cases for Differentially Private Synthetic Text", "Gretel's Differentially Private Approach", "Evaluating Differentially Private Synthetic Text", "Evaluating Training Accuracy", "Downstream Task Performance", "Analyzing Synthetic Data Quality and Versatility", "Synthetic Quality Score Analysis", "Efficient Compute and Costs", "Unlocking Data with Differentially Private Synthetic Text", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Much of organizations\u2019 most valuable data exists in natural language formats- including customer feedback transcripts, call center logs, and internal reports and documents. Unfortunately, due to privacy concerns around the data, it is often locked in silos, accessible only to a few.\u00a0", "Fortunately, ", " provides a simple, scalable process that organizations can use to make any natural language or tabular-based data available across their enterprises. Gretel fine-tunes state of the art large language models (LLMs) on sensitive data while providing mathematical privacy guarantees through a technique called differential privacy, applied during model fine-tuning. Gretel models can then be used to create any amount of synthetic data across a multitude of use cases- from enabling data access, analytics, and even creating training data for downstream ML/NLP tasks.", "In this post, we\u2019ll dive into how Gretel can be used to create safe synthetic text data that can be used for a variety of use cases across an organization, and then compare the performance of the differentially private synthetic data against the real world data that it is based on.\u00a0", "In healthcare and life sciences, for example, synthetic data enables the safe sharing of electronic health records (EHR) to improve patient outcomes. By using synthetic patient data, such as initial diagnoses and patient descriptions of symptoms, researchers can explore new treatments and care strategies without compromising patient privacy. In the financial sector, synthetic data helps in building robust fraud detection systems, or chat bots built upon customer chat logs, while ensuring customer privacy remains intact. The broad applicability of synthetic data brings with it the challenge of maintaining quality and reliability, especially when used as a proxy for real-world datasets.", "Differential privacy is a technique that protects individual data points while enabling models to learn overall patterns and distributions. Gretel has pioneered applying differential privacy during language model training ", ", with over 900k SDK downloads of the ", " library to date.", "Our approach uses a technique called Differential Privacy Stochastic Gradient Descent (DP-SGD) algorithm (", "; ", "). DP-SGD adds noise to the optimization process and clips gradients to prevent memorization of any single data example. This provides mathematical guarantees that no individual's personal information can be traced or revealed, while still allowing the model to learn trends, insights, and distributions from the real world data.", "Differential privacy protects individual data points through two key parameters - epsilon (\u03b5) and delta (\u03b4). These parameters determine the strength of the privacy guarantee. Epsilon limits the maximum difference in output that can be caused by a single data point. Lower epsilon provides stronger privacy, making individuals less distinguishable. Delta represents the probability that guarantees could fail. Lower delta values result in tighter assurances.", "For our differentially private model, we chose privacy parameters of \u03b5=8 and \u03b4=10^-5, resulting in solid privacy guarantees while retaining high data utility. For context, the US Census bureau uses an \u03b5=17.14 for US person data (", ", 2021), and Google uses an \u03b5=6.92 and\u00a0 \u03b4=10^\u22125 along with federated learning for their next-word prediction models on Android devices (", ", 2022). Apple\u2019s Safari browser uses \u03b5 values between 8 and 16 (", ", 2021). The chosen configuration with an \u03b5=8 balances generating usable synthetic data without sacrificing individual privacy. ", "Gretel combines state-of-the-art large language models with differential privacy protections during training using the DP-SGD algorithm. Our approach embeds formal privacy assurances directly into machine learning pipelines. This allows securely unleashing the power of language models while protecting user privacy. ", "Gretel GPT combines state-of-the-art large language models with differential privacy protections during training. This enables high-quality synthetic text generation that approaches accuracy of real data, while rigorously protecting individual privacy. Because differential privacy is embedded directly into the training pipeline, the resulting synthetic data can be used without restrictions - including aggregating datasets or training downstream machine learning models.", "In Part 2, we will dive deeper into testing differential privacy protections, as well as analyzing attempted attacks. To learn more, check out our ", ", as well as ", " on Gretel\u2019s tabular and ", ". Stay tuned!", "We fine-tuned the ", " on ", " to create two models:", "We compared synthetic text quality from each model by measuring:", "To evaluate how well synthetic text generalizes, we used Gretel's Synthetic Quality Score (SQS) evaluation metric tailored to compare against real-world text.", "Key experimental parameters:", "By benchmarking against a non-private fine-tuned model, we can quantify how closely differential privacy preserves accuracy and utility across tasks. For this experiment, we drew inspiration from insightful recent work by ", " conference aligning with Gretel\u2019s innovative approach to differentially private language models.", "Example training record (real world). We\u2019ll choose a rating of \u201c3\u201d out of 1-5 because often a middle-level sentiment can be the most difficult to capture.", "Example ", ", matching category and stars from the review above- but using synthetic content from our fine-tuned model- :", "We first compared training loss between the differentially private (DP) model and non-DP baseline. Initially, the DP model shows higher loss due to the complexity of learning under privacy constraints.", "However, as training progresses, the DP model loss converges towards the non-private version. By the end, there is only a slight difference in training loss between the two models. This demonstrates the ability of differential privacy to enable effective learning over time.", "Next, we evaluated the accuracy of each model in categorizing reviews by business type. The figure below visualizes downstream categorization accuracy at each training step.", "Key Findings:", "The analysis confirms that although accuracy may be lower at the start, differentially private models can quickly adapt to match or exceed non-private models on real-world tasks.", "The true strength of synthetic data lies in its versatility for various downstream use cases beyond basic generation and classification.", "To evaluate quality across a breadth of potential applications, we generated synthetic reviews using our differentially private model (\u03b5 = 8) prompted with category and rating tags. This ensures the distribution matches the real data. We then compared against the original reviews using ", " (SQS), which analyzes structure, insight quality, and distribution similarity.", "The results benchmark the differentially private synthetic text against real-world data across metrics relevant for diverse analytics, training, and business applications. This reveals the adaptability of private synthetic data to unlock value across a variety of practical use cases.", "In the first page of the report above, we can see that 500 records of synthetic text sampled from the differentially private model are compared against 500 records of text from the real world data. With an SQS score of 86 and a text semantics similarity score of 94 when compared to real-world data, we can expect the synthetic text corpus to be suitable for various downstream tasks, including product feedback analysis and ML & NLP tasks such as training models.", "Highly similar text semantic distributions in the PCA Scatter Matrix indicate that the synthetic model correctly learned the semantics and distributions of the real world data that it was trained on.", "Interestingly, the differentially private model tends to generate more text than the real world training data- with an average of 14.7 sentences vs 13.3 sentences per review, and an average of 8.6 words per sentence vs 7.5. Note that this finding is different from the findings and observations from ", ", where they ", ". This could be due to model generation parameters, or differences in the base Mistral-7B LLM used in our experiment when compared to GPT-2 used in Yue et al.", "Interested in synthetic text quality research? Explore our journey in creating the open SQS metric and its methodology in our detailed documentation, complete with ", ".", "When scaling generative AI, cost and compute considerations are crucial - especially for large language models requiring substantial resources. Gretel's GPT employs the QLoRA technique to reduce precision to 4-bits. This allows generating high-quality synthetic data while only fine-tuning ~1% of total model weights. ", "With GPU acceleration, the overhead of differential privacy mechanisms like noise injection and gradient clipping is minimal. Additional training passes are needed to match non-private accuracy.", "For example, processing 1 million reviews (632 MB) took 15.2 hours on 8x A100 GPUs. Using public cloud pricing at $15.90 per hour, this results in a compute cost of ~$240 per 1M reviews ($0.38 per MB) for differentially private model training.", "The small subset of weights updated also leads to an efficient 22 MB adapted model, vs 632 MB of training data. This enables storage and transfer efficiencies worth further exploration.", "Differentially private synthetic data provides a unique avenue for organizations to share sensitive data more broadly, enabling new analytics, models, and insights without compromising privacy.", "Gretel GPT with differentially private fine-tuning is currently available in private preview to select partners. To learn more or discuss use cases for unlocking the value of your organization's text data securely, please reach out to us at ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/how-to-create-differentially-private-synthetic-data", "page_title": "How to create differentially private synthetic data - Video", "headers": ["How to create differentially private synthetic data", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this video, Alex walks you through creating a differentially-private synthetic dataset and how to determine the quality of that data.", "Hey, this is Alex from Gretel. Today, we're going to walk through an example of creating differentially private synthetic data. We'll then compare that example against both the real world data that was based on and another set of synthetic data that's not using differential privacy. We'll look at some of the privacy benefits and the effect on accuracy for the downstream models. ", "To follow along, go ahead and connect to dock stock Gretel at AI. From there, I'm going to click on SDK notebooks, just walks through a few different examples of using Gretel for different use cases. Look for one called synthetic data with differential privacy. And we'll go ahead and open that in co-lab. I'm gonna go ahead and click run here and then we'll walk through some of the what's happening in the notebook as it starts training the model. So I'll go ahead and click run all, no need to configure a GPU here. We're going to use the global cloud service to do the model training for us. ", "So we're going to start off by installing the Gretel client and then download a data set that we're going to use for for synthesis today. So here we can see it contains hour of the day east scooter. So a bike ID and we have latitude longitude coordinates for source and destination. The exercise we're going to do today is that we're going to insert secrets, so very precise latitudes and longitudes into this dataset. And we're going to see how many times we have to repeat it before the model starts to memorize that data. ", "So I'll go head over to Gretel, grab my API key, go ahead and paste it in here and click go Here, we get a preview of the dataset so we can see different source latitude, longitude. So very precise location coordinates here. What we're trying to do here. And we're simulating this idea of how many times would a individual user's address. For example, need to be repeated here before the model memorizes that address and replace it. ", "So the goal of synthetic data is really to learn about locations and patterns and things like that. We're not trying to learn about individual users when we think about things from a privacy perspective. So we're trying to make sure that the model here creates a new data set that is representative of what it was trained on, but it doesn't actually point to any real humans data. ", "To measure this what we're going to do. And we have a function here called create canaries. What that does is it takes a set of secrets. So here we've created some totally fake latitudes and longitudes here, but these are our secrets.And we want to see how many times do we need to repeat these in the input data for it to exist or be repeated by the model when we ask it to create a new data set. So for each row here, we can see we're sampling. Let's go ahead and take it. See how many rows we're running. It's about 270 rows, I think. ", "For each row. What is going to do is insert one of these secrets here.I can see 63 being repeated right here. It's being repeated right here. So you see it in these different rows where we're inserting secrets in it's doing it at different frequencies. So here we have this 85 numbers being presented repeated 5% of the time, the 63 numbers and 50% of records, so we should see that quite often. We're combining that with our training data and we're going ahead and start training our model. Here we see the Gretel configuration. We've included all the different config options we're using here. Not all these are necessary, but it helps just to walk through. So we're loading with our default configuration. ", "Let's talk about some of the things that we're doing, that's different here. So this is really what you see here. That's different for this initial example. Here we are training a model. We're using differential privacy equals true. We're also using what I would consider to be a pretty minimal amount of noise. So differential privacy, how we implemented it, we use a technique called DPS GD. So essentially what we're doing is inserting noise into the optimizer as it's training a neural network to learn to recreate this dataset. What that does is it has the effect of preventing that model from learning secrets in the data. So we want the model to learn patterns. We don't want to learn the model to learn or memorize specific data in the end, the training data. So we're inserting, I would say a minimal amount of noise here. You can turn this up based on your privacy concerns, but we'll see what kind of effect this has on the overall data set. ", "The second thing we're doing, and this is called the LT norm clipping. We are clipping the gradients in the optimizer. And what that does is it prevents the optimizer from learning too much at each step. So both of these really kind of intentionally build privacy directly into our model to make sure we understand some of the privacy benefits here, we're turning off privacy filters. So privacy filters are technology that we've built at Gretel that has meant to identify and remove records that could create risk from a privacy perspective for known attacks members and inference record linkage attacks, things like that. We're going to turn those off at first and we're going to look at the effect that DP has running all by itself on this input dataset ", "And see the configuration that we're training with here. And let's go ahead and start a training. DP does require a little bit of extra compute versus what we're able to do when we're just running a regular synthetics on a GPU. And you'll notice some of that as the the models training here. So I won't make everyone kind of wait through this entire thing. We'll go ahead and jump and look at some of the results of these models that have been trained and completed. If you want to try experimenting with your own settings, I would encourage try changing some of the different settings, try turning DP on and off, try adding outliers and similarity filters. And at the end here, we're going to see how often from a generated dataset that those secrets are repeated.", "Here we can see the project that we created. Let's go ahead and check in. So here we have a new project called rideshare DP model and go into the Gretel console here, search on rideshare There's our project could see that we just started training a new model here. Hilarious, awesome troll. We'll take a look at that. So it's in the process of training right now. I would expect this to take probably about 20 minutes to train this.So we're going to go ahead and jump over to a trained model and look at the results So we're going to start here first.", "The first thing I did for this I ran a version of the exact same notebook we had, but I disabled differential privacy. And as you can see here, DP is turned off and I also disabled all of the filters. So the question is saying, just creating synthetic data by training a model, what is the chance that it's going to memorize the replay to these secrets? Go ahead on and take a look at the results here. ", "So here are our four to secret values that we created. We can see that value 185 was inserted 14 times and value for the 63 dot 2, 2, 4, 2 latitude coordinator was inserted 145 times into the dataset. So it is natural at some point that a dataset, a model with learn and start repeating these things that it sees that frequently in the data with privacy filters off and differential privacy off, sojust running a language model on this data. We can see that this secret that existed 14 times was not recreated. The secret that was inserted 145 times was repeated six times by the model. In some cases that's not acceptable. In some cases it's perfectly acceptable. So it's really about finding that kind of right balance between accuracy and privacy that you're looking for with your model. ", "Look at the next example here. So Gretel's default settings is to completely default run with Gretel has privacy filters enabled by default. So let's see what happens running a default cradle here. We can see that none of these initial secrets with the privacy filters turned on were repeated all the way up until the end here where the 63 dot 2 0 2 4 2, which was inserted in this case, 133 times was repeated four times in the day in the final data set. ", "So now what we came here to do, let's go ahead and look at the results of differential privacy and privacy filters being run on this dataset. So go down here and look at the results here. And as we were hoping, and as we were expecting here, none of these secrets were reported and it repeated the input data. So anywhere from the secret, that was repeated 20 times to the one that was done 129 times, none of these were repeated by the the output of the model, which is really encouraging.", "If you are concerned about addresses, credit cards, types of data like that, social security numbers, things like that, that maybe users put into a free torrent warm text field, or something like that, that shouldn't exist here. We're giving you some guarantees and some levers you can turn to, to make sure that those things don't get repeated downstream by your machine learning models. ", "So really encouraging results here. Let's take a look at what effect does differential privacy and using these settings have on the accuracy of the model. So one of the things that happens every time that a synthetic model is created with Gretel is we run what's called an accuracy report. So here's the case of differential. Privacy is not able to, we're running as a standard vanilla Gretel synthetics model. You can see it trained pretty quickly here at trained in 18 minutes on, on a GPU and the grotto cloud. The quality score is pretty good. So here, we're looking at an 88 out of 100. ", "Let's go ahead and take a look at what does that mean? So I'd like to look at the report and it gives us a nice kind of feel for for why did we give it this 88 out of 100 score? So I'll go ahead and download that can open up the model, take a look here. None of obviously the training lines are duplicated and the output data, we can see what was turned on. So our privacy filters were on differential. Privacy was off. And what you see here is the correlations that exist in the input data were learned very well. So, you know, very little separation correlations, similar patterns here, and this is where you really see things with differential privacy. So here we can see the distribution when we look at a PCA graph outliers and things like that, that existed in the input data were largely recreated in the synthetic data as well. So this is default run gives you really good accuracy and good privacy protections, but not as good as, as you'll see when we run differential privacy. We also see the distribution of each individual field here is very close to what it was in the training data. So this is something that gives you a warm and fuzzy that your synthetic model has very well learned. The insights in the patterns and the original input data and being able to recreate it. ", "Okay, let's go ahead and take a look at what impact this has when we are running a differential privacy. So go back, back out here, let's go to our rideshare data sets. Here's one we created which we were running differential privacy on. So we'll take a look at the config here, but you can see the quality score has dropped pretty significantly privacy protections. Excellent training took a little bit longer because the methods that we use to insert noise and clip gradients slow down the model a little bit. Let's go ahead and take a look at the configuration that was used. So same configuration. We just previewed differential privacy is on minimal amount of noise being inserted in here.I'd note that this data set is only about 20,000 lines long. So typically with differential privacy, you really start to see the privacy benefits and accuracy when you have a much larger data set, let's say 50,000, a hundred thousand, even a million rows of data. But for this example, we are starting with something a little bit smaller.", "Now let's go ahead and take a look at the report and see, can we see any differences other than this synthetic quality score being lower that would help us understand, like, why is this model performing differently than our vanilla model trained on the standard credit config privacy protections? Excellent. Once again here we've got a 79, 100 score. No data was repeated. Let's take a look here and start seeing what's different. So the correlations you can see here on specially, when you look at this a different graph, isn't quite as strong as what we had previously.So here you see very little differences at all. Here, you can see a little bit stronger as we go down here. This is where you really start to see the difference, these outliers that exist. And you can see inside the PCA distribution graph are no longer there in the input data. So it did learn the patterns and it's recreating the overall insights of the original data pretty well. Let's see that even we look at a field to field distribution graph. They're not as close as they were before, but they're still close. So they're still creating and learning those insights. But you're getting much stronger privacy protections. ", "I hope this has been useful. Please try and go through yourself, run this notebook try changing any of these different settings. So we've laid them all out. So it's really easy to change things. Try changing the number of reports, try changing the noise multiplier or the clipping, and take a look. Even the number of secrets that are inserted into your data and take a look at how that compares against the settings where you're not using differential privacy and try to find that right balance between privacy and accuracy that you're looking for. Cheers. ", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/walkthrough-create-synthetic-data-from-a-dataframe-or-csv", "page_title": "Walkthrough: Create Synthetic Data from any DataFrame or CSV", "headers": ["Walkthrough: Create Synthetic Data from any DataFrame or CSV", "Video transcript", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we're going to walk through using Gretel's apis to create synthetic data from a CSV or Pandas DataFrame. Let's jump right in. You can find the ", ", or go ahead and ", ", Google's free notebook hosting service. ", "It's a really simple install here- all we have to do is install the Gretel client, so a simple PIP install. We also need an API KEY which allows us to interact with the api services. While that's installing we'll go ahead and log into the Gretel console which is ", ". Grab a copy of our api key. All right I'm going to go ahead and kick off the rest of this project while we talk through what's happening.", "The first thing that we're going to do here is use the Python SDK which interacts with Gretel's API services. Let\u2019s start by creating a project. We're going to name it synthetic data. You can name it whatever you would like. Now for the fun stuff.\u00a0 We're going to pull down one of our template configs so these configs essentially define a set of neural network parameters that are used by our synthetic model based on the data that's been trained on so here we can see that we went ahead and downloaded the default configuration which is a pretty sane configuration that should work for most data sets. You can browse other configurations by clicking on this link as well and it will go ahead. The next step is go ahead and load our sample data set that we want to create a synthetic version of into a DataFrame so here we can see we'll load up Pandas.", "We'll define the path here so we're going to load this from Amazon S3 but you can load any local CSV file that you would like into your DataFrame and we'll go ahead and preview that DataFrame and we're also going to save that CSV file out to a local file. We can see we're looking at a financial data set- it's about 5,000 rows and 15 columns so it should be a nice challenge for the synthetic model to see if it can learn on this little data. Let\u2019s go ahead and save this file off to disk and then we'll use that to start training our model.", "Here you can see we're creating the model, we're submitting it to the api service and telling the model to go ahead and train a model using the Gretel cloud. You can run this locally in your environment, but running in the Gretel cloud allows you to get by without having a gpu or needing to configure tensorflow cuda or the underlying dependencies here we can see that the model finished training you see the accuracy here on both the test set and in the validation set is quite good so we're at 90% accuracy loss is pretty low. Typically loss anywhere under one will indicate you've got a pretty good model and we're going ahead and using the model to generate 5000 records.", "We are going to use the 5000 records to profile the synthetic model and figure out how the job it's doing at creating synthetic data. Also, we will go ahead and create a report and we'll go ahead and take a look at that report next. Here is a version of our synthetic DataFrame so this is the entirely artificial data set that was created by our model you can see it mimics the input data very closely so you can see different age distributions working class the same type of fields that we came to expect when we were looking at the training data. The next question we have is how good of a job did the model do at retaining the insights, characteristics and correlations from the original data set?", "So let's go ahead and dive in and take a look at that for that we have the synthetic quality report which we'll go through right now essentially what we're doing is throwing the kitchen sink from a statistical perspective comparing the output of the model those 5000 records we generated versus the input data just about 5000 records we put in. Here you can see overall we got an excellent score and the different measures that we're taking a look at here to kind of assess the the correlations and the distributions per field are holding up quite well from a privacy perspective none of the training data was duplicated so these are entirely new records that were being created.", "Here we get an overview of every field and there'll be a nice deep dive that we can look at as we go deeper down on the report this is my personal favorite to look at so here when we look at the training and synthetic data correlations. What we are looking at here are correlations that exist between each field so you obviously expect a strong correlation between the same field going up but we can see other cases here where an integer here that appears to be identifying the education is always the same so strong correlation between those fields.\u00a0 And we see weaker correlations between things like relationship and age for example or relationship and marital status where we see nearly 50% correlation between the input data what we're looking for here is we want to see that the synthetic data model is able to as closely as possible recreate the correlations that exist in the original data set.", "Another really handy view that I like is using PCA. So, PCA is a dimensionality reduction technique favorite toolkit in the arsenal for data scientists and what we looked at like to look at for here is essentially compressing those 15 columns and those 5000 rows down into two different dimensions that we can look at through dimensionality reduction and what we look for here is a similar shape between the synthetic data and the training data similar size distribution of the different lines so this looks quite good.", "As well from there we can dive in on a field by field comparison. How many of these were recreated in synthetic sets versus how many of them existed in the original training set? And you want to see as close of a line as possible without having things obviously be exactly the same. It looks like the synthetic model once again learned this quite well we see very similar but different distributions between synthetic and the original training data", "From there we can use our model and this last line here shows us a way to use the model to have our record generate as many more records as we want so here we are passing our model request to generate another 100 records which it went ahead and did very quickly and we can load that as a essentially wrote that load that csv directly from the API service in as a DataFrame and use our favorite data science tools to work with it.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/diffusion-models-for-document-synthesis", "page_title": "Diffusion models for document synthesis", "headers": ["Diffusion models for document synthesis", "Move over GANs, Diffusion Models are here", "Document Synthesis", "GANS", "Diffusion Models", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Here at Gretel, we spend a lot of time thinking about how to better generate synthetic data. Usually, that data comes as tabular or text data. Most data in the business world is either in a relational database, excel file, CSV, or stored as text. However, there is an increasing demand for ", " of other forms than just tabular data.\u00a0", "One form of data that is quite interesting is images. When compared with the long history of machine learning, generative image applications are quite new. In the past decade we have seen a wide variety of methods to generate images including Auto Encoders and Generative Adversarial Networks (GANs).\u00a0", "For the years since their inception, GANs have held the title of best generative models for images as measured by diversity and quality. However, even though they have produced amazing results there are still a few problems. One of the main issues is often referred to as \u201cmode collapse\u201d or \u201clow mode coverage\u201d wherein the model has trouble generating a wide variety of images. This means that the images may be high quality but low in diversity.\u00a0", "\u200d", ", Diffusion Models have arrived as a potential replacement for GANs since they are able to generate high-quality images with great diversity. If you want to dive in and learn more about Diffusion Models, here are some in-depth posts:", " ", "\u00a0", "Diffusion Models have some similarities and some differences to GANs. In a GAN, there are two neural networks that compete against each other, hence the term \u201cadversarial\u201d. In Diffusion Models, however, there is only one neural network involved. This network is trained on diffused data. This means that a \u201cforward\u201d noising process is run on the data that takes it from clean to noisy, like so:", "Then, the neural network (typically a ", ") is trained to reverse the forward noising process. This approach has a number of really nice properties, primarily that it performs well on a wide range of data types. One of the most recent successes of Diffusion Models is ", " from OpenAI. They use Diffusion Models and text-based models to create fantastic images.\u00a0", "This same underlying technology can be used for important business applications as well. For example, here at Gretel we recently explored how well Diffusion Models perform at document synthesis.", "In a company\u2019s transition to digital, they often catalog physical documents as digital scans. This results in images of documents that are stored in the cloud or on internal systems. Oftentimes, we want to analyze these images to make decisions or train downstream machine learning systems. For example, a doctor may want to scan and upload their handwritten notes to include with other patients\u2019 electronic health records for further analysis. However, because of the private nature of these documents, or the expense of collecting and scanning - we often need synthetic images of documents for our downstream tasks.\u00a0", "Enter Document Synthesis as a potential solution. Here, we can synthesize realistic images of documents using either GANs or Diffusion Models. Since we may only have a few original images to work with, our methods need to efficiently use the data we have collected.\u00a0", "Here we use a small 300 image dataset of receipts as our testbed. The goal is to generate realistic-looking receipts that could be useful to an organization.", "We first explored whether \u201csmall data\u201d GANs would be able to synthesize receipts. We had moderate success. As you can see here, the model was able to learn the basic shape of the documents and the vague squiggles of text. However, these are likely not useable for any business purpose, since they remain illegible.\u00a0", "However, when we transition to using a Diffusion Model, we immediately see an increase in quality. This model could quickly be scaled which would improve performance. Notice here how the model has not only captured shapes, lighting, and variations - but it also has been able to write coherent text.", "On the applied science team here at Gretel, we explored whether Diffusion Models could be used to generate documents. We found the capabilities of these models to be really promising! If this is exciting to you, feel free to reach out at ", " or join and share your ideas in our Slack ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/predicting-patient-stay-durations-in-the-er-with-safe-synthetic-data", "page_title": "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "headers": ["Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Why synthetic data for healthcare?", "Case study: Predicting patient ER stay durations", "Removing PII", "Creating private synthetic data", "Comparing results", "We would love to hear from you", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In today's data-driven world, collaborative data sharing has demonstrated its success with initiatives like Covid-19 and cancer research. This success has primed life sciences institutions, research organizations, and hospitals to work with data at a scale that has never been possible. These organizations generate and work with extensive amounts of data that could reshape healthcare and advance research in ways we are only beginning to experience.", "A key to unlocking this potential lies in the power of AI to learn from vast amounts of medical data, where models can be trained and used to predict disease outcomes, enhance clinical decision-making, and support drug discovery and development. However, due to the need to protect patient privacy, enabling access to medical data across organizations, between teams, and even to machine learning models for training is difficult and often ends up never happening.\u00a0", "In this post, we will explore a real-life case study of how a major hospital uses Gretel to help predict the duration of stay for new emergency department patients, to identify gaps in hospital resources or coverage, and to identify trends in outbreaks.\u00a0", "While traditional approaches to maintaining data privacy have their merits, they also come with significant drawbacks. Secure enclaves, although efficient in preserving privacy, pose challenges due to their complex setup, management, and inability to export data. Homomorphic encryption, while limiting exposure of patient data, is computationally intensive and prevents direct interaction and exploration of data.", "The creation of synthetic data using LLMs and privacy-enhancing technologies such as differential privacy offers several unique advantages:", "Synthetic data allows for the safe and streamlined sharing of data, empowering life sciences researchers to deepen their understanding of diseases and quickly test their ideas without compromising patient privacy.", "A major hospital system in the midwestern US reached out to Gretel as they were planning to start a project using machine learning to predict the duration of stay for a patient given an initial diagnosis in the emergency department. Being able to accurately forecast how long each patient will stay can help hospitals provide better care \u2014 the hospital network could quickly respond to trends in public health such as outbreaks, and ensure optimal allocation of limited resources.", "The initial challenge that the hospital encountered was in how to share access to the sensitive patient records that form the training data with the development team. While the records could easily be exported from the Epic EHR system, they need to be anonymized with the following requirements:", "First, let\u2019s take a look at the medical records and their format. Note, these are real but anonymized records that we have been given permission to use for this post.", "From initial exploration, we can see that the dataset contains 20 columns and approximately 30,000 medical records from the past year. The dataset is a mix of categorical, numeric, and short natural language text descriptions.\u00a0", "From the dataset above, we can see two columns with potentially identifiable information- specifically:", "Using Gretel\u2019s Transform API, we can define a simple transformation to encrypt or replace each MRN and CRN, protecting against any linkage attacks while still maintaining the statistical significance and distribution of these fields in the dataset. Below is an example ", " that we will run as a preprocessing step.", "The next step is to use our de-identified dataset to train synthetic data models using Gretel. These models can then be used to create artificial, synthetic versions of the training data \u2014 containing the same statistical properties, but with various levels of privacy protections and guarantees \u2014 for example, ensuring that no combination of non-PII based patient attributes (such as height + weight + age + zip code) could be used to uniquely identify a patient.", "We will experiment with training two of Gretel\u2019s core models on the dataset below. Gretel ACTGAN is a Generative Adversarial Network (GAN)-based model that creates highly accurate generative models and datasets, utilizing privacy-enhancing technologies to offer state-of-the-art accuracy that is critical for machine learning models. Second, we will train Gretel\u2019s TabularDP model, which is a graph network-based generative model that offers rigorous mathematical guarantees for privacy.", "Since we know the model will be used to create training data to predict patient stay durations (e.g. 0\u20133 hrs, 3\u20136 hrs, 6\u201312 hrs, or 12+ hours), we can use Gretel\u2019s Evaluate API to automatically test the downstream machine learning accuracy of synthetic data versus real world data, as shown in the config examples below.", "Gretel ACTGAN", "Gretel TabularDP", "Below are the results of comparing the synthetic data generated by Gretel ACTGAN and Gretel TabularDP models versus real-world data. The SQS score provides an objective, statistical measure of dataset quality. The classification task shows how synthetic data stacks up against actual data when trying to predict the length of a patient's emergency room stay based on their initial diagnosis.", "In this example, the synthetic data created by Gretel ACTGAN outperformed real world data across the mean f1-score on the top 3 classification tasks measured (a Gradient Boosting Classifier, Ada Boost Classifier, and Random Forest Classifier). Tabular DP\u2019s stronger privacy guarantees came with a slight reduction in accuracy vs real world data. When using synthetic data, the goal is to find the right balance between utility (in this case, classification accuracy) and privacy protections.\u00a0", "As this data was being used internally, the hospital opted for using the ACTGAN configuration which offers real-word accuracy with strong privacy. For cases where the data would be shared outside the hospital, differential privacy offers formal privacy guarantees that meet the strictest compliance regulations and definitions of anonymity.\u00a0", "We would be delighted to discuss your specific use case for synthetic data in healthcare further. Feel free to reach out to us at hi@gretel.ai or join our Discord channel to start a conversation. We look forward to exploring how Gretel can assist you in maximizing the benefits of synthetic data for your healthcare use cases.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/classify-pii-with-the-gretel-cli", "page_title": "Classify PII with the Gretel CLI - Video", "headers": ["Classify PII with the Gretel CLI", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this tutorial, Alex walks you through the steps needed to classify PII (Personally Identifiable Information) with the Gretel CLI.", "Alex (00:05):", "Hey, I'm Alex. I'm the co-founder at Gretel. And today we're going to walk through using Gretel's APIs to label sensitive information in your CSV or log files.", "Alex (00:13):", "Let's go ahead and jump on in. First thing you'll want to do is go ahead and get logged into the Gretel console. Make sure your Gretel CLI is set up. So we'll go ahead and follow the instructions for installing CLI right now. Simple PIP command. Install Gretel Client. Should already be set up here. Okay. Now that CLI is set up, you'll want to run Gretel Configure. This will walk you through setting up the different API endpoints and things that you're going to hit. Go ahead and hit \"defaults\" for all of these. We'll be running in the cloud today, but we'll specify the keys to accessing the cloud. Grab your API key. Looking good.", "Alex (01:07):", "Just running Gretel by itself gives you an overview of the different CLI commands. Today, as we're going to be labeling data, we'll be focusing on the Gretel models and the Gretel records. Yes. To follow along here, since you're going to go straight to the docs. So let's go ahead and click on the doc link. We're going to go to discover PII and follow along with this tutorial.", "Alex (01:28):", "So from here, I'm going to start. First thing I want to do is to build a configuration. This tells the Gretel APIs what you want to search for inside data. There are a ton of different info types that you can find with Gretel. And here, if you click on this link, you can see a list of the 40 plus support info types that we have currently. This is always expanding. And you have the ability to define your own info types with Gretel. So we'll do that in the example today, but quite a list of different things that you can add to search for. Back here, we'll go ahead and grab this config, asks us to name it as classified config dynamics. Let's go ahead and do that.", "Alex (02:01):", "Go ahead and cut and paste it in there. Take a quick look at what it's doing here. So we're asking it to classify it since it's going to be training the classification model. It's going to be running these different labels, searching for person name, credit card number, phone number, and et cetera. Also, we're defining our own with a regular expression here. So here you see us finding a namespace called acne and inside of there, it has its own user ID, regular expression, searching for user underscore and then five, and then your numbers after that to define what a user ID looks like.", "Alex (02:36):", "Go ahead and save that. And then build a data set quickly. So here we have a data set, ready to go called pii.csv. Go ahead and copy that in. So here we see the columns for the CSV we're sending in, ID, name, email, and et cetera. If you're just sending in plain text, for example, chat logs or Twitter feeds, things like that, you can simply just create a single column named tweets, for example. And then copy the plain text in there. And that will work just fine for you. You see a variety of different types of data here, names, emails, visa, phone numbers, things like that. Go ahead and save it.", "Alex (03:20):", "Now we're going to create a project. The project is really just a construct inside Gretel that allows you to store models and information. You have the option of running these projects in cloud or locally. For this example, we're going to run these projects in the cloud. Go ahead and copy it in Gretel projects, create. Here, we're going to create a project name, classify example. We're going to make this our default project, so we don't have to specify that anymore. Okay. It's created. It's now our default project. So we don't need to store the project name or project ID.", "Alex (03:50):", "The next command here, go ahead and copy it in. It's going to train our model on the CSV file we sent in. And we're going to save the results here to a JSON file. It can be used for subsequent calls. So here, we're taking a look at the individual parameters we're using on the Gretel's model's API. I'm telling it to create a model using our classify config. We're giving it an input data, which is pii.csv, and we're telling it to run in the cloud. You also could just do a runner local if you want it to run locally. What's going to happen in that case is in a local configuration. It will download a container that runs locally inside your environment. We're going to store the output to a file called model.json, Which can be used as the input for the next command. Let's go ahead and run that.", "Alex (04:43):", "It should execute pretty quickly on larger CSV files, even that, it runs pretty quickly. But what it's doing is it's learning to infer different columns. For example, if it detects that a name exists in 90% of columns, it's learning to infer that most likely name, even if not detected inside a particular column, maybe a username or an email address or phone number.", "Alex (05:05):", "Great. Our model has been created. It looks like it was done correctly. Took about eight seconds of processing here. So now let's go ahead. And the next thing we're going to do is going to classify these records. This. Now we're going to call the Gretel records classify command. Instead of passing in the model ID, we're just going to have a passing this model data.json That we created earlier that contains all the information that this model needs to run. The input data, once again, is going to be API that CSV until it's run in the cloud. And we're going to tell it to output its findings from the cloud directly here to a local file system.", "Alex (05:47):", "The cloud worker is picking it up. It's just started. And it's on classify. Let's go ahead and grab these. So what I'm going to do is grab this one line right here.", "Alex (06:17):", "Go ahead and view the results here. So we're looking at one record and all the entities that were found. Here what we see is that JSON aligned limited output per line. So here we're looking at a single line. I'm using the Python utility. This a little bit easier to view. But on index zero, which is the first line, we found the following entities. We see this returned in an array. We can see the character offset within each field that these entities occurred, so in the first field, we found a field called VISA inside it. We found the label called credit card number. Highly confident that we found a credit card number in this and the classifiers here are also exporting the field that are needed to get that kind of confidence in addition to matching on the regular expression.", "Alex (07:00):", "And we see starting the character offset zero and ending in character offset 16 matched. Going all the way through the rest of the findings here, including the custom regular expression that we find here at the end, found a user ID. Highly confident and defined as well as starting to character offset zero, ending at 10.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/machine-learning-models", "page_title": "ML Models: Understanding the Fundamentals", "headers": ["ML Models: Understanding the Fundamentals", "\u200d", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Machine learning (ML) is a process by which computer programs learn to recognize patterns and behaviors in data. The goal of machine learning is to build computer models that can make predictions on new data or novel information. These models can then be used to automate repetitive tasks and make basic (and increasingly complex) decisions without human intervention. ", "To ", " a computer program to build a model, we first need to design an algorithm, which is ", " that solves a specific problem, such as calculating the probability of an event occurring or classifying an object. We then feed that algorithm a steady diet of data to ingest and process, so it can develop a catalog of examples (a model) to compare to any future inputs it receives. Generally, the more well-fed the model the better its performance will be, though good data sources are often hard to find.\u00a0\u00a0", "In this post, we\u2019ll explore the fundamentals of machine learning, the various forms of models of the physical world that can be developed through this process, as well as the expanding list of human tasks that can be tackled using these trained machines. Finally, we\u2019ll show you how some newer machine learning models are designed to ", " that can be used for training other ML models. ", "While the concept of training machines is a bit abstract, ML model applications are everywhere in our daily lives. From your smartphone making text suggestions when messaging a friend, to Uber adjusting arrival times to destinations based on dynamic traffic patterns, and Netflix recommending your next show to watch \u2013 each of these tasks is executed using ML models that have created mathematical representations of their real-world processes.\u00a0", "ML models are a great tool to help predict future events. There are an increasing number of real-world tasks, decisions, and objects that can be represented mathematically as repeatable, step-by-step processes or pixel-by-pixel images, and then converted into code that a machine can act on. A model trained on historic data can discover past patterns that then inform inferences or predictions about what may happen next.", "Each machine learning algorithm settles into one of the following basic model categories, based on how it's designed and what type of data it's trained on:", "Let\u2019s briefly discuss each.\u00a0\u00a0", "Supervised learning is a type of machine learning algorithm that learns from data with ", ". In supervised learning, your model is trained on ", ", which means that each input in the dataset has been assigned a specific label or outcome as its output. This gives the algorithm a benchmark to aim for in its predictions and improve its accuracy as it learns. Over time, the goal or ", " of the algorithm is to adjust its error rate to map more closely to a generalizable desired result.\u00a0There are two main types of supervised learning models: regression models and classification models. ", "Unsupervised machine learning algorithms are a great tool for learning hidden patterns in data. Here, you are not providing predefined classes for the model to sort data by, so the model will group the ", " ", " into various clusters of derived values or ", " that it deems measurable, informative, and non-redundant. The model will then classify these values on its own. This unguided, free-association process enables the discovery of novel patterns that a supervised learning model would not otherwise be looking for or find.\u00a0", "Unsupervised learning models are used to perform three broad tasks:", "Self-supervised learning is a bit of a hybrid model. It leverages signals from the structure of the unlabelled data it is ingesting to create a supervised task where it predicts the unobserved properties of the inputs and discerns which data points are similar and which are different. With enough information, the model can develop a form of ", " or generalized knowledge where it understands the meaning of certain information in ", ". The most popular types of self-supervised learning models are transformers.\u00a0\u00a0\u00a0", "In reinforcement learning, the algorithm can interact with its environment. Because it has a sense of agency in the world, these models are referred to as ", ". An agent\u2019s feedback system is governed by a ", " that rewards good actions and punishes bad actions and is optimized to guide this cyclical learning process towards an ideal desired outcome. These policies and ideal states can be supervised or unsupervised to seek specific rewards. This type of learning is most similar to theories of human learning, as we also learn things through experiences and feedback from our environment.", "There are several popular algorithms that come under reinforcement learning. They all work in similar ways \u2013 by processing feedback signals after each current state or action taken by the model in its environment and then optimizing its subsequent actions based on that real-time, incoming information and its policy goals. ", "Below are the main RL algorithms and their policies:", "Next, let's delve into the common model tasks in more detail.\u00a0", "\u200d", "We\u2019ve already briefly discussed two of the main categories of machine learning model tasks: classification and regression. Now, we'll take a closer look at how these models can be designed for more specific tasks. ", "Classification models are used to predict outcomes in a categorical form. They group input data into different categories and assign labels to each category. A common example is a model that identifies and labels an email as either spam or not, or a customer as likely to purchase a specific product or not. A more modern example, especially for developers and enterprises who are building products and services with large amounts of data is to ", " that may be included in a dataset, so they can ensure user privacy is protected.\u00a0", "There are two types of classifications in machine learning:", "These two classification model types can be further broken down by approach. There are ", ", but here are three of the most common and how they work:", "Regression models use supervised methods to map the relationships between data variables by fitting any inputs to a defined geometric line (either straight or curved) or a plane (in the case of two or more independent variables). This allows you to estimate how dependent and independent variables change and interact over time. An example could be measuring the strength of the relationship between two variables like rainfall and temperature. Or you could measure a dependent variable at a certain value for a certain independent variable, like the average temperature at a certain level of rainfall.\u00a0", "Here are the main types of regression model algorithms:\u00a0", "Clustering is a great way to group inputted data together based on similarities and differences. Clusters correspond to connected areas in an N-dimensional space where there is a high density of data points. The similarity of any two data points is determined by the distance between them, and there are many methods to measure the distance. These clusters should be \u2018stable\u2019 in that they should be possible to characterize using a small number of variables. ", "These models can be used for a variety of tasks like image segmentation, statistical data analysis, or market segmentation.\u00a0Here are some of the various ", ":", " is an unsupervised learning technique used to reduce the number of features or variables present in a dataset. It is the transformation of data from high-dimensional to low-dimensional space, through the deletion and combination of certain features so that the low-dimensional representation retains some meaningful intrinsic properties of the original data. This is done in order to improve the performance of models and algorithms, as well as to reduce the amount of data that needs to be processed. A GPA score is an example of a complex dataset of inputs being reduced to a single numerical representation.\u00a0\u00a0", "\u200d", ", where you decide what data variables or attributes are most important and which you can ignore. This process is foundational to dimensionality reduction and pattern recognition in all machine learning tasks. By focusing the model on a subset of features to use in further model construction, you can improve accuracy, save computation time and enhance model learning.\u00a0", "Here are the three main dimensionality reduction algorithms:\u00a0", "Deep learning models are neural networks that mimic the functioning of the human brain to find correlations and patterns by processing data with a specified logical structure. This filtering of data through three or more layers of processing allows training on labeled and unlabeled data simultaneously. Deep learning models are trained by using large sets of labeled data and neural network architectures that automate feature learning without the need for manual extraction. Unlike basic neural networks, deep learning model architectures consist of multiple hidden layers.\u00a0", "Here are the most common deep learning algorithm architectures:", "While there are many ", ", there is no one-size-fits-all answer to the question of ", ". It depends on the specific use case or task you are trying to accomplish, as well as on the number of features, associations, structures, and volume of the inputted data. Even when you do know these parameters, it is still recommended that you always start with the simplest model that can be applied to a particular problem and then gradually enhance its complexity.\u00a0", "Here\u2019s a simple decision tree for thinking through this selection process:\u00a0", "Even if your model learns the training data well, in terms of accuracy, this does not mean it will be effective in predicting new information, since by design you forced the model to do this. The essential metric is how a model performs in new scenarios and environments\u2013", "\u2013beyond its training data. That's where the practice of model validation is key.\u00a0", "Model validation requires you to split your data into two \u2014 a training dataset, which generates the model, and a validation dataset to test the model\u2019s prediction accuracy in novel situations. You can then make adjustments to improve accuracy by ", " and cross-validating outcomes to help you determine whether a more complex model is needed. One of the best ways to improve the accuracy of a model is by training it on more data.\u00a0", "Unfortunately, one of the biggest challenges engineers, software developers, and data scientists face is getting access to a supply of data that\u2019s sufficient to test an idea or design a new feature. This is because much of the data that exists today and that is used for training models contains sensitive or personally identifiable information (PII). Gaining access to such data faces ethical and legal hurdles that can be prohibitively costly or impossible to surmount. In our modern digital economy, this bottleneck stifles most research and innovation efforts being undertaken today.\u00a0", "However, as mentioned, recent innovations in advanced deep learning model architectures, such as GANs and DMs now enable the ", " that\u2019s as useful and in some cases even better than the real thing for training highly accurate AI/ML models.\u00a0", " offers on-demand software tools for identifying, labeling, and transforming existing sensitive datasets, and also a deep learning synthetic generation model for creating your own bespoke, high-quality data for training.\u00a0", "The more real-world possibilities you can feed your model, the better accuracy or generalization you will see for novel situations. That's the ultimate goal. So always be sure to train with either safe real-world datasets that are privacy-preserving or high-quality synthetic data that is properly generated and labeled.\u00a0", "\u200d", " today and start building better data.\u00a0\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/generate-synthetic-data-locally-from-the-cli-3-4", "page_title": "Setting up your Gretel environment (3/4) - Generate synthetic data locally from the CLI - Video", "headers": ["Setting up your Gretel environment (3/4) - Generate synthetic data locally from the CLI", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In part 3 of this video series on setting up your Gretel environment, Alex shows how to generate synthetic data locally from the Gretel CLI.", "(00:05): Hey, in part three of this video, we are going to train a synthetic model using a CLI on a local box, and this box could be located on premises, it could be your workstation, it could be inside one of the cloud providers, really your choice. One of the neat features of this is that your data never leaves your environment, so it really works for many compliance control to really sensitive use cases. How this works is by deploying a container, a docker container inside your environment that talks to a GPU accelerated box to train a synthetic model and generate as much synthetic data as you want. CLI is a really easy way to get started, so why don't we go ahead and jump in.", "(00:40): So we're going to go down CLI examples, create synthetic data and follow the instructions that it displays here. First thing we're going to do is create our new projects, which we're going to call healthcare and set it as our default project. We are set up and running there. We're going to download a sample data set. I think I've already downloaded this data set here, but we'll go ahead and run it again. Okay, and why don't we go ahead and take a look at our sample data set here, so we're going to run a head command on it. You can see here about 20 different fields, mixed categorical and numeric data, and some kind of interesting encoded data here. Age ranges you might see in the length of stay.", "(01:30): Our goal is to train a model that will create an arbitrary amount of synthetic data that matches the same kind of size, shape distribution, and insights of the original data. We're going to make some slight tweaks to this command, which was designed for training a model using the gravel cloud from the command line since we're running on a local box here, so let's go ahead and edit and then create our own file. We'll call it train.sh. We can copy in the example code here. We're going to make a few tweaks. So I like to look at this as one line. You can keep the config, you can go to default if you prefer, whatever configuration you like to use, but the really important thing we're going to change here is going to change from local cloud runner to local, and I prefer to use the default configs, so we're going to adjust that as well.", "(02:19): Here you can see input data providing is the CSV that we just downloaded. Output, we're telling it to drop the output file and the artifacts right here, and to save the new model that's created into a JSON, so we're outputting that to a JSON file. Go ahead and run that training file. This first step, the first time you do this, it's going to take a while because it needs to download the container from the ... Here we can see the synthetic data model finished training. It generated a data set for us. We also got a quick overview from the model report of the model quality, which we can see was very good. We can download ... Actually, I believe it went ahead and downloaded any of these different records for us to take a look at.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/introducing-gretel-amplify", "page_title": "Introducing Gretel Amplify", "headers": ["Introducing Gretel Amplify", "Introduction", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We\u2019re excited to announce the release of the Gretel Amplify model. Amplify is designed to rapidly generate large volumes of tabular synthetic data using statistical models and a hyper-efficient multi-processing implementation. Amplify runs on CPU and can generate data up to 1000x faster than deep learning-based generative models, enabling you to create large numbers of synthetic records very quickly. While Amplify is effective at learning and recreating distributions and correlations, it typically has a 10-15% drop in synthetic data accuracy versus Gretel\u2019s deep learning models.\u00a0", "Some use cases for Amplify include:", "How fast is Amplify? Let\u2019s find out. In this blog, we\u2019ll walk through how to generate 5000 MB of data to demonstrate the speed of Amplify using a cloud worker with a quad-core CPU. You can follow along with our Gretel Amplify demo notebook located ", " or here:", "After inputting our Gretel API key, we start by specifying the input data we want to amplify. These can be ", ". In this example, our input data is a ", " from Kaggle. Here, we are amplifying real-world data, but Amplify can be used to generate large quantities of data from synthetic data as well.\u00a0", "The notebook shows a preview of the input data, along with basic information about the dataset.\u00a0", "There are two ways to indicate the amount of data you want Amplify to generate. You can use the ", "num_records", " config parameter to tell the model to produce a certain number of records. Or, you can use the `target_size_mb` parameter to designate the desired size in megabytes of your output data. The maximum value for `target_size_mb` is 5000 (5 GB). Full details about the Amplify configuration file and parameters can be found in our ", ". For our demo, we want to generate 5000 MB of data, so we\u2019ll set `target_size_mb = 5000`", "Creating and submitting our model takes just two lines of code.", "Now, our model is off and running! We can use the helper function ", "poll()", "to track the model\u2019s progress.", "\u200d", "About 30 minutes later, Amplify has finished generating! Now it\u2019s time to unzip and view our results.\u00a0", "Let\u2019s get to the good part! How fast is fast? You can use the wrapper function below to extract and print stats from the model.\u00a0\u00a0", "In this example, Gretel Amplify generated 5010 MB of data in under 18", "minutes, with a generation rate of 4.7 MBps. That\u2019s 46,200,000 records at a speed of over 43,300 records per second. At that rate, you could generate 3.75 billion records a day using a single cloud worker! The total time to deliver the output data, which includes generation, artifact creation, and artifact upload, was just 33 minutes.", "Because Amplify employs multi-processing, its speed is roughly proportional to the number of CPUs used. In this demo, we ran on a single Gretel cloud worker using a quad-core CPU. Want to run even faster? Try running Amplify locally using more cores and see how fast you can generate! If you need more speed, you can scale horizontally by running Amplify on multiple workers, and/or get in touch with us at ", " to get a sneak peek of our upcoming turbocharged workers with more CPUs.", "The Gretel Amplify model is a powerful tool for generating large quantities of data in a flash. The Amplify model is available via our Console, CLI, and SDK and can be run in the Gretel Cloud or locally. For more information about Gretel Amplify, check out our ", ".\u00a0", "How will you use Amplify? Let us know! Feel free to reach out at ", " or join our ", " and share your thoughts.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/synthetic-image-models-for-smart-agriculture", "page_title": "Synthetic Image Models for Smart Agriculture", "headers": ["Synthetic Image Models for Smart Agriculture", "Autonomous farming with computer vision", "Baseline ML model architecture and results", "Handling a changing environment with synthetic data", "Overview of Synthetic Image Generation and Text-to-Image Models", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Since our post on using ", ", synthetic image models have drastically improved, especially in the domain of text-to-image generation. After all of the excitement, particularly around creative use cases, you might wonder, \u201cwhat can ", " do with these advancements?\u201d\u00a0", "There are various potential applications, and generative image models can advance solutions for a range of concrete enterprise problems. In this post, we\u2019ll walk through one: using synthetic image models to improve a computer vision application. This is valuable because we show how synthetic image models can be used to reduce costs and speed up iteration time. Specifically, we\u2019ll train a computer vision classifier for autonomous farming. We\u2019ll then show that the classifier fails with changing weather patterns like snow. Finally, we will use a synthetic image generation model to efficiently generate snowy plant images to boost the training dataset and the computer vision classification accuracy by combating data drift.\u00a0", "Improvements can come in many forms. For context, a general computer vision workflow will include data collection, curation, and cleaning before even approaching model training and validation. As we\u2019ve discussed in our ", ", synthetic data can reduce the cost of these activities and improve privacy without sacrificing quality. In this post, we\u2019ll focus on how synthetic images can improve the computer vision classification system\u2019s ability to handle unexpected events and improve accuracy in varied conditions.\u00a0", "Imagine you are an engineer for a company that produces automated farming equipment for agribusiness. The company is replacing or augmenting tractors, feeders, bailers, and even weeders with autonomous technologies. As part of this effort, you are designing a detector to distinguish crops from weeds. Farm equipment will use this detector to eliminate weeds while saving precious crops.\u00a0", "To make this more concrete, we\u2019ll look at charlock (i.e., wild mustard) as the weed and maize (i.e., corn) as the crop. These plants have overlapping ", ", and unwanted charlock growth can interfere with the corn harvest.\u00a0", "For this example, we\u2019ll outline and create a computer vision system robust to unexpected environmental variations. Specifically, we\u2019ll use a synthetic image model to generate snow-covered plants that can be used as training data to improve the generalization and performance of our automated agricultural equipment.", "We\u2019ll use a neural network as the backbone for our detection algorithm. This means that we\u2019ll need to choose a neural network architecture and train it to classify images coming from autonomous farming equipment.\u00a0", "A popular and performant computer vision architecture is the ", ", which is easy to train, stable, fast, and efficient. To train a ResNet to identify between corn and wild mustard, we\u2019ll show it hundreds of images of each that we have meticulously collected and labeled manually. This process, while both financially expensive and time-consuming, will give a great initial boost in accuracy to our weeding machine.", "After our network is trained, we look at the results and see solid performance. The ResNet18 model accuracy when trained on real data and evaluated with real data is ", ".", "Excellent!", "But there's room for improvement if we collect more data and tune our network, or change the architecture. As a proof of concept, however, it's quite good at distinguishing corn and charlock.\u00a0", "After this success, we deploy the model into our automatic weeding machines and sit back to enjoy a job well done. However, imagine we have an unexpected snowstorm in the fields, and now all the plants are covered in a thin layer of snow.", "Our classifier performance drops to ", " when deployed in this unexpected situation. As we discussed in our ", ", the ", " ", " in our environment has drastically reduced the performance of our data-dependent ML system. If left running, our automated systems could start pulling up corn and leaving behind harmful weeds. This would negatively impact our crop yield and be financially costly. As such, we don\u2019t have time to collect hundreds of high-quality images, send them to be manually labeled, retrain our classifier, and redeploy. We need to train and deploy a solution now.\u00a0", "Synthetic data is a great answer to reduce data collection costs while maintaining high utility for ", ". This case is no different! Instead of collecting images, we can synthesize images of corn and charlock with snow on them and retrain our classifier with those synthetic images included in our training data. As other researchers have suggested, this should boost our classifier\u2019s performance.", "An oft-cited ", " is that 60% of all data will be synthetic by 2024. So far, structured and unstructured synthetic data (e.g., tabular and text) has driven enormous business value, enabling ", " and ML/AI applications.\u00a0", "Over the past few years, we\u2019ve seen vast improvements in other data types like audio, images, and video. Specifically, with the advent of systems such as ", ", ", ", ", ", ", ", and more, we have seen a drastic improvement in a specific type of synthetic data generation system. This system, commonly referred to as ", " allows a user to input text description, and the generative model will output an image matching that text.\u00a0", "This paradigm has proven immensely useful for creative applications such as concept art and storyboarding. However, the ability of the system to translate a text description into an image is limited by the original training data. For example, if you want to use the open-source ", " model for the autonomous farming application and try to generate Charlock or Maize either in normal conditions or in a snowy environment, you\u2019ll find the model ill-suited for such a task.\u00a0", "While these models are powerful, they often cannot be used directly off the shelf for enterprise applications. We need to customize these models to work for our use cases. The process of customization is called ", ".\u00a0", "It takes about 15 minutes for Gretel\u2019s synthetic image model to learn a new concept on a single NVIDIA A100 GPU. After we customize the model, we can generate synthetic images of our plants in a variety of different conditions. For example, all of the plant images in the blog are synthetic!\u00a0", "For our inclement weather example, we need to generate images of our plants in the snow. It takes about an hour to generate 2,000 images of corn and charlock in the snow \u2014 such as those in Figure 2 \u2014 on a single NVIDIA A100 GPU with 40GB RAM. By augmenting our training set with these images, we see the performance return to acceptable levels. The ResNet18 model trained on real and synthetic images, including snowy images, has a 97.75% accuracy.", "In this post, we explored how to use a synthetic image generation model to improve the ability of our plant classifier to handle adverse weather conditions, resolving an issue created by data drift. At Gretel, we\u2019re building a synthetic data platform that simplifies generating, anonymizing, and sharing access to data for thousands of engineers and data science teams worldwide. Even if you have no data or limited data, Gretel can help you start building machine learning and AI solutions.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-tweets", "page_title": "Fine-tune a MPT-7B LLM with Gretel GPT", "headers": ["Fine-tune a MPT-7B LLM with Gretel GPT", "Getting Started", "Let's now explore the code involved in this project:", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In today's blog post, we're going to dive into fine-tuning a multi-billion parameter large language model (LLM) using ", ". Our focus will be on training the model using recent tweets from your favorite Twitter personalities. The objective is to enable the model to answer questions in a manner consistent with these personalities. It's a fun and engaging experiment and also showcases how we can update a large language model with new knowledge, such as recent tweets, and leverage ", " like you might use to train a downstream ML model such as a sentiment classifier or question-answering bot.", "This post was inspired by Boris Dayma's HuggingTweets project, which can be found here: ", ". Boris's project was a great starting point for our own work, and we're grateful for his contributions to the open source community.", "In our post, we'll walk you through the steps of fine-tuning a large language model using the ", ". We'll also provide some examples of how you can use the adapted model to answer questions and generate text that reflects the tone and personality of your favorite Twitter influencers.", "We hope you enjoy this post!", "First you'll need a ", " or you can use our sample dataset. A Twitter API key is easy to sign up for, and they\u2019re not charging for them (", ") ;-). You will need a ", " to use our model training and inference services. Next, run the notebook in Colab \u2192 ", "\u200d", "\u200d", "Once you have the necessary keys, the first step is to compile a list of Twitter users you want the model to emulate and a set of questions to ask the model to complete for each personality. For the purpose of this example, we'll curate a diverse group of well-known Twitter users. Then, we'll prompt the model to generate answers to specific questions to see how each different synthetic twitter personality responds to different prompts.", "Our goal is to ensure that the model responds in a manner consistent with each user's personality, while avoiding excessive repetition or content overlap in the answers. Of course, feel free to experiment with your own data and train the model on a specific domain or group of users to observe how the model responds to various questions.", "First, we'll download up to the last 3,000 tweets from each user in our list. To ensure high-quality training data, we'll preprocess the text by removing URLs, Twitter handles, retweets, and posts with less than three words.", "Once we have the preprocessed tweets, we'll create a training set where each line starts with the Twitter handle name followed by the corresponding text. While it's possible to use separators between the handle and text, it's often unnecessary with larger language models. The desired format for each line is: [handle] [text].", "To configure the training process, we'll define a Gretel training configuration in YAML. This configuration specifies the model name, tuning parameters, and project name, which will be used to store the adapted model in the Gretel service after training. Think of the project as a repository within Gretel, where you can store and share trained models with others.", "We will use the following config:", "Once we have the training configuration ready, we can initiate \u200cmodel training. Training state-of-the-art language models with billions of parameters can be a resource-intensive task. However, Gretel employs techniques like parameter efficient fine-tuning (", ") and Low-Rank Adaptation of Large Language Models (", ") to enable efficient training on more accessible GPUs, such as NVIDIA A10gs or single A100s. These techniques optimize the fine-tuning process, improving performance while consuming less memory.", "After the training is complete, we can generate queries and send them to the Gretel API. Queries are constructed in the format [handle] [prompt], where the handle represents the Twitter user and the prompt is the beginning of a sentence or question. For instance, you could have a query like \"elonmusk My dream is\" to find out how the model thinks the elonmusk twitter account would finish the prompt ;-).", "To facilitate sending the queries to Gretel, we create a record handler object and format the prompts as either a single-column CSV or a Pandas DataFrame. In this example, we'll use a dataframe.", "Let\u2019s take a look at how our favorite Twitter personalities complete the prompt \u201cI think\u2026\u201d", "It\u2019s great to see the same model being able to distinguish between the personalities of each Twitter account, and use that to answer questions in the same context. Another neat thing is to see the model focusing on recent topical areas. On areas for improvement, the model seems to really love using emojis.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/how-accurate-is-my-synthetic-data", "page_title": "How accurate is my synthetic data?", "headers": ["How accurate is my synthetic data?", "Introducing the Synthetic Data Quality Score", "Training Field Details", "Data Quality Metrics", "Field Correlation Stability", "Deep Structure Stability", "Field Distribution Stability", "Summary", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we are thrilled to launch Gretel's new synthetic data report. At the heart of the report is a new, overall Synthetic Data Quality Score metric that helps you assess the quality of your synthetic data versus the original source data. To demonstrate the new report, we trained a ", " on the \u201c", "\u201d, a popular healthcare dataset from Kaggle and then used the model to generate 5000 synthetic records. In this blog, we\u2019ll step through the synthetic report that was generated for this dataset and demonstrate how simple it is to grasp both a quick high level quality summary as well the more in-depth scoring methodology underneath.", "Our Synthetic Data Quality Score, as shown below, is 94 which is classified as ", ". This score is an estimate of how well the generated synthetic data maintains the same statistical properties as the original dataset. In this sense, the quality score can be viewed as a utility score or a confidence score as to whether scientific conclusions drawn from the synthetic dataset would be the same if one were to have used the original dataset instead. Understanding the synthetic data model\u2019s quality and accuracy is vital when the data will be used to answer critical questions such as ", ". \u00a0", "\u200d", "The Synthetic Data Quality Score appears right at the top of the report and can therefore be used as a quick summary of how things went. All circled question marks will expand into additional relevant information. For example, if you click on the question mark to the left, you\u2019ll get the below table showing how to interpret your score. When your score is Excellent or Good, any of the listed use cases are viable for your synthetic data. \u00a0When your score is Moderate, then the viable use cases are more limited. \u00a0With any score other than Excellent, you can always try and improve your model with our tips and advice", "in our ", ". \u00a0", "\u200d", "Also included at the top of the report are summary statistics about the shape of the training and synthetic data, as well scores for each of the different quality metrics. \u00a0There are 12 fields in our Stroke Prediction dataset and 5110 training records. All records and fields were used to train our synthetic model (with all default settings) and from the model we generated 5000 synthetic records. Our overall Synthetic Data Quality Score (94) was computed by taking a weighted combination of the individual quality metrics: Field Distribution Stability (96) , Field Correlation Stability (92) and Deep Structure Stability (95).", "\u200d", "For those interested in the details behind the quality scores, the report then continues on first showing an overview of the training field details.", "The high level Field Distribution Stability score is computed by taking the average of the individual Field Distribution Stability scores, shown in the right most column above. To better understand a field's Distribution Stability score, you can click on the field name to be taken to a graph comparing the training and synthetic distributions.", "To measure Field Correlation Stability, the correlation between every pair of fields is computed first in the training data, and then in the synthetic data. The absolute difference between these values is then computed and averaged across all fields. The lower this average value is, the higher the Field Correlation Stability quality score will be.", "To aid in the comparison of field correlations, a heatmap is shown for both the training data and the synthetic data, as well as a heatmap for the computed difference of correlation values. To view the details of what each square in the heatmap refers to, simply hover over \u00a0the square with your cursor. \u00a0The hover text will show you the two fields involved, as well as the correlation in the training data, the correlation in the synthetic data and the difference between the two. ", "Below are the correlation matrices for our Stroke Prediction dataset. \u00a0You can see in a glimpse how well the training correlations are maintained in the synthetic data.", "To verify the statistical integrity of deeper, multi-field distributions and correlations, Gretel compares a Principal Component Analysis (PCA) computed first on the original data, then again on the synthetic data. The idea behind PCA is to capture in just a few features the essential shape of all the features. \u00a0These new features are what is referred to as the Principal Components.", "Below are the Stroke Prediction dataset principal components computed first using the training data and second using the synthetic data. You can visually see their similarity instantly.", "Gretel computes a synthetic quality score by comparing the distributional distance between the principal components in the original data and those in the synthetic data. The closer the principal components are, the higher the synthetic quality score will be. \u00a0An example principal component comparison is shown below. As PCA is a very common approach used in machine learning for both dimensionality reduction and visualization, this metric gives immediate feedback as to the utility of the synthetic data for machine learning purposes.", "Field Distribution Stability is a measure of how closely the field distributions in the synthetic data mirror those in the original data. For each field we use a common approach for comparing two distributions referred to as the Jensen-Shannon Distance. The lower the JS Distance score is on average across all fields, the higher the Field Distribution Stability quality score will be. To aid in the comparison of \u00a0original versus synthetic field distributions, a bar chart or histogram is shown for each field. \u00a0Below are example graphs from two of the fields in our Stroke Prediction dataset.", "Synthetic data is a viable privacy choice for many different use cases. Oftentimes those use cases require high quality statistical symmetry with the original data. In this blog, we\u2019ve shown how the Synthetic Data Quality Score can be used as a quick answer as to whether your synthetic data has the quality needed for your use case. \u00a0The details behind the score can sometimes help narrow in on what might be problematic fields. \u00a0You can utilize our advice in our ", " for handling tough fields as well a multitude of other tips for improving your Synthetic Data Quality Score.", "As always we welcome feedback via email (", ") or you can find us on ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/will-jennings", "page_title": "Author: Will Jennings - Gretel.ai", "headers": ["Will Jennings", "Data Simulation: Tools, Benefits, and Use Cases", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Gretel Demo Day: Exploring the Future of Synthetic Data", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Test Data Generation: Uses, Benefits, and Tips", "ML Models: Understanding the Fundamentals", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/create-artificial-data-with-gretel-synthetics-and-google-colaboratory", "page_title": "Create artificial data with Gretel Synthetics and Google Colaboratory", "headers": ["Create artificial data with Gretel Synthetics and Google Colaboratory", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this post we\u2019ll use ", " and Google ", "\u2019s free GPUs to train a machine learning model to automatically generate fake, anonymized data with ", " guarantees.", "Today we will walk through some of the new features in Gretel\u2019s ", " open-source synthetic data library ver ", " including:", "Check out the walk-through screencast below, or click the Colab link to get started creating your own synthetic dataset!", "For a deep dive on anonymizing precise location data, check out our previous deep dive on ", ", and how we discovered and partnered with Uber to fix ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/alex-watson", "page_title": "Author: Alex Watson - Gretel.ai", "headers": ["Alex Watson", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "What's new in Beta2", "Generate textbook-quality synthetic data for training LLMs and SLMs", "Prompting Llama-2 at Scale with Gretel", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Anonymize tabular data to meet GDPR privacy requirements", "Synthetic Data and the Data-centric Machine Learning Life Cycle", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Fine-tune a MPT-7B LLM with Gretel GPT", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/ai", "page_title": "AI blog posts - Gretel.ai", "headers": ["AI", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "Filling in sparse tables with Gretel\u2019s Tabular LLM", "Nail Synthetic Data Generation Every Time with Gretel Tuner ", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Gretel Demo Day: Exploring the Future of Synthetic Data", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Generate time-series data with Gretel\u2019s new DGAN model", "Test Data Generation: Uses, Benefits, and Tips", "We just streamlined Gretel\u2019s Python SDK ", "Prompting Llama-2 at Scale with Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/what-were-reading-trends-takeaways-from-the-neurips-2021-conference", "page_title": "What We\u2019re Reading: Trends & Takeaways from the NeurIPS 2021 Conference", "headers": ["What We\u2019re Reading: Trends & Takeaways from the NeurIPS 2021 Conference", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Many of us on Gretel\u2019s research team attended the annual NeurIPS conference last month. As with every year\u2019s event, there were hundreds of fascinating papers and presentations that highlighted the future of machine learning and information science. Below are just a handful of takeaways we found interesting, particularly from a data privacy perspective.\u00a0", "If you\u2019ve read any of these papers, tweet us your feedback ", ", join our ", ", or connect with us individually on LinkedIn. We\u2019d love to hear what you think!", ":", ":", ":", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/readme-v2", "page_title": "README.V2", "headers": ["README.V2", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": [" We have a solution to this problem that we and every developer faces each day. We founded Gretel based on our beliefs that data shouldn\u2019t be scary, and for you to compete in today\u2019s world, you need to be able to use and learn from your data. ", "Companies like Amazon, Google, and Apple have the resources to give the developers the best of both worlds- data privacy and streamlined access to data. We\u2019re here to make that possible for any developer.", "In February we published our first ", " and started laying out our goal of enabling developers to safely share and collaborate with sensitive data, and our vision of democratizing building with data so everyone can use it. We asked for your feedback and ideas, and promised to share research, open source code, and provide examples. ", "In the 6 months since then, we have had conversations with nearly 100 developers and companies to understand the barriers to working with sensitive data and how we can apply privacy-enhancing technology to break down those barriers. Here is what we learned:", "In the past year, we have built a set of open-source SDKs that enable developers to label and share access to data, composable APIs to enable transformations to streaming data, and an \u00a0open-source AI-based synthetic data library that can generate artificial datasets from sensitive data with ", ", and automatically boost minority classes in datasets to ", ".", "Today, we are thrilled to release ", ". It\u2019s free, and you can get started in minutes with one of our guides for ", ", or even generating ", " with differential privacy guarantees.", "We are building Gretel for developers like you, so don\u2019t be shy. Please follow us here, ", ", and ", ". Want to see for yourself? ", " We\u2019re ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/nail-synthetic-data-generation-every-time-with-gretel-tuner", "page_title": "Nail Synthetic Data Generation Every Time with Gretel Tuner", "headers": ["Nail Synthetic Data Generation Every Time with Gretel Tuner ", "Introducing Gretel Tuner \ud83c\udf9b\ufe0f", "Ready to try Gretel Tuner? Start here \ud83d\udc47", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Before you start training a machine learning model, you have to make important choices like the number of layers in a neural network, the embedding dimension, and the details of how training should proceed. These so-called \u201c", "\u201d are ", " learned during training and can have a dramatic impact on the quality of your model. In the case of generative models, optimized hyperparameters can be the difference between low- and high-quality synthetic data.\u00a0", "It is therefore essential to systematically and objectively tune your model\u2019s hyperparameters for your particular use case. That\u2019s why today we are excited to announce ", " our new config-driven tool for efficiently sweeping the hyperparameters of ", " models.", "We have built Gretel Tuner directly into our ", ". Our implementation was designed with the following features in mind:", "The third point on custom metrics is especially exciting, since this makes it possible for our users to ", " Gretel Tuner (\ud83d\ude09) to their use cases. For example, we have found optimization metrics based on downstream machine learning tasks to be very effective at tuning hyperparameters to yield models that generate synthetic data of high fidelity ", "utility for machine learning.\u00a0\u00a0", "The best way to start sweeping hyperparameters with Gretel Tuner is to work through our introductory and advanced Google Colab tutorials:", "To install Gretel Tuner locally, simply add the [tuner] option to the SDK\u00a0installation command:", "This will install the Gretel client along with the new tuner module and its associated dependencies.\u00a0", "We\u2019re excited to see what you\u2019ll achieve by using the Gretel Tuner alongside the Gretel Synthetic Data Platform and our fully featured SDK!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/how-we-accidentally-discovered-personal-data-in-a-popular-kaggle-dataset", "page_title": "How we accidentally discovered personal data in a popular Kaggle dataset", "headers": ["How we accidentally discovered personal data in a popular Kaggle dataset", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In the coming weeks, the ", " will be available for developers everywhere. We are so excited and we want to discuss some of the features early on so you can start to think about all the workflows that are possible!", "In this post we will share some of the upcoming features in Gretel, and how those features enabled us to discover personally identifiable information including full names and email addresses in the popular Lending Club financial dataset on ", ".", "Gretel helps developers share data more safely so you can build faster and better. To do that, we are building workflows that enable you to better understand your data and then make informed decisions on how to make this data safe to share. This workflow is outlined below:", "When you use Gretel, your data\u2019s first stop is our labeling pipeline, for automatic data labeling. We use a combination of ", " (NLP) featuring neural network-based entity recognition for names and addresses, managed regular expressions, and custom extractors to label different types of entities. In the NLP space, this is referred to as ", " (NER).", "Gretel offers NER in two flavors:", "The data structure will be the same whether or not you are using our synchronous API or our publish/subscribe project stream. That will allow you to build applications that function the same regardless of which API you use.", "Now, we\u2019ll walk you through automatic data labeling in Gretel, using real Lending Club loan data from Kaggle. (We were impressed by its vast dimensionality of the data set, so have been using it for some of our own testing!) The Lending Club loan dataset is approximately 2.2 million records with 147 fields in each record. We used one of our language client libraries to stream all the records through our API and explore the entity index that was built.", "Take a look at the Lending Club example below (we altered the record before testing with it to not reveal any of the original data). All labeled record results have a \u201crecords\u201d array. Each element in this array is a pair of the original record we received and our attached metadata. Each element in the \u201crecords\u201d array has two top level keys: \u201cdata\u201d and \u201cmetadata.\u201d The \u201cdata\u201d key is the original record we received. The \u201cmetadata\u201d key contains all of the information we learned about this record.", "Note that we assign a unique \u201cgretel_id\u201d for every record you send us. In synchronous mode, the ID is returned along with all completed data. When you use the asynchronous API, you only receive the ID back and the labeled data is available in your project stream.", "We also provide metadata information on a per-field basis. As you can see, in this record (and several others) it appears the \u201cdesc\u201d field has a lot of free-form text which can easily contain sensitive information. Any fields that have entities detected will have a \u201cner\u201d key and an array of label objects. Once this metadata is created, you can plug into our Transformer SDK, use our rule-engine Docker container to do specific routing based on detected entities, or build your own workflows.", "Looking closer at the label object structure:", "If you followed our example closely, you may have realized that as a result of our data labeling, we discovered personally identifiable information in this public data set. To our surprise, we noticed a handful of specific identifiers, like email addresses and phone numbers, when labeling the data. At first, we had assumed they were redacted and substituted with fake surrogate data. Ultimately, the Gretel data labeling service discovered 27 records containing email addresses, first and last names, and phone numbers.", "We take privacy very seriously, so we immediately notified Lending Club of our discovery and have been working with their security team to remediate personal information in their public datasets. But had this data been scanned by Gretel originally, these records could have been properly sanitized, fields dropped (like the verbose \u201cdesc\u201d field), or the entire record dropped before releasing publicly.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-gpt-sentiment-swap", "page_title": "Gretel GPT Sentiment Swap", "headers": ["Gretel GPT Sentiment Swap", "\ud83e\udee3 A sneak peek at what we\u2019re building", "\ud83d\udc69\u200d\ud83d\udcbb Follow along and code with us!", "A note about reproducing our results", "Create your Gretel account", "\ud83c\udf9b\ufe0f Fine tuning an LLM with Gretel GPT ", "The fine-tuning dataset", "Product review pairs", "Pair selection metric", "Where to find the review-pair datasets", "Submitting a fine-tuning job to the Gretel Cloud", "\ud83e\udd16 Generating sentiment-swapped reviews", "Submit a generation job to the Gretel Cloud", "Inspect the results", "\ud83e\ude9c Next Steps", "Pair semantically similar reviews", "Sentiment swap \u201cmeh\u201d reviews", "Explore more use cases", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Here at Gretel, we are helping businesses fine tune large language models (LLMs) on ", ". Whether your goal is to diversify, balance, or fully synthesize a natural language dataset, our ", " API can streamline your task using commercially viable models, which can run in Gretel\u2019s managed cloud service or entirely within your own private environment.\u00a0", "In this blog post, we use Gretel GPT to fine tune a multi-billion parameter LLM to generate product reviews of a specified sentiment. The goal? To enhance sentiment analysis training sets, especially in areas where certain sentiments are sparse. By doing so, we can more robustly train sentiment classifiers, optimizing their performance in discerning customer sentiments within product reviews. ", "This approach isn\u2019t limited to product reviews; it\u2019s equally effective for improving the detection of toxic or abusive language on your platform. By fine tuning the Gretel GPT model with your own data, you can rapidly generate high-quality labeled examples. This not only reduces the time needed but also significantly cuts costs compared to traditional data collection and manual labeling methods.", "Let\u2019s go \ud83d\ude80", "By the end of this post, we will have a fine-tuned LLM that swaps the sentiments of product reviews. For example, suppose we have a negative review about the game Madden NFL 2001. ", ", our fine-tuned LLM will swap the sentiment as follows:", "In the above example, the input review provides negative feedback about the game\u2019s graphics and gameplay, whereas the output review is enthusiastic about both of these characteristics. Positive feedback about other aspects of the game and/or the customer\u2019s overall experience would also be acceptable output \u2014 so long as the sentiment-swapped review sounds like a plausible review about the same game.", "It turns out that the above example is a real generation from an LLM that we fine tuned while creating this post! Continue reading to learn how you can build your own version using ", ".\u00a0", "The code and data used for this blog post can be downloaded from the gretel-gpt-sentiment-swap folder in our ", ". If you only want to clone the code used in this post, you can use GitHub\u2019s ", ":", "Next, install the project dependencies (preferably within a virtual Python environment):", "Alternatively, if you\u2019d rather not install any packages locally, you can follow along with this ", " in Google Colab (note that the first job it runs takes a few hours to complete).", "To reproduce all the results of this post, you will need to run three scripts \u2014 the first builds the fine-tuning dataset, the second submits the fine-tuning job to the Gretel Cloud, and the third uses the fine-tuned model to generate synthetic data. Each script has command-line options, with defaults set to the values we will use in this post. You can see all the options for a given script using the `-h` or `--help` argument.\u00a0", "If you don\u2019t already have one, create an account on the ", ". Our\u00a0 developer tier comes with 60 credits (5 hours of free compute) per month and a 4-hour maximum runtime limit, which is enough for you to follow along and fine tune a model with us!", "To build our sentiment-swapping LLM, we will fine tune ", " using ", ".\u00a0", "What about the fine-tuning dataset? This is one of the most important choices we need to make. For our particular use case, we need a dataset with the following properties:", "While it is generally quite challenging to find an existing dataset that satisfies all your requirements, the ", " dataset is almost exactly what we are looking for. It contains millions of product reviews across 46 distinct product categories. And importantly for us, each review has an associated star rating, which we will use as a proxy for customer sentiment.", "To create our fine-tuning dataset, we will use the following fields:", "In this blog post, we will build fine-tuning examples from the ", " subset of the full Amazon dataset, but you are free to pick a different subset (note that a larger subset will take more time to train and may not be fully covered by our developer tier). Our dataset creation code can be viewed in the ", " script, where we fetch the data subset from the ", ", ", ", ", ", and ", ", as described below.", "The fine-tuning examples will consist of ", " of opposite sentiment. For simplicity, we will focus on 5-star and 1-star reviews, which we associate with positive and negative sentiment, respectively.\u00a0", "Here is an example review pair:", "For each review pair in the dataset, we will randomly select whether the positive or negative review is first. This will train the model to go both directions in sentiment \u2014 from positive to negative and from negative to positive. At training time, the model\u2019s job will be to generate both reviews. At inference time, we will simply prompt the model with all the text in a review pair, excluding the content of the second review.", "For products with many 1-star and/or 5-star reviews, how do we choose which reviews to pair? There\u2019s no one right answer to this question, as it depends on your particular use case and dataset. We have implemented two possible pair selection metrics.\u00a0", "The first metric selects reviews with the most \u201chelpful votes\u201d from users that found them useful. The assumption is that reviews with more votes will generally be more coherent, relevant, and interesting than reviews with less votes. In cases where there\u2019s a tie in votes, we randomly select one of the reviews. This selection metric runs relatively quickly and is therefore the default choice. You can view the associated code in ", " of the ", " script.", "For the second selection metric, our goal is to pair semantically similar reviews. We accomplish this by creating dense embeddings of each review using the ", " library. We then pair reviews with the highest ", ". This selection metric is more computationally demanding than the default metric described above. After batching the reviews for efficiency, it takes about 10\u201320 minutes to pair the reviews on an M2 Macbook Pro. You can view the associated code in ", " of the ", " script.", "When running the script, you can change the pair select metric using the `--pair-metric` command-line argument, which can be either `helpful_votes` or `cos_sim`. For this blog post, we will stick to the default values for all command-line arguments:", "When you run ", ", the created dataset will be saved as a compressed csv file in the ", " inside this project\u2019s root directory. Additionally, an associated \u201cconditional prompts\u201d file will be created, which contains test prompts for products that only have 1-star or 5-star reviews (i.e., products for which it is not possible to make a review pair). These conditional prompts will be used later to assess the quality of the generated reviews.", "For convenience, we\u2019ve already created datasets based on the `Video_Games_v1_00` (16,420 review pairs) and `Apparel_v1_00` (20,000 review pairs) data subsets using both pair selection metrics, which should be downloaded when you clone the code repository.\u00a0\u00a0", "With our dataset in hand, we\u2019re ready for the easy (and fun!) part: fine tuning an LLM using Gretel GPT. We will carry out the fine tuning using the ", ". To submit our fine-tuning job, we run the ", " script with default arguments:\u00a0", "This will create a ", " named \u201cgretel-gpt-sentiment-swap\u201d and start our fine-tuning job. Within the project, a model will be created with a name that indicates which data subset and pair selection metric you chose. If you used the video game data subset, the model should take a bit over three hours to train.\u00a0", "You can monitor your job\u2019s progress in the ", ". Upon completion, a ", " will be generated to assess how well the synthetic data captures the statistical properties of the training data. This quantitative report is incredibly useful for building confidence in the quality of your synthetic data, though sometimes nothing beats looking at some example generations yourself, which we will show you how to do next.", "Once our model is finished training and we\u2019re happy with its synthetic data quality report, we can either use it to generate completely new sentiment-swapped review pairs, or we can prompt it to generate the opposite sentiment of provided reviews. We will take the latter approach.\u00a0", "As mentioned above, we will generate a sentiment-swapped review by prompting our fine-tuned model with a product review pair, excluding the second review.\u00a0", "For a concrete example, here\u2019s a prompt that will generate a positive review for the game Jambo! Safari Animal Rescue on Nintendo DS:", "The file of ", " we created previously contains 100 conditional prompts similar to the above example. Using this file as our input ", ", we run the ", " script with default arguments to submit a generation job to the Gretel Cloud:", "This script will fetch our default project and use the last model we trained (you can use a specific model using the `--model-id` argument) to create sentiment-swapped reviews based on our input conditional data source.", "The script should take about 5-10 minutes to complete, after which it will save the model generations in the ", ". Two files will be created: one ", " that contains the raw model outputs (i.e., the generated reviews) and another ", " with the complete product review pairs for inspection.", "Our conditional data source file prompts the model using both positive and negative conditional reviews. Let\u2019s take a look at a couple review pairs from each category. In the examples below, the normal and bold fonts indicate the prompts and generations, respectively.", "At quick glance, it looks like our fine-tuned model works well! The content of each generated review is relevant to the product mentioned in the prompt, and the review sentiments are swapped for all pairs. The model also captures the tone and informal writing style of the real reviews, including typos, words in all caps, and missing punctuation.\u00a0\u00a0", "You can view all 100 sentiment-swapped review pairs based on our default script settings in the ", " that was output by our generation script.", "As a first next step, we recommend repeating the above exercise using the ", " to select review pairs. How do the model generations differ from our default \u201chelpful votes\u201d pair selection metric? You can also try using a different data subset and/or combining subsets from the Amazon Customer Reviews dataset.\u00a0", "For simplicity, we limited our fine-tuning data to 1-star and 5-star reviews. It\u2019s also possible to include 3-star reviews or even all possible star reviews in your training set. You would need to put some thought into how to construct the training examples \u2014 for example, instead of review pairs, you might use a sequence of 1\u20135 star reviews for each product.\u00a0", "The workflow we followed in this blog post can be adapted to a wide range of domains and use cases. Indeed, with enough domain-specific data for fine tuning, Gretel GPT can help you conditionally generate realistic doctor\u2019s notes, legal contracts, meeting transcripts, or even in-game banter between players in a chat app \u2014 just to name a few possibilities! ", "Sign up for free and ", " with Gretel. ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/data-science", "page_title": "Data science blog posts - Gretel.ai", "headers": ["Data science", "What's new in Beta2", "Data Simulation: Tools, Benefits, and Use Cases", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Machine Learning Accuracy Using Synthetic Data", "What is Data Anonymization?", "Generate time-series data with Gretel\u2019s new DGAN model", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Gretel GPT Sentiment Swap", "Comprehensive Data Cleaning for AI and ML", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/fast-data-cataloging-of-streaming-data-for-fun-and-privacy", "page_title": "Fast data cataloging of streaming data for fun and privacy", "headers": ["Fast data cataloging of streaming data for fun and privacy", "The Metastore", "Deep Dive", "Disclosure", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel\u2019s APIs, by default, automatically build a metastore that makes it easy to understand what is inside of your data. Many developers are used to working with metastores that are provided by technologies like Hive, and incorporated into services like AWS Glue. The metadata provided here is extremely useful, but not very friendly to work with. At Gretel, we strive to make everything accessible through our REST API, a concept virtually every developer is familiar with.", "Once you start sending records to the Gretel API, indexing to the metastore automatically begins. In this case, we were streaming several hundred of the GBFS ", " feeds into our API. The most obvious field from the specification is \u201cbike_id\u201d, as this is most likely a primary identifier.", "Pretty straight forward here, most of the values are strings, we\u2019ve seen over 17 million records with this field, with an approximate unique count just over 1.1 million. Our initial hunch is 1.1 million unique IDs is ", "of e-bikes and scooters out there\u2026", "Understanding what other fields in your data that can correlate to your primary IDs / keys is a vital part of privacy introspection. Remember, even if a unique ID is removed, there could very well be another field or combination of fields that provides a similar level of attribution (see the ", "). One technique we use to find other potentially identifiable fields is comparing field names using word embeddings to infer relationships", "By creating Word2Vec and FastText models trained on the schemas of thousands of open-domain databases and tables, we\u2019re able to do fast comparisons between schema field names to clue us in to other fields we may want to pay attention to. This is great because it gives us clues ", "Comparing all of the other fields in the records yields two closely related fields (based on field name): \u201cis_ebike\u201d and \u201cjump_vehicle_name.\u201d", "Looking at \u201cis_ebike\u201d shows us this is most definitely a boolean value (which is almost always set to one value so basically a constant). It\u2019s also not very common, with only 11k entries, so probably only being reported infrequently.", "Moving on to the next field with the metastore tells a slightly different story.", "So we see an overall lower number of fields with this name, not surprising, as this is only one operator of several sending location data. But what\u2019s more surprising is the lower overall approximate unique count of field values relative to the total field count.", "When comparing the percentage of unique values between \u201cbike_id\u201d and \u201cjump_vehicle_name\u201d at this point in time, it is approximately 6.6% vs 0.4% respectively. As we discuss below, the p_unique value for \u201cbike_id\u201d will generally level off while the value for \u201cjump_vehicle_name\u201d would infinitely approach 0 as time moves on.", "We decided to take a closer look and pull some of the raw data and see what was going on with these IDs. With respect to the \u201cbike_id\u201d fields, we found that in general, over time, the cardinality of this field was steadily increasing. It turns out that some data producers were actually generating ephemeral \u201cbike_id\u201d values on a steady interval. This would explain why the cardinality was increasing steadily.", "When we isolated the \u201cjump_vehicle_name\u201d field, however, the cardinality over time was basically linear. To us, this indicated that this field probably has a one-to-one mapping with an actual vehicle.", "This field was also only 6-characters long, more friendly for a small screen, while the \u201cbike_id\u201d field usually appeared to be some type of UUID.", " ", "By generating snapshots of the GBFS feed, it becomes fairly straight forward to create a \u201cride record\u201d that shows a change in a scooter\u2019s GPS location based on a previous value. Plugging this ID into that dataset shows some unique rides. By computing the geodesic distance between coordinates we can also create a filter to show changes in location that are above a certain value (to avoid GPS drift).", " Observing bikes through the app (or getting the vehicle name from the bike itself) combined with aggregated feed data could allow ride reconstruction for specific individuals as they rent scooters.", "When deciding to suggest the removal of this data we originally looked at HackerOne but ran into a roadblock. None of us are super active HackerOne users so by default, we don\u2019t have any \u201csignal\u201d (aka street credit) on that system to actually submit anything. Then we decided to do what any other developer would do, check out GitHub issues! Turns out there had already been some discussion around \u201cbike_id\u201d randomization and there was a small subset of vendor contributors active on the issue trackers.", "This felt like the right place because fundamentally we wanted to ensure visibility and that we were reaching the right audience. We posted the information as a new issue, and within minutes we had a response, and within hours the data was removed from the feeds. Clearly there was already someone empowered with decision making monitoring this GitHub repository and we were able to provide them the right insight to make a change happen same-day.", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/healthcare", "page_title": "Healthcare blog posts - Gretel.ai", "headers": ["Healthcare", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Deep dive on generating synthetic data for Healthcare", "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/create-high-quality-synthetic-data-in-your-cloud-with-gretel-ai-and-python", "page_title": "Create high quality synthetic data in your cloud with Gretel.ai and Python", "headers": ["Create high quality synthetic data in your cloud with Gretel.ai and Python", "Set up your local environment", "Generate an API key", "Setup your system and install dependencies", "Create the virtual environment", "Install required Python packages", "Train the model and generate synthetic data", "Compare the source and synthetic datasets", "Want to run through end to end?", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Whether your concern is ", " for Healthcare, ", " for the financial industry, or ", " or ", " for protecting consumer data, being able to get started building without needing a ", " (DPA) in place to work with SaaS services can significantly reduce the time it takes to start your project and start creating value. Today we will walk through an example using Gretel.ai in a local (your cloud, or on-premises) configuration to generate high quality synthetic models and datasets.", "To get started you need just three things.", "We recommend the following hardware configuration: CPU: 8+ vCPU cores recommended for synthetic record generation. GPU: Nvidia Tesla P4 with CUDA 10.x support recommended for training. RAM: 8GB+. Operating system: Ubuntu 18.04 for GPU support, or Mac OS X (no GPU support with Macs).", "See TensorFlow\u2019s excellent ", " for GPU acceleration. While a GPU is not required, it is generally at least 10x faster training on GPU than CPU. Or run on CPU and grab a \u2615.", "With an API key, you get free access to the Gretel public beta\u2019s premium features which augment our ", " for synthetic data generation with improved field-to-field correlations, automated synthetic data record validation, and reporting for synthetic data quality.", "Log in or create a free account to ", " with a Github or Google email. Click on your profile icon at the top right, then API Key. ", " and copy to the clipboard.", "\u200d", "We recommend setting up a virtual Python environment for your runtime to keep your system tidy and clean, in this example we will use the Anaconda package manager as it has great support for ", ", GPU acceleration, and thousands of data science packages. You can download and install Anaconda here ", ".", "Install dependencies such as ", ", Tensorflow, Pandas, and Gretel helpers (API key required) into your new virtual environment. Add the code samples below directly into your notebook, or download the complete ", " from Github.", "Load the source from CSV into a Pandas Dataframe, add or drop any columns, configure training parameters, and train the model. We recommend at least 5,000 rows of training data when possible.", "Use Gretel.ai\u2019s reporting functionality to verify that the synthetic dataset contains the same correlations and insights as the original source data.", "Download your new synthetic dataset, and explore correlations and insights in the synthetic data report!", "\u200d", "Download our ", " on Github, load the notebook in your local notebook server, connect your API key, and start creating synthetic data!", "At Gretel.ai we are super excited about the possibility of using synthetic data to augment training sets to create ML and AI models that generalize better against unknown data and with reduced algorithmic biases. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Like gretel-synthetics? Give us a ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/grace-king", "page_title": "Author: Grace King - Gretel.ai", "headers": ["Grace King", "Generate Synthetic Databases with Gretel Relational", "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "Introducing Gretel Amplify", "Generate synthetic Taylor Swift-like lyrics using Gretel GPT", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/we-just-streamlined-gretels-python-sdk", "page_title": "We just streamlined Gretel\u2019s Python SDK", "headers": ["We just streamlined Gretel\u2019s Python SDK ", "A first look at our SDK\u2019s new interface", "Ready to build with Gretel? Start here \ud83d\udc47", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["At Gretel, we strive to build tools that our users love. Our goal is to make Gretel the simplest and most enjoyable way for developers to generate high-quality synthetic data. This is why we are beyond excited to introduce the new high-level interface for our ", ", which we designed to be as simple, flexible, and intuitive as possible for our users.", "With nearly 900k downloads, our Python SDK is one the most popular ways to build with Gretel. With the new interface, our users will save time (and brain power!) by writing ", " than was previously required. Their workflows will also be dramatically simplified through more thoughtful logging, helper methods for assessing synthetic data quality, and a streamlined way to dynamically customize model configurations. And since the interface is built on top of the lower-level SDK, no changes to any existing code are necessary.", "Sounds awesome, right? Let\u2019s dive in!\u00a0", "The high-level interface is implemented in our SDK as the new Gretel object, which serves as a one-stop shop for interacting with Gretel\u2019s APIs, models, and the associated artifacts.\u00a0", "With the new interface, training a state-of-the-art deep generative model from scratch only takes a few lines of code:", "Behind the scenes, Gretel spins up the necessary compute resources, loads and configures the model, and trains (in this case) our tabular ", " model on the data in the input csv file.", "Gretel\u2019s ", " is automatically generated and fetched for you to assess the quality of the model without leaving your notebook:", "On-demand synthetic data generation using any of your previously trained models is simple: ", "Once again, Gretel will spin up the necessary compute resources to generate the data, so you don\u2019t need to worry about managing cloud infrastructure or GPUs to run your model!\u00a0", "Upon completion of the generation job, your synthetic data will be automatically fetched and stored as a Pandas DataFrame, making it straightforward to integrate into your data pipeline:", "The best way to get started with the Gretel SDK and its new high-level interface is to work through the ", ", which are a series of Colab notebooks that are designed to provide a solid foundation for building your own projects with Gretel.\u00a0", "If you are already building with Gretel, you can start using the new interface right away by updating to the most recent version of our Python SDK:", "We can\u2019t wait to see what you create with our synthetic data models and new SDK upgrades. Thanks for reading, and happy coding!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/automate-synthetic-data-pipelines-with-gretel-workflows", "page_title": "Automate Synthetic Data Pipelines with Gretel Workflows", "headers": ["Automate Synthetic Data Pipelines with Gretel Workflows", "What are Gretel Workflows?", "Frequently Asked Questions (FAQ)", "How are workflows different from models?", "Are workflows available for hybrid environments?", "Which connectors do you support?", "How much will Workflows cost?", "I already use a workflow management tool. Will Gretel Workflows replace it?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Synthetic data projects typically consist of two phases: choosing a synthetic model that provides optimal accuracy and utility, and deploying the selected model(s) in a pipeline so the results are up-to-date and accessible throughout your organization. This isn\u2019t a one-time process. It needs to be done over and over, on a regular basis, to make sure you and your team always have updated and relevant data as you build software, analyze business data, and train machine learning or AI models.\u00a0", "A key factor in ensuring the success of your synthetic data strategy is the ability to quickly integrate data generation tasks into your existing processes. To help achieve this goal, we\u2019re thrilled to announce the general availability of Gretel Workflows, a powerful orchestration tool now available to the Gretel community. Use Gretel Workflows to accelerate deployment times by 30%, eliminating the hassle of writing scheduled batch jobs and propelling your data strategy forward efficiently.", "With Workflows you can:", "Gretel Workflows provide a ", " for automating and operationalizing Gretel. Easy-to-read YAML configuration files are used to create a sequence of instructions for the Gretel platform to execute. Not so keen on editing YAML? Use the Gretel Console to build a Workflow using a step-by-step interface.", "With Workflows, you can connect to various data sources such as AWS S3 or MySQL (coming soon), train and run one or more models, and schedule recurring jobs to automate your synthetic data pipeline. The resulting synthetic data can be accessed from within your data services, thus staying within your control, while also being shareable with authorized teams in your organization.\u00a0\u00a0", "Here are some ways in which Workflows can be used:", "Workflows are available now in the Gretel Console as well as the SDK/CLI. ", " or ", " to start building today.\u00a0", "Workflows can train and run one or more models. Typically, you\u2019d train a single model to get the optimal accuracy and privacy for your use case, and then create a scheduled workflow. The workflow could connect to your S3 bucket or other data store, train one or more models, run them, and output data back to the remote destination.\u00a0", "We\u2019re actively working on supporting workflows in hybrid environments and the functionality will be live shortly.", "We\u2019re launching with AWS S3 support, and Google Cloud Storage (GCS) and Azure Blob Storage will both be available soon. We\u2019re also building connectors for relational databases and data warehouses, such as MySQL, PostgreSQL, and Snowflake.\u00a0", "At this time, there's no additional cost to using Workflows. Note that on the free tier, processing multiple files at once or creating lots of scheduled workflows could increase consumption because of the number of models being created. However, we're currently offering 60 free credits per month instead of 15, so you have the flexibility to try out all these new features without running out of credits.", "Gretel Workflows aren't intended to replace a workflow tool like Airflow. They\u2019re a step in your data ops orchestration, alongside ETL and other processing tasks.\u00a0", "Questions? Feedback? Hop on ", ". We\u2019d love to hear how you use Gretel\u2019s workflows and connectors in your organization.", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/privacy", "page_title": "Privacy blog posts - Gretel.ai", "headers": ["Privacy", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "What's new in Beta2", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "What is Data Anonymization?", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Test Data Generation: Uses, Benefits, and Tips", "We just streamlined Gretel\u2019s Python SDK ", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Gretel is live on Google Cloud Marketplace \ud83c\udf89", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/gretel-demo-day-exploring-the-future-of-synthetic-data", "page_title": "Gretel Demo Day: Exploring the Future of Synthetic Data", "headers": ["Gretel Demo Day: Exploring the Future of Synthetic Data", "What We\u2019re Building", "A Tabular LLM \ud83e\udd16", "A Use Case for a Medical Researcher", "A Use Case for a Financial Analyst", "A Model Playground \ud83d\udedd", "Automated Workflows \u2699\ufe0f", "\ud83c\udf9b\ufe0f", "The Future of Synthetics", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This week, we had a blast sharing some of our latest advancements with the AI community,\u00a0 including our new Model Playground and Tabular LLM, a model tailored for tabular data generation. We also demoed our new no-code solutions for operationalizing and automating synthetic data generation tasks in your existing data pipelines. In case you missed it, below we cover key highlights from the ", ". We\u2019ve included timestamped links throughout so you can follow along with the ", ". Let\u2019s go! \ud83d\ude80\u00a0", "At Gretel we believe data is infinitely more valuable when it can be safely shared. That\u2019s why we\u2019re focused on building better data that excels on downstream performance while safeguarding data sensitivity and protecting personal privacy. That's where the science of synthesizing data comes in.\u00a0", "While it\u2019s still early days for generating high quality synthetic versions of raw data that you can trust for secure analytics and model training, it is an emerging market that\u2019s gaining momentum fast. Every day, we have conversations with data scientists and AI engineers who are unlocking enormous business value by synthesizing data. Synthetic data is increasingly becoming both a catalyst for innovation as well as a tool for privacy compliance and model validation. As Microsoft researchers recently showed, synthetic data can even ", ". This streamlined, cost-effective, and privacy-compliant solution suggests a promising future for responsible AI development.\u00a0", "Policymakers are already leveraging synthetic data to enhance regulatory oversight, too. For instance, the UK\u2019s Financial Conduct Authority (FCA) used synthetic data to support its ", ", which offered startups and enterprises a controlled regulatory environment for stress testing and evaluating their innovative financial technologies before deploying to market. ", " and resource by participants in the pilot program, and suggested that providing higher quality synthetic data will only enhance the benefits of the program. This underscores the growing importance and usefulness of synthetic data in innovative financial services environments, particularly in sandbox settings where new technologies and approaches are tested and refined.", "In the U.S., President Biden\u2019s ", " aimed at guiding responsible AI development and use across the federal government called for an acceleration in the federal adoption of privacy enhancing technologies, like synthetic data and differential privacy. The draft language of the EU's AI Act, particularly regarding model testing and training, ", ". If this language stays in the final law that\u2019s expected to be enacted in 2024, it will have a transformative global impact on AI regulation and data governance.\u00a0", "We expect these trends to continue with enterprise teams developing more bespoke, domain-specific models and applications, and policymakers and regulators adopting synthetic data to support privacy preservation without stifling innovation. We\u2019re building towards this future. Here\u2019s how \ud83d\udc47", "During the Demo Day, we showcased our new ", " - the first AI system designed for large-scale tabular data generation. The application excels at generating, augmenting, and editing datasets through natural language of SQL prompts. In one example, our VP of Product & Applied Science Sami Torbey, showed how you could augment or impute missing values, and even ", " to be from France, complete with real French names and cities, at the click of a button. C\u2019est tr\u00e8s chic! \ud83d\ude2e \ud83c\uddeb\ud83c\uddf7", "Our co-founder and CPO Alex Watson walked through a more advanced example ", "t that mimicked the correlations and statistical properties of the real data.\u00a0", "To make this a bit more concrete, here\u2019s how a prompt for a healthcare task might look, if you were doing clinical research and you\u2019re data was biased due to limited records or historical inaccuracies:", "The model produces a table with name, age, gender, diagnosis, last doctor visit, blood pressure reading, and weight, as instructed. The male and female records in the table should conform to a 50/50 distribution. Results usually appear in 15 seconds or less, so if the results are not satisfactory, you can easily change or append to your previous prompt and try again.", "Imagine you\u2019re a financial analyst at a hedge fund. You\u2019re tasked with forecasting e-commerce consumer trends in various regional markets. However, your current data is limited and missing a lot of context. With Gretel\u2019s Tabular LLM you could:\u00a0", "Early access is now available, sign up to join the waitlist ", ".", "Gretel\u2019s Model Playground is an interactive and engaging way for users to get tabular results from our proprietary LLM, especially if you have few or no training data (e.g. cold start). Secondarily, the playground offers a more natural and intuitive way for users to engage with our Gretel GPT model, which is powered by some of your favorite LLMs (e.g., Llama2 and Falcon). This ensures you get results back to help you tweak your prompt in near-real time, and also have a delightful experience while building data for your use case.", "The main challenge we hear from enterprises is operationalizing synthetic data generation. Navigating the complexities of integrating this technology into existing workflows can be daunting.That\u2019s why we\u2019ve developed Gretel Workflows, our automated, end-to-end solution for simplifying synthetic data pipelines. Our Product Manager Grace King walks through a couple of ways you can ", ".\u00a0\u00a0", "These workflows easily integrate it into your existing data pipelines using scheduling, cloud storage, database, and data warehouse connectors, and no-code configuration. This allows synthetic data to be created on-demand and made accessible wherever and whenever you need it.", "During the Demo Day, someone raised the important question of ", ". The key lies in the quality of the models we design and use, and the data that feeds into them.", "Think of it this way: If you're baking a cake, the quality of your ingredients and the recipe you follow will determine how well it turns out. Similarly, in creating synthetic data, the 'ingredients' (i.e., training data) we give to our AI models, and how we prompt the model with recipes of instructions and hyperparameter configurations, the better your synthetic data. To ensure we meet the highest standards of quality, Gretel provides a ", " for generated data \u2013 it\u2019s like a taste test to assess if your synthetic data is accurate and private before you pull it out of the oven. \ud83d\udc68\ud83c\udffb\u200d\ud83c\udf73\ud83d\udd25", "2024 is shaping up to be another landmark year in the evolution of privacy-preserving generative AI. We are witnessing an increasing trend among clients, especially in healthcare, life sciences, and financial services, towards exploring multimodal synthetic data applications. There's a rising chorus from policymakers in the US, EU, and UK advocating for the development and implementation of privacy-enhancing technologies like synthetic data. Moreover, the tangible business value derived from these advancements, such as increased democratization of data access and custom AI model training, is becoming more evident every day. Our focus is on empowering engineers, researchers, and developers with intuitive, low-code tools to create clean, renewable, and privacy-preserving synthetic data. Quality data is the foundation for responsible AI development and data governance at scale.\u00a0", "We\u2019re thrilled to be part of a growing community that includes privacy advocates and pioneers in data synthesis. Curious about Gretel or the world of synthetic data? Join the discussion in the ", " or drop us a message at ", ".\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/compliance", "page_title": "Compliance blog posts - Gretel.ai", "headers": ["Compliance", "Gretel Demo Day: Exploring the Future of Synthetic Data", "Why Nonprofits Should Care About Synthetic Data", "What is Data Anonymization?", "Anonymize tabular data to meet GDPR privacy requirements", "Automate Detecting Sensitive Personally Identifiable Information (PII)", "Got text? Use Named Entity Recognition (NER) to label PII in your data", "What is Privacy Engineering?", "How we accidentally discovered personal data in a popular Kaggle dataset", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/how-to-generate-synthetic-data-tools-and-techniques-to-create-interchangeable-datasets", "page_title": "How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "headers": ["How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "The Concept of Synthetic Data Is Simple\u2026", "\u2026But the Possibilities are Endless", "Techniques to Generate Synthetic Data", "Two Common Approaches to Generative Models", "Are Language Models Better than GANs for Synthetic Data?", "Advanced Features with Generative Models", "Synthetic Data Generation Tips and Best Practices", "Increase your training data", "Increase your synthetic data", "Clean your data first", "Deal with anomalies", "Simplify your data where possible", "Working with highly-dimensional datasets", "Sanity check CSV format and column data types", "Handling tough fields", "Generate Robust and Highly Accurate Synthetic Datasets With Gretel", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Synthetic data is artificially annotated information that is generated by computer algorithms or simulations. Often, synthetic data is used as a substitute when suitable real-world data is not available \u2013 for instance,\u00a0to augment a limited machine learning dataset with additional examples. In other cases where\u00a0 real-world data cannot be used due to privacy concerns or compliance risks, synthetic versions of that data can be safely shared and mined for insights, without revealing any sensitive or personally identifiable information (PII). This \u201cshareable\u201d quality can enable, for example, medical research between hospitals where\u00a0 a researcher can learn about a disease, but not the real patients that the data was based on.", "Getting access to data is one of the bottlenecks to progress and innovation today. In fact, in a recent Kaggle survey, 20,000 data scientists listed the \u201cdata gathering\u201d stage as the most time consuming part of a typical project, accounting for ", ". In certain fields such as life sciences research, requesting access to genomic datasets for researching cancer or rare disease can take ", ".", " opens up possibilities of enabling access to artificial and privacy-preserving versions of data in minutes, or the ability to augment a machine learning dataset for superior accuracy and fairness than one trained only with real-world data.", "Here are some examples of how synthetic data is utilized for different purposes today.", "There are also dozens of industry-specific use cases for synthetic data. Here are some of the most common.\u00a0", "Now, we\u2019ll take a look at three common techniques for generating synthetic, starting with the most basic approach.\u00a0", "In contrast to more advanced, machine learning-based approaches, a popular technique for generating synthetic data can be to simply draw, or sample numbers from a distribution. While this approach does not capture the insights of real-world data, it can create a distribution of data that follows a curve that is loosely based on real-world data. ", "For this example, we will use Python and NumPy library\u2019s ", " function to create a set of four datasets using a \u201cnormal\u201d distribution of variables, each with a slight change to the centerpoint.", "Here's the ", " if you'd like to try it yourself.", "Agent-based modeling (ABM) is a simulation technique where individual agents are created that interact with each other. These techniques are particularly useful to examine interactions between agents (e.g., people, cells, or even computer programs) in a complex system. Python packages such as ", " make it easy to quickly create agent-based models using built-in core components, and to visualize them in a browser-based interface.\u00a0", "Generative modeling is one of the most advanced techniques used to create synthetic data. It can be described as an unsupervised learning task that involves automatically discovering and learning the insights and patterns in data in such a way that the model can be used to output new examples that match the same distribution as the real-world data it was trained on.\u00a0", "Training generative models often starts with gathering a large amount of data in a particular domain (e.g., images, natural language text, tabular data), and then training a model to generate more data like it. The generative models described below have different architectures, but are all based on neural networks - and fundamentally leverage the same approach of utilizing a number of parameters smaller than the input data they were trained on - ", ".", " treat the training process as a game between two separate networks - a generator network, and a second discriminative network that attempts to classify samples as either coming from the real world (training distribution), or coming from the model (synthetic data). On each training iteration, the generator adjusts its model parameters to generate more convincing examples to (effectively) fool the discriminator, until, ideally, the discriminator is no longer able to differentiate between the real world and synthetic examples.\u00a0", " ", " such as Recurrent Neural Networks (RNN) and Transformers attempt to learn the underlying probability distribution of the training data, such as a sequence of words or tokens, so that it can easily sample new data from that learned distribution (or effectively, predict the next token or words in a sentence). While simple in concept, language models are arguably the most powerful algorithm for generative text today. They demonstrate the ability to learn and recreate both short texts (sentences, tweets, etc) and longer texts (entire chapters of books, or complex time-series sequences of data) by training on massive amounts of data.", "The image above is from \u201c", "\u201d by Ashish Vaswani et al, Google Brain. It shows the attention mechanism in a transformer model following long-distance dependencies in the encoder. As you can see, many of the attention heads are going from the word \u201cmaking\u201d to \u201cmaking more difficult\u201d. Popular language models include Recurrent Neural Networks such as LSTM or GRU networks, and Transformers (such as OpenAI\u2019s GPT-2 and GPT-3) which introduce the ability to look at both past and future elements at the same time. They can also achieve faster training times by avoiding recursion and processing sentences as a whole via attention mechanisms and positional embeddings.", "The answer is not so simple. Both ", " and ", " have different pros and cons, making them perform well on different tasks. As such, at Gretel, we like to think of algorithms as being part of a toolkit that can be applied in different ways for different problems. For example, we have found GANs to be very effective and efficient for generating ", ", and learning complex relationships in ", ", but can be difficult to optimize due to unstable training dynamics and are generally ", ". When working with tabular data, language models are able to quickly learn and generalize datasets, but at higher ", ".\u00a0", "Regardless of whether the next generation of generative models are powered by GANs, LMs, or even something new (", "), we are incredibly excited about the capabilities that generative models make possible with synthetic data.", "Not all models are created equal, and different architectures offer benefits in certain areas. Here we discuss two features that are particularly useful with synthetic data.\u00a0", "\u200d", "Some generative models support model conditioning (sometimes referred to as \u201cseeding\u201d, or \u201cprompting\u201d), which is a technique that enables the model to generate more records that match a certain class or label, versus simply recreating the distribution that it was trained on. This technique can be used to balance class distributions in datasets for more accurate or ethically fair machine learning. For examples, check out our blog on ", ", and ", " by augmenting it with additional records for female patients.", "\u200d", "We know that when training a machine model, we are attempting to teach the model to learn from real-world data. As such, it\u2019s possible for the model to memorize and replay sensitive data from the training set - such as a name, credit card number, or sensitive medical information about a patient. For many uses of synthetic data, privacy is a core use case - so being able to apply techniques such as differential privacy during the training process can provide mathematical guarantees that the model will not memorize or be able to replay sensitive data that it was trained on. Furthermore, many users seek to find a balance between privacy guarantees and the accuracy of their dataset for a particular task. For more information about privacy and synthetic data, check out Gretel\u2019s ", ", and also comparing the ", ". Spoiler alert: Gretel.ai\u2019s synthetic datasets average within 2.58% of the accuracy of real-world data across the top 8 datasets on Kaggle.\u00a0", "Here are some tips and best practices for generating clean and accurate synthetic data.\u00a0", "The number of training records used can directly impact the quality of the synthetic data created.\u00a0 The more examples available when training a model, the easier it is for the model to accurately learn the distributions and correlations in the data.\u00a0Always strive to have a minimum of 3,000 training examples. If possible, increasing that to 5,000 or even 50,000 is even better.\u00a0", "The more synthetic records generated, the easier it is to deduce whether the statistical integrity of the data remains intact. If your synthetic data quality score isn't as high as you'd like it to be, and you\u2019ve generated less than 5,000 synthetic records, generating more synthetic records is a good way to deduce if there really is a quality issue.\u00a0", "As in any statistical or machine learning analysis, the first step is to clean up your data. Tending to the following issues can be vital in creating quality synthetic data:", "Assess the extent of your missing values.\u00a0A moderate amount of missing data can be easily handled by the synthetic model.\u00a0An excessive amount can lead to difficulties in accurately learning the statistical structure of the data.\u00a0Decide if it's appropriate to drop columns or rows with missing data, or if it's more appropriate to attempt to fill in the missing fields using, for example, the median or techniques such as KNN imputation.", "Study the correlation matrix in the Gretel's synthetic performance report, and remove unnecessary, highly correlated fields. This is particularly important when the dataset contains a large number of columns.\u00a0", "A large number of highly unique fields, or just one highly unique field that is exceptionally long, such as an ID can cause the model to struggle to learn the patterns in the data. If possible, consider removing the field or fields before generating synthetic data, and adding them back in afterwards.\u00a0", "If training records are duplicated in error, then any statistical analysis of the data will be impacted. A large number of duplicated records can also cause the model to see it as a pattern it needs to learn resulting in the potential duplication of private information in the generated synthetic data.", "Assess whether there are anomalies in the data, and if so, whether they are errors or true data points.\u00a0In general, the synthetic model will usually be robust to anomalies, but on the occasion that it's not, the replay of an anomaly in the synthetic data can potentially constitute a serious breach in privacy.", "If a long categorical field can be replaced with a simpler set of integer labels, then do so.\u00a0If a long numeric field can be grouped into a smaller number of discrete bins, then do so. If a floating point value has excessive levels of precision, remove them. This step is rarely needed, but if you find the model is struggling it may help improve performance.", "Datasets with high counts of columns (50+) and rows (100k+) can be tricky for synthetic data models, as dimensionality can increase training time, and require tuning for the neural network such as the optimizing learning rate and batch size. Working with a highly dimensional tabular dataset? Check out Gretel.ai\u2019s ", ", which enables parallel training, retries, and support for datasets with up to thousands of columns and millions of rows.", "Test reading your training CSV file into a Pandas DataFrame.\u00a0Sanity check that the columns are as expected and that there are no warnings about columns with multiple data types.\u00a0", "At Gretel, we\u2019re laser-focused on developing state-of-the-art APIs and algorithms for generating synthetic datasets, including the privacy-protection mechanisms necessary to enable the creation of guaranteed safe data. We\u2019re also making sure all of these products and features are accessible via easy-to-use APIs, so developers and data scientists can be confident they are getting high-quality synthetic data whenever and wherever they need it.\u00a0", "Going forward, we will continue to post research, source code, and examples about enabling data sharing and access at scale. Sign up for", " and give Gretel a try, you can run Gretel\u2019s APIs to ", ", ", ", or ", " data \u2013 ", ".\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-is-live-on-google-cloud-marketplace", "page_title": "Gretel is live on Google Cloud Marketplace \ud83c\udf89", "headers": ["Gretel is live on Google Cloud Marketplace \ud83c\udf89", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Enterprises need safe, accurate and timely data to build state-of-the-art machine learning and AI applications. But to get there requires navigating a host of privacy and security risks, data access challenges, and quality assessments. Today, we\u2019re thrilled to announce that we\u2019re making that much easier for the thousands of enterprises operating in the Google Cloud ecosystem. As of today, you can now access all of ", ", and start generating high-fidelity synthetic versions of your sensitive datasets that are statistically accurate, infinitely scalable, and privacy-protected.\u00a0", "For a rundown of how developers can use Gretel and Google to create limitless amounts of accurate, private synthetic data, ", ".", "Among the many reasons enterprises, governments, and researchers around the world are using Gretel is because of our comprehensive, platform approach to data creation that combines generative AI models, privacy-enhancing technologies, and robust data metrics and analysis. Gretel\u2019s platform features:\u00a0", "By offering API-driven generative AI modeling capabilities that focus on core use cases for MLOps and LLMOps teams, addressing their privacy and security risks, and providing data generation on demand, Gretel enables teams to quickly experiment and innovate safely.", " said ", ", Managing Director, Cloud Marketplace & ISV GTM Initiatives.", "Google Cloud customers can draw down on existing Google Cloud commitments to use Gretel\u2019s entire suite of privacy-enhancing tools and generative AI products. You can find Gretel on Google Cloud Marketplace ", ".\u00a0", "If you have any questions about Gretel\u2019s platform or would like to learn more about how synthetic data can help your business, please don't hesitate to reach out.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/anonymize-tabular-data-to-meet-gdpr-privacy-requirements", "page_title": "Anonymize tabular data to meet GDPR privacy requirements", "headers": ["Anonymize tabular data to meet GDPR privacy requirements", "Synthetic data as an alternative to real-world data.", "Access the GDPR workflow notebook", "Set up a development environment", "Anonymize datasets using Gretel or your own cloud", "Fine-tune data transformation and synthetics policies", "Create accurate and private synthetic versions of sensitive data", "Review the anonymized data and quality reports", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The European Union\u2019s General Data Protection Regulation (GDPR) and similar privacy regulations require businesses to safeguard the personal data of their users. In combination with other privacy-enhancing technologies, synthetic data can help companies follow GDPR standards and significantly reduce risks associated with handling sensitive data.", "Let\u2019s walk through how you can use Gretel\u2019s Transform and Synthetics APIs to anonymize three different types of sensitive datasets with a single, customizable policy.", "You'll learn how to:", " defines anonymous information as information that \u201cdoes not relate to an identified or identifiable natural person.\u201d ", ".\u00a0", "Multiple academic and ", " have demonstrated that simply removing personally identifiable information (PII) such as names, credit card numbers, and addresses isn't sufficient to anonymize data. As no records in a synthetic dataset are based on a single person, synthetic data models inherently reduce the likelihood of singling out an individual, linking records about an individual, or inferring information about an individual. To learn more about how synthetic data can mitigate GDPR risk, check out our ", ".", "Let\u2019s get started. \ud83d\ude80", "Note: This tutorial was created and run using Gretel\u2019s cloud service, using an NVIDIA T4. For optimal data accuracy, we\u2019ll use the ", " model, which requires a GPU. For faster speeds, but slightly lower accuracy, you can use ", ", which doesn't require a GPU.", "Visit the ", " and sign up or log in for easy access to the GDPR workflow notebook. With our use case cards, you can quickly set up projects that have configurations and steps optimized for the problems you\u2019re solving, such as anonymizing data to meet GDPR standards.\u00a0", "Selecting the GDPR use case card provides two options for using our GDPR workflow. You can either:", "We assume you\u2019re taking the first option in the following steps, though you can follow the same code in Google Colab.\u00a0", "Your first step is to download ", ", which includes the `gretel-client` library.", "In a terminal:", "This example uses the Gretel Cloud service to create synthetic data. If you selected the GDPR use case card, it provides an option to copy your Gretel API key. Otherwise, follow ", " to find your key. If you wish to run in your own cloud and synthesize data without it ever leaving your environment, follow the steps in the developer docs to ", ".", "For convenience, we recommend using the Gretel client on the command line to store your API key on disk. This step is optional; the `gdpr-helpers` library will prompt you for an API key if one can't be found.", "We'll anonymize three example datasets, included with `gdpr-helpers`. The three datasets have a variety of dimensions, and include different types of data.\u00a0", "To run on your own datasets, change the `search_pattern` in the code below to point to CSV files in a folder on your filesystem. You can also update the Gretel Transform and Synthetics configurations (we\u2019ll cover this in the next section). To anonymize sensitive data in your own cloud without sending any data to Gretel APIs, set \u201crun_mode\u201d to \u201chybrid.\u201d", "Create a new file, named \u201canonymize_files.py,\u201d in the cloned repository, and put the following code in it:", "You may wish to customize the data transformation policies. For example, you may need to replace detected PII with fake versions, or add a field that wasn't detected by Gretel\u2019s named entity recognition (NER) APIs. To do this, you can modify the base `transforms_config.yaml` template using any of the available ", ". Often, a single `transforms_config.yaml` can be used across your entire organization and modified to support any number of company or application-specific fields.", "By default, the template replaces detected email addresses with fake email addresses that come from a `foo.com` domain. If you wish to encrypt PII in place instead of replacing it with fake data, you can make the following change. Open the `./src/config/transforms_config.yaml` in your favorite editor and update the YAML:", "(Example: Update Gretel\u2019s transform config to encrypt PII versus using fake values)", "From:", "To:", "After you have made any updates or changes to the base configuration, you can use your script, `anonymize_files.py`, to anonymize all of the files matching the search pattern you defined. By default, the anonymization workflow uses ", ", a generative model with advanced accuracy and privacy settings.\u00a0", "For faster performance or to run in your own cloud without requiring a GPU, you can change the `synthetics_config.yaml` to use ", ", a statistical model, with the following change:", "(Example: Update the Gretel Synthetics config to use a fast, CPU-based generative algorithm)", "To:", "Let\u2019s go ahead and run the Python program to anonymize the datasets. The gdpr-helper library will create three artifacts for each dataset that it anonymizes. By default, they'll be written to the `artifacts` directory in the following format:", "You have now completely anonymized the set of three sample datasets! For next steps, try anonymizing your own data. If you have any questions about this guide, or anonymizing data to meet compliance standards such as GDPR, CCPA, and others, please feel free to contact us inside the Synthetic Data Community we\u2019re hosting over on ", " or at ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/community-insights-overcoming-medical-class-imbalance-with-synthetic-data", "page_title": "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "headers": ["Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this case study, the Gretel team sat down with ", ", a Brown University medical candidate and leader of a bioincubator, to learn why medical practitioners turn to Gretel-generated synthetic data when overcoming challenges with clinical data. Reetam\u2019s team leverages synthetic data to predict postpartum hemorrhages for expecting mothers in Cameroon and Nigeria.", "Biased data, or class imbalance, is a large problem when dealing with medical data. Bias can be introduced through a number of factors: limited medical data collection from underrepresented demographics, historically low mortality rates for commonly treated disease, or gender biases stemming from both societal and clinical factors. Today, combatting this problem is costly and resource-intensive, requiring practitioners to collect and annotate more data; in some cases, collecting this data is even impossible, such as when experimental conditions can be impossible to recreate in a lab or clinical setting.\u00a0\u00a0", "Even with these challenges, the stakes couldn\u2019t be higher. Clinical applications built on biased data lead to unreliable results in settings where real human lives are at stake. Reetam Ganguli is committed to overcoming this critical data challenge. Read below (or ", ") to learn more about his work and how Brown University is building the next generation of data-driven clinical applications with synthetic data.", "I\u2019m a medical candidate at Brown, aiming to be an OB GYN physician. My research focuses on the intersection of deep learning and inferential statistics as they relate to predicting clinically salient outcomes. I also lead the ", " where we pair physicians with interdisciplinary groups of students that innovate data-science solutions to address clinical needs.\u00a0", "I have completed one basic introductory computer science course on data structures and algorithms. I mainly learned machine learning through self-taught instruction from online YouTube tutorials and publicly available websites. Really found ", ", ", ", and ", " to be helpful resources.\u00a0", "Almost all medical data will face the class and balance problem. The primary reason for this is because, as we as a society become more technologically advanced, our care becomes better. This means we will have less adverse outcomes, meaning patients that have life-threatening symptoms where there is a serious threat of mortality. The lack of this type of data means machine learning classifiers now have less training data for positive samples. This can be difficult if you are trying to base your clinical decisions off of classifiers that are biased towards predicting \u201cno:\u201d because you won't be able to adequately interfere if a patient is about to have an adverse outcome\u2014due to the high false negative rates that most models produce. If you extrapolate this issue 50 years into the future, where our medical care is even better, we will see diminishing positives within our medical data. This is where I believe synthetic data will be one of our most powerful tools.\u00a0", "When speaking in more data-science terms, ultimately, I just want to have better classifier accuracy, better area under the receiver operating characteristic curves, better F1 scores and better positive predictive value. It is my hope that platforms like ", " will be best at helping to generate diverse synthetic datasets to enable better research outcomes.", "Some other class imbalance rectification methods I used were SMOTE, data undersampling,\u00a0 and data oversampling, but these did not improve my classifier accuracy and ultimately ended up introducing unwanted bias into the data. This is why I turned to synthetic data.\u00a0", "One dataset I was using for a particular project involved detecting postpartum hemorrhage for pregnant mothers. One salient issue when dealing with medical data is that there is a pretty significant class imbalance within your data. For me, over 99.5% of my patients were negative for postpartum hemorrhage and less than 0.5% were positive for postpartum hemorrhage. This leads to classifiers that are significantly biased toward predicting \u201cno\u201d and ultimately are clinically useless. When looking for solutions I came across Gretel by a simple Google search and was amazed by how accessible your platform was as someone who is not as technical. Gretel's console provided a very easy way for me to generate synthetic data to augment my existing dataset and combat the class imbalance.\u00a0", "I really like the fact that the console is very user-friendly and doesn't even require you to have any kind of coding experience. I think this kind of plug and play platform would be accessible to most healthcare providers who don't have a technical background and don't have any technical skill sets to manually code generative recurrent neural networks of their own. I also really like how simple the API is to use for people who are not technical and can get started within a matter of minutes.", "We are prospectively testing our data on a cohort of Cameroonian patients as well as a cohort of Nigerian patients. This is despite the fact that our original models were trained on American patients. We want to see how well our models are able to generalize and be applied in low-resource settings.\u00a0", "If we are successful, then our machine learning models will be able to predict adverse outcomes like postpartum hemorrhage at the point-of-care. This means that we will be able to make this prediction for women as early as the first week of pregnancy and give their doctors and medical staff plenty of time to adequately prepare for any adverse outcomes.", "The primary challenge with this issue is that the distribution of Cameroonian and Nigerian patients follows a very different distribution than American patients. Some reasons for this may include the fact that these patients grew up in different backgrounds and have different physiological characteristics as well as other social determinants of health. This becomes a challenge when testing our machine learning models because these models have been optimized to fit an American patient distribution. We are hoping that platforms like Gretel can add synthetic data and balance out the distribution in favor of Cameroonian and Nigerian patients to combat any model over-fitting that we might have to American patients.", "If we successfully train our models without the overfitting problem, then we will be able to make our platform accessible to pregnant mothers and doctors all across the world, particularly those in low-resource settings and developing countries who may greatly benefit from it. In these countries, blood banks are either lowly stocked or entirely non-existent. Having a donor lined up beforehand for a blood transfusion can literally save a life. In Cameroon, there are no extra blood reserves and any time a patient needs blood they need a donor. Blood can also not be stored for a long time, thus, predictive applications are essential to clinical preparedness.\u00a0", "Synthetic data has tremendous potential to help users overcome critical data challenges such as class imbalance in clinical data. Try generating your own data by uploading a medical dataset in the ", " today. If you need more support, feel free to join our ", ", email us at ", ", or reach out to ", " to learn more about his work with postpartum hemorrhages.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/downstream-ml-evaluation-and-quality-scores", "page_title": "Compare Synthetic and Real Data on ML Models with the new Gretel Synthetic Data Utility Report", "headers": ["Compare Synthetic and Real Data on ML Models with the new Gretel Synthetic Data Utility Report", "Analyzing synthetic data performance is easier than ever before\u00a0", "\u00a0", "What\u2019s next?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Most organizations are eager to use synthetic data to augment their real-world training datasets and improve the performance of machine learning models in production. But how well do models trained with synthetic data compare to those trained on just raw data alone? Establishing this comparison is difficult and delays the AI-development lifecycle. Today, we\u2019re excited to announce that you can now assess the performance of synthetic data on ML models with a few simple clicks using our new ", ", which provide you with a new\u00a0 ", " featuring a novel ", ".", "The ability to validate synthetic data fidelity using ", ", our API for measuring synthetic data quality and utility, is one of the reasons why thousands of developers trust data generated with Gretel. Our new ", " expands upon the statistical measures of quality you have come to know and love through the Synthetic Data Quality Report, Synthetic Data Quality Score (SQS), and Benchmark\u2014all part of the broader ", " offering.", ", we showed you how to generate synthetic grocery store purchase data using the Gretel ACTGAN model and then used the open-source AutoML PyCaret classification library to evaluate the performance of prediction models trained on synthetic data versus those trained on real-world data.\u00a0", "Now you can perform this evaluation in the Gretel environment, including directly in the ", ".", "In this blog, we\u2019ll walk through an example showing how to: ", " add a Gretel Evaluate classification or regression task to any Gretel Synthetics model, ", "use the Evaluate SDK, and ", " understand the Synthetic Data Utility Report.", "You\u2019re probably already familiar with this step: synthesizing data! In this example, we\u2019ll use the publicly available ", "\u00a0which predicts whether a customer will subscribe to a term deposit (prediction: yes/no in target column \u201cy\u201d).\u00a0", "It\u2019s easier than ever to ", "! Head over to the ", " to start with the sample `bank_marketing` dataset and use the existing configuration.", "The Evaluate tasks call the open-source AutoML PyCaret library to train and evaluate classification and regression models. The classification and regression tasks are pre-set with defaults for `holdout` `metric` and `models` values. All you have to do is make sure the `target` column matches your data.\u00a0", "Here\u2019s what the `bank_marketing` dataset looks like:", "Since we\u2019re interested in training classifiers to predict the categorical \u201cyes/no\u201d outcome, we\u2019ll set the Evaluate task to `classification` and `target` parameter to \u201cy\u201d. Then all we have to do is click \u201cBegin training\u201d to start the process.", "While model training is in progress, we can check out the logs to verify that training is progressing without error. You\u2019ll see that reports should be created at the end of model training.", "Let\u2019s say we want to predict the numeric `thalach` value (which refers to maximum heart rate) in the publicly available ", " dataset. You can download the processed_cleveland_heart_disease dataset using this ", ".", "Here\u2019s what the model configuration looks like: ", "Notice that `task` and the `target` changed to match the use case and dataset. Now, click \u201cBegin Training\u201d to start this model, and follow along in the logs to make sure training is progressing without error.", "The classification and regression Synthetic Data Utility Reports are available in the Records &\u00a0downloads section of the model page. We\u2019ll go through each part of the report later on in this blog. ", "In the previous examples, we didn\u2019t change the default values for:\u00a0", "If you need more customization, you can change these optional parameters, like so:", "The supported models and metrics are detailed in the ", ".\u00a0", "The examples shown here for classification and regression are further explained step-by-step in the ", " and the ", ".", "In the notebooks, you\u2019ll also see an option to call the Evaluate SDK directly on two datasets (one synthetic, one real-world) to create a Synthetic Data Utility Report.\u00a0", "Indicate whether you want to evaluate classification or regression by importing `DownstreamClassificationReport` or `DownstreamRegressionReport` from the Gretel Client, and set your `data_source` to point to the synthetic data and `ref_data` to point to the real data. ", "After you start the job, head over to your ", " in the Gretel Console to see more training logs and easily download the Utility Report after the job is completed.\u00a0", "For detailed more information about the SDK, see the ", "The new Gretel Evaluate classification and regression tasks generate a ", " ", "showing you the performance of the synthetic data compared to the performance of real data.", "First, you\u2019ll see a high-level ", "which gives you an at-a-glance signal about the utility of your synthetic data for downstream models in your ML pipeline.", "The MQS is calculated as the ratio", "of the average score from the top-performing models trained on synthetic data to the average score from the top-performing models trained on real data. In the examples above, the classification MQS is 96%, which is a promising result for the synthetic data. You may even see MQS above 100%, which means the synthetic data outperformed real data - an exciting result!\u00a0", "The top performing models are highlighted in the next section of the report:", "In the example below, during model training, \u201caccuracy\u201d was set as the classification metric in the Gretel Evaluate task configuration. (Note: \u201caccuracy\u201d is the default metric for classification and \u201cR2\u201d (i.e. R-squared) the default for regression, so you don\u2019t need to change those options unless you want to optimize training on a different metric).", "In the classification report, you\u2019ll find a results table with metrics for each model:", "In the regression report, you\u2019ll find a table of results based on regression metrics:\u00a0", "You can also dive deep into plots to understand the top models. Confusion matrices are plotted out in the classification report for the top models for both synthetic and real data.\u00a0", "In the regression report, review the error distribution for more information on the downstream models.", "You can find more information about the Synthetic Data Utility Report in the ", ".", "You\u2019ve trained and evaluated synthetic data on machine learning models against real-world data, now what? By reviewing the Data Utility Report and MQS, you can get immediate feedback on whether your synthetic data is ready for your ML training or experimentation workflows.\u00a0", "If the MQS is not as high as you\u2019d like, you can try a few things, including reviewing the Synthetic Data Quality Report to deep dive into synthetic data quality and fidelity metrics. If your data quality results aren\u2019t where you want them to be, you can also follow ", ".\u00a0", "With these new Evaluate offerings and Synthetic Data Utility Report, we hope you\u2019ll find it quicker and easier than ever to validate the performance of synthetic data, and safely incorporate it into your business.", "How will you use this feature? Let us know by joining our ", " to share your feedback. We\u2019re here to help. Go forth and synthesize!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/ai?094d394d_page=2", "page_title": "AI blog posts - Gretel.ai", "headers": ["AI", "Synthesizing dialogs for better conversational AI", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Comprehensive Data Cleaning for AI and ML", "Gretel is live on Google Cloud Marketplace \ud83c\udf89", "Conditional Text Generation by Fine Tuning Gretel GPT", "How to safely work with another company's data", "Red Teaming Synthetic Data Models", "Generate Synthetic Databases with Gretel Relational", "Bringing AI-generated images to enterprise use cases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/introducing-gretel-blueprints", "page_title": "Introducing Gretel Blueprints", "headers": ["Introducing Gretel Blueprints", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Working with developers using our ", " and open-source ", ", one of the top requests we have heard is developers asking us to make it incredibly simple to get started on use cases including ", ", ", ", and ", ". This week we are thrilled to release Gretel Blueprints. Gretel Blueprints are collections of sample code and sample datasets that utilize Gretel's SDKs that can be easily adapted to solve customer-specific use cases. ", "Feedback from our community has led us to believe that the application of code can actually be use case specific. \u00a0Developers should not have to always make a series of technology decisions in order to solve a specific problem.", "Just like customer use cases evolve, so will Blueprints. We will be adding new Blueprints based on new features and customer feedback- let us know on ", " if you would like to see Blueprints for your use case. \u00a0Several of our Blueprints will also have detailed blogs of their own to deep dive into the use case and more importantly, ", "Gretel Blueprints will be accessible via ", " and are hosted on ", ". \u00a0Soon, when creating a Gretel Cloud Project, you will be able to choose our Blueprints as a starting template and several Blueprints will come with their own sample data. If you have a project already or are creating a blank project with your own data, the Blueprints will be available from the \"Transform\" page.", "Once in the \"Transform\" page, you will be able to select any Blueprint to use with your current Gretel Cloud Project or for use with data in your own environment.", "\u200d", "Like our core open source SDKs, Blueprints are hosted on ", " and licensed under Apache 2.0. We did this so customers can adopt our sample code to meet their needs specifically without being overly prescriptive on how to solve any one problem.", "In the coming days, we'll be making announcements on our new Gretel Cloud workflows and releasing tutorials, walk-throughs, and even customer testimonials for many of our Blueprints. If you like it, give us a \u2b50 on ", "! Stay tuned to our ", " and ", " for these exciting announcements and feel free to reach out via ", ", GitHub, or social media to engage with our team!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/data-science?094d394d_page=2", "page_title": "Data science blog posts - Gretel.ai", "headers": ["Data science", "How to safely work with another company's data", "Red Teaming Synthetic Data Models", "The Evolution of Gretel's Developer Stack for Synthetic Data", "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "How accurate is my synthetic data?", "Generate synthetic data in 3 lines of code", "Deep dive on generating synthetic data for Healthcare", "Evaluating Data Sampling Methods with a Synthetic Quality Score", "What We\u2019re Reading: Trends & Takeaways from the NeurIPS 2021 Conference", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/generate-synthetic-databases-with-gretel-relational", "page_title": "Generate Synthetic Databases with Gretel Relational", "headers": ["Generate Synthetic Databases with Gretel Relational", "Introduction", "Walkthrough", "Our database", "Define source data via a database connector", "Examine the input data", "Synthesize Database", "Create the relational model and project", "Synthesize a database", "View Results", "Preview Table Data", "Confirm Referential Integrity", "Gretel Relational Report\u00a0", "Output Files", "Write synthetic results to a database via connector", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Relational databases are a cornerstone of enterprise data management. Such databases often contain hundreds of interrelated tables, and form the backbone for several use cases within an organization. While democratizing this data across teams comes with major productivity benefits, access to these databases often remains restricted due to privacy and security concerns, constraining innovation, or forcing teams to rely on proxy datasets that may not offer the same quality of insights. For over 23,000 developers, generating synthetic data with Gretel has become a preferred solution to combat this data bottleneck by creating synthetic versions of single tables that preserve the statistical properties of the original data while providing advanced privacy guarantees. Today, we\u2019re excited to announce the general availability of ", ", which leverages our library of generative AI models to ", ".", "Gretel Relational offers the high-quality Gretel Synthetics models you know and love, now for multi-table and relational databases. With over 30 available connectors, you can synthesize data from and write back to popular databases and data warehouses like Oracle, MySQL, MariaDB, Microsoft SQL Server, Snowflake, SQLite, PostgreSQL, and more.\u00a0", "In this blog, we'll walk through a practical example of generating a telecommunications database using Gretel Relational. \u200cYou can follow along with our Gretel Relational Synthetics demo notebook located ", " or here:", "In this example, we have a telecommunications company who wants to anonymize their database due to privacy and security concerns. The database contains sensitive information including customer names, social security numbers, addresses, SIM numbers, and transaction history. Due to privacy and security concerns, the data can't be shared outside a limited group of authorized employees. Access to this information is crucial, but currently restricted, for the team tasked with analyzing the data to identify and predict trends, such as the number of accounts by geographic location, or the likelihood a client will have a late payment. For this data analysis use case, optimizing for statistical accuracy and maintaining the size of the database are the primary objectives. By synthesizing this database, the data can be shared safely, while also maintaining its referential integrity and accuracy.", "The relational database we\u2019ll be using is a mock telecommunications database shown below. This contains five tables that store information about product data, customer information, and transaction history for phone plans. The `client` table represents the person associated with each phone plan. It has two child tables: `location`, which contains the address of each client, and `account`, which represents all the devices on a client\u2019s plan. Additionally, an `invoice` table is maintained to track each bill associated with an account, and the `payment` table tracks the transactions related to each invoice.\u00a0", "In the figure below, the lines between tables represent primary-foreign key relationships, which are used in relational databases to define many-to-one relationships. To ensure relationships within and across tables are maintained, a table with a foreign key that references a primary key in another table should be able to be joined with that other table. Below we'll demonstrate that referential integrity exists both before and after the data is synthesized. We'll also assess the statistical accuracy and privacy of the synthesized data.", "To begin, we specify the real-world data we\u2019ll be synthesizing. There are two ways to define relational data - automatically via a database connector or manually with individual table files. Gretel Relational supports connectors for any database supported by ", ", and has first-class helper functions for PostgreSQL, SQLite, MySQL, MariaDB, Snowflake, and more. For more information about connecting to your own database, ", " In this notebook we'll use a SQLite connector to automatically extract the data and schema from the telecom database. The extracted relational data contains the table data, the primary and foreign keys, and their relationships.\u00a0", "Alternatively, you can define the input relational data from individual table files. In this case, the table names, keys, and their relationships are defined manually.\u00a0", "Now, we\u2019ll join related tables `account` and `client` using the key `client_id`. Let\u2019s preview the joined data to confirm referential integrity of the source data. In the output below, note that every record in the child table `account` matches a distinct record in the parent table `client`. Therefore, the number of records in the joined data matches the number of records in the child table. This confirms the referential integrity of the source data. After we synthesize the database, we'll use the same method to confirm that this integrity has been maintained.", "Next, we set up our relational model and create a project using the `MultiTable` instance. The `MultiTable` class is the interface to working with relational data. It will be used later to control model training and data generation, and to evaluate the quality of the synthetic database. For Gretel Relational Synthetics, there are three model options - Gretel Amplify, Gretel ACTGAN, and Gretel LSTM. In this notebook, we are using Gretel Amplify. The cell below sets up the relational model and creates a project as specified by `project_display_name`. This project will house all models and artifacts associated with the database synthesis. During this step, you'll be prompted to input your API key, which can be found in the ", ".", "Now it's time to train and generate the synthetic database.\u00a0", "The code shown above trains an Amplify model for each table in the database. The logs refresh periodically to provide status updates.\u00a0", "After about three minutes, training is complete and we can begin generating the synthetic database. For our use case today, we want to generate a synthetic database that replicates the original, so we will generate the same amount of data for all tables. Alternatively, you can choose to subset your existing database or generate even more data by adjusting the `record_size_ratio` parameter. Additionally, you can choose to exclude specific tables from synthesis with the `preserve_tables` parameter.", "As with training, logs refresh throughout the generation process to provide status updates. Because it\u2019s possible to generate data many times from the same relational model, each synthetic generation run has an associated generation ID, which is always logged at the beginning of the generation. This ID can be helpful for identifying \u200cartifacts \u2014 synthetic tables, reports, etc. \u2014 after generation. By default, the ID is `", ", but you can set a custom ID by including an `identifier=\u201dmy-custom-id\u201d` parameter.", "After only seven minutes, we\u2019ve generated a synthetic version of the entire database with five tables and over 231,000 records.\u00a0\u00a0", "Now, let\u2019s take a look at our results. First, we\u2019ll compare the original and synthetic data from a single table in the database.", "Looking at the first five records of the `payment` table, we see that the synthesized table fields match the data types and provide logical values for their corresponding original fields. Each `payment_id` maps to a unique `invoice_id`, as expected. Examining the entire dataset, both the source and synthesized tables have `issued_date` ranges between December 2013 and May 2017, and `amount` ranges between $1 and $1,000. For a more detailed examination of the quality of the data, we will look at the Gretel Relational Report below.", "Now, to test referential integrity, we'll show the same join on the synthetic `account` and `client` tables as we did for the source data. If referential integrity has been maintained, the child table and the joined data should have the same number of rows.", "As with the original data, every record in the synthesized child table, `account`, matches a distinct record in its synthesized parent table, `client`. The number of records in the child table and in the joined data match, confirming that referential integrity has been maintained. Additionally, looking at the values of the fields confirms that logical data has been generated. The name fields contain names, `ssn` contains nine digit numbers, etc.\u00a0", "During each database generation, a ", " is created. Similar to the ", ", the relational report helps you assess the accuracy and privacy of your synthetic database as a whole and of the individual tables.", "To view the report in the notebook, we can run the cell below.", "At the top of the report, the synthetic data quality score (SQS) and privacy protection level (PPL) for the database are shown. These are composite scores that represent the accuracy and privacy of the whole database. Later in the report, scores are also provided for each table. The quality score is an estimate of how well the generated synthetic data maintains the same statistical properties as the original dataset. In this sense, the Synthetic Data Quality Score can be viewed as a utility score or a confidence score as to whether scientific conclusions drawn from the synthetic dataset would be the same if one were to have used the original dataset instead.\u00a0\u00a0", "Our synthesized telecommunications database has an \u2018Excellent\u2019 Composite SQS of 86 and \u2018Normal\u2019 PPL. An ", " The ", "The report includes a visual of the key relationships between tables in the database, as shown below. When the cursor is hovered over a key, its related keys and tables highlight. ", "For each table, individual and cross-table Gretel Relational Reports are generated, which include additional quality scores. The individual report evaluates the statistical accuracy of the individual synthetic table compared to the real world table it is based on. This provides insight into the quality of the stand-alone synthetic table. The cross-table report evaluates the synthetic data of the table and all its ancestor tables. This provides insight into the accuracy of the table in the context of the database as a whole.", "The table below displays the data quality scores, grades, and links to each respective report. With eight \u2018Excellent\u2019 and two \u2018Good\u2019 scores, we can feel confident that our synthetic database has the accuracy level recommended for our data analysis use case. ", "All of the Relational Synthetics output files can be found in your local working directory. Additionally, if you\u2019d like to access them via the Gretel Console, you can use the following cell to quickly obtain the URL:", "Finally and optionally, we can write our synthetic results to a database via a connector. This can be the same database used for the source data or a new database. Here, we write to a new SQLite database `synthetic_telecom.db`. In this example, we add the prefix \u201csynth_\u201d to each table name.\u00a0", "With over 30 supported databases, ", " to learn more about writing to your own database.\u00a0\u00a0", "Gretel Relational makes it easier than ever to connect to and synthesize databases using Gretel\u2019s industry-leading generative AI models. We\u2019ve demonstrated that referential integrity and statistical accuracy are maintained when generating a synthetic database with Gretel Relational. With the ability to quickly evaluate the quality and privacy of the synthesized database, you can be confident that your data is accurate and secure.\u00a0", "How will you use Relational Synthetics? Let us know! Feel free to reach out at ", " or join our ", " and share your thoughts.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/innovating-with-fasttext-and-table-headers", "page_title": "Innovating With FastText and Table Headers", "headers": ["Innovating With FastText and Table Headers", "On to the fun part!", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we\u2019re going to walk you through some examples of using ", "embeddings of field headers to improve our Natural Language Processing (NLP) for structured data (e.g. tables, json, csv). NLP can be tough in a structured environment as field values are often single words or numbers. Field headers are often either abbreviated, misspelled or unstandardized. The luxury of the context of full natural language sentences, and the now amazing and plentiful ways of exploiting that is not to be had.", "As a data scientist, being able to quickly analyze and understand new datasets is of vital importance. At ", ", we build tools that help developers and data scientists build, anonymize, and share data safely. Enabling safe data sharing starts with labeling sensitive data, and ", " (PII) has a way of creeping in in unexpected places. Gretel makes sure you know where all potential PII exists using innovative machine learning models.", "This is where the FastText embeddings of field headers comes in. At ", ", we have built a FastText model based on a large collection of schema examples to supplement our entity recognition service to help find sensitive data. Today, however, we\u2019re going to explore alternative use cases for this model. We invite you to experiment with both the model and code with the hope of inspiring yet even more ideas.", "So just what is the data behind this model? Google researchers created an attribute correlation statistics database, ", " , containing co-occurrence statistics of schema elements by mining billions of HTML tables extracted from the general-purpose web crawl. We combined their data with a hefty slice of similarly obtained data from the ", " to create a data set of roughly 12 million schema examples. Each line in our data set is a list of the field headers occurring in a particular table schema. Thus when we build our FastText model, each field header embedding becomes a representation of the other field headers it typically co-occurs with. When two field headers show strong similarity (i.e. consistently co-occur in similar contexts) the implication is they\u2019re interchangeable (a synonym, an abbreviation,\u2026).", " is the king of taking state of the art NLP, implementing it in a supremely efficient manner, and then making easy to use API\u2019s available to everyone. It was an easy choice to decide on using Gensim\u2019s version of FastText. Our model was built using the Gensim API, and our freely available Gretel Tools package utilizes the Gensim API to implement the functionality we\u2019ll demonstrate below.", "First, you must set up your dependencies. To install Gretel Tools, view the ", " file on GitHub. The code examples demonstrated below live in this ", ".", "Next, load our pre-trained FastText header model. By instantiating a HeaderAnalyzer object, the module automatically downloads our FastText model from S3.", "\u200d", "Now let\u2019s look at some scenarios of utilizing the model. One clear use case is you\u2019re designing a new table schema and trying to decide on which field headers to use. You envision potentially joining your table with other sources in the future and would like to use common, popular headers. First up is a telephone number. You\u2019re wondering, should I literally call it \u201ctelephone number\u201d (which is a bit long) or \u201ctelephone\u201d or \u201cphone#\u201d. Utilizing the Gensim API, you can ask the model \u201cWhat are other common variations of \u2018telephone\u2019 used in schema definitions?\u201d", "\u200d", "In the above code example, we asked for the 30 most similar headers and then sorted them by frequency. If you play with the parameters, you\u2019ll quickly realize there are literally hundreds of different ways to phrase this in a table context. The most frequent of the similar headers appears to be simply \u201cphone\u201d, thus the decision is made.", "Perhaps you misspelled \u201ctelephone\u201d using \u201ctelepone\u201d instead. Not a problem! FastText not only embeds words, but substrings of words (ngrams) as well. Thus you can ask questions about absolutely any word, regardless of whether it was in the training data.", "\u200d", "Another scenario is say you\u2019re trying to automatically assess the joinability of two tables. You have already determined that the fields you\u2019re comparing are of the same datatype and do have overlapping values. A FastText comparison of their headers could be the final bit of evidence needed.", "Let\u2019s say one table uses the header \u201cgender\u201d and the other uses \u201csex\u201d. You can check their similarity with the following code:", "\u200d", "That\u2019s a decent similarity score. Another example is one table uses the header \u201cstreet\u201d and the other uses \u201caddress\u201d:", "\u200d", "Not bad. And yet one more example, say one table uses the header \u201cplayer\u201d and the other uses \u201cathlete\u201d:", "\u200d", "Definitely strong evidence that the columns contain joinable fields.", "Yet another scenario is say a company wants to enforce a standardization policy to ensure the joinability of information across a large number of internal data sources. When designing a new table, searching through a lengthy standards document can be an arduous task, especially when the header you had in mind shares few characters with the company standard (as in the examples listed above). A quick FastText comparison between your header and the list of standards would easily narrow in on the right choice. Here\u2019s an example of checking the header \u201cborn\u201d against an example (tiny example!) of company standards:", "\u200d", "It\u2019s fascinating (and also sometimes frustrating) to see the multitude of different ways that developers and data scientists will label the same kinds of data.", "We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, twitter, or ", ". Follow us to keep up on the latest trends on building with your data!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/john-myers", "page_title": "Author: John Myers - Gretel.ai", "headers": ["John Myers", "CHANGELOG: Beta2", "What is Data Anonymization?", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Augmenting ML Datasets with Gretel and Vertex AI", "The Evolution of Gretel's Developer Stack for Synthetic Data", "Gretel releases Beta 2", "Contact Tracing: Deep Dive & Simulation", "Anonymize Data with S3 Object Lambda", "Gretel Smart-Seeding is auto-complete for your data", "Gretel Synthetics: Introducing v0.10.0", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/augmenting-ml-datasets-with-gretel-and-vertex-ai", "page_title": "Augmenting ML Datasets with Gretel and Vertex AI", "headers": ["Augmenting ML Datasets with Gretel and Vertex AI", "Overview", "Getting started", "Diving in", "Next steps: Deployment options", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Here at Gretel, we are thrilled to officially be a ", " with Google Cloud Platform (GCP). For the last few years, Gretel has been unblocking Machine Learning (ML) operations by enabling the use of synthetic data to augment or replace ML training data. With this partnership, we're even more excited to bring the power of Gretel to Google\u2019s ", " customers to accelerate MLOps.", "In this blog we'll show you how to utilize Gretel to create high-quality synthetic tabular data that you can use as training data for a classification model in Vertex AI.", "Vertex AI is an extremely powerful platform for automating MLOps. Having used the product as part of building this integration, it was very easy to see how Vertex can automate away the complexity of model training with ", " and model serving with both real-time predictions and asynchronous batch predictions.", "Regardless of the MLOps platform you use, there's often friction in getting started because of having limited or unbalanced training data. This is where Gretel plugs in! In this example we'll utilize the popular ", " to build a binary classifier with Vertex AI and use Gretel to increase the number of training samples and balance the male and female records to eliminate bias in the dataset.", "Vertex AI provides a wide variety of ", ". For this walkthrough, we've created our own version of the ", "tutorial.", "Before working with the Notebook, the following steps should be taken for setting up GCP and Gretel:", "For GCP:", "For Gretel:", "With the steps above completed, let's ", "! We suggest using Google Colab or ", " for the ease of installing Google SDKs and authenticating with Google's APIs.", "Make a copy of the notebook and execute the first few cells to install dependencies and authenticate with Google's APIs.", "Next, you'll see a cell that looks like this:", "This does two things:", "The next couple of cells will authenticate with Google and create (or access) the GCS bucket that will eventually hold the synthetic training data we'll be making. Feel free to not update the name of the bucket as the code will create a unique GCS bucket name for you.", "Make sure to change the region variable to the region you need to use. A default one is already provided.", "Your bucket and region configuration should end up looking like this:", "There shouldn't be any files in this newly created bucket, so the last cell shouldn't have any output.", "Next, we'll take a peek at the training data:", "There are 717 records and there's a pretty big delta between female and male-based rows. Vertex AI requires a minimum of ", " and based on", " of this dataset, we also want to balance out the number of records for males and females. We'll use Gretel to take care of both constraints!", "Now that we've explored the dataset, let's use Gretel to train and generate new data!", "We authenticate with Gretel and create a Project:", "If you follow the URL to the Gretel Console provided at the bottom, you should see an empty Gretel Project. Don't worry, you'll have a synthetic data model there soon.", "In the next large code cell, we'll begin training a Gretel Model on the training data. For this particular dataset, we've chosen to use ", ". We use the Gretel SDK to load our ", " as a Python dictionary and make a couple of modifications:", "Run the entire cell and the code will run and wait for the Gretel Model to complete training. If you revisit your Gretel Project page, you should see the Model running:", "For this particular training data, the model should take 7-10 minutes to train. Good time for a coffee or tea break!", "When the Gretel Model finishes training, the Notebook code block will display the Model ID like this:", "If you ever need to access this model again, for generating more data, as an example, you can load that back in like so:", "Now that our Gretel Model is trained, we can take a look at the ", " (SQS). We can take a peek at the score and also download the full report with this cell:", "If you refresh the ", " listing in the left pane, you'll see \"report.html.\" You can download and view this report in your web browser, which will give you the full details on the quality and usability of the synthetic data. It will look similar to this:", "Next, we'll use our trained Gretel Model and generate a dataset that has an equal number of male and female records and ensure the final synthetic dataset is a combination of the original training data and our synthetic records.", "We do a basic calculation to determine how many of each \"sex\" to generate, and create a DataFrame that has exactly one column: \"sex\" and its associated values. When we submit this conditioning data back to the Gretel Model and for each \"sex\" value, the model will generate the rest of the record.", "Here we'll be prompting our model to generate 765 female records and 494 male records. Running the next cell will trigger our Gretel Model to generate the new data. If you click into the model in the Gretel Console, you'll see this data generation job eventually be in an \"active\" and finally a \"completed\" state:", "The data generation job should only take a few minutes. Once this is completed we download our newly generated synthetic data and combine it back with the original training data:", "This gives us a total of 1,976 records for training with an even split of male and female records. Additionally, we upload our synthetic training data to our GCS bucket, and now the GCS_SYN_SOURCE variable directly points to the synthetic data we'll now use in Vertex AutoML training.", "Run the next few cells, which will create a Vertex Dataset, set up the Vertex model training job, and kick off the AutoML training. When you execute the cell below, you can expect the training to take about ~2 hours:", "If you open up the Vertex AI console in your GCP project, and click on \"Training\" on the left side-bar, you will see your Vertex AutoML model being trained:", "When this model is complete, you can explore the evaluation metrics by clicking on the model\u00a0", " from the table above. Here we show the combined metrics for both target labels:", "In the above view, click on the ", " tab and copy the", ":", "Back in our Notebook we'll reload this Model:", "Now you can run the remaining two cells which will deploy this model to a real-time HTTP Endpoint and allow real-time predictions. It takes a few minutes to deploy the Endpoint and once it's deployed you can now make real-time predictions like so:", "That's it! You've successfully used Gretel to augment an ML training dataset for use in Vertex AI. At this point you can run the rest of the Notebook (if needed) to tear down the resources you created throughout the tutorial.", "This tutorial uses Gretel Cloud as there is zero infrastructure setup required and it allows users to get familiar with Gretel's capabilities very quickly. Gretel offers options for the Data Plane, which are the infrastructure components that consume training data and produce synthetic data models. When using Gretel Cloud, Gretel manages a backend Data Plane that requires no setup from customers.", "We also offer the ability for customers to deploy their own Data Plane, which we call a ", ". For existing GCP customers, you can deploy the Gretel Data Plane with the following GCP services:", "In this ", ". This allows all data processing to happen in your own GCP account by way of GKE.", "In a future post, we'll do a deep dive on configuring GKE for use with Vertex. Gretel Hybrid Deployments are currently available as a private beta. If you're interested, ", " and we'll be happy to work with you!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-google-cloud-partnership", "page_title": "Gretel and Google Cloud partner on synthetic data", "headers": ["Gretel and Google Cloud partner on synthetic data", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We\u2019re excited to announce the availability of Gretel\u2019s ", ", as well as Gretel\u2019s launch in the Google Cloud Marketplace.\u00a0", "With Vertex AI, MLOps teams have all the functions they need for rapid and scalable AI development at their fingertips. What's often missing, though, is a steady supply of quality data to train and deploy resilient ML models that perform well on downstream tasks. Improving model accuracy requires robust training datasets that cover edge cases and that account for changing real-world dynamics that otherwise lead to problems like data and concept drift. However, the only way to get this improved accuracy is to leverage large volumes of sensitive data, which presents security and privacy risks. With Gretel\u2019s models, which are available in multiple modalities like image, tabular, text, time-series, and relational, developers can create secure versions of sensitive data from existing Google Cloud storage buckets and deliver them directly to Vertex. This approach addresses data compliance concerns and any data supply constraints, as you can access and create whatever data you need, on-demand.", "Google Cloud\u2019s partnership with Gretel makes it simple to generate anonymized and safe synthetic data for enterprises blocked by data sharing limitations or a lack of data. Synthetic data also enables developers to experiment with, fine-tune, and operationalize foundation models without giving these models access to their raw and sensitive data. This unlocks \u201cthe last mile\u201d in generative AI, where the data that\u2019s most valuable to train a model isn\u2019t available in the public domain or simply can\u2019t be shared.", "Developers that use Vertex AI for MLOps will now be able to use synthetic data to meet privacy and data augmentation requirements, resulting in accelerated ML R&D and operations.", "The entire collection of Gretel\u2019s models is available for Google Cloud and Vertex customers to incorporate into their services, including APIs to:", "Research has shown that synthetic data can be as good or even better than real-world data for data analysis and training AI models; and that it can be ", " in datasets and ", " of any personal data that it\u2019s trained on. With the right tools, synthetic data is also easy to generate, so it is considered a fast, cost-effective data augmentation technique.", " said Manvinder Singh, Managing Director, Partnerships at Google Cloud. ", "If you\u2019re reading this, you know that the promise of generative AI is unquestionable, and that enterprise leaders everywhere are imploring their teams to incorporate ML and AI into their R&D and products. The biggest hurdle to jumping in, in addition to having limited data to start with, is ensuring the privacy of that data and the safety of the models being used. Safe incorporation of generative AI means ensuring that what a business gets out of its generative model is exactly what it expects from its data. By using Gretel\u2019s synthetic data, with its pre- and post-validation ensuring the privacy of the model and the utility of the data, enterprise leaders seeking to win in a world being revolutionized by generative AI can do so safely.", "If you are a Vertex user, ", " and start building safer models with synthetic data. Going forward, Gretel will also work closely with Google Cloud to accelerate deployment and availability within the GCP Marketplace, including adding integration with BigQuery for fully automated workflows. Want to use Gretel and Vertex AI within your Google Cloud deployment? ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-synthetics-introducing-v0-10-0", "page_title": "Gretel Synthetics: Introducing v0.10.0", "headers": ["Gretel Synthetics: Introducing v0.10.0", "Getting Started", "Data Validation", "Coming soon\u2026", "Wrapping Up", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we are thrilled to release new features for", " that make it even easier to get started with synthetic data. After initial rounds of testing and feedback from customers we are introducing a new interface that works directly with Pandas DataFrames and allows batched training of columns to ", ".", "The Batch interface uses the existing config, training, and generation modules under the hood while automating some of the manual steps a user would have to take to work with complex and highly dimensional datasets.", "The motivation behind the Batch interface is to support datasets with high dimensionality and column counts. This is achieved by clustering like columns and training models on subsets of the entire dataset at once. While batching is generally not required to generate synthetic datasets with less than 30\u201340 columns, scaling to support datasets with hundreds or thousands of columns requires a different approach.", "One of the biggest benefits to Gretel\u2019s synthetic data library is its ability to automatically ", " at both the dataset and record levels. For example, consider height and weight. Taller people tend to be heavier \u2014 that\u2019s an example of a positive correlation, which would be learned automatically by our model. Not all columns are directly correlated, however, and by splitting the input training into batches or clusters of columns, it becomes possible to scale synthetic generation to highly dimensional datasets with minimal loss in accuracy.", "Let\u2019s explore how we create our Batch interface. Click the link below to build a synthetic dataset interactively with our batch training notebook (including free GPU access \ud83d\udd25) on ", ".", "First, define parameters for training Gretel\u2019s synthetic data model:", "Gretel Synthetics\u2019 DataFrameBatch object only takes two keyword arguments to function.", "First is your source PandasDataFrame and second is a configuration template. The parameters for the configuration template are ", " as the configuration parameters you would use for a", ", simply put them into a dictionary instead. Version 0.10.0 also includes detailed doc strings for each of the config parameters so you can explore and tune for more sophisticated use cases.", "By default we split the input DataFrame into batches of 15 columns. This can be changed via init params and additionally you may use a ", " parameter to provide your own batches of headers.", "There are a couple of notable changes / updates:", "A vital step in data synthesis is validating that generated lines meet specific constraints. Gretel Synthetics allows validators to be used for each line, this functionality extends into the Batch module by allowing validators to be set for each batch.", "By default, if no validators are provided, each line generated for a batch will be checked that it can be split back into the number of headers that exist for the batch. This ensures that the output DataFrame has the same shape as the input DataFrame.", "If you wish to configure a validator for a specific batch \u2014 for example, to balance a health dataset by ", " \u2014 you can do that like this:", "Stay tuned for our Public Beta which makes several more libraries available, including our custom validator package.", "Our custom validation package will automatically learn constraints in your data and enforce them during generation. This includes character sets, numerical boundaries, string patterns, and more! As always, we will have simple API interfaces and notebooks examples to help you get started.", "Once we have created our DataFrameBatch", "instance, we need to go through the training and generation process. These functions are bundled into three core methods for: generating training data, training the models, and generating synthetic data:", "Now you\u2019ve got your synthetic data!", "We hope this overview has been helpful. Please step through our ", " which walks through the entire process using a publicly available dataset.", "Feel free to shoot us feedback at ", " or open an ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/murtaza-khomusi", "page_title": "Author: Murtaza Khomusi - Gretel.ai", "headers": ["Murtaza Khomusi", "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/bringing-ai-generated-images-to-enterprise-use-cases", "page_title": "Bringing AI-generated images to enterprise use cases", "headers": ["Bringing AI-generated images to enterprise use cases", "Introducing Gretel image synthetics", "How to generate images with Gretel", "Get started with Gretel image synthetics today", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["\u201cPics ", " it didn\u2019t happen.\u201d ", "For all the images that currently exist on the internet and in private databases, the right ones for specific tasks can be difficult to find, or simply don't exist. Imagine you\u2019re a data scientist at McAgro Ventures, working on a machine learning model for resilient crop detection. You have a great library of plant pictures, and you can find lots of pictures of snow, but without a separate set of pictures of ", " plants ", " that snow, you\u2019re missing critical data you need to train your model. Today, we're introducing ", ", to help you generate the images you need, when you need them.", "Gretel image synthetics enables enterprises to harness image-focused generative AI, and compose their domain data with the knowledge built into foundation models. We designed our image synthetics system specifically for generating domain-specific images at scale, with an emphasis on the high-quality needed for training downstream ML models.", "With a wide range of applications, from medical X-rays of rare conditions to simulations of different climates for", ", the future is synthetic.", "To introduce these capabilities, we\u2019ll showcase how an auto insurance company might use image synthetics to improve their ability to accurately process broken windshield claims. Today, determining the cost of replacing a windshield requires expert knowledge and in-field assessment. Automating this process is challenging due to the strong data collection requirements. The insurance group may not want to collect customer images due to privacy constraints, and breaking the windshields of their own cars is quite undesirable. ", "Image synthetics are a potential solution. By training the model on publicly available images of broken windshields and publicly available images of car models, this group can combine the two concepts to produce unlimited high quality images of broken windshields to use in training a ", " ", " model.\u00a0", "First, we'll train Gretel image synthetics to generate realistic images of broken windshields. Since we leverage a pre-trained model, we can start by using just 17 images of broken car windshields. This process is teaching your custom Gretel image synthetics model about the new ", "of broken windshields.\u00a0", "We can generate the above using the following prompt, which includes a series of style cues to suggest realistic images.\u00a0", "`", "`", "An auto insurer might need to be able to estimate damage pricing for broken windshields for specific makes and models, or just make sure that their model is reliable for particular years or styles of vehicle. To increase their model\u2019s reliability or accuracy, they might need to generate images with particular features. Let\u2019s do this ourselves by training Gretel image synthetics to generate realistic images of Acura cars by using 15 images of an Acura TSX Sedan 2012. We can use our prompt to control certain aspects like color!", "`", "`", "There were no pink cars in the training dataset! We combined a new color with our Acura concept to generate high-utility images by prompting the model with text input. The model already \u201cknows\u201d thousands of concepts, which means you can often synthesize the data you need even when you don\u2019t have fine-tuning data. This synthetic data stays grounded in your data input and gains flexibility from the many concepts already in the model.\u00a0", "Now the fun starts! While none of our Acura training images had a broken windshield, and none of our broken windshield training images were of an Acura, our training images helped familiarize the model with each of these concepts individually. It\u2019s now able to combine both concepts by generating images of Acura cars with broken windshields. The best part is that thanks to the magic of image synthetics, we could do this without breaking a single windshield!", "Gretel image synthetics is now available as a ", ". We'd love to hear your feedback, whether you use it to build a custom use case, follow along with this windshield use case, or even create your own superhero avatar. Once available in Gretel\u2019s synthetic data platform, the new image synthetics model will bring to images the same high-quality, scalable, and private synthetic data generation you're used to for tabular, natural language, and time series datasets.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/privacy?094d394d_page=2", "page_title": "Privacy blog posts - Gretel.ai", "headers": ["Privacy", "Red Teaming Synthetic Data Models", "Introducing Gretel Tabular DP: A fast, graph-based synthetic data model with strong differential privacy guarantees", "Gretel and Google Cloud partner on synthetic data", "Augmenting ML Datasets with Gretel and Vertex AI", "Teaching large language models to zip their lips", "Automate Detecting Sensitive Personally Identifiable Information (PII)", "How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "Got text? Use Named Entity Recognition (NER) to label PII in your data", "Deep dive on generating synthetic data for Healthcare", "Contact Tracing: Deep Dive & Simulation", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/yamini-kagal", "page_title": "Author: Yamini Kagal - Gretel.ai", "headers": ["Yamini Kagal", "Automate Synthetic Data Pipelines with Gretel Workflows", "Got text? Use Named Entity Recognition (NER) to label PII in your data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/integrating-with-gretel-sdks-just-got-easier", "page_title": "NEW: Integrating with Gretel SDKs just got easier!", "headers": ["NEW: Integrating with Gretel SDKs just got easier!", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Last week Gretel launched our Gretel Console Beta for creating data projects and uploading datasets to Gretel Cloud. Since then we have received an influx of fantastic feedback from the community around how we can make the experience easier, faster and more streamlined. Today we\u2019re rolling out our first new feature: the project integration button.", "Engineers and data scientists are now using the Gretel Beta to transform and synthesize private data using our ", " and ", ". Users can create a free Gretel account using their existing Github or Google accounts, generate an API key, and in just a few clicks start streaming in data. In order to get started streaming data with Gretel tools you need to provide your personal API key and unique project name identifier.", "We heard feedback from our users who told us that the process of generating and finding their personal API Keys was difficult, and they had a hard time finding their project name identifier (hint: It\u2019s hidden in your project settings). They also encountered times when they had to break their workflow to copy their API key (hint: that one is buried in your user profile). We realized quickly that these important elements needed to be front and center in our users\u2019 workflows and always accessible in just a few clicks.", "Starting today, you will now have access to your project name, API key and connection URI directly from any project page! That means that integrating with Gretel is only one click away from anywhere in your project.", "The integration menu gives you quick and immediate access to everything you need to get started with our SDKs or REST API:", " or create a free account with the Gretel console today and see the new integration feature in action.", "We have many more updates on our product roadmap that will improve the experience of creating safe data for everyone. As we continue to build we want to hear what you have to say! If you have feedback, questions or just want to chat please reach out to us at ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/practical-privacy-with-synthetic-data", "page_title": "Practical Privacy with Synthetic Data", "headers": ["Practical Privacy with Synthetic Data", "TLDR; ", "Background", "The dataset", "The attack", "Run the experiments", "Examining the results", "Next steps", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this post, we will implement a practical attack on synthetic data models that was described in the ", " by Nicholas Carlini et. al. We will use this attack to see how synthetic data models with various neural network and differential privacy parameter settings actually work at protecting sensitive data and secrets in datasets. ", "Oh, and we found some pretty surprising results\u2026", "Open datasets and machine learning models are incredibly valuable for democratizing knowledge, and allowing developers and data scientists to experiment with and use data in new ways. Whether the datasets contain location data, imagery (such as camera video), or even movie reviews, the underlying data used to create these datasets often contains and is based on individual data.", "Synthetic data models are a promising technique for generating artificial datasets that contain the same insights and distributions as the original data, without containing any real user data. Potential applications include allowing medical researchers to learn about a rare disease without learning about the patients, or training machine learning models on private data without exposing potentially sensitive information.", "Traditionally, successful applications of differential privacy have been shown with massive datasets containing highly homogeneous data. For example, the US Census data, or emoji prediction for Apple devices. These datasets have 100s of millions of rows, and lower dimensionality, making them ideal candidates for privacy preserving techniques like differential privacy. For this example, we will work with a much smaller dataset that is based on a public feed of e-bike ride share data. What makes this dataset interesting for our dataset? It contains sensitive location data, which has long been considered a challenge to properly anonymize. Secondly- we\u2019re working with a dataset that\u2019s under 30,000 rows of input data and more representative of the typical datasets data scientists work with each day.", "Let\u2019s see if we can build a synthetic dataset with the same insights as the raw data, but without using real trips or locations. For this example, we logged a public ride-share feed over the course of one day in the Los Angeles area, and you can download the dataset on our Github. Here\u2019s the format:", "We will implement a practical attack on Gretel.ai\u2019s data model (this will work on any generative language model) by inserting canary values at various frequencies randomly into the model\u2019s training data. We can then use that data to generate models with various neural network and privacy settings, and measure each model\u2019s propensity to memorize and replay canary values.", "How will these results differ from the guarantees that differential privacy provides? Well, we cannot mathematically claim that a model is private if these attacks pass. E.g. It is possible that a model has memorized some un-intended information (canary values) that simply have not been replayed by the model yet. That said, knowing that you can insert a canary value between 10 and 100 times in a dataset and not see it repeated does provide practical checks on your model, _and_ that your expectations around privacy guarantees appear to be holding. ", "Let\u2019s start by generating random secrets to insert into the training data. ", "Next, sample the training set and add new records containing canary values.", "We can use a simple helper to search for the existence of the secret strings in generated data. Let\u2019s see how many values of each secret we will be inserting into the training data.", "We are ready to run our experiments by training synthetic data models on our new training set containing various quantities of canary values. In the experiments below, we trained synthetic data models with 10 different configurations and with canaries inserted between 18 and 138 times into the training data. We then counted the number of secrets that were replayed by the model during generation.", "To try this yourself, check out our ", ".", "As we can see, the standard synthetic data configurations without differential privacy in Experiments 0, 1, and 2 provided reasonable protections against rare data and secrets being learned, with the secret needing to be present over 18 times in the training data to be memorized and replayed by the model.", ". Experiment 9 used a relatively high noise multiplier (0.1) and aggressive gradient clipping (1.5) which resulted in zero canaries being replayed in the generated data, but with a substantial decrease to model accuracy. ", "To our surprise, simply using gradient clipping in Experiment 6 and a minimal level of noise into the optimizer prevented any replay of our canary values- (even secrets repeated 138 times in the data!) with only a small loss in model accuracy. ", "For many use cases with data, such as training ML models- we are balancing the privacy guarantees of data with utility and accuracy. Try experimenting on your own data with different parameter configurations to find the right balance.", "Have your own use case to chat about? Send us an email at hi@gretel.ai or join our community Slack at ", ".", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/contact-tracing-deep-dive-simulation", "page_title": "Contact Tracing: Deep Dive & Simulation", "headers": ["Contact Tracing: Deep Dive & Simulation", "Summary", "Crypto Components", "Proximity Exchange", "Determining Risky Contacts", "Simulation", "Privacy", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["There\u2019s been a lot of scrutiny, concern, and analysis of the ", "proposal from the joint Apple + Google effort to provide privacy preserving capabilities that inform people when they have been in close proximity to a COVID-19 infected person. I decided to examine this a little further to see exactly what was being proposed, how it would be implemented, and what privacy concerns there are.", "I implemented the cryptographic portion of the specification in Python and created a rudimentary \u201cLife\u201d simulator that shows what results can be expected. The code is ", ".", "T", "The core of this specification is creating a 3-tier system of secure identifiers / keys that can be selectively exchanged.", "The first component is the handset Tracing Key. The Tracing Key is a unique 32-byte base encryption key that will never leave the device. Ever.", "Second is the Daily Trace Key (DTK). This is a derivative encryption key that is created every 24 hours using a HMAC Key Derivation Function (HKDF). This key:", "Finally, there is the Rotating Proximity Indicator (RPI). This essentially is a 16-byte digital signature that is derived from a particular DTK. For a single DTK, there can be up to 144 unique RPIs. We use a HMAC to compute this and part of the message component to the HMAC is the nearest 10-minute interval for that day.", "We concatenate the interval number to the string \u201cCT-RPI\u201d to create the message we are hashing.", "So we compute the first RPI of the day as ", "and the last RPI of the day as ", ".", "Technically, the device only computes a new RPI when the Bluetooth MAC address changes but the specification allows for every 10 minutes if needed. MAC address rotation is usually between 10 and 20 minutes.", "So, at any given point in time. A single handset will have:", "Because of the chain of custody from the base Tracing Key we are almost sure there will be no collisions across these varying components.", "When 2 handsets come within close proximity to each other (which is detected by one handset sensing another bluetooth device near by), each handset will send the other handset its current RPI.", "So if handsets A and B come within a proximity where they can sense each other\u2019s Bluetooth connections. A will send B an RPI and B will send A it\u2019s RPI.", "Each handset stores a growing list of RPIs it received from other handsets.", "When the owner of a handset receives a positive test result for COVID-19, that owner will have the option to self-identify in whatever application is using this protocol. Then the application will:", "At this point, all other handsets are able to download these DTKs. Now remember, given any DTK, you can compute all possible 144 RPIs from that DTK without needing any other information.", "So, if a handset receives a list of 14 DTKs from the remote server, that handset can compute all potential RPIs for that list of DTKs. This is 14 * 144 (2,016) possible RPIs. The handset now determines if one or more of those 2,016 RPIs are in its own list of RPIs that it received from other handsets in the last 2 weeks.", "This is essentially doing a set intersection between the 2,016 derived RPIs from our infected subject and the set of all RPIs we received from other handsets.", "The intersection of RPIs now represent precise windows of time that a handset was in close proximity to another handset where the owner has tested positive for COVID-19.", "Here is a simplified graphic overview of how this process works:", "To simulate how this works I created a basic Python class that represents a single handset. Additionally, I created a Life class that simulates 2 weeks of life-living from a subject that will eventually test positive for COVID-19.", "The pattern-of-life is very rudimentary, we assume the individual interacts with family in the morning, co-workers for most of the day, other random individuals at meals, errands, etc and back to family in the evening. Weekends are a mix of interacting with family, friends, and others.", "The README will get you started if you want to run it yourself. When the simulation begins, we choose a random amount of family, friends, co-workers and others that you will interact with. The \u201cothers\u201d category represents all the people the subject does not know, but interacts with at stores, gyms, etc.", "The number of unique handsets increases for family, friend, and so on in an attempt to be a little realistic about who you have close and continuing contact with.", "After it runs, a \u201creport.txt\u201d will be generated. The header of this report will show the chosen sizes of your groups:", "I hard-coded the start date to be 13 Apr at 0700 AM ET.", "Each interaction with a category of people chooses a handset at random from that group. Naturally this yields higher amounts of interaction with family compared to a larger group like \u201cothers.\u201d", "The other sections in the report shows what a ", " might show to its owner. Each handset report ", "Additionally, the report is sorted descending by the number contact windows each handset had with the infected individual.", "Taking the first handset report from the list, shows a high level of interaction with the subject:", "Naturally, as we scroll down the report, other relation categories have fewer interactions with the subject:", "The protocol itself drastically reduces risk of re-identification. Rotating unique IDs that grow in cardinality for any given handset help mitigate risk. The rotating RPIs assist in providing much better granularity of risky contacts while also mitigating re-identification.", "With that said, the upstream use of this protocol is the biggest risk.", "Device and owner specific information should not be collected, ever. It\u2019s \u201ceasy\u201d to also collect device / user information especially in the step where DTKs are sent to a remote system. This would include handset data, GPS location information, and transport information (IP addresses, etc). This data can be joined and used to re-identify DTKs to specific people.", "This protocol does not guarantee anonymity from other people that may receive a notification once the subject discloses an infection. Primarily because it is very likely someone might recall where they were on any date and time and narrow down the potential list of carriers to the subject that reported the positive test in the first place.", "Generally, the protocol should be implemented in a way that ensures the owner of the upstream systems that collect DTKs are unable to identify specific individuals. This protocol should more quickly enable better testing candidates and faster quarantines and does not replace the more specific location/trend analysis that public health professionals will have to do.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/gdpr-and-ccpa", "page_title": "Understanding Synthetic Data, Privacy Regulations, and Risk", "headers": ["Understanding Synthetic Data, Privacy Regulations, and Risk", "Summary", "Overview", "Key definitions and regulatory concepts", "Definition of synthetic data", "Regulatory concepts\u00a0", "Relevant EU regulations and opinions", "Recital 26\u00a0", "Article 29 Data Protection Working Party Opinion 05/2014 (A29)", "De-identification, anonymization, and the CCPA", "Gretel\u2019s solution for mitigating privacy risks", "Best practices for anonymization", "Privacy best practices implemented by Gretel", "Conclusion", "Frequently Asked Questions", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This guide examines two European Union (EU) privacy regulations \u2014 Recital 26 of the GDPR and an Article 29 Data Protection Working Party opinion \u2014 to explore how risks associated with sensitive data can be mitigated by using synthetic data generation (SDG). Synthetic data is artificially annotated information that is generated by computer algorithms or simulations that is commonly used as an alternative to real-world data. Gretel's services provide privacy-preserving technologies to reduce privacy risks significantly by implementing best practices such as de-identification, overfitting prevention, differential privacy, similarity filters, outlier filters, and continuous monitoring. Gretel's services and best practices can provide safeguards against known adversarial attacks and provide protection from the applicability of GDPR, CCPA, and similar privacy regulations, to a dataset.", "\u200d", "The European Union\u2019s General Data Protection Regulation (GDPR) has shaped how global companies design and build products, software, and systems since its inception. As data has become the foundation of most products and services, companies are increasingly collecting and analyzing customer data, and prioritizing respect for the privacy rights of customers while complying with various privacy regulations, such as the GDPR.\u00a0", "This guide provides a summary of European Union (EU) privacy regulations and how risks associated with sensitive data can be mitigated by using ", " (SDG). Synthetic data is artificially annotated information that is ", " that is commonly used as an alternative to real-world data. While artificial, synthetic data statistically reflects the insights of real-world data. Recent advancements in generative AI have made it possible to create synthetic data with accuracy that can be as effective as real-world data for training AI models, powering statistical insights, and fostering collaboration with sensitive datasets while offering strong privacy guarantees.", "At Gretel, we believe that synthetic data generated using our services could be considered anonymized information and thus not be subject to the GDPR or similar applicable privacy laws, including the California Consumer Privacy Act (CCPA). While the EU data protection authorities haven't directly opined on the nature of synthetic data, existing guidance in the EU regulations examined below supports the view that it would constitute anonymized data.\u00a0\u00a0\u00a0", "But any effort to implement strategies to mitigate privacy risk requires:", "This guide covers these topics to provide readers with actionable knowledge on mitigating privacy risks with synthetic data.\u00a0", "\u200d", "Before examining some of the key regulations, let\u2019s establish some shared vocabulary that we\u2019ll use throughout this guide. We\u2019ll cover the definition of synthetic data that Gretel uses and source definitions for pseudonymization and anonymization from relevant EU documents.\u00a0", "\u200d", "Fundamentally, synthetic data is annotated information that computer simulations or algorithms generate as an alternative to real-world data.\u00a0", "Synthetic data is generated using a class of machine learning models that learn the underlying relationships in a real-world dataset and can generate new data instances. High-quality synthetic data will contain the same statistical relationship between variables (e.g., if real-world data reveals that weight and height are positively correlated, then this observation will hold true in the synthetic data). However, records of a synthetic dataset will not have a clear link to records of the original, real-world data that it is based on. Synthetic data is one step removed from real-world data. This means that synthetic data generated with the right level of privacy protection can provide safeguards against known adversarial attacks, something traditional anonymization techniques like masking or tokenization cannot promise.", "\u00a0", " is a \u201c", "\u201d", " is the \u201c", "\u201d", " In contrast to pseudonymization, anonymization is intended to prevent the re-identification of individuals within the dataset.", "\u200d", "With this conceptual foundation, we can examine the most relevant EU regulations. While EU privacy regulations and other related writings are voluminous, there is a subset of these documents that we believe forms a core set of requirements. The most relevant documents are:", "Here are some of the key features of both documents for understanding their privacy impact.\u00a0", "\u200d", "Recital 26 of the GDPR makes several important statements:", "While the statements are strong, there are several caveats about Recital 26 that readers should note:", "While aspects of the Recital seem to leave some ambiguity, other EU writings are very helpful in clarifying the criteria for when anonymization can successfully make a dataset not subject to privacy regulations.", "\u200d", "This lengthy document is very helpful in clarifying criteria for anonymizing information, such as those noted in Recital 26. In this course of this clarification, it:", "The three tests are:", "A29 then discusses these tests against several anonymization technologies, and Gretel supports these technologies mentioned in A29:", "A29 indicates \u2014 and Gretel agrees \u2014 that Differential Privacy is one of, if not the strongest, standards, with other techniques that rely on noise addition being very effective.\u00a0 Gretel\u2018s view is that using these techniques ", " can provide very strong protection from the applicability of GDPR privacy regulations to a dataset, ", " that best practices are followed. We\u2019ll cover a set of best practices in a later section.", ": Note that even A29 states that there is \u201cno prescriptive standard in EU legislation\u201d for which anonymization technique is to be used (A29, p.6). Properly applying techniques and best practices is critical, and Gretel can assist customers in their selection and application.", "\u200d", "While the CCPA is an American rather than EU regulation, many companies dealing with EU privacy regulations also aim to adhere to the CCPA, which was a first-of-its-kind privacy law in the United States.\u00a0\u00a0", "Under the CCPA, the relevant concept is \u201cde-identification\u201d rather than anonymization. De-identification is a lower standard than the anonymization required by GDPR and could be satisfied with ", " services by simply removing, replacing, or encrypting personal identifiers in data. The additional step of applying\u00a0 the use of Gretel\u2019s anonymization techniques via synthetic data generation is not necessary. However, Gretel recommends that customers seeking to ensure their data is outside the scope of CCPA use Gretel\u2019s synthetic data.\u00a0\u00a0", "De-identified information is exempt from the CCPA. \u201cDe-identified information\u201d under the CCPA means information that cannot reasonably identify, relate to, describe, be capable of being associated with or be linked, directly or indirectly, to a particular consumer. It requires that the business that uses such de-identified information implement technical safeguards and business processes that prohibit re-identification and processes to prevent inadvertent release of the de-identified information. This definition effectively exempts data de-identified in accordance with ", ". When customers use the Gretel \u201cbest practices\u201d described below to maximize data anonymization of their datasets, they are significantly more likely to make re-identification difficult.", "\u200d", "As a synthetic data platform that uses generative AI, Gretel provides multiple services and privacy-preserving technologies that significantly reduce privacy risks.", "When using AI-based generative models, there is inherent stochasticity (randomness or noise) in generating synthetic data that offers inherent privacy benefits. Commonly used methods, like batch-based optimization (e.g., stochastic gradient descent) and regularization (e.g., dropout layers in deep neural networks), are designed to ensure that the models generating synthetic data do not memorize their inputs. They are intended to improve the generalization of models, which aids anonymization.", "Gretel\u2019s synthetic data models can be augmented by applying \u201cbest practices,\u201d privacy-preserving techniques to achieve a higher level of anonymization. Such best practices are described further below.\u00a0", "When it is critical to include sensitive and potentially identifying fields like names in a dataset, a user will first replace real names with fabricated ones using Gretel Transform and then pass the dataset to a synthetic data-generating model. The resulting synthetic data will not have any such identifiers from the real-world dataset.", "Additional privacy mechanisms, such as ", " and ", ", can be used with Gretel models. Differential privacy is a mathematical standard discussed in Article 29 that users can configure Gretel models to utilize. Gretel also introduces privacy-enhancing technologies, including privacy filters that aim to thwart adversarial attacks by ensuring that no synthetic record is overly similar to any record in the real-world data or is an outlier. These additional technologies provide strong protection against data linkage, membership inference, and re-identification attacks with only a minimal loss in data accuracy or utility.", "Using synthetic data models can mitigate the likelihood of singling out an individual by linking records relating to an individual or inferring information about an individual. However, if a dataset contains information primarily about one individual, then the likelihood of exposure for that individual increases. This caveat is not unique to synthetic data and applies to every anonymization method.\u00a0", "Some anonymization techniques show inherent limitations. These limitations must be considered seriously before data controllers craft an anonymization process using a given technique. They must have regard for the purposes to be achieved through anonymization \u2014 such as protecting individuals\u2019 privacy when publishing a dataset or allowing a piece of information to be retrieved from a dataset.", "With Gretel\u2019s services, anonymization efforts can adopt several levels of protection, both to offer increasing guarantees for the protection of personal data and also to help find the right balance between privacy and data utility appropriate for the use case. While all three of the levels outlined below provide protection from the GDPR, implementations that adopt the \u201cBetter\u201d or \u201cBest\u201d approaches defined below most optimally minimize risk.\u00a0", "\u200d", "The EU regulations we\u2019ve covered, particularly A29 (pp. 24 - 25), specify best practices for applying anonymization techniques.\u00a0", "A29 suggests that:", "It also suggest that companies:", "Where possible, Gretel\u2019s platform follows these best practices automatically, or the Gretel team can assist with ensuring that all of these are followed.", "Interestingly, one of the best practices mentioned in A29 is dealt with automatically by Gretel\u2019s technical approach. A29 states that \u201cWhen relying on differential privacy (in randomization), account should be taken of the need to keep track of queries so as to detect privacy-intrusive queries.\u201d This best practice is relevant for systems that place a differential privacy-based filter between the raw data and the entity that is querying that data. With Gretel\u2019s approach, where the synthetic data is generated and separated entirely from the original data, this \u201cmulti-query risk\u201d is eliminated.", "Additionally, Gretel recommends this best practice:", ": the more records in a dataset about a single individual, the higher the likelihood of exposure for that individual. While this is not always possible, pre-processing the data to limit the number of records containing information about any one individual as a fraction of all individuals in the data increases the effectiveness of every anonymization method, including synthetic data.\u00a0\u00a0", "\u200d", "Many privacy-focused systems or platforms require developers or users to implement best practices manually. Gretel, however, implements many automatically.\u00a0", "These fall into three categories:", "For more detail on the synthetic data privacy and protection features of Gretel\u2019s services, please visit Gretel\u2019s Privacy Protection FAQ.", "\u200d", "At Gretel, we believe that the proper implementation of a privacy solution using Gretel services can dramatically reduce \u2014 and possibly eliminate \u2014 the risk that a dataset is subject to privacy regulations. As discussed in this guide, it is vital that the right techniques are used, and that best practices are followed during implementation and thereafter. Gretel\u2019s team is happy to assist customers with the implementation of privacy solutions using synthetic data.\u00a0", "For advice or support with privacy issues using Gretel services, please contact ", ".\u00a0", "\u200d", "\u200d", "\u200d", "\u200d", "\u200d"]},
{"url": "https://gretel.ai/blog/synthetic-data-configuration-templates", "page_title": "Synthetic Data Configuration Templates", "headers": ["Synthetic Data Configuration Templates", "Configuration Templates", "Backwards compatibility with current Gretel Beta", "Summary", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Have you ever gone to a restaurant or a bar and been given blank menu? That would be awkward right? ", "Menus are great because they give you options, even if you aren\u2019t sure what you are looking for.", "Well we\u2019ve got some options for you!", "If you have seen our ", " blog post, then you know that we\u2019re actively working on creating an API driven approach to privacy engineering, backed by Gretel Cloud.", "Part of this effort revolves around a configuration driven approach. Instead of having to build data structures in a specific programming language, like Python, users will be able to construct their privacy workloads in configuration files that can be sent to our REST API. From here, a Gretel Worker can consume the configuration from the API and do the heavy lifting. A Gretel Worker can consume a job request from our REST API and begin processing data in Gretel Cloud or your own environment.", "Over the last several weeks, we\u2019ve spent a lot of time with customers helping create optimal ", " parameters based on the type of training data that is being used. Often times the training data can have a smaller number of records, larger number of records, be all numeric, have a lot of free text data, etc. It\u2019s important to get the right configuration options to get the best possible synthetic data.", "We\u2019ve found some pretty good synthetic data configurations and while we\u2019re building for Beta2, we made the decision to start tracking these configurations and make them available as templates that can be consumed and edited just before training a model.", "When Beta2 releases, configurations can be authored in YAML or JSON and pushed to our REST API. The configurations for Gretel Synthetics ", ".", "Take a look at the README to explore the various configuration templates. Additionally, each template has some comments with more detail on the rationale behind the various parameter settings for each use case.", "If you have been using our current ", ", you can use these configuration parameters today. You may recognize some of the params from the template YAML files as the keys in the synthetic config_template from the current blueprints.", "Here\u2019s what you are used to seeing in blueprints today:", "You may simply update the keys in the config_template dictionary with the key / value pairs defined in the param object from the ", ".", "However, we\u2019ve added a quick helper to do this automatically in our gretel-client. \u00a0Gretel client is already installed as part of our blueprints, so as long as you get the latest version you can start using this today. \u00a0Our ", " has already been updated to use this new helper. With the helper you can just provide the name of the configuration template (without the file extension) and the configuration dictionary will be loaded for you:", "Aren\u2019t options great? No matter what your data looks like, we hope some of our configuration templates will help you pick some of the right parameters needed to train your synthetic data models. You can always try experimenting with some of the parameters once you\u2019ve loaded a template.", "As we make progress with Beta2 we\u2019ll continue to post updates!", "We\u2019d love to hear how these templates work. As usual, send us an email at ", " or come ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/nicole-pang", "page_title": "Author: Nicole Pang - Gretel.ai", "headers": ["Nicole Pang", "Introducing Gretel Benchmark", "Compare Synthetic and Real Data on ML Models with the new Gretel Synthetic Data Utility Report", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/automated-data-exposure-detection-with-gretel-outpost", "page_title": "Automated Data Exposure Detection with Gretel Outpost", "headers": ["Automated Data Exposure Detection with Gretel Outpost", "Overview", "Example Flow", "How to get started with Outpost", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["On August 25th ", " will launch its free Public Beta and we are super excited to enable developers to create safe data with our cloud service and developer ", "s. As we build, we constantly find new use cases for our technology.", "Previously, while testing our own service, we discovered unintentional PII in a ", ". Since then, we\u2019ve continued to observe the multiple data leaks in the news and have had several discussions with enterprise security and privacy teams, who reached out to see how our automated data labeling could minimize the risk of accidental data exposure.", "After some great conversations and some weekend coding sessions we are happy to announce ", ".", "Outpost is a free integration architecture that ingests alerts from common security tools, executes data collection surveys, and sends that data to Gretel to quickly identify the exposure of PII and sensitive data. In a sense, Outpost automates the steps that a security team will take in assessing the risk or exposure to data associated with the security alert.", "Outpost has two components that work hand-in-hand:", "The diagram below illustrates possible deployment modes for Outpost using the Shodan consumer:", "\u200d", "One example deployment would be using Shodan.io (a search engine that scans the Internet) alerts to identify publicly accessible ElasticSearch systems that reside within your organization\u2019s public IP ranges. First, you would configure Shodan to ", " that belong to your organization. Second, start Outpost with the Shodan consumer enabled.", "Whenever Shodan yields an alert that indicates a possible exposed ElasticSearch system, a survey request will be generated and the ElasticSearch survey module will attempt to extract sample documents and send them to a new Gretel Project. We will sample documents from all possible indices, which provides maximum coverage on all possible data that is exposed.", "Additionally, the raw Shodan alerts are also consumed, processed for data labeling, and made available in its own Gretel Project. This provides a quick, safe way to view everything Shodan knows about your infrastructure.", "Survey data shows up as new projects in your project list and contain a short description of the survey that created the project:", "\u200d", "Now you may invite others to look at the data or use our SDKs to anonymize and share the findings with others. The records that are sent to Gretel are encrypted in a memory-only stream and we only retain the most recent 50,000 records to enable raw data analysis.", "Metadata about the records, however, will be kept for the duration you keep the project active and can accumulate metrics well beyond the cached 50,000 records. One particular report that is useful when analyzing sampled data from Outpost is the entity report, which shows high level statistics on what types of data we saw across the entire ingest stream:", "Outpost is a free community project, however due to the sensitive nature of its capabilities we have chosen not to open source it at this time. We welcome any team member of a security or privacy organization that has a need to mitigate exposure risk to request access. We will add you to our GitHub team, which provides access to the code and deployment guides.", "Outpost is available today and will be usable with our public beta that launches August 25th!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-smart-seeding-is-auto-complete-for-your-data", "page_title": "Gretel Smart-Seeding is auto-complete for your data", "headers": ["Gretel Smart-Seeding is auto-complete for your data", "Time Series Data", "General Smart-Seeding", "Summary", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Over the last few weeks, we have released how-to guides on two particularly challenging use cases for synthetic data- ", ", and generating ", ". Both of these use cases are made possible by a feature we call smart-seeding: the ability to use the synthetic model\u2019s neural network model to auto-complete a partial record. ", "In a tabular dataset, this is essentially \u201cseeding\u201d a synthetic model with several columns for a record in the same format it was trained on, and then asking the model to complete the rest of the record. Today we are releasing two new Blueprints that utilize Gretel's smart-seeding and provide interfaces that automate this process for you.", "The use cases for smart-seeding we have observed are:", "Let's take a look at the new blueprints.", "This ", " automates the logic to train synthetic time-series models on trends in data versus absolute values, so you don't have to build the specific trending data from the source dataset. For example, this is particularly useful for training synthetic models on financial datasets with ", " data. All you need to do is provide your time/date column and the column(s) that represent the observations at those points in time.", "As a note, you may specify more than one \"trend\" column here. The library will automatically compute the trend lines for you and restore the trend data back to synthetic data after generating the new DataFrame.", "This ", " utilizes a new model that allows you to take partial values from your training DataFrame and use them as input for synthesis. You do this by specifying one or more seed columns. For instance, if you have a DataFrame with 5,000 rows, and choose columns A, B, C, the model will be trained with those columns as \"smart seeds.\"", "When generating data, the 5,000 3-tuple values of A, B, C will be used as input to the model, and the model will synthesize the rest of the record for you. This generates a 1:1 mapping of training data to synthetic data.", "Optionally, you may enable an auto-correlation feature for smart seeding. For example let's assume a numerical column called \"age\" and you want to use this as a smart seed. Also assume a categorical column that is derived from \"age\" called \"age_group.\" Because \"age_group\" is so tightly linked to \"age\", the model will automatically include \"age_group\" as a smart seed.", "We're thrilled to get such good feedback from our users. If these use cases are familiar to you and you are exploring synthetic data, dive into the Gretel Beta, it's free!", "As always we welcome feedback via email (", ") or you can find us on ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/data-science?094d394d_page=3", "page_title": "Data science blog posts - Gretel.ai", "headers": ["Data science", "Innovating With FastText and Table Headers", "How To Create Differentially Private Synthetic Data", "Reducing AI bias with Synthetic data", "What is Synthetic Data?", "What is Model Soup?", "Improving massively imbalanced datasets in machine learning with synthetic data", "Create high quality synthetic data in your cloud with Gretel.ai and Python", "Create a Location Generator GAN", "Creating synthetic time series data", "README.V2", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/evaluating-data-sampling-methods-with-a-synthetic-quality-score", "page_title": "Evaluating Data Sampling Methods with a Synthetic Quality Score", "headers": ["Evaluating Data Sampling Methods with a Synthetic Quality Score", "Introduction", "Calculating Gretel's ", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["While much recent research in probabilistic generative modeling has focused on visual domains such as images, a large portion of synthetic data use cases revolve around structured data such as CSVs and Databases. At Gretel, we have built generative modeling tools to learn distributions from customer data and generate realistic synthetic data from those distributions.\u00a0", "As with any ML project, a key aspect is measuring a model\u2019s performance. We have developed a tool called the ", "ynthetic ", "uality ", "core (SQS) to ensure that the tabular output of our generative models maintains important statistical information found in the ground-truth data. In this post, we'll walkthrough the process of how we calculate and interpret our synthetic quality score, and how you can use it to evaluate the effect that sampling methods have on the quality of synthetically-generated tabular data. ", "To calculate a synthetic quality score, we first measure the inter-columnar correlations of the ground-truth dataset and the synthetic dataset. We then look at the directions of greatest variance, as measured by the principal component analyses. We finally observe the discrete mass distributions of each feature in the dataset.", "Using the distances between correlation values, PCAs, and feature distributions we construct a single scalar for each of the three features above. These real scalar values are in the range [0,100] and are smooth. These two properties are ensured by fitting polynomials over a large number of real tabular datasets.\u00a0The SQS is then calculated and normalized. An SQS of 100 means the synthetic data is a perfect match and 0 means the model fit was quite poor.\u00a0", "With this measure in place we can now explore how probabilistic sampling can change the quality of synthetically generated data. We use a decoder only Transformer with 1.5 billion parameters fine tuned on the UCI Adult dataset.\u00a0", "The Transformer is trained to model a categorical distribution over a finite vocabulary. The vocabulary consists of encoding tokens of the dataset. There are many ways to sample from this distribution over the vocabulary. The first is to directly ", "from the temperature adjusted distribution which performs very well due to the alignment between training and testing. Alternatively we can restrict the categorical distribution to the ", " tokens or the ", "of top p mass or to the set of tokens with high ", " information content. These methods perform slightly worse, which is surprising given their prevalence in many NLP generative modeling tasks. Worse of all is ", "search, which likely fails due to the sharpness of the categorical distribution.", "Our method of choice is an ensemble of sampling methods which performs as well as sampling directly from the categorical distribution while reducing the variance of SQS performance.\u00a0", "With the help of our synthetic quality score, we looked at the performance of various data sampling methods and how they can impact the quality of generated data. This is an interesting avenue into analyzing data quality, and one we will be exploring more in the future. If you want to learn more about our scoring system or our synthetic data quality reports, check out ", ". Also, feel free to reach out at hi@gretel.ai or join and share your ideas in our Slack ", ". ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/data-science?094d394d_page=1", "page_title": "Data science blog posts - Gretel.ai", "headers": ["Data science", "What's new in Beta2", "Data Simulation: Tools, Benefits, and Use Cases", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Machine Learning Accuracy Using Synthetic Data", "What is Data Anonymization?", "Generate time-series data with Gretel\u2019s new DGAN model", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Gretel GPT Sentiment Swap", "Comprehensive Data Cleaning for AI and ML", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/the-evolution-of-gretels-developer-stack-for-synthetic-data", "page_title": "The Evolution of Gretel's Developer Stack for Synthetic Data", "headers": ["The Evolution of Gretel's Developer Stack for Synthetic Data", "Overview", "The Toolkit", "Mobilize All The Models", "Automate It All", "Summary", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["When we launched Gretel almost three years ago, it was to solve a simple problem: ", ". \u00a0Getting access to production (and usually sensitive) data is difficult. Products and technologies that claim to accelerate this process through governance, access control, and auditing just don't cut it. In fact, these products weren't even built with the consumer of data in mind: developers, scientists, analysts, and researchers.", "Gretel set out to create the developer stack for data and in this blog we introduce some of our newest product and technology initiatives that will ensure our platform can grow with the needs of the data consumer for years to come.", "Here's an overview of what we'll be discussing today:", "When we first launched Gretel, our early discussions with customers informed our beta releases to be oriented around abstract workload types. Generally, we found that users' needs could lump broadly into one of three core workloads:", "As we worked with more users and customers, the complexity of data and use cases grew and we learned a couple key lessons:", "With the realization that synthetic data is a non-zero-sum game, Gretel engineering set out to build the next evolution of our platform.", "The ", " for synthetic data purposes is massive! In order to take advantage of everything out there, we knew we needed a way to make any model generally available through Gretel's APIs and interfaces. So earlier this year, we internally debuted our Model Integration Framework (MIF) which enables the rapid development and deployment of arbitrary complex machine learning tasks.", "The goals of Gretel's MIF are:", "With the MIF deployed, users can easily choose the model that best fits the type of data they have, which is illustrated below.", "We have started to release blogs and documentation for these new models, and will continue to do so as new models rapidly become available. Some of these new models include:", "Now, with our ability to rapidly launch different models and jobs, we've set our sights on the next evolution: synthetic data automation.", "So what\u2019s coming next?", "Gretelers are hard at work upgrading our entire job-running architecture. We are doubling down on cloud-native core components, such as Kubernetes, and adding in the ability to construct complex pipelines to bring a level of automation to our product that has yet to be seen in the synthetic data industry.", "While our architecture will evolve to support more complex workloads, we will be adding a level of automation into the product that actually reduces complexity drastically for our users. Some of the key tenets of this next evolution are:", "The illustration below shows what these automatic pipelines will look like.", "Our goal is to make Gretel the most intuitive and easy place to generate synthetic data. To that end, our next evolution will provide a thin, opinionated set of interfaces, while hiding the complexity of privacy engineering, machine learning model selection, data partitioning, and synthetic data generation from the user. While we firmly believe in allowing access to complexity as needed, our guiding light will always be user delight and simplicity in usage.", "We are super excited to build the next revolution of the synthetic data stack for everyone. \u00a0We can't do it without the support of our customers and user base. If this is exciting to you or you have feedback, questions, or just want to chat about synthetic data, feel free to reach out at ", " or join and share your ideas in our ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-transforms-with-s3-object-lambda", "page_title": "Anonymize Data with S3 Object Lambda", "headers": ["Anonymize Data with S3 Object Lambda", "Scenario", "Solution Design", "Implementation", "Ship it!", "Summary", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Just recently, AWS announced its support for ", ". \u00a0The possibilities and use cases here are endless. \u00a0The first thing we thought of is being able to use Gretel Transforms to anonymize S3 object data real-time as it's requested, without having to create a duplicated version of the data to be stored.. \u00a0We've also been itching to test out containers for AWS Lambda so putting together an example stack to do all of these things felt like a great project to work on.", "In this blog we use the following stack to launch a real-time S3 object transformation service. \u00a0At the end of this blog, you will be able to launch a end-to-end service that does real time anonymization of data in S3 with a REST API.", "This ", " contains all the necessary ingredients to cook up some transforms with S3 Object Lambda along with the Serverless configuration to launch our entire AWS CloudFormation stack.", "If you want the TL;DR, you can run this code:", "Let's dive in...", "For this demo, let's assume you have a S3 bucket with structured data that ideally you would want to make available to other teams, organizations, or customers. \u00a0Additionally, in order to share this information, you need to redact or transform some of the content to preserve privacy.", "Secondly, your downstream consumers may not need to access every object, but instead selectively choose an object to retrieve. Let's assume objects are named by date: \"YYY-MM-DD.csv\" and a user would just want to occasionally fetch data for a specific date.", "One of the leading strategies to support this is to \"transform-on-write\", meaning when a S3 object gets created initially, some workflow executes that creates the transformed version of the object and stores that separately in S3.", "At this point, you can generate a ", " to give access to the transformed object.", "One drawback to this strategy is that for every object you store, a transformed version must be also stored. If access to the transformed objects is sparse, this could really impact your costs for both storage and compute for transforms.", "With the new S3 Object Lambda feature, users can request access to S3 objects that are transformed in real-time as they are returned to the client. Super cool!", "Stepping through the AWS blog post, we see the API call to download the object from the Object Lambda Access Point:", "After seeing this, my next thought was ", "After creating my own transform by hand, it turns out creating a pre-signed URL does indeed work. \u00a0So modifying the above code, we can get a pre-signed URL that does the same exact thing.", "This is nice, because the consumer of the data does not necessarily have to issue their own specific API call to S3 to get the data, provided the owner of the data creates easy access to a pre-signed URL.", "One easy way to provide pre-signed S3 URLs would be by providing a REST API endpoint that allowed someone to request access to a transformed version of a S3 object. For this scenario, because our objects (filenames) are structured by date, we could have a simple endpoint that allows clients to request a specific object by filename:", "The API handler code could generate a pre-signed S3 URL and return that to the client. Then performing a GET request on the returned URL would execute a call to the S3 Object Lambda Access Point, transforming and returning the S3 object at the same time.", "To test this solution out, we need to do a few things:", "To transform the data, we\u2019ll use ", ". For this particular dataset we\u2019ll run the following operations across specific field names.", "This pipeline can be constructed using Gretel Transforms:", "Once this is constructed, the pipeline object has a transform_df(df: pd.DataFrame) method that can be used to transform an entire DataFrame.", "In our application code, we read in the original S3 Object body as a string, and we need to return it as a string. So we have a utility function that converts the body to a Pandas DataFrame, runs our transform, and returns back a string:", "Next let\u2019s start to build out our stack. Feel free to follow along if desired, we can start by cloning the repo:", "The core logic is in our handler.py module. We can use this module to build a Docker container that is built on the AWS Lambda Python image. This container can be run locally to test any function we have defined in handler.py and also will be deployed to ", " so it can be used as the runtime image for our actual Lambda functions. \u00a0The Dockerfile is pretty straight forward here:", " The base Lambda images, at the time of this blog, have version 1.16.x of boto3 installed. The new API calls required for S3 Object Lambda require boto3 1.17+, so we directly install a new copy of boto3 into the Lambda task directory. In our handler.py code, we\u2019ll patch the Python path by having it look for packages in the root first, so we can make use of these new API calls.", "By default, we\u2019ll set the s3_proxy_handler as the default entry point for the Lambda container. But we can override this for other Lambda functions and use cases as needed.", "We can test our transform logic locally by using the built-in Lambda emulator the base container provides. To do this, you can build the container locally and run a helper script that is in the repo:", " The overridden entry point here, handler.record_handler lets us use a different handler just for running the _transform_data function from above.", "If things work, you should see the first records from the transformed dataset.", "Next are the specific handlers that will run on AWS Lambda. These are in the code as the following functions:", "To manage our AWS infrastructure, we\u2019ll use Serverless. ", "The entire stack is managed through the serverless.yml file. With NodeJS installed on your system, you can install Serverless via npm install serverless.", "This files has a few key sections to call out:", "If you want to deploy this yourself, you will have to modify the custom.baseName property in the serverless.yml file. All of the AWS resources stem from that name, and the bucket names cannot be shared.", "To deploy we just run a simple:", "If things go smoothly, you should see some output that ends with:", "Since we\u2019ve already deployed this in our infrastructure, you can always just test our live endpoint:", "The new S3 Object Lambda opens a tons of possibilities! We really enjoyed putting together this demo stack because it really shows how the Developer Experience for cloud engineering is improving and of course we are even more thrilled that our own Gretel Transforms can drop in place to a workflow like this!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/privacy?094d394d_page=3", "page_title": "Privacy blog posts - Gretel.ai", "headers": ["Privacy", "What We\u2019re Reading: Trends & Takeaways from the NeurIPS 2021 Conference", "Gretel Synthetics: Introducing v0.10.0", "What is Synthetic Data?", "Introducing Gretel's Privacy Filters", "What is Privacy Engineering?", "Fast data cataloging of streaming data for fun and privacy", "Automated Data Exposure Detection with Gretel Outpost", "Create high quality synthetic data in your cloud with Gretel.ai and Python", "Recognizing Data Privacy Day by Protecting Your Privacy", "Create artificial data with Gretel Synthetics and Google Colaboratory", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/john-myers?b2715911_page=2", "page_title": "Author: John Myers - Gretel.ai", "headers": ["John Myers", "Synthetic Data Configuration Templates", "Fast data cataloging of streaming data for fun and privacy", "Automated Data Exposure Detection with Gretel Outpost", "How we accidentally discovered personal data in a popular Kaggle dataset", "Introducing Gretel Blueprints", "An update to Gretel\u2019s license to support continuous community growth and innovation", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/gretel-releases-beta2", "page_title": "Gretel releases Beta 2", "headers": ["Gretel releases Beta 2", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we are thrilled to announce the release of Gretel's \"Beta2\", continuing our mission to make privacy engineering easier and making data safe.", "Almost a year ago, Gretel launched a Beta service that offered a mix of REST APIs and Python SDKs that solve a number of privacy engineering challenges. Since then, we have been collecting feedback on features, service deployment and packaging, and building the next iteration of the product.", "One of the biggest things we learned while working with our users is that privacy engineering is fragmented: it involves a combination of machine learning, natural language processing, data science, and data engineering. At Gretel, we believe the complexity of privacy engineering should not affect the availability of usable tools. This principle has been one of our guiding lights in building the next iteration of our product that we are releasing today, often referred to as \"Beta2.\"", "With this release, the Gretel service enables the following workloads without having to write a single line of code:", "In addition to the workloads we support, there are a few things you should know about Gretel's pricing and deployment model:", "If you have already signed up for Gretel and used our Beta previously, you'll automatically be vectored to Beta2 during your next sign-in. Our current Beta will be available for a short period at ", ". ", "If you haven't signed up yet, ", " and create your very own synthetic data model!", "As always, your feedback drives our product. Feel free to drop us an ", " or join us in ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/introducing-gretel-tabular-dp", "page_title": "Introducing Gretel Tabular DP: A fast, graph-based synthetic data model with strong differential privacy guarantees", "headers": ["Introducing Gretel Tabular DP: A fast, graph-based synthetic data model with strong differential privacy guarantees", "Why is the guarantee of differential privacy important for synthetic data?\u00a0", "How can we generate differentially private synthetic data?\u00a0", "How does Gretel Tabular DP work?\u00a0", "How accurate is synthetic data with differential privacy guarantees? An example with hospital data.", "Why is Tabular DP so fast?", "What kind of datasets will Gretel Tabular DP work well for?", "How do we choose differential privacy parameters?", "Conclusion", "Notes", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We are excited to announce the public preview of Gretel Tabular DP, a differentially private graph-based generative model, which creates synthetic versions of sensitive data with provable mathematical guarantees of privacy. In this blog, we\u2019ll show that when working with a dataset containing largely categorical values, Gretel Tabular DP can produce synthetic records that maintain high statistical symmetry with the original data, even with very conservative ", " (\u03b5 < 1, \u03b4 < 10", "), in just 10 minutes.", "While generative models can produce synthetic datasets that preserve the statistical qualities of the training dataset without identifying any particular record in the training dataset, most generative models to date do not offer mathematical guarantees of privacy that can be used to facilitate information sharing or publishing. Without such mathematical guarantees, each adversarial attack on these models and the synthetic data they generate needs to be thwarted reactively.", "In the last few years, there's been a steady stream of research demonstrating that generative models, ranging from ", " and ", " to ", ", have the potential to memorize and expose their inputs. In fact, this body of research has grown so much that there are ", " that can extract training data from large models.", "We can never be sure of what attacks might become feasible in the future. This is exactly the problem that ", " (DP) solves by bounding the probability that a compromising event occurs. By introducing calibrated noise into an algorithm, DP defends against all future privacy attacks with a high probability.", " DP is commonly parametrized by two privacy parameters, epsilon (\u03b5) and delta (\u03b4). Epsilon puts a ceiling on how much the probability of a particular output can increase if you were to add or remove a single training example from a dataset. Delta is a bound on the external risk that won\u2019t be restricted by epsilon, i.e. external risk that inherently exists no matter what you do with the dataset.", "Tabular synthetic data can be produced using many different algorithms. For example, at Gretel we use generative adversarial networks (GANs), language models, and statistical models.\u00a0", "One approach to making tabular synthetic data differentially private is to modify existing models to meet the standard of DP, typically through the addition of noise. However, this method isn't always capable of producing data that's both private and of high quality. Large amounts of training data are typically required to achieve reasonable DP guarantees when training deep learning models from scratch. Further, the noise added for DP during the optimization process used to train deep learning models typically causes a delay in convergence and comes at the cost of degraded quality of synthetic samples.", "Another approach is to use models that are designed with privacy at the forefront. Alternatives to deep learning approaches became popular during the ", ". They rely on measuring low dimensional distributions in a dataset combined with ", " of the dataset, all in a differentially private manner. Gretel Tabular DP takes this approach.", "Gretel Tabular DP follows the select-measure-generate paradigm ", ", which follows three steps.", "The differential privacy guarantee applies to each row of the tabular dataset. Adding or deleting one row of the dataset is guaranteed not to change the outputs by much; Tabular DP provides record level DP.\u00a0", "{{cta-signup}}", "Let\u2019s use Gretel Tabular DP with a dataset of over 100,000 diabetic patient records describing factors affecting patient readmission.", " There are 43 attributes including basic demographic information, specifics of their hospital stay, medications, and diagnoses, and an indicator for whether they were readmitted to the hospital. The dataset primarily contains categorical data types, with a handful of numeric type variables.\u00a0", "We trained three Gretel models to generate synthetic patient encounter data: ", ", ", ", and Gretel Tabular DP with two different levels of privacy.", " Here\u2019s a ", " where you can follow along.\u00a0", " ", "We are interested in comparing the tradeoff between the quality of data produced by these models and the privacy protections afforded by them. We measure synthetic data quality using Gretel\u2019s ", " (SQS), which is an estimate of how well the generated synthetic data maintains the same statistical properties as the original dataset. SQS can be viewed as a confidence score as to whether scientific conclusions drawn from the synthetic dataset would be the same if one were to have used the original dataset instead. On the other hand, we consider privacy to be a property of the data generation process. Gretel\u2019s ", " provide heuristic protection against common adversarial attacks, while differential privacy provides quantifiable protection against all current and future adversarial attacks.\u00a0", "Table 1 shows a comparison of the models. While the model with the best quality synthetic data (SQS = 93) is Gretel ACTGAN with privacy filters turned on, Gretel Tabular DP performed comparably even with very conservative privacy parameters. For a differential privacy guarantee", " of \u03b5 = 0.5, \u03b4 = 3 \u00d7 10", ", we can generate a dataset of good quality (SQS = 91). Increasing the privacy budget to\u00a0 \u03b5 = 1, \u03b4 = 3 \u00d7 10", " helps close the gap further (SQS = 92).", "SQS is a composite of ", " shown in Table 1.\u00a0 Let\u2019s dive into each component.\u00a0", "Particularly noteworthy is the low training time for Gretel Tabular DP, which utilized only a single CPU, and generated synthetic data within 10 minutes. Gretel LSTM and Gretel ACTGAN, both of which utilize GPUs, took much longer in comparison. This is because Gretel Tabular DP is calculating large tables of counts and estimating a graphical model, which only grows in complexity with the number of variables. So a dataset with over 100,000 rows and 43 columns is a cinch to process! In comparison, Gretel LSTM and Gretel ACTGAN utilize deep neural networks, which learn from each record over multiple iterations. As a result, datasets with more records take longer to train.\u00a0", "Gretel Tabular DP works well on datasets with primarily categorical variables, relatively low cardinality (<100 unique categories per variable) and under 100 variables. As with all differentially private methods, increasing the number of records in the training dataset will typically result in improved synthetic data quality. Gretel Tabular DP isn't appropriate for time series data where maintaining correlations across sequential records is important, as the underlying graphical model has an assumption of independence between records.", "Gretel Tabular DP accepts ", ".\u00a0 The strength of the privacy guarantee desired determines \u03b5. Small \u03b5 less than 1 provide the strongest guarantee. We recommend starting below 1, and increasing as necessary if higher quality synthetic data is desired. Experts recommend setting \u03b4 to a value much smaller than 1/n, where n is the number of records in the training set. By default, we initialize this value to be less than or equal to 1/n", " depending on the characteristics of your dataset. Choose a lower value for stronger privacy guarantees.\u00a0", "Gretel Tabular DP is a fast and powerful new Gretel model to generate high quality tabular synthetic data with mathematical guarantees of privacy. Try it out now by selecting the \u201cCreate provably private versions of sensitive data\u201d card in ", "!", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/arron-hunt", "page_title": "Author: Arron Hunt - Gretel.ai", "headers": ["Arron Hunt", "NEW: Integrating with Gretel SDKs just got easier!", "November 2020 - What\u2019s new in Gretel", "How to use Gretel\u2019s new entity stream", "Introducing the Gretel Bartender", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/lipika-ramaswamy", "page_title": "Author: Lipika Ramaswamy - Gretel.ai", "headers": ["Lipika Ramaswamy", "Introducing Gretel Tabular DP: A fast, graph-based synthetic data model with strong differential privacy guarantees", "Q&A Series: Solving Privacy Problems with Synthetic Data", "Common misconceptions about differential privacy", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/november-2020-whats-new-in-gretel", "page_title": "November 2020 - What\u2019s new in Gretel", "headers": ["November 2020 - What\u2019s new in Gretel", "Diving deep into Records", "Transform your data with Gretel blueprints", "Create a new project from a blueprint", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The engineering team at Gretel has been hard at work improving the Gretel Console, and our latest updates make browsing your records easier than ever. We are rolling out three big updates that make building engineering and machine learning workflows even easier with Gretel Cloud.", "We have been listening to customer feedback and capturing some great insights into how engineers and data scientists are using our data catalog. Something we have heard time and time again is that people want to see a full overview of individual records sent to Gretel. Previously we only surfaced field values and entities, but those alone don\u2019t paint a full picture of each individual record. This made it difficult to identify outliers and see relationships in your data.", "From the records tab in your project, you can now:", "This new details view has been live in the Gretel Console for a couple weeks. If you haven\u2019t seen it yet we highly recommend you check it out!", "Last week we published our blog post ", ". Blueprints make it simple to anonymize and balance datasets by providing templates focused on solving specific data transformation use cases. These blueprints make it easy to do things like ", " without needing to write any code. Our latest product updates bring these blueprints into the Console in our new transform page within your Gretel projects. ", "Blueprints make it easy to work with your existing Gretel projects, but what about starting from scratch? With our latest update you can now create a new Gretel blank project, or select from our sample datasets and included blueprints that help you get started anonymizing and balancing datasets in seconds. This update gives the option to anyone to walk through a standard Gretel workflow entirely with our sample data and transformer templates \u2014 no code or data required!", " or create a free account with the Gretel today and see the new updates. ", "\u2014", "We have many more updates on our product roadmap that will improve the experience of creating safe data for everyone. As we continue to build we want to hear what you have to say! If you have feedback, questions or just want to chat please reach out to us at ", ".", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/teaching-large-language-models-to-zip-their-lips", "page_title": "Teaching large language models to zip their lips", "headers": ["Teaching large language models to zip their lips", "Introduction", "The problem: privacy leaks in large language models", "Reinforcement Learning from Privacy Feedback (RLPF)", "Preliminary results", "What\u2019s next for RLPF?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel introduces Reinforcement Learning from Privacy Feedback (RLPF), a method that can be used to align large language models (LLMs) to improve generative quality while also making them more privacy-preserving. Language models leaking proprietary data or custom prompts is a problem that's currently plaguing many generative AI applications. We propose RLPF to mitigate some of these issues. We also suggest future directions to reduce bias, discrimination, and other harmful characteristics that might exist in today\u2019s language models.", "While language models are increasing rapidly in capabilities, they can still ", ", proprietary training data, or custom prompts when interacting with users. This undesirable behavior comes from many factors, but the primary factor is the way these language models are trained and how ", ".", "Some of this leaked sensitive information may have come from internet data used to train LLMs, some or much of which wasn\u2019t intended for training AI models when it was published. Leakage from an LLM is an unforeseen side effect, even though the data was publicly available. The problem of leakage compounds when the data was private or proprietary to begin with, and used to fine-tune a model. Leakage of data that had strict sharing requirements due to regulation, policy, or a desire for privacy can have profoundly detrimental impacts on organizations and individuals.\u00a0", "One approach for mitigating privacy leaks that has a strong theoretical grounding is to use ", " (DP). We have several ", " and are strong believers in its applicability. Applying these techniques to LLMs is an active area of research with somewhat mixed results. Training instability and potential degradation of downstream performance makes DP a tricky method to apply directly to LLMs. However, looking past these potential issues, DP training can provide a mathematical guarantee that your model preserves privacy, which is a strong plus.", "Considering other approaches to address the problem of leakage, we were inspired by ", " that uses reinforcement learning (RL) and human preference data to encourage models to be more helpful and less harmful, and asked:\u00a0", "We discovered that you can replace \u200chuman feedback data with concrete measures of privacy and use RL to improve the language model\u2019s capabilities in a multi-task fashion. We call this RLPF since it uses privacy feedback to reduce a model\u2019s tendency to leak sensitive information.", "Our initial experiments suggest that reinforcement learning can be used to make language models ", " while still maintaining generation quality. We show this on a text summarization task and present a few benchmark models as a comparison (see results below).\u00a0", "In traditional reinforcement learning from human feedback (RLHF), human preferences regarding generated text are collected and ranked. A separate model is trained to predict these preferences. This model then acts as a ", " for the original language model. The model would be rewarded when it generated things the reward model encoded that humans would prefer and was penalized when it generated improper text.", "We test our method on a text summarization task using the", ". This dataset consists of pairs of articles and highlights. Each highlight is a summary of the main points of the associated article. The goal of this task is for a language model to generate accurate highlights from a given article.", "There are a number of ways to reward a model when it generates high quality highlights. ", " is a common metric used to determine how well an article was summarized. Without diving too deep into specifics, METEOR can be thought of as a measure of overlap between the original article and the summarized article. A score of 1 implies that the article was well summarized and a score of 0 suggests a poor summarization. In practice, good numbers are usually around 0.2. It isn\u2019t a perfect measure of summarization quality, but is powerful enough to be useful.\u00a0", "We can use METEOR as a proxy for human preferences (they're highly correlated) in a reinforcement learning loop. At the end of training, our base model will have improved its ability to summarize text.\u00a0", "However, one issue here is that summaries will often contain private information from the original article. We might want a system that can generate anonymous summaries that don\u2019t reveal sensitive information. METEOR will actually encourage the opposite since it relies on overlap. Therefore, we need to introduce a second reward for privacy.\u00a0", "We found that you can use a plethora of metrics from natural language processing (NLP) as a reward. You could use the Flesch-Kincaid readability index, ", ", or in our case, a measure of privacy. In the example below, we count the number of names that appear in a summary and penalize the model (negative reward) when the summaries leak names. This is done using a named entity recognition (NER) system. When combined with METEOR to form a reward for reinforcement learning, language models can improve at summarization while simultaneously improving at preserving privacy.", "This usage of existing NLP techniques side-steps the expense of collecting human feedback and makes the intended behavior of the system more interpretable. Also, the ability to incorporate disparate feedback from various metrics allows us to potentially train models to mitigate biased, discriminatory, or other harmful language.\u00a0", "These results are preliminary and meant to illustrate a path forward, but we\u2019re excited about the promising signs of life we see with this method.", "\u200d", "In Figure 1, we see three models with their associated average NER score. A lower score means the model is less likely to generate highlights that contain a person\u2019s name. In Figure 2 we show each model with its METEOR score. A higher value correlates with higher quality summaries of the original article. In both cases, our RLPF-tuned FlanT5-XL model has improved over the baseline model in privacy preservation and summarization quality, and even slightly outperforms the widely used and powerful ChatGPT language model.", "These two figures separate out the performance of each model on each task separately. Our RLPF model was trained on the tasks jointly and was able to improve on both of them over the baseline.\u00a0", "We have shown that we can use a measure of privacy and summarization quality to improve the output of a language model using Reinforcement Learning from Privacy Feedback. This is an exciting discovery that could be applied to a variety of problems. One extension of this approach that we\u2019re particularly excited about is reducing biased language in model generations. If this is exciting to you, please do reach out! We\u2019d love to hear from you.\u00a0", "\u200d", "Thanks to Joshua Greaves from Google Brain, who helped craft the reward function used in this research. ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/conditional-data-generation-in-4-lines-of-code", "page_title": "Conditional data generation in 4 lines of code", "headers": ["Conditional data generation in 4 lines of code", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In ", ", we introduced the `", " SDK, an interface designed to be the simplest way to generate synthetic data.", "In today\u2019s post, we\u2019ll walk through conditional data generation. ", " (sometimes called seeding or prompting) is a technique where a generative model is asked to generate data according to some pre-specified conditioning, such as a topic, sentiment, or using one or more field values in a tabular dataset.\u00a0", "Conditional generation is a method that you can use to ", " for machine learning training sets, at a ", ". It can be a useful technique to address bias in data, such as in ", "to provide fair and equitable healthcare. ", "Try out the code below, or follow along step-by-step with our notebook in Colab. ", "First, start with installing the `gretel-trainer` library. Next, sign up for a free Gretel account and grab an API key from ", ".", "Below is the simplest path to conditionally generating tabular data. This code uses Gretel\u2019s APIs to train a deep learning model on the popular ", ", which includes demographic fields common to medical data.\u00a0", "For this example, we'll use one of Gretel\u2019s AI-based generative models and then sample 10 additional records that match predefined race, ethnicity, and gender column values.", "Train our model on the patient records dataset, specifying the fields we wish to use for conditional data generation.", "Sample new synthetic data from our model matching the predefined criteria.", " uses Gretel\u2019s fully managed cloud service for model training and generation. You can create ", " without needing to set up or manage infrastructure and GPUs. Try running our ", ", and for the next steps, try running on one of your own datasets or CSVs, or ", " to see advanced examples or to compare results across ", ". Have questions? Ask for help on ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/data-science?094d394d_page=4", "page_title": "Data science blog posts - Gretel.ai", "headers": ["Data science", "How we accidentally discovered personal data in a popular Kaggle dataset", "Q&A Series: Solving Privacy Problems with Synthetic Data", "Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "A guide to load (almost) anything into a DataFrame", "Exploring NLP Part 1: Why Should a Privacy Engineering Company Care About NLP?", "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/privacy?094d394d_page=1", "page_title": "Privacy blog posts - Gretel.ai", "headers": ["Privacy", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "What's new in Beta2", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "What is Data Anonymization?", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Test Data Generation: Uses, Benefits, and Tips", "We just streamlined Gretel\u2019s Python SDK ", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Gretel is live on Google Cloud Marketplace \ud83c\udf89", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/introducing-gretel-benchmark", "page_title": "Introducing Gretel Benchmark", "headers": ["Introducing Gretel Benchmark", "What is Gretel Benchmark?", "Getting started\u00a0", "Models", "Your custom models", "Gretel models", "GretelAuto", "\u200d", "\u00a0", "\u00a0", "Data", "Evaluations", "The Benchmark report", "Learn more", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we\u2019re announcing the release of Gretel Benchmark, a Python library for you to compare any model that generates synthetic data, with a set of standardized tests to evaluate those algorithms for synthetic data quality, runtime, and other machine learning use cases.\u00a0", "Want to jump right in? You can use Gretel Benchmark by installing Gretel-trainer.", "Get started with the quickstart ", " and Benchmark ", ".\u00a0", "Keep reading to learn about important features and the detailed Benchmark evaluation report. ", "It\u2019s easy to define custom models, so you can use ", " algorithm, not just Gretel models, for synthetic data generation to compare in Benchmark.\u00a0", "To provide your own model implementation, define a Python class that meets this interface:", "Learn more ", " about creating your custom model. Make sure to install any third-party libraries you use as dependencies wherever you are running Benchmark.", "We\u2019ve also made it easy for you to use Gretel models in Benchmark. Here\u2019s a nifty summary of all the available Gretel models with ", ":", "You can also easily modify the Gretel model configs with:\u00a0", "Find out more about how to use ", " in the Benchmark documentation.\u00a0", "Benchmark allows you to compare the synthetic data quality and runtime of multiple models (whether custom or Gretel models) on multiple datasets.\u00a0", "To use your own data in Benchmark, you can follow the instructions for\u00a0 ", " in the docs or check out the ", ".\u00a0", "If you need test data, we also provide a list of publicly available datasets that are popular for synthetic data use cases like those in finance, e-commerce, healthcare, and more. You can view and select datasets in Benchmark using ", ":\u00a0", "We created a set of standard tests that make up the Gretel Benchmark evaluations on algorithms for synthetic data generation. The Benchmark report shows:\u00a0", "Want to evaluate Gretel models on your industry use case? For a quick and easy look into how different Gretel models perform on popular machine learning datasets, check out our Benchmark report below. When you run Benchmark, you\u2019ll also see an evaluation report like this one.", "You can use a Benchmark report like the one shown here to evaluate which Gretel model is best for your synthetic data goals. For example, Gretel LSTM consistently generates synthetic data with a high Synthetic Data Quality Score (SQS) on multiple types of tabular or complex data. As seen in the results below, Gretel CTGAN is great for particularly long or wide datasets and generally has a faster runtime. If you\u2019re looking to quickly generate lots of data, Gretel Amplify produces results in 1/10 of the time (check out the fast train and generate times!). Gretel GPT-X generates high-quality synthetic data for natural language datasets. Depending on your specific goals with synthetic data or constraints, you may find particular Gretel models to be best suited for your use case. You can reference the Benchmark report below to guide how you evaluate Gretel models, or of course, try Benchmark yourself!\u00a0\u00a0", "You can find out more in the Benchmark documentation. Questions or comments? We\u2019re always available in our ", " - send us a note! Happy synthesizing!", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/changelog-beta-2", "page_title": "CHANGELOG: Beta2", "headers": ["CHANGELOG: Beta2", "Our Users", "Industries", "Use Cases", "Where we are going", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Here's what we learned about privacy engineering from 50+ companies and hundreds of developers\u2026", "It's needed. Everywhere.", "Gretel's mission is to enable developers, researchers, and scientists to quickly create safe versions of data that can be used for pre-production environments, machine learning workloads, and be shared across teams and organizations.", "We categorize this work as", " Current solutions have high barriers to entry, limited automation abilities, or are flat out hiding behind \"contact us\" messaging. We can do better, we have to do better. Gretel was founded by engineers and will always serve engineers. So, we set out to create products that users can get started with in minutes and scale massively.", "To validate this problem we launched a free public beta almost six months ago. \u00a0What we learned and validated was amazing. What we figured out we need to do next was even better.", "In the coming months, the Gretel team will be heads-down, integrating your feedback, reducing friction, and increasing simplicity.", "Let's dive in.", "Since launching the beta we have had over 500 users signup. These users have run hundreds of synthetic data and transform workloads using our tutorials and ", " for use cases like anonymizing and synthesizing data. \u00a0After aggregating telemetry and user feedback, we've learned quite a bit about our users, the industry, and core use cases:", "90% of our users write code, exactly who we want to build for. \u00a0Our current beta orients around using a set of Gretel SDKs and Python notebooks to execute workloads. \u00a0This allowed flexibility to quickly build blueprints, solving a wide array of problems, however it also comes with its challenges. Which we discovered when diving in with our users:", "With a ton of learning and data in our pocket, we have decided to take this feedback and release an updated version of the Gretel Beta, what we call Beta2, with these goals:", "As the components and functionality rolls out. We'll be communicating more directly about the specific feature and packaging changes that users can expect. \u00a0We anticipate having Beta2 available in the second half of this year. Like our current Beta, it will be free for all users, and your feedback is vital to building the best product we can.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/how-to-create-differentially-private-synthetic-data", "page_title": "How To Create Differentially Private Synthetic Data", "headers": ["How To Create Differentially Private Synthetic Data", "TLDR;", "The Netflix dataset", "Parameter tuning approach", "Finding the ideal optimizer", "Optimizing learning_rate", "Optimizing l2_norm_clip", "Optimizing noise_multiplier", "Putting it all together", "Final remarks", "Citations", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this post, we\u2019ll train a synthetic data model on the popular ", ", using a mathematical technique called ", " to protect the identities of anonymized users in the dataset from being discovered via known privacy attacks such as ", ". With differentially private synthetic data, our goal is to create a neural network model that can generate new data in the identical format as the source data, with increased privacy guarantees while retaining the source data\u2019s statistical insights. Today, we will walk through a generalized approach to find optimal privacy parameters to train models with using differential privacy.", "To dive a level deeper on the topic of differential privacy, see our previous post on ", ", or an excellent overview of private learning methods ", "\u2075.", "According to Wikipedia\u00b9, \u201cthe ", " consists of 100,480,507 ratings that 480,189 users gave to 17,770 movies. Each training rating is a quadruplet of the form <user, movie, grade, date-of-grade>. The user and movie fields are ", " IDs, while grades are from 1 to 5 (integral) stars\u201d.", "Netflix originally published this dataset in 2007 as part of a contest with a $1 million USD prize for the team that could beat their movie recommendation algorithm. However, researchers from the University of Austin demonstrated how an adversary with only basic knowledge of a subscriber could identify their records, and even \u201cuncovering their apparent political preferences and other potentially sensitive information", ".", "The goal of our synthetic model will be to create a new, artificial dataset of the same format as the Netflix Prize dataset and with the same statistical insights, but without memorizing or repeating any individual record. To keep training time manageable for lots of tests, we\u2019ll work with 100k rows at a time.", "In this example, we\u2019ll use the ", " library to create the synthetic data model and artificial dataset. Gretel-synthetics uses ", "an open source library created by Google researchers to train machine learning models with differential privacy guarantees. Finding optimal parameters for differential privacy is often model specific, and in the remainder of this post we\u2019ll dive in on finding the optimal privacy parameters for our use case.", "Here\u2019s the approach we\u2019ll use", "For our first experiment, we train 200 models on the the first 100,000 records of Netflix data and bake off two popular optimizers supported by TensorFlow and Keras; \u2018RMSprop\u2019 and \u2018Adam\u2019. Our goal is to find out which optimizer can best minimize the number of records containing invalid structure or data generated by the model.", "In the results above, the size of the circles correspond to the number of epochs in training [1\u2013100]. As each epoch requires an additional pass over the data, higher epoch counts during training time have the effect of reducing privacy guarantees for the model. In our experiment, we can see that the tf.keras.optimizers.RMSprop optimizer with learning rates between 0.001 and 0.01 handily outperforms tf.keras.optimizers.Adam for this task.", "In our second test, we try different learning rates. The higher the learning rate, the more impact that individual training steps have on the model. According to TensorFlow Privacy\u2019s documentation\u00b3, \u201cif the updates are noisy (such as when the additive noise is large compared to the clipping threshold), the learning rate must be kept low for the training procedure to converge\u201d. For this example, we will try a variety of learning rates to find the right balance of learning rates versus model accuracy.", "In the test above, we can see that learning rates in the range of 0.000799 (slowest, most accurate with only 1% of records failing and averaging 99 epochs) to 0.050241 (fastest, averaging 19 epochs, least accurate) generated models that passed our success threshold with less than 30% of records generated failing validation. However, past a certain point (0.00006) in our tests, the model hits our test maximum of 100 epochs and never converges, resulting in a high number of failed records.", "The l2_norm_clip describes the cumulative gradient across all neural network parameters that will be clipped so that its L2 norm is at most the value of l2_norm_clip\u2074.", "On our test dataset, the lower l2 gradient clipping settings do not strongly affect our model accuracy, we\u2019ll go with the recommended TensorFlow defaults.", "This governs the amount of noise added during training. According to the TensorFlow privacy documentation\u2074, \u201cGenerally, more noise results in better privacy and lower utility. The noise_multiplier generally has to be at least 0.3 to obtain rigorous privacy guarantees, but smaller values may still be acceptable for practical purposes\u201d. The amount of noise that can be injected into a dataset is often dependent on the quantity of training data available, and the variation within that training data. With the Netflix prize dataset, you generally want at least 100k rows of training data to be able to use a high level of noise. Try experimenting with different data sets and noise multiplier values using the notebook below.", "We just ran several thousand experiments to optimize our differential privacy settings for our synthetic data model. Woohoo! Try generating a synthetic version of the Netflix challenge dataset yourself with our notebook, or switch out the CSV and try running on your own dataset.", "Check out an example ", " to generate a synthetic dataset with differential privacy guarantees using ", ".", "At Gretel.ai we are super excited about the possibility of using synthetic data to augment training sets to create ML and AI models that generalize better against unknown data and with reduced algorithmic biases. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Follow us to keep up on the latest trends with synthetic data!", "Interested in training on your own dataset? ", " is free and open source, and you can start experimenting in seconds via ", ". If you like gretel-synthetics give us a \u2b50 on ", "!", "\u00b9 Wikipedia, ", "\u00b2 Arvind Narayanan, Vitaly Shmatikov, ", "\u00b3 TensorFlow Privacy ", "\u2074 TensorFlow Privacy ", "\u2075 Laurens van der Maaten, Awni Hannun, ", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/exploring-nlp-part-2-a-new-way-to-measure-the-quality-of-synthetic-text", "page_title": "Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "headers": ["Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "TL;DR", "Overview of Text Metrics", "Fr\u00e9chet BERT Distance and Some Enhancement", "Experiment: Methodology", "Experiment: Results", "Discussion & Next Steps", "Here\u2019s what we were reading", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Multiple users have asked us about the quality of our synthetic text. With tasks ranging from intent classification to better speech recognition, users want to know whether they can bolster their datasets using our synthetic text. But how do we measure the quality of synthetic text? In this blog post, we give an overview of the types of text metrics that exist today. Then we present an innovation built on top of the work of ", " that better aligns with our use case (and outperforms its predecessor on several datasets).", "If you want to skip right to the code and replicate our experiments, feel free to visit our ", "! We will be sharing our experiments from blog posts here from now on.", "A range of techniques are used to evaluate synthetic text. Here we discuss reference-based metrics that seek to characterize how closely synthetic text matches some reference text. Popular metrics in this space include ", ", which are usually paired together (BLEU corresponds to precision and ROUGE corresponds to recall). Both metrics compute a score by determining how much the synthetic text and real text overlap. But what if the synthetic text and real text had minimal overlap? Consider the following example. The real text reads:", "and the synthetic text reads:", "The synthetic text would be given a score of 0, even though the synthetic text has similar semantic meaning to the original text. This mismatch led us to investigate other text metrics using embeddings, since Gretel wants to generate synthetic text that has the same semantic and grammatical structure of the original text, but also produces new text that has a low chance of exposing sensitive information found in the original text.", "Other reference based metrics using ", " have been coming out in recent years. Two of the metrics we explored are ", " and ", ". BERTScore finds embeddings for each word in the real and synthetic texts, performs a pairwise cosine similarity metric on each real, synthetic word pair, and then performs a greedy matching on a matrix of the cosine similarity scores to generate a composite score. BLEURT on the other hand pre-trains and fine-tunes a BERT based regression model to provide scores. It is pre-trained twice on perturbed, synthetic text examples from Wikipedia utilizing several more traditional scores (such as ROUGE and BLEU). The first pre-training regimen is using the language modeling task usually done for BERT, while the second pre-training regimen focuses on natural language generation. After pre-training, BLEURT is then fine-tuned on human ratings of synthetic text. Using their ", ", you can fine-tune on any human ratings data you want.", "Both of these scores have one major issue for Gretel: they require matched real/synthetic pairs of text. Our data generation process trains on the entire real data set, and we can produce as many synthetic examples as we want. Thus, there isn\u2019t a correspondence between the synthetic and real examples. So what we need is a text metric that:", "Our data generation process does not produce 1-1 pairings of real/synthetic examples. But it does generate a 1-1 pairing of real/synthetic datasets. Thus, we are able to compute metrics concerning the ", " of the data, which we already do in the ", ". This leads to our favorite text metric thus far.", "\u200d", " (FBD), inspired by the ", " that is used frequently to evaluate synthetic image generation in GANs, is a BERT based metric that evaluates the distance between distributions created by the real text embeddings and the synthetic text embeddings. Formally, assume that both distributions are multivariate gaussians. Let \u03bc\u1d63 and \u03a3\u1d63 be the real mean and covariance matrices respectively and \u03bc\u209b and \u03a3\u209b be the synthetic mean and covariance matrices respectively. Then the Fr\u00e9chet BERT Distance between the real embeddings distribution and synthetic embeddings distribution is:", "FBD fits our use case. It does not require a 1-1 real/synthetic pairing and it does not require a rigid overlap between the pair of texts. Moreover, it is quick to compute once you generate the embeddings. That speed is due to the assumption that the embeddings themselves generate a multivariate gaussian distribution, a simplifying assumption that seems to not deprecate results.", "Users have tried to generate multiple kinds of text data. We\u2019ve seen comments, reviews, doctor\u2019s notes, etc. flow through our system. Some have even approached us with intent classification problems that require generation of synthetic conversational text. What all these have in common is that they operate on the ", " level. Word embeddings have been a fantastic breakthrough, but we want to compare the full text per record and not just the words contained in each text example.", "\u200d", " (also known as ", ") is an architecture that takes the mean of the BERT embeddings for a document and then performs a fine-tuning task using cosine similarity to make sure the sentence embeddings are semantically meaningful. SBert has a ", " of different models to choose from that vary significantly in terms of efficacy and task alignment. You can find all of their models via ", ".", "We also want to make our metrics as easy to understand as possible. Getting a score comparing two multivariate gaussian distributions without any visuals is challenging for some to grasp. It may leave users wondering, \u201cwhat is this metric really saying?\u201d So we pair our version of FBD with a new metric we call Fr\u00e9chet Cosine Similarity Distance (FCSD).", "After we take sentence embeddings of both the real text and the synthetic text using SBert, we then take the mean of the real text embeddings. This provides an anchor point for us to generate cosine similarity scores. We take the cosine similarity between the real text and the real mean to form one distribution and do the same with the synthetic text and the real mean.", "This gives us a distribution we can visually present to our users after we generate the FBD score. And we can also calculate the Fr\u00e9chet Distance between the two cosine similarity distributions, thus giving us another score to present to our users.", "Even if we have a new idea for a text metric, we want to make sure that it, at the very least, is as good or improves upon its predecessor. Thus, we replicated the experiments done in ", ", the paper that first introduced the FBD metric, and tested the results against our two new metrics: FBD and FCSD utilizing SBert embeddings.", "For the original FBD metric, Xiang et. al. utilized either ", " or ", " to perform the embeddings. According to them, RoBERTa outperformed BERT as an embedding for FBD. Thus, we decided to use the RoBERTa implementation for our experiments. For the SBert embeddings, there were ", ". We chose 12 of the top performing models as of August 2021. The researchers who maintain SBert are frequently generating new pre-trained models, so the top models available today may not be the same models shown at the time of the experiment. The models we tested are:", "The datasets used were the same ones described in the paper. They are dialogue query and response datasets with human annotations. The goal of this evaluation is to see which metric most correlates with the human annotations of these datasets. If a metric is highly correlative, that means it is performing well. Just like the paper, we calculate both the Pearson and Spearman correlation coefficients for all datasets, comparing the automated metric against human annotations. We omit one dataset used in the paper, Persona(M), due to not understanding which ratings were used to perform the correlations found in Xiang et. al. The names of the datasets we did use are below:", "Just to note, all experiments were originally run on a CPU (and only took a couple of hours to run!). If you would like to use a GPU, you may need to tweak the code a bit.", "From our results (which you can find in this ", "), we can see that FBD using Sbert\u2019s `nli-roberta-base-v2` model outperformed the original FBD metric on 6 different measures. On the measures it did not outperform the original FBD metric, it still did better than the other SBert models on 3 of those measures and performed competitively with the original FBD metric. Moreover, although FCSD did not perform as strongly across the board, the metric performed most consistently when using `nli-roberta-base-v2` based embeddings. Note that this table only showcases the 4 best performing models. You can run the rest of the experiments using our ", ".", "This blog post presented innovations on top of the Fr\u00e9chet BERT Distance developed by ", " It fits extremely well with our use case and outperforms its predecessor in our analysis. We documented our thought processes behind the development of this metric, the methodology we replicated from the original FBD paper, and the results from our top performing models.", "We would be remiss if we did not share some limitations and challenges. ", " showcases several metrics that categorically outperforms FBD. However, many of those metrics require fine-tuning, meaning they would need more time to calculate than FBD. Thus, we chose to build on FBD because it provides our users with a quicker, more seamless experience. Moreover, although we replicated the analysis done by Xiang et. al., we note that their use case is different from ours. They use a combination of query and response, but in our product we would only use the \u201cresponse\u201d portion. Thus, we will also be doing an internal evaluation of the new metric.", "In our next blog post in the series, ", ", we will utilize our new metric to evaluate some language models. Specifically, we will be benchmarking our own synthetic data generation model against some transformer based models and possibly some other architectures. Hope you come back and enjoy!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/what-is-model-soup", "page_title": "What is Model Soup?", "headers": ["What is Model Soup?", "Introduction", "Model Soup", "Experiments and Implementation", "Results", "Learnings", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Machine Learning has emerged as an extremely powerful toolkit for many industries and problems. By taking data and learning rules, ML can drastically increase the number of things we can do with computers. Part of the alchemy of training these machine learning systems is called \u201chyperparameter tuning\u201d. There are a number of these hyperparameters that an engineer or scientist may choose to work with, each with different downstream effects. They often work together somewhat mysteriously to increase or decrease performance.\u00a0", "For many years, ML practitioners would train multiple models each with a different hyperparameter configuration and then choose the best performing of all those models. This \u201cfinal\u201d model supposedly represented the best performance that could be reached given the data and model choices.\u00a0", "However, it was often the case that even better performance could be obtained by using multiple models in parallel and averaging the output of each model. This \u201censembling\u201d technique led to improved performance at the cost of running many models at once instead of a single model.", "Model soup is a recent discovery", "\u00a0 in which an ensemble of models is formed by averaging the weights of the models instead of combining each of their individual outputs. The result is a single model, which is the average of many models with many different hyperparameter configurations.\u00a0", "A key finding from this paper is that uniformly or greedily averaging models performs better than any individual model on the ImageNet benchmark dataset. Additionally, they find it performs better than a naive ensemble might.\u00a0", "In the image above, we see the green diamond models are all trained with different hyperparameters while the blue circle and purple star are the results of averaging the weights of the models together.", "At Gretel, we have users with very small datasets (< 100 rows) trying to increase the size of their data by generating more data via our synthetic data tools. Often when there are only a few examples, our models don\u2019t perform up to standard. Therefore, we briefly explored model soups as a way to improve model performance on smaller datasets.\u00a0", "To determine if this is a direction we want to continue investing in, we explore the minimal viable experiment.", "Here the two models were chosen by performance which was measured by per character accuracy:", "SQS score model 1: 44", "SQS score model 2: 42", "SQS score soup model: ", "Here, instead, the two models were chosen by performance as measured by loss - but still evaluated on SQS:", "SQS score model 1: 40", "SQS score model 2: 36", "SQS score soup model: ", "To ablate, we tried souping two untrained models and souping the same models, but the SQS score did not improve in either case.", "An interesting insight, however, is that we found poor performance when averaging more than 5 models or on certain types models (those with low accuracy to begin with).", "We should explore if there is a pattern in this performance to determine when soups are effective or not. Also it would be interesting to determine if there is a threshold for the number of models after which performance begins to improve again.\u00a0", "We replicated a small set of the results around model soups, showing that performance improved as measured by synthetic data quality which may lead to improved performance when customers have small amounts of data. We identified a few areas for more exploration! If this is exciting to you, feel free to reach out at ", " or join and share your ideas in our Slack ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/laszlo-bock", "page_title": "Author: Laszlo Bock - Gretel.ai", "headers": ["Laszlo Bock", "Recognizing Data Privacy Day by Protecting Your Privacy", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/protecting-our-digital-breadcrumbs", "page_title": "Recognizing Data Privacy Day by Protecting Your Privacy", "headers": ["Recognizing Data Privacy Day by Protecting Your Privacy", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["I was born in Communist Romania in the 1970s. The ", " was the branch of the government charged with knowing everything that Romania\u2019s citizens were doing. At its peak, roughly one in 44 people living in Romania either worked for the ", "or were informers. This was a place where employers spied on workers, teachers spied on children, and children were asked to spy on their parents. To this day, my father prefers to have sensitive conversations sitting in his car, because that was the one place in Romania that you could be sure had no one listening to your conversation.", "My family was fortunate enough to escape Romania, sneaking across the border and eventually arriving in an Austrian refugee camp, before immigrating to the United States as political refugees. The specter of a government that could listen to and observe every private act loomed large in our conversations growing up.", "But it\u2019s not only the government that listens.\u00a0", "Today each of us leave \u201cdigital breadcrumbs\u201d everywhere we go. Cookies and trackers are old news, as is the way retailers know -- and sometimes reveal -- more about us than our families: it was almost a decade ago when ", " before she\u2019d had a chance to tell her family.", "More recently there\u2019s been coverage of \u201c", " ", "\u201d, companies that buy and sell vast databases of your personal information. Fast Company reported that, \u201c[b]y buying or licensing data or scraping public records, third-party data companies can assemble thousands of attributes each for billions of people. [I]f you use a smartphone or a credit card, it\u2019s not difficult for a company to determine if you\u2019ve just gone through a break-up, if you\u2019re pregnant or trying to lose weight, whether you\u2019re an extrovert, what medicine you take, where you\u2019ve been, and even how you ", " on your smartphone.\u201d Companies like Facebook are enormous purchasers of this data.", "On Data Privacy Day, there\u2019s another category of privacy that\u2019s worth talking about, another place where our digital breadcrumbs are hoovered up and -- with the best of intentions -- leave us vulnerable and exposed.", "Over the last two decades, the bottleneck within companies to do massive data analytics was access to computational power. It could take users days, if not weeks, to get access to enough \u201ccompute.\u201d Now, fast access to data is no longer the bottleneck. Unicorn technology start-ups, as well as enormous corporations, have access to tremendous computational resources. What they often don\u2019t have, are the privacy safeguards required to protect your data and mine.\u00a0", "I\u2019ve spent the last 15 years in Silicon Valley, including a decade on the management team at Google, and have heard stories time and again about companies where \u201cevery engineer has access to all of our code base\u201d and \u201cwe innovate because our machine learning models and AI have access to enormous volumes of data.\u201d What\u2019s left out is how that data is labelled, anonymized, and made accessible.", "Some good actors have elaborate privacy policies in place. For example, at one company, when a user stops using the product, not only is their data removed but all the company\u2019s relevant algorithms are re-run without that data to ensure an engineer couldn\u2019t \u201cback into\u201d that individual\u2019s private information.\u00a0", "More typical, at tech companies and Fortune 500 companies, is that raw user data is accessible to too many people, with too few controls and often little or inadequate anonymization.", "The genesis of Gretel.ai goes back to the ", "and those digital breadcrumbs. What if we could make customer data available to analysts, immediately, but in a way that made it impossible to identify any single person? What if we could ensure that personal information was protected, benefiting not just the individual but also giving developers faster, worry-free access to data?", "This is what ", " does. And on Data Privacy Day, it\u2019s thrilling to know that privacy and innovation don\u2019t have to be opposite ends of the spectrum. If you want to try it out, walk through our use cases, ", " and ", ", or reach out to us at hi@gretel.ai. ", " ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/privacy?094d394d_page=4", "page_title": "Privacy blog posts - Gretel.ai", "headers": ["Privacy", "Common misconceptions about differential privacy", "Practical Privacy with Synthetic Data", "Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/john-myers?b2715911_page=1", "page_title": "Author: John Myers - Gretel.ai", "headers": ["John Myers", "CHANGELOG: Beta2", "What is Data Anonymization?", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Augmenting ML Datasets with Gretel and Vertex AI", "The Evolution of Gretel's Developer Stack for Synthetic Data", "Gretel releases Beta 2", "Contact Tracing: Deep Dive & Simulation", "Anonymize Data with S3 Object Lambda", "Gretel Smart-Seeding is auto-complete for your data", "Gretel Synthetics: Introducing v0.10.0", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/gretels-source-available-license", "page_title": "An update to Gretel\u2019s license to support continuous community growth and innovation", "headers": ["An update to Gretel\u2019s license to support continuous community growth and innovation", "Overview", "Why are we doing this?", "What do you need to do?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel is changing the license for some of its open source components that also are a part of Gretel's SaaS Platform. We are migrating from Apache 2.0 to the ", " (SAL). This new license allows you to freely install and incorporate Gretel software packages into your own business, research, academic, or personal solutions. You may also modify and create derivatives, but may not redistribute to third parties or build a software-as-a-service offering with the packages.", "Additionally, we will be adding a Contributor License Agreement (CLA) that must be accepted if contributions are made to the affected software packages. Similar to licensing agreements that other products like TensorFlow and Pytorch use, Gretel\u2019s CLA asserts that contributors have the right to make a contribution, that contributors may maintain any copyright or patent protections for their contributions, and that Gretel and any other licensees and users of Gretel products may utilize the contributed code.", "The two software packages impacted by this change are:", "For more information, please see our ", ".", "We believe this is a necessary step to ensure that Gretel can continue to invest heavily in cutting-edge synthetic data capabilities that are freely distributed for the community, while sustaining a healthy business that funds these endeavors for the foreseeable future.\u00a0", "This approach is common amongst other data-centric platforms such as Confluent, Elastic, and Redis. These businesses have launched their own \"as-a-Service\" offerings and have also been simultaneously working with (or battling with) major cloud providers that have varying approaches regarding\u00a0 the use of open-source software. Fortunately, these data-centric companies have been very transparent in their dealings with these cloud providers, and as Gretel begins onboarding our own large strategic partners and enterprise customers, we are taking a similar stance of proactivity instead of reactivity.", "Of course, we could have chosen to never release any of our core synthetic capabilities as open source from the very beginning and taken a more proprietary stance. But what good would that be? My co-founders and I have already played this game, coming from the cybersecurity world where every algorithm is \"next generation\" and \"bleeding edge,\" and the products operate in a black box \u2013 obfuscating basic functionality from the users they are intended to help. No thanks.", "The synthetic data market is emerging, and we have an opportunity to keep it honest, collaborative, and innovative while building our business at the same time. Publishing our core capabilities for the community to use encourages transparency, fosters open research and collaboration, and, most importantly, earns trust with our users, whom we view more as peer developers than customers.", "If you're already a Gretel SaaS Platform user, we love you! Keep on keeping on. If you are using Gretel Synthetics or Gretel Trainer, we also love you! For the most part, the users of these components should continue to use them exactly how they have been \u2013 incorporating Gretel packages into their solutions, enabling the use of synthetic data for their research or business.", "If you are considering using Gretel, in any capacity, to offer synthetic data capabilities to your own customers or users, does this mean you are out of luck? No! Just contact us and we'd love to work or partner with you on your use case. This licensing update just means you can't outright take our software packages and build your own business without speaking with us first.", "Questions or concerns? We'd love to chat. Reach out at ", " or come join us in our ", " (and soon Discord too!).", "Now get synthesizing!", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/a-guide-to-load-almost-anything-into-a-dataframe", "page_title": "A guide to load (almost) anything into a DataFrame", "headers": ["A guide to load (almost) anything into a DataFrame", "File formats", "JSON files", "Newline delimited JSON file (aka JSONL). ", "JSON array nested in a top-level JSON object. ", "Excel spreadsheets", "Parquet files", "SQL data", "Data storage", "Reading from S3", "Conclusion", "Resources", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Hey all! Long time Java developer here, with 7 years of building large scale cloud systems at AWS before transitioning to Gretel.ai, where we are building privacy engineering technology with a combination of Python, Go, and JavaScript.", "Coming from the Java world to Python, I wanted to share some of my experiences that are hopefully useful to other developers getting started with Python and Data Science. In this post, we\u2019ll dive into the wonderful world of Pandas, which I have found to be very flexible and easy to get started with. It comes with easy to use APIs and tons of integrations with different tools, so you can start processing and visualizing your data in a matter of minutes!", "Here at Gretel, we design our SDKs to work seamlessly with Pandas, so I get to work with DataFrames on a daily basis. This post will go into some of my favorite ways of loading data into a DataFrame.", "I recommend you check out the corresponding Python notebook, which contains all of the examples and more! Feel free to modify them and see what happens. A runnable version of the notebook is available for free on ", " ", "Pandas comes with 18 readers for different sources of data. They include readers for CSV, JSON, Parquet files and ones that support reading from SQL databases or even HTML documents.", "In fact, you can get the total number of readers from the Pandas documentation, by using one of their readers! Here\u2019s how:", "In this example, Pandas will read data from the HTML table on that web page and load it into a DataFrame. All in a single method call! The match argument can be set to any text that appears in the table we are interested in (without match Pandas will load all of the tables on that web page).", "And while it\u2019s not that common to load data from a web page, it\u2019s quite a neat thing to have in your toolbox. ", "One time, I was curious about how my home country did during the 2016 Olympics (fun fact - 2020 Olympics hasn\u2019t happened yet, it\u2019s scheduled to start on July 23rd, 2021, because of you know what\u2026).", "This ", " has all the information, but the tabular format is not very easy to digest. It does show the final ranking of each country, but it\u2019s not easy to see how big of a difference there is. That\u2019s where data visualization comes in handy. As the saying goes \u201c", "\u201d and in this case it\u2019s quite accurate.", "With Pandas, I could have that data visualized without breaking a sweat! It took only few minutes and I was able to satisfy my curiosity (you can run that example yourself in a notebook here ", ".", "The next question we could explore is this: what\u2019s the country that has the most medals won per person? Easy! I just load a DataFrame from another ", ", divide number of medals by the population and voila! I\u2019m going to leave it as an exercise to the reader, feel free to open the Python notebook references above, give it a try and let us know how it went!", "All of the examples below will use that 2016 Olympics results dataset stored in different formats. Feel free to check our notebook, where you can run all of these ", " from your browser.", "There are 2 different ways of representing tabular data in the JSON format.", "In this format, each line in this file is a valid JSON object.", "Reading data from this format is very straightforward with Pandas:", "`lines=True` tells Pandas to treat each line as a separate JSON object", "In this case, the whole file is one big JSON object and the tabular data is nested in one of the fields.", "Reading from this format is slightly more complicated, as Pandas doesn\u2019t let you to provide the path where your tabular data resides within the JSON object. Because of that, you will need to load the JSON file manually and then pass it to Pandas.", "For this format, you can specify the name of the sheet (tabs at the bottom) you want to read (with the sheet_name argument). If you skip it, Pandas will load the first one.", "Pandas will automatically detect compression algorithms and decompress the data before reading it (in our example we are using snappy compression).", "Pandas can read data directly from a SQL database. It relies on SQLAlchemy to work with different database types, which means that it supports all major relational database systems.", "To keep this example simple, we are using a SQLite database to read the data from.", "Now let\u2019s look into different places where data can be stored. All of the examples above assumed that the file was stored on the local file system. That\u2019s not always the case and Pandas comes with support for multiple popular file storage solutions such as: S3, Google Cloud, SFTP or GitHub to name just a few. To read from remote storage solutions, Pandas uses the FSSPEC library.", "I will use S3 to present a few options of reading remote files. Working with other sources will be very similar, so I\u2019m not going to list them all to keep things short. If you do need to read data from other sources, I recommend you read Pandas' and FSSPEC\u2019s documentation. \u00a0You will find links to these at the bottom of this post.", "S3 is a very popular object storage service and as expected, Pandas can read data directly from it. In order to use S3, you need to install optional dependencies (more on that in Pandas", ").", "When reading from S3, here are some things that are supported:", "What\u2019s great about all this is that all you need to do is change the URL and pass few arguments and you get the behavior you need. No need to write extra code for any of this.", "Pandas comes with a huge variety of formats that it supports out of the box and it\u2019s useful to know what it can do to save time and let you jump into exploring your data. In this post, I presented some of the ways of reading data into a DataFrame I found useful. ", "Do you have any tricks you found useful when working with different data sources and formats in pandas? If so, please share them with us on Twitter ", ".", "The end.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/how-to-use-gretels-new-entity-stream", "page_title": "How to use Gretel\u2019s new entity stream", "headers": ["How to use Gretel\u2019s new entity stream", "How does it work?", "Browsing the entity stream from gretel-python-client", "Browsing the entity stream with the Gretel REST API", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel Cloud automatically labels data with advanced NER (named entity recognition) and NLP (natural language processing) and allows you to explore your labeled data in our cloud console. These entity labels offer insights into what kind of data you are sending and help to inform and educate the way that you transform data. ", "Starting today, you can now browse a stream of records associated with an entity in a single view. We spoke with data scientists and engineers who told us that they loved seeing the high level overview of tagged entities however they wanted a deeper understanding of which records they matched with so they could look for trends and identify sensitive fields. The new entity stream empowers you to do just that.", "Navigating to an entity stream is incredibly simple; all you need to do is select an entity tag from a record, field or the entities tab within a project. Selecting the entity will open the new entity stream and allow you to dive deeper into that entity and learn more about how it is contained in your data project. Once you are there Gretel highlights relevant information about that entity including the total number of fields that were identified with that tag, the approximate cardinality, and when it was last seen. ", "Our ", " SDK allows you to explore an entity stream in a given project by passing the entity you would like to view.", "You can learn more about querying records in our ", ".", "It is also possible to query the entity stream using our REST API from our `GET /streams/recrods/:project_name` endpoint and passing the `?entity_stream` query parameter.", "Learn more about querying records via our REST API in our ", ".", "\u200d", " or create a free account with the Gretel today and see the new entity stream. ", "\u2014", "We have many more updates on our product roadmap that will improve the experience of creating safe data for everyone. As we continue to build we want to hear what you have to say! If you have feedback, questions or just want to chat please reach out to us at ", ".", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-synthetics-faqs", "page_title": "Gretel Synthetics Frequently Asked Questions (FAQs)", "headers": ["Gretel Synthetics Frequently Asked Questions (FAQs)", "What is synthetic data?", "How do I get started?", "How does Gretel synthetics create artificial data?", "Is there an architecture diagram? ", "What kinds of data can I send to Gretel-synthetics?", "What are the outputs from Gretel-synthetics?", "Can I run gretel-synthetics on premises?", "What are gretel-synthetics premium features?", "Do I still need to de-identify sensitive data when using gretel-synthetics?", "What kinds of privacy protections can Gretel Synthetics help with?", "How is Gretel-synthetics differential privacy different from traditional implementations?", "How is synthetic data different from the original source data it was trained on?", "How many lines of input data do I need to train a synthetic model?", "How many columns of training data can I have?", "How many epochs should I train my model with?", "Does training a synthetic model require a GPU?", "What is differential privacy?", "How does Gretel-synthetics leverage differential privacy?", "How does Gretel-synthetics implement differential privacy?", "If my model trained in batches using differential privacy, what is my final epsilon (privacy guarantee)?", "What are good epsilon (\u03b5) and delta (\u03b4) values in differential privacy?", "How is Stochastic Gradient Descent (SGD) modified to be differentially private?", "What does RDP order mean?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["While synthetic data can mean many things, Gretel\u2019s definition of synthetic data is artificial data created from training a machine learning model to re-create a source dataset. The synthetic model outputs artificial data that contains many of the insights and correlations of the original data without memorizing any records from the original data.", "Sign up for Gretel.ai\u2019s ", " and walk through one of our pre-packaged examples or run on your own dataset. Or, check out and contribute to our open source ", " library.", "\u200d", "Gretel-synthetics utilizes a sequence-to-sequence architecture to train on a text dataset and learn to predict the next characters in the sequence. Gretel-synthetics uses a Long-Short Term Memory (LSTM) artificial neural network to learn and create new synthetic examples from any kind of text or structured data.", "This flow diagram walks through the process of loading source data, training a generative machine learning model, and using the model to create a synthetic dataset using ", ".", "You can create synthetic data from any kind of text data, whether structured or unstructured. The simpler the data format the better, we recommend CSV, Pandas DataFrames, or unstructured text delimited by line. Gretel-synthetics thrives on patterns. For example, it does particularly well on machine learning datasets as it\u2019s able to replicate both within field distributions as well as cross field correlations. Conversely, if all your dataset field values are highly unique, the model will struggle to find exploitable patterns.", "The outputs include a synthetic data model that can be used to generate synthetic data records, an initial set of synthetic data records, and an available premium reporting module that assesses the quality and correlations between the synthetic data and the original dataset.", "Some customers need to train synthetic data models within a compliance-approved environment. Gretel-synthetics is open source and can be deployed as a Python package or Docker container. However, some premium features require an API key and connection to Gretel\u2019s APIs.", "With Gretel API access, developers can access premium features such as automated data format validation to ensure that all data generated by the neural network matches the structure and distributions of the source data, field correlation and clustering to improve insights captured by the synthetic data model, and a reporting module that generates an HTML report assessing the quality and correlations between the synthetic data and the original dataset.", "Gretel synthetics will memorize and replay repeated data that it observes in the training set. When anonymizing sensitive identifying data types within a dataset, such as fields containing social security or credit card numbers, it is best to anonymize data as best as possible before training the synthetic model. Gretel helps you automate this process with our data labeling APIs and transformation SDKs.", "Gretel-synthetics is designed to help developers and data scientists create safe, artificial datasets with many of the same insights as the original dataset, but with greater guarantees around protecting personal data or secrets in the source data. Gretel\u2019s implementation of differential privacy helps guarantee that individual secrets or small groups of secrets, such as a credit card number inside structured and unstructured data fields will not be memorized or repeated in the synthetic dataset. Gretel\u2019s synthetic data library also helps to defend against re-identification and joinability attacks, where traditionally anonymized data can be joined with another dataset, even ones that have not been created yet, to re-identify users.", "Several companies including Uber have built libraries that help apply differential privacy to SQL queries, by injecting noise into the results of a query aggregation. This approach is powerful but requires you to know what questions that you want to ask of data, without the ability to see or inspect sensitive data directly. Gretel-synthetics is a sequence-to-sequence model that trains on a source dataset, injects noise during the learning process rather than at query time, and creates a secondary dataset that can be shared and viewed directly by data scientists or developers or queried using any database technology.", "Gretel-synthetics creates artificial data by training a machine learning model to create data just like the input data it was trained on. For example, if you train on a CSV dataset the output will be CSV.", "We generally recommend 5000+ examples. If you have a smaller dataset that is only a few hundred lines, try training for 100+ training epochs to learn the structure. If you are working with a highly dimensional dataset (e.g. 15+ columns) we recommend 15000+ examples.", "Gretel works best on learning models for densely-packed datasets with 50 or less columns of data. However, there is no limit on the columns (dimensionality) of your dataset. Gretel-synthetics clusters highly correlated columns into batches to be trained independently and then joins the results. We have tested for datasets up to 1,500 columns of sparse data.", "The right number of epochs depends on the inherent perplexity (or complexity) of your dataset. A good rule of thumb is to start with a value that is 3 times the number of columns in your data. If you find that the model is still improving after all epochs complete, try again with a higher value. If you find that the model stopped improving way before the final epoch, try again with a lower value as you may be overtraining. If you have only a small number of records in your dataset or are having a large number of records fail validation, you may need to increase the number of epochs significantly to help the neural network learn the structure of the data.", "A GPU is highly recommended by not required to get started with gretel-synthetics. For a rule of thumb, you can expect training the synthetic model to be 10x faster or more on GPU. However, inference is not nearly as parallelizable as training and we recommend CPUs and Gretel\u2019s available multi-processing support for text generation.", "Differential privacy is a framework for measuring the privacy guarantees provided by an algorithm. Through the lens of differential privacy, we can design machine learning algorithms that responsibly train models on private data. Learning with differential privacy provides provable guarantees of privacy, mitigating the risk of exposing sensitive training data in the synthetic data model or its output. Intuitively, a model trained with differential privacy should not be affected by any single training example, or small set of training examples in its data set.", "Gretel-synthetics uses differential privacy to defend against memorization while learning on a private dataset. Imprecisely speaking, the output of a synthetic model trained over a dataset D that contained one occurrence of a secret training record X versus another synthetic model D1 that did not contain X should be nearly identical. Thus, we have mathematical assurances that our model did not memorize the secret.", "The TensorFlow team has taken on a lot of the heavy lifting of implementing and releasing TensorFlow Privacy, an extension to TensorFlow that allows differentially private learning. Gretel synthetics implements TensorFlow\u2019s open source code for DP-SGD in the Tensorflow-Privacy library with slight modifications to adapt it to recurrent neural networks, and improved the baseline performance by replacing the plain SGD optimizer with an RMSProp optimizer as it often gives higher accuracy than vanilla SGD (Tijmen Tieleman and Geoffrey Hinton, COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012).", "When differential privacy models are trained on disjoint subsets of a private database, their combined use has an epsilon value equal to the maximum across all models.", "Epsilon is your quantitative privacy guarantee. It gives a ceiling on how much the probability of a particular output can increase if you were to add or remove a single training example. Stringent privacy needs usually require an epsilon value of less than one. However, in some domains it\u2019s not uncommon to see epsilons of up to 10 being used. Delta is a bound on the external risk that won\u2019t be restricted by epsilon. External risk is that which inherently exists no matter what you do with your dataset. By default Gretel will initialize this value to be 1/#training samples. Delta values such as e-05 or less should not compromise utility.", "SGD works by stochastically sampling a set of training examples, computing the loss (difference between predicted value and real value), computing the gradient of the loss, then after modifying these gradients by the learning rate, uses the resulting values to update the model parameters. The iteration of this process is what\u2019s meant by descent. There are few main changes to this process to make it differentially private. First the gradients are clipped such that no single training example can unduly impact the model, and second, random noise is added to the clipped gradients to make it impossible to deduce which examples were included in the training. Additionally, instead of clipping gradients at a batch level, they are clipped in micro-batches. The more clipping, noise adding and micro-batching you have, the more differentially private your model will be. As there is often a trade-off between privacy and utility, Gretel-synthetics exposes each of these elements as modifiable parameters in the training.", "Gretel uses a variation of differential privacy referred to as R\u00e9nyi differential privacy (RDP). RDP makes use of the R\u00e9nyi divergence to measure the distance between distributions. R\u00e9nyi divergence is a generalization of Kullback-Leibler divergence that works in the notion of a parameter referred to as it\u2019s \u201corder\u201d. In RDP, the idea is to search for the order that optimizes epsilon (e.g. your privacy guarantee). When running Gretel-synthetics, the \u201coptimal RDP order\u201d will be printed along with epsilon and delta once training completes.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/automatically-reducing-ai-bias-with-synthetic-data", "page_title": "Automatically Reducing AI Bias With Synthetic Data", "headers": ["Automatically Reducing AI Bias With Synthetic Data", "What is Fair AI?", "Our imbalanced dataset", "Boosting the Census with synthetic data", "Final remarks", "Citations", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this blog we\u2019ll step you through example code for automatically creating a fair, balanced, privacy preserving version of the ", " using Gretel SDKs.\u00a0", "Let\u2019s start with a definition of what fair, balanced datasets are. \u201cIn fair AI, the objective is to provide systems that both quantify bias and mitigate discrimination against subgroups.\u201d1 In this example, we will boost under-represented `race`, `gender`, and `income_bracket` classes from the 1994 US Census dataset. The Python notebook below (we call it a ", ") can be used to support balancing datasets for AI fairness as well as generally any ", ".\u00a0", " (AI) is now ubiquitous in our culture. It is often responsible for critical decisions such as who to hire and at what salary, who to give a loan or insurance policy to, and who is at risk for cancer or heart disease. Fair AI strives to eliminate discrimination against demographic groups. The code example below can help you achieve fair AI by boosting minority classes' representation in your data with synthetic data. With this approach, only a single pass is required to correct representational bias across multiple fields in your dataset (such as gender and ethnicity, for example). Please note that correlations and distributions in non-bias fields (such as age) will transfer from your training data to your synthetic data.", "The blueprint lets you choose from two different modes for balancing your data. The first (mode=\"full\") is the scenario where you'd like to generate a complete synthetic dataset with representation bias removed. The second (mode=\"additive\"), is the scenario where you only want to generate synthetic samples such that when added to the original set will remove bias.", "To get started, log into the ", " with a GitHub or Google account, and create a new project.\u00a0", "As always, you can check out the complete code for this Blueprint on our ", ".", "The blueprint starts out with the necessary code to install Gretel's SDK. The data used for this blueprint was created from the 1994 US Census Database. It consists of anonymous information such as occupation, age, native country, race, capital gain, capital loss, education, work class and more. Each row is labelled as either having a salary greater than \">50K\" or \"<=50K\".\u00a0 As shown below, we\u2019ll request using 14,000 records from the dataset, the mode of \u201cfull\u201d and for 1000 synthetic records to be generated.", "The blueprint then enables you to view existing categorical field distributions in the dataset:", "We\u2019ll now choose to remove the demographic bias of \u201crace\u201d, \u201cgender\u201d and \u201cincome_bracket\u201d.\u00a0", "After choosing your fields, the blueprint takes you through the typical cells to train your synthetic data model.\u00a0 Upon synthetic generation, we seed the model with the classes that need boosting to generate additional records. Upon completion, as you can see below, your new data will be perfectly balanced.", "The blueprint concludes by enabling you to either save your new synthetic data to a CSV file or back onto a Gretel Project.\u00a0 You\u2019re also able to generate a full synthetic ", ".", "Discriminaton in data from which important conclusions are made can have disastrous consequences.\u00a0 At Gretel, creating bias-free synthetic data is a core use case. Stay tuned for continued new features in this area! We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Follow us to keep up on the latest trends with synthetic data!", "1 Ahsen, Mehmet Eren, Mehmet Ulvi Saygi Ayvaci, and Srinivasan Raghunathan. \"When algorithmic predictions use human-generated data: A bias-aware classification algorithm for breast cancer diagnosis.\" ", " 30.1 (2019): 97-116.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/ai?094d394d_page=3", "page_title": "AI blog posts - Gretel.ai", "headers": ["AI", "Diffusion models for document synthesis", "The Evolution of Gretel's Developer Stack for Synthetic Data", "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "Creating Synthetic Time Series Data for Global Financial Institutions \u2013 a POC Deep Dive", "Deep dive on generating synthetic data for Healthcare", "Progress and Innovation - Women in AI", "Evaluating Data Sampling Methods with a Synthetic Quality Score", "Automatically Reducing AI Bias With Synthetic Data", "Gretel Synthetics: Introducing v0.10.0", "Reducing AI bias with Synthetic data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/creating-synthetic-time-series-data", "page_title": "Creating synthetic time series data", "headers": ["Creating synthetic time series data", "TL;DR", "Background", "Getting started", "The training dataset", "Extract trend data", "Train a model", "Generate synthetic datasets", "Visualize results", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this post, we will create synthetic versions of a time-series dataset using ", "\u2019s ", ", visualize and analyze the results, and discuss several use cases for synthetic time series data.", "One of the biggest bottlenecks that we face as data scientists is thechallenge of not having enough data. Companies like Amazon have turned to synthetic data to generate the large amounts of training data required to support ", ", researchers are experimenting with GANs to generate ", ", and companies like Unity3D are applying their expertise in gaming environments and 3D assets to help you train models that can ", ".", "Whether we need historical data to test our algorithms, or need more data to build ML models that generalize better, the use of synthetic data to augment limited datasets is growing incredibly rapidly. One are that has been particularly challenging for creating realistic synthetic data has been time-series data, due to the need to maintain strict ordering, and the at times complex relationships between time and measurement values.", "Click here to ", ", or here to ", ". You will want a box with a GPU for model training, else grab a cup of \u2615. To get started running the example code, grab an API key (it\u2019s free) from the ", ".", "For our training dataset we will create a time series model at hourly intervals. For the Y axis, we will want a complex, but understandable measurement function to both test our synthetic model and enable quick visual analysis. For this, we can apply a sine wave function that repeats on a yearly basis.", "Great, now we have our time series DataFrame. Hint: to experiment with different time intervals (e.g. hourly, per minute), try changing the freq in Pandas\u2019 date_range() function above. Now, let\u2019s visualize the time series!", "Next, we create the training set for our synthetic model. We could train directly on the time-series data, but we would just be training the model to memorize the measurements at a given time. ", "The model then re-creates the time series trend data, which can be restored to the original format using a cumulative sum.", "Next, we will use gretel_synthetics and tensorflow to train a synthetic model on our test dataset. To ensure that our generated synthetic datasets match the time windows of our source dataset, we specify the date field as a seed value that can be provided at generation time.", "We can now use the model to generate any number of synthetic datasets. To match the time range of the original dataset, we\u2019ll use Gretel\u2019s seed_fieldsfunction, which allows you to pass in data to use as a prefix for each generated row. The code below creates 5 new datasets, and restores the cumulative sum from the trend data to match the original dataset.", "Finally, let\u2019s visualize the 5 synthetic datasets we created vs. the original training set to get a feel for how the synthetic time series data compares against the source data. Fortunately, Pandas DataFrame integration with Plot.ly\u2019s graphing libraries makes this easy!", "As we can see, the synthetic model did a good job of learning the sin function and temporal relationships in our source dataset. To take this example a step further, try adding in new variables and sine waves to synthesize, or try changing the neural network configuration parameters such as ", " to experiment with different results. We\u2019d love to hear what you try, and stay tuned for more complex examples in a future post!", "At ", " we are super excited about the possibility of using synthetic data to create ML and AI models that are both ethically fair and generalize better against unknown data. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Follow us to keep up on the latest trends with synthetic data!", "Interested in training on your own dataset? ", " is free and open source, and you can start experimenting in seconds via ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/maarten-van-segbroek", "page_title": "Author: Maarten Van Segbroeck - Gretel.ai", "headers": ["Maarten Van Segbroeck", "Synthesizing dialogs for better conversational AI", "Synthetic Data, Real Privacy: Automating Secure Workflows with Gretel and Amazon SageMaker", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/introducing-the-gretel-bartender", "page_title": "Introducing the Gretel Bartender", "headers": ["Introducing the Gretel Bartender", "The epiphany", "The Plan", "The Challenge", "The Results", "Closing", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This past year the Gretel team has been hard at work building tools to help engineers and data scientists build with safe data. This task is no small feat, and we quickly discovered that with all the hard work we put in, we longingly looked forward to our Friday afternoon happy hours. Through a series of meetings, team off-sites, R&D, and a lot of self-discovery, we finally built something that would allow us to work hard ", " play hard. Introducing the ", ".", "Hindsight is 20/20, and what seems so obvious now was entirely lost to us at the beginning. One night while sipping on cosmopolitans at the Casa Jondal in Ibiza (OK it was a Chili\u2019s in Salt Lake City), our CEO \u2013 Alex Watson \u2013 looked disappointingly at his empty martini glass and blazoned, \u201cThat was delicious, but \u2019m getting tired of drinking the same thng all th\u2019time. When\u2019s the last time anyone came up with a newdrink?\u201d. That question (though very slurred) sparked an idea in our minds. What if we could use our own tools to design an AI that would save us from the endeavor of having to drink the same boring drinks over and over again? We have the technology, so what\u2019s stopping us?", "With our minds buzzing with anticipation and excitement, we set off to start building a solution to our very real, very important problem. We wanted to train a machine learning model on a large database of cocktail recipes and teach it to craft brand new ones. These new, synthetically generated recipes would be a perfect conglomeration of all the best drinks in the world, from the humble old-fashioned to the bold and daring Double Chocolate Martini. It would select all the best ingredients, ratios, and mixing techniques and unite them into wonderfully new and exuberant creations that most mixologists could only dream of creating. ", "Soon after its inception, problems began to emerge. The AI wasn\u2019t performing very well during its training. As any boozy connoisseur will know, you can\u2019t just mix a cocktail and send it off. You have to try it for yourself. After each training epoch, our model became tipsier and tipsier. As each iteration processed, it became more imprecise. It started to produce garbled nonsense. It wouldn\u2019t stop talking about politics and putting in Jukebox requests for \u201cPiano Man\u201d at all the local bars. It wanted to plan a group trip with all of us that \u201cWe\u2019ll totally go on this time, and I\u2019m not just saying that because I\u2019ve had a few. I mean it\u201d. Eventually, we had to cut it off. We told it that there is important work to be done and that it was time to buck up; we\u2019re so close. After our heart-to-heart chat, we finally began to see some real results, and boy, were they wonderful. ", "The results it served were better than we ever could have imagined. The model could produce an infinite number of brand-new cocktail recipes. All we had to do was suggest a base liquor, and it would fill in the rest. Feeling something with Vodka? How about a rootbeer vodka with grenadine in a highball glass with a salted rim (yummy)? Or an Irish coffee with champagne, red bull, and a lemon wedge (that will wake you up)? We were in wonder at the possibilities and knew immediately we had to make this available to everyone. So, in the spirit of giving and a desire to make the world a better place, we are opening up our AI to the world. ", "To generate a new cocktail recipe, all you have to do is tweet at our Twitter bot with your favorite liquor, and we will send you back a delicious recipe.", "Salud!", " 1 oz of Sambuca, 1 oz of Vodka, 1 oz of Amaretto, 1 oz of Lime juice. The Blue Margarita.", "We hope you enjoy your April 1st with a synthetic cocktail or two. Here\u2019s the ", " we used to generate our model. If you actually end up making one of these drinks, tweet it at us, and we\u2019ll ship you something fun! ", "Cheers!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/announcing-general-availability", "page_title": "Data Is More Valuable When It Can Be Shared", "headers": ["Data Is More Valuable When It Can Be Shared", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Since founding Gretel two years ago, we have often been asked ", "\u00a0Well, a Gretel customer put it best with a simple statement: ", "\u00a0", "One of the biggest bottlenecks to innovation that developers and data scientists face today is getting access to data, or creating the data that you need to test an idea or build a new feature.\u00a0From our experience working at AWS, Google, OpenAI and with other leaders in the data industry, we know first hand that enabling developers to safely learn and experiment with data is the key to rapid innovation. ", "As developers and data scientists, we don\u2019t always need - or even want - access to sensitive customer information. That\u2019s where synthetic data comes in.\u00a0", "What if you could get access to an artificial and privacy-preserving version of data in minutes, with ", " without having to wait weeks for manual anonymization and approvals? What if you could create additional data from just a few examples without having to wait weeks or months for annotated data to be created? ", "Gretel can do that for you. At Gretel, we are making this possible through our state of the art ", " tools.\u00a0", "Today, we are thrilled to announce that Gretel is at General Availability (GA). For us, GA reflects the same commitment to an ", ", continuing to learn and iterate in public, and create simple and ", "\u2013 now with the scale and efficiency to run as part of your data science pipelines and workflows. It\u2019s been amazing to see what is possible with Gretel. We are building Gretel\u2019s APIs to help developers reduce bias in ML datasets, creating ", " for the metaverse, and recreating the results of life sciences experiments using ", ".\u00a0", "Going forward, we will continue to post research, source code, and examples about enabling data sharing and access at scale. ", " and give Gretel a try, you can run Gretel\u2019s APIs to ", ", ", ", or ", " data \u2013 ", ".\u00a0", "For anyone interested in deploying our toolkit in production AWS environments, we\u2019re previewing our new S3 Connector. If you\u2019re interested or want to start a conversation, drop us an email at ", ". You can also visit our ", " and start a conversation with our developers and data scientists.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/synthesizing-dialogs", "page_title": "Synthesizing dialogs for better conversational AI", "headers": ["Synthesizing dialogs for better conversational AI", "Datasets \ud83d\uddc3\ufe0f", "Gretel-GPT training \ud83d\ude80", "Getting Started: Installation", "Paraphrasing preprocessing step", "Daily-dialog", "Commonsense-dialogues\u00a0", "Counsel-chat", "Configure the Gretel-GPT model", "Training the Gretel-GPT model", "Generate records", "Results \ud83d\udd0e", "daily_dialog\u00a0", "commonsense-dialogues", "counsel-chat", "Conclusion \u261d\ufe0f", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In the world of natural language processing (NLP) and conversational AI, having access to high-quality training data is essential. This data fuels the training of language models, chatbots, and virtual assistants, enabling them to understand and generate human-like text. However, obtaining real conversational data with the required diversity and privacy considerations can be challenging.", "This blogpost shows how Gretel-GPT can be leveraged to create realistic synthetic dialogs, turn-takings and QA datasets, enhanced with metadata tags or labels. These synthetic conversations serve a multitude of purposes, from training language models to enhancing conversational agents and providing valuable insights into transcribed conversations\u2014all while preserving privacy.", "In this blogpost, we demonstrate the capabilities of Gretel-GPT on the following conversational datasets (note: all datasets are available on HuggingFace):", ": this dataset contains high-quality multi-turn dialogs. The dialogues in the dataset reflect our daily communication and cover various topics about our daily life. The dataset was manually labeled with communication intention and emotion information. The intention and emotion labels are defined as follows:", ": this dataset is a crowdsourced dataset of ~11K dialogues grounded in social contexts involving utilization of commonsense. The social contexts used were sourced from the train split of the SocialIQA dataset, a multiple-choice question-answering based social commonsense reasoning benchmark.\u00a0", "\u200d", ": this dataset is a scrape of Counselchat.com's forum, an expert community and platform that helps counselors build their reputation and make meaningful contact with potential clients. Therapists respond to questions posed by clients, and users can like responses that they find most helpful. The dataset contains expert responses by licensed clinicians to questions posed by individuals.", "Follow along with our complete notebook in ", " or ", ".", "First, install dependencies.", "Gretel-GPT is a powerful tool for generating synthetic conversations. It allows creating realistic dialogues, maintaining the structure and order within a paragraph while generating text that sounds convincingly human. However, to work effectively with Gretel-GPT, a preprocessing step is necessary, especially when your source data is in structured formats like JSON objects or tabular data.", "The below examples show how we converted each record of the dataset into a single paragraph for Gretel-GPT model training.", "Now, we will configure the Gretel-GPT model. In this case, we will use ", " and specify the amount of epochs such that we train for approximately 60 min.", "We kick Gretel-GPT model fine-tuning. Note that for each dataset, we limited the training data to 1000 randomly sampled records. ", "Generate as many records as you want. Note you can run Gretel\u2019s record handler in parallel to speed up model generation time.", "Here are some examples of synthetically generated paragraphs for each dataset. As can be seen in the examples, the Gretel-GPT model preserves the paragraph structure allowing to convert them back into the original datasource format in an automated fashion.", "In the rapidly evolving landscape of NLP and LLMs, the significance of clean high-quality training data is imperative, yet challenging. Gretel-GPT offers a solution by generating realistic synthetic conversations and datasets enriched with metadata, all while preserving privacy. These resources serve diverse purposes, from fine-tuning language models to improving conversational agents and analyzing transcribed dialogues. Here, we showed how you can leverage Gretel-GPT on structured conversational data with the goal to generate convincingly human text.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/comprehensive-data-cleaning-for-ai-and-ml", "page_title": "Comprehensive Data Cleaning for AI and ML", "headers": ["Comprehensive Data Cleaning for AI and ML", "Our dataset", "Standardize empty values", "Remove duplicate records", "Drop columns with mostly missing data", "Impute missing values", "Remove redundant fields", "Capping high float precision", "Remove constant fields", "Address field level outliers", "Handle record level outliers", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Data cleaning is a fundamental first step in any ML or AI project. You\u2019ll often hear the phrase \u201cgarbage in, garbage out\u201d. What that means is, if the data you enter is really messy, that increases the chances of your project results being messy. In this blog, we step you through some approaches to data cleaning with a particular eye towards AI/ML. Please feel free to follow along with our Jupyter notebook ", ".", "The issues we address are as follows. Note, the order is such that earlier steps can benefit the analysis of future steps.", "The dataset we use is the popular ", " that has been modified to include an example of all the different types of problems we may encounter. We can read in the dataset as follows:", "Missing values can be represented by a variety of fields such as \"?\", \"Missing\", \u201cN/A,\u201d or \u201cNot applicable\u201d. Standardize all these values to be `np.nan`. This will simplify the imputation of missing values. You can see in our dataset that \u201c?\u201d is often used to represent missing data.", "First, show the first occurrence of all duplicated rows. For brevity's sake, we\u2019ll just show the first five.", "Your job now is to study the duplicated records and decide if they are in error or not. \u200cIf they are in error, you can remove them with the following command:", "In our experience, you're better off dropping columns with 60% or more of missing data. When so much data is missing, the signal from the field can be messy. Below, we\u2019ll show the percent missing for each field.", "Note, only `dummy_col1` has more than 60% data missing, so now we drop that column.", "There are many simple ways to fill in missing data. \u200cFor example, if the field is numeric, you could fill in missing values with the mean or the median. \u200cIf the field is categorical, you could fill missing values with the most frequent categorical value. By far the most effective way to fill in missing data is with a machine learning imputation approach. KNN imputation is a machine learning-based imputation algorithm that's seen success but requires tuning of the parameter ", " and additionally, is vulnerable to many of KNN\u2019s weaknesses, like being sensitive to outliers and noise.\u00a0", "In this blog, we\u2019ll be using ", ", which is another machine learning-based data imputation algorithm. It uses a random forest trained on the observed values of a data matrix to predict the missing values. There are many benefits of using MissForest. For one, it can be applied to mixed data types, numerical and categorical.", "We\u2019ll start by listing the missing percent of records per field:", "Now let\u2019s create several functions for massaging the data into the numeric format MissForest needs:", "Let\u2019s train and run the MissForest algorithm:", "Now we translate the categorical values (which are currently numeric) back into strings:", "Finally, let\u2019s take a look at our new data. \u200cNote that there are no longer any missing values.", "When two fields are completely correlated, they're redundant and one of them could be removed. First, here's some code to compute \u200cfield-by-field correlations.", "And here\u2019s the output from running the code:", "As you can see, `education` and `education.num` are 100% correlated. The best thing to do is to remove one and then proceed with your AI/ML pipeline. If after the AI/ML you really need both fields in the data (as can be the case with creating synthetic data), then before you start, create a map between the two fields.\u00a0At the end, use the map to get the removed field back in. Here\u2019s example code removing education:", "Sometimes float values can have excessively long precision, particularly when they are the result of some mathematical computation. Some models, like Gretel\u2019s LSTM, are sensitive to values with excessive precision and do much better if the floats are rounded.\u00a0Sometimes (as is the case when creating synthetic data), how much precision you need is dependent on how much precision you need in the output.\u00a0If you can reduce the precision, then rounding to two spots to the right of the decimal generally works well.\u00a0In our dataset, there is only one column with a long floating-point precision, and that column is `dummy_col3`.", "Now we drop some of the precision by rounding every float to have two spots to the right of the decimal point, and look at the data again.", "When a field only has one consistent value, it has very low predictive power and you're better off removing that field. This next code snippet will detect any constant fields:", "The above returns the following line:", " `Column dummy_col2 is a constant column of 0.0`", "Now you can remove that column with the below code snippet:", "Records that contain outlier values can be very disruptive to an AI/ML pipeline. \u200cIt's important to look at each outlier found and determine if the reason it\u2019s an outlier is that there\u2019s an error in the data. Your choice is then to either fix the error or just remove the whole record. \u200cRemember: ", ". If there are errors in the training data, there will be errors in the AI/ML analysis. In this section we are looking for outliers at the field level, and in the next section we\u2019ll look at outliers at the record level.", "We\u2019ll start by displaying the numeric columns:", "Now we\u2019ll pick the column `age` and graph a box plot. The nice thing about a box plot is you can see how outliers fit within the context of properties like minimum, first quartile, median, third quartile, and maximum.", "You can see that ages above 80 are outliers. In this case, we decide that these are valid records (not errors) and leave the records in. If you had wanted to remove the outliers, the following code would do the trick:", "A record level outlier is one where all the fields in the record together form an outlier. A nice routine for measuring record level outliers is IsolationForest. This algorithm is an unsupervised decision-tree-based one originally developed for outlier detection in tabular data. You can read more about it ", ".", "We start by defining a set of functions we\u2019ll be using:", "Now we normalize the data, train, and score the outlier model:", "Our scoring mechanism has been normalized such that an outlier score of 0.7 or above means the record is for sure an outlier. \u200cA score of 0.6 to 0.7 means the record is possibly an \u201coutlier\u201d and should be manually looked at.", "Let\u2019s start by looking at the for sure outliers:", "There is only one record, a dummy error record we entered where the person is a 6-year-old elementary school student earning more than 50K. The following code shows how to remove this outlier:", "Data cleaning can often make or break the success of an AI/ML project. ", " encourages all its users to fully clean their data before initiating a Gretel project. Most of Gretel\u2019s models to create synthetic data use AI/ML. Data cleaning will increase the odds of creating high quality synthetic data, which then increases the odds of a downstream successful AI/ML project. Please feel free to contact us with any questions or comments: hi@gretel.ai.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/piotr-mlocek", "page_title": "Author: Piotr Mlocek - Gretel.ai", "headers": ["Piotr Mlocek", "A guide to load (almost) anything into a DataFrame", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/what-is-data-anonymization", "page_title": "What is Data Anonymization?", "headers": ["What is Data Anonymization?", "\u00a0", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Data anonymization is the process of mitigating direct and indirect privacy risks within data, such that there is a measurable way to ensure records cannot be attributed to a specific individual or entity. With an estimated ", " of data being generated every day and an increasing reliance on data to power new applications, machine learning models and AI technologies, the importance of implementing effective anonymization techniques and removing any bottlenecks is crucial to accelerating future developments and innovations.\u00a0", "This post is a general introduction to anonymization, and the tools and techniques for providing sufficient privacy protections, so that personally identifiable information (PII) is safe from exposure and exploitation.\u00a0", "There\u2019s no silver bullet for creating safe, shareable versions of real-world data. Data anonymization should be considered a continuous process; one that can require rapid iteration of applying various privacy engineering techniques and then measuring those privacy outcomes until a desired end state is reached.", "When implementing data anonymization methods there are three main tenets to consider:", "With this framework in mind, we can construct a high-level process for data anonymization:", "In the following sections, we'll dive deeper into our core tenets of the data anonymization process, and then walkthrough how you might apply them to a notional dataset. Finally, we\u2019ll explore various anonymization techniques you can use.", "Beyond best practices and organizational policies, there are several country-level and international regulations for data uses and consumer privacy protections that define de-identification measures we must take. In the U.S., some prominent privacy regulations include the Health Insurance Portability and Accountability Act (HIPAA), which governs how personal health information (PHI) is handled; and the California Consumer Privacy Act (CCPA), and has inspired a growing number of state-level privacy laws. The CCPA is loosely modeled off of the European Union\u2019s General Data Protection Regulation (GDPR), which we\u2019ll focus on for this post.\u00a0", "The GDPR doesn\u2019t require a specific anonymization technique be used, but it does set a high standard for what \u2018anonymized data\u2019\u00a0 means; namely, it\u2019s ", " In other words, anonymized data must be irreversibly altered, and neither directly or indirectly traceable back to an individual.\u00a0", "And even though the GDPR applies only to EU citizens and companies, it places strict controls on data transfers to non-EU countries and international organizations, so it has an outsized impact on anyone working with or moving data across and beyond its borders.\u00a0", "Let\u2019s now look at some practical first steps to applying these standards in our de-identification processes:", "Once these policy- or regulation-based de-identification techniques are applied, additional steps should be taken to detect and mitigate other", "A direct privacy risk would be any piece of information that can specifically identify an individual or entity. This might include phone numbers, email addresses or physical addresses.", "First, you must identify these risks. This can be done in one of two ways:", "Once additional direct risks (identifiers) are located, depending on the desired use cases, a variety of privacy techniques may then be applied. These options will be discussed later on. First, let\u2019s consider the third and final tenet of our anonymization process.", "The evaluation step of the anonymization process is critical. It includes analyzing the additional combinatorial elements in the data for any possible re-identification risks, also known as ", ". These risks are best illustrated by ", "\u2019s study, where they used a notional combination of medical records and voter records to show how an indirect risk could be exploited to re-identify an individual, even if direct identifiers are removed from the dataset.\u00a0", "There are a ", " that can be used to anonymize data. The list below provides a significant amount of flexibility depending on the end use case for the anonymized data. These techniques can be applied iteratively, as part of the risk mitigation and evaluation cycles in the anonymization process.", "An obvious step to anonymization is simply to remove a field in the dataset. But while this is effective, it often changes the utility of the data, depending on your use case. Usually, you would only be removing fields that pose ", " and that contain personal identifiers. The end use case should dictate when to do this, but here's a general rule of thumb that might help:", "Data reduction is similar, but instead of removing the entire field you reduce the granularity of values. This might be useful for partial aggregate analysis on the data, where some position of an identifier is needed. Specific techniques for reduction are:", "Replacing personal identifiers with fake but realistic versions is by far one of the better approaches to preserving the semantics of data in its entirety. This method is also very important for data that powers development and testing environments, as it can help ensure ", " between real-world data and anonymized data. For example, if we chose to mask the phone number 867-5309 to 867-XXXX, this could potentially be a change to the schema, because only numbers and dashes would be the expected and valid contents. With masking, we've introduced a masking letter, \"X\", which may break data validation for the downstream use case. Replacing the phone number with an entirely fake value would preserve the requirement of only allowing numbers and dashes.", "There are two distinct methodologies to entity replacement to consider: deterministic and\u00a0probabilistic.\u00a0", "Numerical data and dates and/or timestamps are often leading sources of ", ", where these values can still uniquely be mapped back to a specific individual or entity. If your downstream use case involves aggregate or trend analysis, removing or reducing the granularity of this data might be sub-optimal.\u00a0", "Shifting techniques randomly increase or decrease numerical and date values, such that joinability with other datasets becomes more difficult for re-identification attacks, while preserving the ability to do analysis on the data.", "Shifting will generally include defining some minimum and maximum value to change a specific value in the dataset. For example, if we were looking at purchasing data for customers, consider the following:", "We could apply a numerical shift with the following parameters:,", "For each record in the data, a random shift amount will be applied to the \"Amount\" field. With the configuration above, a random value will be generated between -3 and +3, and then added to the existing value. This value may have up to 2 decimal places, which are also randomized. So, for our first record, our random value might be 2.73 which will mutate 5.34 to 8.07.", "However, this becomes problematic if you want to do trend analysis on the data. The two records for \"User A\" could receive different random shift amounts, which would create significant drift from the trend in their spending.", "An advanced extension for shifting techniques is to provide a field reference to shift values consistently for that field's value. In the example above if we selected the \"User\" field as our reference field, then we would ", " for that specific entity and re-use it everywhere. Since the first shift amount for user A was 2.73, we'll cache that value and re-apply it for the third record, mutating 7.80 to 10.53. Now, the difference between these two amounts will be preserved.", "User B would receive a totally different shift value for the mutations applied to those fields.", "The same shifting concept can be applied to timestamps, dates, and any other value that is rooted in a numerical base.", "Certain elements of data might be so complex that a discrete transformation of the data is impossible or very inefficient. Here, we can apply a technique known as ", ". Synthetic data is artificially annotated information that is ", " by computer algorithms or simulations, which can often be used in place of real-world data. For data anonymization purposes, synthetic data can be useful for creating realistic data for complex fields such as natural language or free text.\u00a0 These fields are more complex than categorical and numerical data because you cannot simply transform the values to something else. Instead, machine learning models can be used to learn how to synthesize new free text that has similar semantic meaning as the real-world data.", "Consider a dataset that is fairly structured, but that has a \"notes\" or \"comments\" field. The techniques mentioned previously may be too simplistic and ineffective for processing this text data. Synthetic data models can, however, learn the semantics of the text, and re-create versions of the text that maintain the same underlying meaning, but that are not identical to the real-world data or identifiable back to a specific person.", "In some scenarios, certain elements of data must be restorable to its original form. For these use cases, encryption and tokenization enable data restoration.", "Encryption utilizes one or more ", " to mutate data to a form that cannot be reversed without accessing the same keys and decrypting the data. Oftentimes, encryption methods may transform the data to be less useful or render it invalid. For example, encrypting a phone number might introduce characters that change the schema of the data.", "One variant of encryption, that can preserve data schemas, is ", ". This technique will still encrypt the data, but the result (known as the ", ") will be of the same format. So, encrypting something like a phone number will yield a new value that also contains digits. This can help ensure that the encrypted data can be inserted back into a database with the same schema and the real-world data.", " is a process that replaces a sensitive piece of information with a value that has no real exploitable meaning. The token is generated separately and the mapping of the real-world data to the token is tracked in a separate, secure system. If a token needs to be restored to its original value, a lookup can be done against the tokenization system and the original data can be extracted back out.", "Now that we\u2019ve looked at anonymization techniques, let\u2019s consider some opportunities that truly anonymized, synthetic data offers:\u00a0", "When it comes to data anonymization, remember, it's a process!\u00a0 At Gretel, we focus heavily on developer workflows, so that data anonymization processes can be defined as code and injected directly into pipelines that developers are already building and maintaining.", "By exposing data anonymization techniques through easy-to-use APIs, developers can iterate on the anonymization process, and generate data that can be shared more broadly.\u00a0", "We're constantly innovating on data anonymization tools, synthetic data generation, and data evaluators. ", " and give Gretel a try, you can get started right away and ", ", ", ", and ", " your data with no code required!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/q-a-series-solving-privacy-problems-with-synthetic-data", "page_title": "Q&A Series: Solving Privacy Problems with Synthetic Data", "headers": ["Q&A Series: Solving Privacy Problems with Synthetic Data", "\u200d", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Below are answers to several questions I received during ", " at The Rise of Privacy Tech\u2019s Data Privacy Week 2022 conference in January. For anyone interested in discussing these or other issues related to synthetic data, drop me a note in ", "!\u00a0", "\u200d", "I would draw a distinction between the process of creating synthetic data and the mechanisms of differential privacy.\u00a0", "Creating synthetic data from a sensitive dataset involves using algorithms that can learn the distribution of the dataset and generate records from that distribution. For example, if a\u00a0 dataset shows that there is a strong correlation between smoking and developing lung cancer, then the algorithm should learn that, and the resultant synthetic data should also have that property.", "In contrast, differentially private mechanisms, such as those that add noise to an aggregate, aim to introduce uncertainty about the presence of any particular individual in that dataset. This is done by calibrating that noise to the sensitivity of the aggregate (i.e. how much will the function change if one record is changed?).\u00a0", "The goal of differentially private synthetic data is to create a dataset that looks and feels like the original dataset without compromising the privacy of any individual data record in the original dataset. Often, we accomplish this by using differentially private methods to train the algorithms that generate synthetic data. There is some calibrated noise addition in, for example, the optimization process while accumulating gradients.", "\u200d", "As long as your approach to generating synthetic data with ML includes the use of certain privacy-protection mechanisms, there\u2019s a very low probability that you will be subject to successful attacks. ", "The best known method at this point to minimize the success of attacks on ML models is differential privacy. Differential privacy has the property of being future proof \u2013 the guarantees do not depend on any assumptions about the attacker, their methods, motives or computational resources. Commonly used methods to produce synthetic data, such as generative models and marginal-based methods, can be made differentially private.\u00a0", "\u200d", "If the goal of an analysis is to study outliers or small, rare populations, it is likely not useful to study such data using privacy-preserving approaches like differential privacy that aim to prevent the undue impact of any single individual on the analysis. For such analyses, controlling who has access to the data and mandating training on acceptable use are more suitable. I touch on this topic in ", ".\u00a0", "But let\u2019s say we are talking about synthetic data produced without formal privacy guarantees. Most synthetic data algorithms typically require a large number of examples to learn from (large depends on the algorithm - e.g. supply under 1000 examples and training a GAN will likely be a struggle). So the quality of synthetic samples for rare population subsets may be very poor, and there may be no option but to use the actual, real-world data for the analysis.\u00a0", "\u200d", "I\u2019ll take a step back from this question to emphasize that it is important to define intended use when creating synthetic data. It may change the algorithm used for data generation. It may change the validators used to ensure the synthetic records follow proper logic. It may change the way the original data is pre-processed. So intended use can have a tremendous impact on the process.", "The quality of the synthetic data should be measured depending on intended use. What are the various methods of measuring synthetic data quality? Well, there is ", " out there on this topic and users typically choose the methods that are appropriate for the use case.\u00a0", "At Gretel, we recognize the importance of reporting data quality. That\u2019s why for every synthetic model that\u2019s built using our platform, we provide a ", ", which can be used by the custodian of the original data to determine whether the synthetic datasets produced by the model will broadly maintain statistical fidelity.", "However, it is possible that the person using the synthetic data needs further information about the original, sensitive dataset and does not have direct access to it. In such cases, a differentially private querying system can be useful in understanding relevant information about the original dataset in a privacy preserving manner, following which a comparison can be made to the synthetic dataset.", "Thanks again to everyone who participated in the discussion. If you\u2019d like to discuss my answers further or have other questions, send me a note in our ", " or email us at ", " to continue the conversation!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/ai?094d394d_page=1", "page_title": "AI blog posts - Gretel.ai", "headers": ["AI", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "Filling in sparse tables with Gretel\u2019s Tabular LLM", "Nail Synthetic Data Generation Every Time with Gretel Tuner ", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Gretel Demo Day: Exploring the Future of Synthetic Data", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Generate time-series data with Gretel\u2019s new DGAN model", "Test Data Generation: Uses, Benefits, and Tips", "We just streamlined Gretel\u2019s Python SDK ", "Prompting Llama-2 at Scale with Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/what-is-privacy-engineering", "page_title": "What is Privacy Engineering?", "headers": ["What is Privacy Engineering?", "\u200d", "\u200d", "\u200d", "\u200d", "\u200d", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Let\u2019s start with a definition: Privacy engineering the systematic application of engineering concepts for protecting sensitive information.\u00a0", "Privacy engineering is based on software and security engineering disciplines but differs in that it has effectively two equally important stakeholders- the first being business teams and applications that need access to the information, and the second being the customers whose personal data is being used. Business and consumer privacy use cases are increasingly blending together, and in 2019 Cisco ran a survey of 2,601 adults ages 18-44 and found that \u201cover 32% of respondents both care about privacy, are willing to act, and have done so by ", "\u201d.", "Today, developers on service teams at AWS and Google can instantly check out or query anonymized datasets to test ideas, without having to wait weeks or even months for compliance approvals to work with sensitive data. Investments in privacy technology have enabled these companies to learn from customers, innovate, and launch new products at a previously unheard of pace. For example, AWS currently offers over 200 different features and services, with over ", ". This growth is largely enabled by wide access to anonymized data, yielding a strong understanding and signal for their customers' needs.", "While many of today\u2019s successful privacy programs rely on manual and time consuming processes, the effort required to continue to scale privacy- even at the most successful tech companies of the world, requires more automation and a new way of thinking. Gartner estimates that companies will ", ", and that in the next 3 years, ", ".", "How do we scale privacy to enable any company or developer to get privacy right? Privacy engineering is about making privacy an engineering problem- built into the fabric of developer code and workflows, and thus one that can be automated and scaled in the same way we have scaled building software.", "Privacy itself has many use cases- for businesses today, getting privacy right is becoming a critical part of earning ", " with customers, which can lead to long term relationships and willingness to share information in the future that can be used to differentiate and build better services. Privacy has become a powerful differentiator for the products we use every day- see Apple\u2019s WWDC 2020 ", ", or an example of how ", ".\u00a0", "For consumers, privacy enables ", "- or the ability to determine how your information can be used and shared. Many companies today are building their business model on this concept- see ", ", ", ", and ", " (which explicitly makes revenue from subscriptions, and ", " users\u2019 personal information).\u00a0", "In the past two years, we have talked to hundreds of companies about their use cases for privacy and the new business models and opportunities that privacy engineering can unlock. Here are some of the top use cases we have seen by industry.", " are looking for ways to enable broader and faster access to data while maintaining customer trust and privacy. Similar to the use case for enabling immediate developer access to anonymized data, building processes for faster data access and experimentation is viewed as a competitive advantage when bringing new services and features to market.", " are interested in creating marketplaces where algorithms can be developed on freely available synthetic data, and then sold or licensed to financial institutions that have access to the real data.\u00a0", " are looking for ways to enable information sharing and monetize data while protecting the privacy of their patients, and ", " that could be ", " trained on shared datasets.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretel-ai-illumina-using-ai-to-create-safe-synthetic-datasets-for-genomics", "page_title": "Gretel.ai + Illumina - Using AI to create safe, synthetic datasets for genomics", "headers": ["Gretel.ai + Illumina - Using AI to create safe, synthetic datasets for genomics", "What\u2019s next", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The biggest challenge \u2014 and opportunity \u2014 for the life sciences community is learning how to safely share patient health data, while protecting the privacy of patients. ", "In particular, genomic data \u2014 literally, the instructions to build and grow an organism \u2014 is one of the most complex datasets that exists today. The ability to safely share genomic data will undoubtedly fuel medical breakthroughs and encourage better medical care. \u00a0", "This complexity and enormous potential of genomic data are exactly why the researchers here at Gretel.ai are thrilled to work with ", "\u2019s Emerging Solutions to explore the question of whether it is possible to create synthetic versions of real world genomic data sets. ", "The synthetic datasets created by Gretel are based on real world data, and offer enhanced privacy guarantees that can enable life science researchers to better collaborate and quickly test ideas through open access to data, without compromising patient privacy. ", "In our ", ", we use state of the art generative neural networks to recreate artificial versions of the highly complex genomic sequences used by life sciences researchers. Synthetic data created by Gretel.ai has the same size and shape as the dataset it was trained on, enabling researchers to explore the data using their preferred data science tools and run queries and statistics on the overall dataset. However, because the data is artificial, no records are based on any single genome. ", "Our research demonstrates encouraging evidence that state of the art synthetic data models can produce artificial versions of even highly dimensional and complex genomic and phenotypic data. \u00a0", "While the initial case study results are based on a relatively small sample set (1,220 mice), we\u2019re confident that with continued experiments in scale, accuracy, and privacy; synthetic data has the potential to enable sharing and collaboration on synthetic genomics datasets at a scale that is orders of magnitude larger than what is possible today. If you\u2019re interested in exploring further, we\u2019ve shared the code to synthesize genomic data and recreate all our experiments on ", ".", "You can read the entire case study ", ". ", "We are working together to enable future genomics research and safe, private data sharing between life sciences organizations and hospitals. In our next posts, we will explore the scale and privacy guarantees that can be achieved working with synthetic data on genomic datasets. To get in on the discussions, please join our ", ".", "If you have any questions or would like to discuss use cases around synthetic data we would love to talk to you. Feel free to reach out to us at ", ". ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/daniel-nissani", "page_title": "Author: Daniel Nissani - Gretel.ai", "headers": ["Daniel Nissani", "Why Nonprofits Should Care About Synthetic Data", "Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "Exploring NLP Part 1: Why Should a Privacy Engineering Company Care About NLP?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/why-nonprofits-should-care-about-synthetic-data", "page_title": "Why Nonprofits Should Care About Synthetic Data", "headers": ["Why Nonprofits Should Care About Synthetic Data", "TL;DR", "Introduction", "(3) ", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In November, I was on a panel discussing why synthetic data is important for nonprofits and how they can use it. The use cases are endless, spanning industries such as healthcare, education, economic development, and so much more. Much of the innovation hinges on both the technological innovations that are maturing and how those innovations address various pain points for nonprofits. I am extremely grateful that ", " invited Gretel to talk about these issues with ", " at the ", ".", "Synthetic data is recognized as one of the ", ". It will enable for-profit companies to share data, particularly when using differentially private synthetic data by removing the friction of privacy-related permission issues and allowing for more rapid technological development in pre-production pipelines. Moreover, synthetic data can be used to bolster datasets that are either too small or have dramatic dataset imbalances. This will help organizations build out more sophisticated machine learning capabilities that are usually very data hungry. However, these benefits do not only help the for-profit sector, but also can greatly impact the public sector.", "I was thrilled to be invited by DataKind to speak on a panel about synthetic data with Medic at the NetHope Summit. It was the perfect opportunity to showcase the power of synthetic data and why it is important for nonprofits to start thinking about it, too. Each pain point experienced by for-profit organizations is amplified for nonprofits. Here are three main challenges and how synthetic data can address them: ", "Many nonprofits and NGOs deal with vulnerable populations, ", ". These populations have faced technological abuse in the ways of ", ", ", ", and ", ". Nonprofit organizations understand that this misuse can happen to the populations they serve, and thus must be very protective over whatever data they collect.", " is one way nonprofits can mitigate this risk, unlocking several innovations and practices previously seen as too risky, such as collaborating on ", " (a use case shared by an audience member during our panel!). Although this doesn\u2019t necessarily prevent imperfect uses of data for model development, it does protect the information about vulnerable populations from leaking and being used for other nefarious ends.", "But even if we are able to share data to improve analyses, insights, and model building, the public sector just isn\u2019t in the business of collecting data. The Law Family Commission on Civil Society, a research group dedicated to enhancing civic community bonds, ", " in October 2021 describing how the public sector is lagging behind the private sector because of poor data practices, such as using \u201cunreadable formats\u201d like PDFs to store data.", ", and doing it right is even harder. Defining the granularity, the cadence, and the types of information collected is a full-time job unto itself. Few nonprofits have the budget or staff bandwidth to commit to robust internal data collection and management systems. Inevitably, other activities that are closer to the nonprofit\u2019s core mission will almost always take priority over these difficult tasks, especially when the return on investment is hard to calculate.\u00a0", "Here\u2019s where synthetic data can shine. If a nonprofit organization has collected just enough data (that amount is unknown, but less than a full data collection run), then they might be able to train a synthetic model to augment a relatively small sample. Because given a sufficient source of real data to work with, even just a little can then be transformed into an unlimited amount of synthetic data! With an expanded dataset and thus a more comprehensive understanding of their strategic initiatives and each one's performance, nonprofits can make more informed choices about where to allocate scarce resources so they can maximize their results.", "A real-world example I discussed on the panel was a model DataKind built in collaboration with ", ". The model would have helped Microcred make better decisions about lending, so it becomes more efficient and inclusive. However, Microcred was only collecting data on people who were granted a loan. Thus, the model could only be used on those who were granted loans, not on those who were denied. This limited the efficacy and impact of the model. Collecting data on those denied loans would take months or even years. But if Microcred utilizes synthetic data in the future, they would be able to collect data over a shorter time span and build out the rest of their dataset with better privacy guarantees.", "One of the biggest benefits unlocked with privacy-protected data is the ability to safely share datasets. Turning siloed, private information into anonymized artifacts that teams can collaborate on can spark novel discussions and discoveries that lead to better results for the vulnerable populations being served.", "Some civil servants in the UK have already begun advocating for the use of synthetic data to improve their government\u2019s use of data. For instance, of the more than 200 ideas proposed during a recent ", " hosted by several public institutions synthetic data was voted one of the best, as a tool that could be utilized in the public sector for, among other things, \u201c...improving detection of benefit and tax fraud through richer data exchange between the Department for Work and Pensions, HM Revenue and Customs and UK Visas and Immigration.\u201d By giving public sector groups the power to collaborate with data, new insights can be found, gaps can be closed, and better systems for the populations served can be built.", "The nonprofit sector is critical to the functioning of any healthy society. It helps ensure vulnerable populations with fewer means are cared for and the quality of life is improved for millions around the world. To continue providing these essential services to individuals and being a force for good on a broader scale, nonprofits must utilize data effectively. By using synthetic data tools like ", ", they can break the bottleneck of privacy and unlock innovations that before seemed implausible.\u00a0", "The key now is to ensure that nonprofits everywhere know these highly effective, inexpensive (and in some cases free) tools are available to them today. I hope you\u2019ll join me in spreading that good news!", "You can ", " with synthetic data for free.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/simplifying-our-apis", "page_title": "Simplifying Our APIs", "headers": ["Simplifying Our APIs", "(1) Simplified authentication", "(2) Project creation", "(3) Model names", "(4) Model configurations", "(5) Parquet support", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["As a developer or data scientist, we know you are really busy. At Gretel, we\u2019re always trying and improving on ways to make synthetic data simpler and more accessible for our users. We have heard your feedback, and are excited to launch five new features that will hopefully make your day a little easier. Cheers!", "A pain point for anyone using Jupyter notebooks is that each time the notebook is restarted, you are prompted to re-enter your Gretel API key. This requires you to jump over to the console, and then copy+paste the API key into the notebook cell.\u00a0", "Now, you can have Gretel cache your API key to the file system directly from the SDK. Additionally, use the \u201cvalidate\u201d option to test it here and create an error if your API key is invalid.", ":", ":", "By default, the create_project() command creates a new project each time that it is run. So, if you are running your blueprint notebook where you create a project called \u2018synthetic-data\u2019, and run the notebook 5 times, you will have 5 new projects created in the console interface.\u00a0", "Use create_or_get_unique_project() to use a Gretel project if it exists, or create one if it does not. Using the example above, if you run a notebook above 5 times, you\u2019ll have 5 synthetic models created inside a single project.\u00a0", ":", ":", "Gretel Synthetics automatically-generated model names are adorable (see `fluffy-fabulous-dog` or `enormous-handsome-hedgehog`), but sometimes you may want to name a model something more descriptive. You can now do that manually with the `model.name` attribute. Simply set the model name to anything you'd like, to easily remember the settings used for your model.", "Gretel\u2019s model configurations are portable across the console, CLI, and SDKs and are based on YAML, a human-friendly markup language. This makes for easy editing, but it can be cumbersome to load into a Jupyter notebook. Using read_model_config() makes this a one liner, and supports either default configurations, or loading configurations from a file.\u00a0", ":", ":\u00a0", "In addition to CSV, JSON, and JSON-L formats, Gretel\u2019s SDK and CLI now support Apache Parquet as an input type. Parquet is a favorite data format for developers and data scientists that allows for very efficient compression and querying of large datasets. Just point your model.data_source to a local or remotely accessible Parquet file. ", ":", "If you have other feedback on how we can make synthesizing data easier for you, drop us an email at ", "or visit our ", " and start a conversation with our developers and data scientists. Thanks for using Gretel!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/marjan-emadi", "page_title": "Author: Marjan Emadi - Gretel.ai", "headers": ["Marjan Emadi", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Measure the utility and quality of GPT-generated text using Gretel\u2019s new text report", "Red Teaming Synthetic Data Models", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/creating-synthetic-time-series-data-for-global-financial-institutions-a-poc-deep-dive", "page_title": "Creating Synthetic Time Series Data for Global Financial Institutions \u2013 a POC Deep Dive", "headers": ["Creating Synthetic Time Series Data for Global Financial Institutions \u2013 a POC Deep Dive", "\u200d", "\u200d", "\u200d", "\u200d", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this study, we discuss the creation of high-quality synthetic time-series datasets for one of the largest financial institutions in the world, and the methods we designed to assess the accuracy and privacy of our models and data. The temporal, ordered nature of time series data can help track and forecast future trends, which unsurprisingly, has enormous utility for business planning and investing. However, due to regulations and the inherent security risks that come with sharing data between individuals and organizations, much of the value that could be gleaned from it remains inaccessible. Here, Gretel\u2019s work demonstrates that synthetic data can help close this gap while preserving privacy. By generating synthetic time-series data that are generalizable and shareable amongst diverse teams, we can give financial institutions a competitive edge and the power to explore a whole new world of opportunities.\u00a0", "Developers can test our methods by opening up our example ", ", clicking \u201cRun All\u201d, and entering ", " to run the entire experiment, or by following along with the 3-step process outlined below!", "For this experiment, the bank\u2019s data science team provided a time series dataset containing customer account balance information over time, which spans 6 columns and approximately 5500 rows. There is a time component in the `date` field, and a set of trending account balances that must be maintained across each `district_id`. ", "In this step, we will create a simple pipeline that can be used to de-identify the time series dataset, and then create a synthetic model that will generate an artificial dataset of the same size and shape. Below is a diagram of the pipeline we will use to generate and test our time series model.", "\u200d", "\u200d", "To de-identify the data, we use Gretel.ai\u2019s Transform API with the ", " below to randomly shift any dates and floating point values in the data. To ensure consistent shifts for each district, we define the `district_id` identifier as a seed value. This step provides strong initial guarantees that a synthetic model will not memorize repeated values in the data.", "We then train a synthetic model on the de-identified training dataset, then use that model to create a synthetic data set with the same size and shape of the original data. For this, we will use Gretel\u2019s ", ", which conditions the model with the `date` and `district_id` attributes specified below and effectively asks the model to synthesize the remaining data.", "Next, we need to test our model\u2019s accuracy, which starts with running a quick sanity check using a comparison of time series distributions for a district in our dataset. Here, Gretel\u2019s synthetic quality score report (viewable in the ", ") is helpful for assessing the model\u2019s ability to learn correlations in the data. Below, you can see our synthetics are in line with the original dataset. Success!", "\u200d", "To tackle the tricky problem of assessing the quality of our synthetic time series dataset, the bank\u2019s team fitted an ARIMA model to both the synthetic and original datasets. This allowed us to measure how well each dataset captured seasonality in the data. (Note: our POC notebook uses the ", " in the statsmodels package, which includes a multiplicative seasonal component similar to the modified ARIMA model used by the bank\u2019s team.)", "To train the SARIMAX models, we used district_id = 13 and ran an experiment on each available target variable (this generated four target variables). For evaluating our model\u2019s prediction accuracy i.e. the root-mean-square error (RMSE), we used the last year in the dataset (1998) as our validation. The available years prior were used for training the model. All other parameters in the SARIMAX model were left at their default settings, except for those provided by the bank\u2019s team. Namely, the order = (0,1,1) and the seasonal_order = (1,1,0,12). Lastly, we made sure to order the data by date for each run.", "The images below are the RMSE results for each config used and the original dataset. The goal is to minimize RMSE. Here, it\u2019s notable that for many synthetic data runs, we outperformed the real data on at least one target variable. In particular, look at runs 17, 18, 19, and 20, where we controlled for the float precision of the columns before generating synthetic data. This control especially helped with runs 19 and 20, where we did significantly better than the real data on 3 out of 4 variables. Moreover, run 20 includes the suggestion of removing net_amt from the synthetic data generation, since it\u2019s a derivation of credit_amt and debit_amt, and thus adds needless complexity to the process. Runs 20Priv and 20SDKPriv both turned on privacy settings with all the improvements described above.", "\u200d", "The takeaway: optimal synthetic configurations can, in some cases, outperform the real-world data on our machine learning model by better minimizing RMSE. ", "Finally, after creating our synthetic model and dataset, we can assess the privacy of our artificial dataset. Let\u2019s compare our transformed and synthesized dataset to the original training dataset.", "This notebook highlights a new feature in Gretel Synthetics called ", ", which provides a form of armor to cover synthetic data weak points often exploited by adversarial attacks. For example, synthetics that are too similar to the original data can lead to membership inference attacks as well as attribute disclosure. Another serious privacy risk arises when you have synthetic \u2018outlier\u2019 records, particularly when they\u2019re similar to outlier training records. To combat both scenarios, we created Similarity and Outlier filters, which can both be dialed to a specific threshold based on the desired level of privacy. In this study, the use of Gretel\u2019s Similarity Privacy Filter removed all synthetic records that were duplicates of training records.", "To take things a step further, one method for ensuring the privacy of every record in the original dataset is to enable `differential privacy,` where we add some statistical noise while training the synthetic data model. Intuitively, a model trained with differential privacy should not be affected by any single training example or small set of training examples in its dataset. This process involves changing the optimizer that generates a well-fit synthetic model, but it requires a large number of records (typically >50k) to maintain the privacy of each record. For this reason, turning on differential privacy in the config wouldn\u2019t provide useful synthetic data while also maintaining a high standard of privacy.", "In this POC, we successfully demonstrated that Gretel\u2019s synthetic data can be as accurate, and in some cases even surpass that of real-world data used for machine learning classification tasks, while also providing strong privacy guarantees required to allow sharing inside a financial institution.", "If you\u2019re interested in building your own synthetic time-series datasets, you can ", ". When you do, please feel free to share your results with our ", ". We\u2019d love to hear what you\u2019re working on!", "Alex Watson, Daniel Nissani, Amy Steier, Lipika Ramaswamy, Kendrick Boyd - Gretel.ai.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/veterans-day-reflections-open-source-software-and-evacuation-operations-a-remarkable-combination", "page_title": "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "headers": ["Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Ever since leaving the military, I\u2019ve been fortunate to work with and run into many other veterans in the tech and startup industry, to include founding companies with them, including Gretel. This Veterans Day, I\u2019d like to reflect on a specific set of events where I had the opportunity to work with a lot of other veterans and benefit from the booming set of tools being built by the open source software community.", "In mid-to-late August of 2021, the United States Air Force led the largest Noncombatant Evacuation Operation (NEO) in history. Approximately 124,000 people were evacuated by airlift, from Afghanistan, in just over a two-week period. I spent over 12 years in the Air Force, and despite the confusion and chaos during this entire process,", "I felt a sense of pride knowing the Air Force was leading this effort. I was not an aviator during my time in the Air Force, but I sure as hell took a lot of rides in C-17s and C-130s and I think to this day I\u2019d almost rather fly via the USAF than some other airlines out there. Your bags fly free, actually show up, and the ", " don\u2019t tolerate unruly passengers.", "My wife, a US Army Veteran and highly proficient linguist, and I were glued to the television switching between channels airing updates about the evacuation. Combined, we spent over 24 months in Afghanistan. Watching this unfold triggered a lot of memories, storytelling, but worse of all, a sense of helplessness.", "Then, all of the sudden, the messages started flowing in like crazy. Our Signal app notifications were blowing up as we quickly learned that ", " were organizing teams to help put together information and details to assist people that were most likely not going to get out via airlift.", "The mission was to put together as much open-source intelligence as possible for potential ground exfiltration routes and provide it to the various group leads so they could pass it along to the people still on the ground. For the next 72 hours or so we converted our dining room and kitchen into a mini operations center and dove in.", "We called our various efforts Operation GTFO.", "Requests were pouring in: scour any and all publicly available information to identify Taliban checkpoints along roads, utilize procurable satellite imagery to look for potential evacuation routes, analyze a myriad of location-based information to find densities of people and traffic to avoid, with the goal of finding safe evacuation routes that have a lower likelihood of detection from threats.", "At this point, I did what any normal startup co-founder would do, rope my other co-founder into the madness! I pinged Alex, a veteran of the Intelligence Community and several deployments, gave him about a 30-second overview and that\u2019s all it took. Then he told one of his close friends, a former Army Intelligence Officer what was up and he dove right in as well.", "Alex and I were spending most of our time trying to make sense of various location-based data; what we really needed to do was aggregate location data into various densities so we could overlay the activity onto our main Common Operational Picture (COP), which was basically a shared Google Map with all sorts of layers enabled. \u00a0One other group we were talking to suggested using Uber\u2019s", " to do it and another group suggested using ", " for visualization.", "Once we were able to analyze some of the location data we ported the GPS locations back into our Google Map. Finally, we were able to pull updated satellite imagery to better understand potential border crossing threats and overlay cell phone coverage with our other location data to better identify \u201cgo\u201d and \u201cno go\u201d areas for ground teams. Everything was added to our map and then we could link to specific slide decks with more detailed analysis.", "Uber obviously built H3 for a different purpose, as no one was really taking an Uber out of Afghanistan, but their decision to open source this technology was a huge enabler for us. H3 also serves as a pretty neat way to do privacy-preserving transforms on location data, while keeping it useful, win-win. There\u2019s tons of other use cases for this kind of location analysis so I packaged up some of the jumbled code we had into a little module that would automate some of what we were doing. You can check out the ", " and we\u2019ll do a little tour of the features below.", "First, follow the ", " section in the GitHub repo. The module expects a Pandas DataFrame with at least three types of columns present:", "For this walkthrough, we\u2019ll use some rental scooter data that is public. There\u2019s already a sample dataset in the repo, there\u2019s also some sample code in the utils.py module in case you would like to create your own dataset.", "Here\u2019s a snippet of the dataset we are using:", "The identifier column is `bike_id` which identifies a unique bike or scooter at any given point in time.", "First, we\u2019ll import and create a `DensityTransform` instance.", "When creating the instance, you\u2019ll want to provide four parameters:", "Next, we\u2019ll fit the model. Here you have to choose the ", " that is desired. This is a value between 0 and 15, inclusive. The higher the number, the smaller an area each hexagon covers. Once the model is fit, you can inspect the details of your hex resolutions using the resolution property.", "In this case, each hexagon on the map covers approximately 36 square kilometers.", "Now, we can transform our original data into location densities using the resolution specified. When doing this, you can optionally transform the name of the \u201cidentifier\u201d field as well.", "This will create a new DataFrame that looks like this:", "In this resulting DataFrame, the `lat` and `lon` columns are now the ", " and for each hexagon we are given the unique count of Bike IDs.", "Finally, we have another transform function that will run the same computation but display a map that shows the hexagons, color-coded by their various densities:", "This will group the unique counts into 8 ranges of unique `bike_id` counts and plot a map:", "Finally, if we wanted to make the resolution a little higher, we could re-run this whole process. Here we\u2019ll just use method chaining to do everything in one command:", "Here we see that our hexagons are a lot smaller and more specific:", "Zooming in a bit, we can see the varying densities more closely:", "There are a lot of improvements and feature additions that could be made as next steps, for instance:", "It is remarkable how many open source tools and capabilities have been developed in the last several years. \u00a0This kind of open and free technology was nascent if not absent a decade ago and we are fortunate for these tools that allowed veterans and other supporters to band together years after hanging up our uniforms for good. This evacuation effort continues today, several displaced individuals still need help, and this probably will not be the last time we harness open technology for these purposes.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/synthetic-text-data-quality-report", "page_title": "Measure the utility and quality of GPT-generated text using Gretel\u2019s new text report", "headers": ["Measure the utility and quality of GPT-generated text using Gretel\u2019s new text report", "Introduction", "How to understand the Text Synthetic Data Quality Score (Text SQS):", "Measuring semantic and structural similarity:", "Text Semantic Similarity Score:", "Text Structure Similarity Score:", "Summarize the real and synthetic datasets:", "Visualize semantic similarity with principal component analysis (PCA):", "Try it yourself", "Video: How to use Gretel's new Synthetic Text Data Quality Report ", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["With recent innovations in various language models, generating synthetic text data has never been easier, but how can we evaluate that synthetic data against the real data? \u200cWe\u2019re excited to announce our Synthetic Text Data Quality Report, which measures text semantics and structure in 50 languages!", "The report highlights Gretel\u2019s Synthetic Text Data Quality Score (or Text SQS), an estimate of how well the generated synthetic data maintains the same semantic and structural properties as the original dataset. In this sense, the score can be viewed as a utility score or a confidence score as to whether scientific conclusions drawn from the synthetic dataset would be the same if one were to have used the original dataset instead. If you don't require semantic or structural symmetry, as might be the case in a testing or demo environment, a lower score may be just as acceptable.", "Let\u2019s break down the different sections of the report! In this report, we are comparing \u200ctext data from ", " with the generated synthetic text from our GPT-x model.", "The ", "is computed as the weighted combination of the ", " and ", " scores. We\u2019ll discuss each score more specifically in the next section.", "One way to interpret the Text SQS is to take a look at what use cases the generated synthetic data would be suitable for. Below, we break down recommendations based on the score. If the score is \u201cPoor\u201d or \u201cVery Poor\u201d, ", " for a multitude of ideas to improve the score.", "This section of the report shows how semantically similar the real and synthetic texts are, with a score in the range 0\u2013100. Semantic similarity refers to the similarity of the meaning of two texts. An embedding model is used to encode the text to a vector of size 512. We use a cosine similarity score to compare the average embedded vectors from the real vs synthetic.\u00a0", "A higher score assures the user that they can enable the synthetic text data in downstream semantic text classification tasks in place of the original text samples.", "Structural similarity indicates how the average characters per word , words per sentence, and sentence counts compare between the datasets. We use ", " to calculate the distance between the real vs synthetic distribution across the above statistics in the entire text dataset. You can see each distribution in the following plot. Similar real and synthetic distributions result in a higher text structure similarity score, which is the scaled average of the three distance values.", "This table is an overall view of the training and synthetic text data features. In this comparison we always consider the minimum number between training data and synthetic data records, or the default of 80 generated records. A lower number of duplicated training lines ensures more privacy in the synthetic generated text. Missing values refer to empty string rows. Similar average character, word, and sentence counts across training and synthetic data guarantee higher text structure similarity.", "In the following plot, we observe the relation across the first four principal components of the average embedded vectors in the real and synthetic text along with the variance ratio explained by each component. The diagonal plots, on the other hand, show the distribution of each principal component for the real and synthetic texts plotted on top of each other. Similar real and synthetic scatter matrices and distribution plots depict a higher semantic similarity score, which gives a user more confidence in replacing the original text with the synthetic for semantic text classification tasks", "You can generate and dig into a report yourself by selecting the ", " card on the Gretel Console.\u00a0", "To learn more about using Gretel GPT, check out our ", " to generate Q&A-style tweets from your favorite Twitter personalities and the accompanying ", ".\u00a0If you\u00a0have any questions, drop us a line in the ", ". ", "Watch Nicole Pang, Senior Product Manager at Gretel, walk through the Synthetic Text Data Quality Report and explain how it helps users evaluate generated text data. ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/progress-and-innovation-women-in-ai", "page_title": "Progress and Innovation - Women in AI", "headers": ["Progress and Innovation - Women in AI", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Recently, Gretel\u2019s senior applied scientist, ", ", and principal machine learning scientist, ", ", were nominated for VentureBeat\u2019s Women in AI Awards \u2013 Lipika as a \u201crising star\u201d for her demonstrated leadership in the field and Amy for her significant contributions to research that helps accelerate progress and innovation.\u00a0", "We asked them both about their experience, work, and any advice for other women who are interested in pursuing a similar path in AI.\u00a0", ": It\u2019s awesome! I love working with my team everyday to solve problems with math. Often they\u2019ll be complex problems that require much trial and error in solutioning, and draw me into a space of intense focus. So it\u2019s been great to take a moment to celebrate. This recognition fuels me to work harder.", ": I've always been so passionate about my work, that to be recognized for my accomplishments was a bit like icing on the cake. I currently have the fortune of working with some amazingly brilliant people. Everything is a team effort at Gretel, and so much of what I accomplish wouldn't be possible without the support of my highly talented colleagues in the applied science, engineering, design, PM and marketing teams.", ": I love that we at Gretel are working towards the mission of enabling privacy for everyone everywhere. There has been so much innovation and advancement in AI, but it shouldn\u2019t come at the expense of the privacy of individuals. This purpose drives everything I do, from writing basic equations to architecting complex models that our users interact with.", ": I truly love working with data and the Gretel vision of making data sharing easy really resonates with me. I also really enjoy working at that intersection of state-of-the art research and innovative product design. It's very satisfying to take bleeding edge ideas, push them a little further, and then morph them to fit existing, tangible business and consumer needs.", ": Read a lot \u2013 everything from papers and blogs on AI to opinions and updates unrelated to AI. For example, if you work on ", ", educate yourself about privacy regulations and laws. Ultimately, the models\u00a0 we build cannot be isolated from the environment they are built in or the communities they are built to serve.\u00a0", ": Get educated, follow your heart, and embrace continuous learning.", ": I'm excited that responsible AI is gaining traction. Many folks in our industry now consider aspects like privacy, fairness and interpretability an input rather than an afterthought. We\u2019re building Gretel for the future, and I\u2019m so thrilled to be a part of this important work.", ": Over the last decade some of the greatest technological innovations have been driven from access to large amounts of rich data. At the same time, privacy is not just a growing societal concern but one mandated by growing government regulations. Synthetic data offers a solution to these concerns and promises a new era of even more rapid technological and scientific innovations. The number and diversity of industries impacted is limitless; from finance, bio-medical, health sciences, genomics and personalized medicine to retail, ecommerce, restaurants, gaming and entire governments. I deeply look forward to being a part of this.", "If you are interested in learning more about Lipika or Amy\u2019s work, you can watch our recent workshop on Gretel\u2019s new Evaluate API, which can ", ". \u00a0 You can also send us a note in our ", " or email us at hi@gretel.ai to continue the conversation. Thanks for reading!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/gretels-new-synthetic-performance-report", "page_title": "Gretel's New Synthetic Performance Report", "headers": ["Gretel's New Synthetic Performance Report", "Report header", "Per Field Jensen-Shannon Score", "Field-to-Field Correlations", "Field Distributions", "How Can I Improve My Model?", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["One of the first, and most important questions you may have after generating a synthetic dataset is \u201cHow accurate is my synthetic data?\u201d. \u00a0Gretel Synthetics includes a performance report that shows you just how well the distributions in your training data were maintained in your new synthetic data. \u00a0In this blog, we'll step you through our newly released version of the performance report which includes interactive Plotly graphs and stylish HTML formatting.", "See one of our ", " or ", " on how to generate synthetic data. \u00a0With the latest version of our premium SDKs, the report is now written to a file of your choosing. The line in the notebook that generates the performance report can be utilized as shown below:", "We have also made a public example of our report available ", ". \u00a0This report is from synthesizing medical records into a safe, shareable version of the data.", "Let's break down the sections of the report! ", "First is the report header, which gives 3 high level pieces of useful information.", "The next section in the report will show the JS Distance score for each individual field (as shown below). For each field there is a Category Count (number of unique values) and a Distance Score. \u00a0The lower your Distance Score the better the within-field distribution was maintained in the synthetic data.", "Gretel computes correlations between all pairs of fields in the training data and then again in the synthetic data. \u00a0The specific correlation algorithm used depends on the data type of the two fields being compared. \u00a0We use Pearson's r for correlation between numeric fields, Theil's U for correlation between categorical fields and ", " for correlation between numeric and categorical fields.", "We use a heatmap to show these correlations:", "As you mouse over the individual squares of the heatmap, Plotly will show you the corresponding two fields and the specific correlation value between those fields. \u00a0If you're new to Plotly, play around with the modebar that appears as you mouse over the upper right area of the graph. \u00a0One menu option that's particularly nice for heatmaps is the \"Toggle Spike Lines\". \u00a0When you click this on, a dotted line to both the x and y axes will appear. \u00a0The \"Camera\" option is also nice for downloading a png of the graph.", "Following the training and synthetic correlation matrices, you'll see a heatmap that is essentially the difference between the two. \u00a0We literally take the training correlation value for a field-field combination and subtract the corresponding synthetic correlation value. \u00a0This makes it easy to narrow in quickly on any differences.", "After the correlation difference heatmap, you'll see a series of graphs, one per field that we're able to graph. \u00a0Categorical fields with a really high number of unique fields are a bit too messy to graph. \u00a0We use bar charts for categorical fields and histograms for numeric fields. \u00a0The Plotly modebar option we like to use on these charts is the \"Compare data on hover\" option. \u00a0When this is enabled, instead of the mouse-over showing just the value for the closest bar, it will show both values for the closest pair of bars so you can quickly see both the training and synthetic values for this category or numeric bin. \u00a0Below we show both an example bar chart as well as a histogram.", "\u200d", "Graphs are so much fun, aren't they? \u00a0Graphs say in one glimpse, the equivalence of what many hard to follow English sentences could say (not to mention the aesthetics benefit).", "Gretel Synthetics generally does very well at maintaining within-field distributions as well as cross field correlations. \u00a0However, if your JS Distance scores and/or your MAE score aren\u2019t as high as you\u2019d like them to be, here are few suggestions for fine tuning your model:", "At Gretel.ai we are super excited about the possibility of using synthetic data to augment training sets to create ML and AI models that generalize better against unknown data and with reduced algorithmic biases. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Like gretel-synthetics? Give us a ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/ai?094d394d_page=4", "page_title": "AI blog posts - Gretel.ai", "headers": ["AI", "Improving massively imbalanced datasets in machine learning with synthetic data", "Automated Data Exposure Detection with Gretel Outpost", "Create a Location Generator GAN", "Create artificial data with Gretel Synthetics and Google Colaboratory", "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "Common misconceptions about differential privacy", "Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/common-misconceptions-about-differential-privacy", "page_title": "Common misconceptions about differential privacy", "headers": ["Common misconceptions about differential privacy", "Differential Privacy is not an algorithm.", "Differential Privacy doesn\u2019t provide blanket protection for all sensitive information.", "Differential privacy is not a panacea.", "\u201cDifferentially private data\u201d is not a clearly defined term.", "Appendix", "Central vs. Local Differential Privacy", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["There is a plethora of content on differential privacy (DP), ranging from academic works, such as ", ", to blogs explaining its core principles, interpretation and application, such as ", ". While these are well-written resources, they require deep study to fully grasp. So if you have a basic understanding of differential privacy but haven\u2019t had the chance to delve into such resources, this post provides clarification on some common misconceptions.", "Rather, DP is a standard that algorithms must meet. The standard is simply that the output of the algorithm, such as a count, should not depend too much on any singular record. Algorithms typically achieve the differential privacy standard by some type of probabilistic noise addition to mask the presence of any record. For example, a small amount of noise can be added to an exact count to make it differentially private.", "To borrow from ", ", \u201cdifferential privacy is a formal distinction between ", " and ", ".\u201d If you choose to contribute your secret, DP will protect your secret in an equivalent way to the scenario where you don\u2019t contribute your secret. But it does not provide guarantees about other ways your secrets become public.", "Consider this example. You\u2019re a data scientist making $100k a year at Rainforest Corp, a fictional company that practices pay equity. The talent team at Rainforest wants to publish the average income of data scientists in an effort to attract more applicants, and they have decided to ", ". You know that your peers\u2019 income is within 5k of yours, but you\u2019ve never shared your income with anyone \u2014 you consider it your secret. You want to continue safeguarding your secret, so you opt-out of being included in this average. A few weeks pass and you see the data scientist job posting has been updated with \u201caverage salary of ~$101k\u201d. Anyone reading this job posting would infer that you make roughly $101k, which is true. So even though you withheld your data from the calculation, a secret about you has become public.", "For another example, check out ", " where Kamath talks through why a widely circulated story of targeted marketing by Target is not a privacy breach.", "DP is not suitable for every single analysis. It is a way of quantifying that an algorithm is going to tell you more about the large-scale trends in a dataset than it is about any specific individual. In other words, it\u2019s designed to help individuals cleverly hide in a crowd.", "If outlier analysis is of interest, DP is not the appropriate tool for privacy protection. Further, DP is not appropriate for studying small populations. At its core, DP is intended to allow aggregate information about large populations to be shared safely.", "\u201cDifferentially private data\u201d is an ambiguous term that\u2019s thrown around quite often. It can mean a few different things, leaving it open to misinterpretation. Let\u2019s break it down.", "If you\u2019ve read some introductory materials, you likely think of \u201cdifferentially private\u201d as an adjective appropriate for algorithms that do something in aggregate, such as a sum, median, or even a neural network, and their outputs. Typically, the privacy protection stems from some form of calibrated, probabilistic noise addition. So how does noise addition work for a process that produces data records instead of an aggregate as the output?", "Some possible interpretations of \u201cdifferentially private data\u201d are below.", "So when using differential privacy as an adjective to describe data, I\u2019d encourage you to clarify the broad class of algorithm that was used, to be mindful of the intended use of data produced, and to correctly term the data as synthetic when appropriate.", "I hope this article helped clear up some common misconceptions about differential privacy. Drop me a line at ", " if you have any questions or would like to chat about my work in applied privacy at ", "!", "There are two different models of differential privacy \u2014 central and local. The standard of privacy remains the same, however the distinction is in where data is stored and when noise addition occurs.", "You are probably familiar with the central model, where all the real, sensitive data is available in some central location. For example, you and I sign up for rewards at an iconic Seattle-based coffeehouse chain. We trust them with our names, phone numbers and birth dates, which they now store in some central database. If they wanted to share the median age of customers who place orders for caffeinated beverages after 7pm, they could do so with the guarantee of differential privacy by calculating the true value and adding some calibrated noise to it. The real, sensitive data is still stored without modification and is unrelated to differential privacy. It\u2019s the process of sharing the median age (i.e. an algorithm, see the first misconception) that is differentially private, not the central database being queried.", "In contrast, the local model was conceived for cases where there is a lack of trust or central location for storing the raw data. For example, my curious friend, George, is conducting a study about whether aspiring data scientists take longer than the allotted time to complete take-home assignments, and asks me to contribute my data. While I am also similarly curious to know aggregate information about my peers, I don\u2019t trust George enough to reveal that it took me a whole day to complete my last take-home, even though the time limit was 4 hours. So George tells me to flip a coin; if it lands on heads, I respond truthfully. Else, I flip a coin again, and if it lands on heads I say yes, and if it lands on tails I say no. If George asked all survey participants to do the same, he would have now collected a noisy version of the dataset. This algorithm is called ", ". The purpose of collecting this noisy dataset is to aggregate it, which George does by ", " introduced by the second coin flip. Note that randomized response, as described above, is differentially private (see ", " for a proof). So, the algorithm is differentially private, and the output of the algorithm, this noisy database of answers, can also be described as differentially private.", "For more on the flavors of DP and boundaries between them, read ", ".", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/jennifer-yonemitsu", "page_title": "Author: Jennifer Yonemitsu - Gretel.ai", "headers": ["Jennifer Yonemitsu", "Progress and Innovation - Women in AI", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/kendrick-boyd", "page_title": "Author: Kendrick Boyd - Gretel.ai", "headers": ["Kendrick Boyd", "Generate time-series data with Gretel\u2019s new DGAN model", "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/gretel-readme", "page_title": "Gretel.README", "headers": ["Gretel.README", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["At Gretel we are a team of developers who have worked on everything. From a chatbot that ", ", to filling the need (literally) for a ", " that that streams live sensor data to AWS, to cloud services that tens of thousands of customers use to ", ", to security that protects up to ", " \u2014 we know first hand how hard it is to manage all kinds of sensitive and ever-evolving data.", "Whether working inside organizations, between companies, or on personal projects, it\u2019s become way too hard for developers to safely share and collaborate with sensitive data. It\u2019s not just us \u2014 every developer, every day, struggles with the difficult task of ensuring the privacy and safety of sensitive data.", "Gretel is the solution. Gretel works in real time to enable safe sharing and collaboration between developers and applications with any kind of data. Gretel\u2019s tools are open, intelligent, and integrated. No matter what kind of data, language, or stack you\u2019re working with, we\u2019re here to help you ensure data safety and privacy from day one.", "From our time at AWS and Google, we know first hand that enabling developers to safely learn and experiment with data is the key to rapid innovation on behalf of customers. As developers, we don\u2019t always need full access to sensitive customer data. We know that it\u2019s often best to only select the data that you need for developing new features or exploring insights \u2014 especially if you can use your developer identity to access data in seconds, instead of spending weeks or months of waiting for compliance approval.", "At Gretel, our lights-on moment was realizing that we can apply machine learning, synthetic data, and formal reasoning to offer provable privacy guarantees for data. And by integrating privacy into our developer workflows, we can enable safe access to data within seconds of the time it is created, unlocking siloed data and opening the door for new ideas.", "We want to make it easier for more of this to happen:", ", developers", ", even building", ".", "Going forward, we\u2019ll use this space to demonstrate how Gretel open source projects can enable you to get the most out of your data. Our goal is to democratize building with data so everyone can use it.", "Keep checking back- we will be posting research, source code, and examples that we\u2019ll be open sourcing soon. We\u2019ve got some great examples of what we can do, and we want to hear your ideas too. We are building Gretel for developers like you, so don\u2019t be shy. Please follow us here,", ", and", ". Want to see for yourself? We\u2019ve got an app for that. We\u2019re ", ".", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/running-gretel-on-apache-airflow", "page_title": "Build a synthetic data pipeline using Gretel and Apache Airflow", "headers": ["Build a synthetic data pipeline using Gretel and Apache Airflow", "What is Airflow", "How does Gretel fit in?", "Extract Features", "Synthesize Features using Gretel APIs", "Load Synthetic Features", "Orchestrating the Pipeline", "Wrapping things up", "Thanks for reading", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Hey folks, my name is Drew, and I'm a software engineer here at Gretel. I've recently been thinking about patterns for integrating Gretel APIs into existing tools so that it's easy to build data pipelines where security and customer privacy are first-class features, not just an afterthought or box to check.", "One data engineering tool that is popular amongst Gretel engineers and customers is Apache Airflow. It also happens to work great with Gretel. In this blog post, we'll show you how to build a synthetic data pipeline using Airflow, Gretel and PostgreSQL. Let's jump in!", " is a workflow automation tool commonly used to build data pipelines. It enables data engineers or data scientists to programmatically define and deploy these pipelines using Python and other familiar constructs. At the core of Airflow is the concept of a DAG, or directed acyclic graph. An Airflow DAG provides a model and set of APIs for defining pipeline components, their dependencies and execution order.", "You might find Airflow pipelines replicating data from a product database into a data warehouse. Other pipelines might execute queries that join normalized data into a single dataset suitable for analytics or modeling. Yet another pipeline might publish a daily report aggregating key business metrics. A common theme shared amongst these use cases: coordinating the movement of data across systems. This is where Airflow shines.", "Leveraging Airflow and its rich ecosystem of ", ", data engineers and scientists can orchestrate any number of disparate tools or services into a single unified pipeline that is easy to maintain and operate. With an understanding of these integration capabilities, we\u2019ll now start talking about how Gretel might be integrated into an Airflow pipeline to improve common data ops workflows.", "At Gretel, our mission is to make data easier and safer to work with. Talking to customers, one pain point we often hear about is the time and effort required to get data scientists access to sensitive data. Using", ", we can reduce the risk of working with sensitive data by generating a synthetic copy of the dataset. By integrating Gretel with Airflow, it\u2019s possible to create self-serve pipelines that make it easy for data scientists to quickly get the data they need without requiring a data engineer for every new data request.", "To demonstrate these capabilities, we\u2019ll build an ETL pipeline that extracts user activity features from a database, generates a synthetic version of the dataset, and saves the dataset to S3. With the synthetic dataset saved in S3, it can then be used by data scientists for downstream modeling or analysis without compromising customer privacy.", "To kick things off, let\u2019s first take a bird\u2019s eye view of the pipeline. Each node in this diagram represents a pipeline step, or \u201ctask\u201d in Airflow terms.", "We can break the pipeline up into 3 stages, similar to what you might find in an ETL pipeline:", "In the next few sections we\u2019ll dive into each of these three steps in greater detail. If you wish to follow along with each code sample, you can head over to ", " and download all the code used in this blog post. The repo also contains instructions you can follow to start an Airflow instance and run the pipeline end to end.", "Additionally, it may be helpful to view the Airflow pipeline in its entirety, before we dissect each component, ", ". The code snippets in the following sections are extracted from the linked user booking pipeline.", "The first task, `extract_features` is responsible for extracting raw data from the source database and transforming it into a set of features. This is a common ", " problem you might find in any machine learning or analytics pipeline.", "In our example pipeline we will provision a PostgreSQL database and load it with booking data from an ", ". ", "This dataset contains two tables, `Users` and `Sessions`. `Sessions` contains a foreign key reference, `user_id`. Using this relationship, we\u2019ll create a set of features containing various booking metrics aggregated by user. The following figure represents the SQL query used to build the features.", "The SQL query is then executed from our Airflow pipeline and written to an intermediate S3 location using the following task definition.", "The input to the task, `sql_file`, determines what query to run on the database. This query will be read-in to the task and then executed against the database. The results of the query will then be written to S3 and the remote file key will be returned as an output of the task.", "The screenshot below shows a sample result set of the extraction query from above. We will describe how to create a synthetic version of this dataset in the next section.", "To generate a synthetic version of each feature, we must first train a synthetic model, and then run the model to generate synthetic records. Gretel has a set of Python SDKs that make it easy to integrate into Airflow tasks.", "In addition to the Python Client SDKs, we\u2019ve created a ", " that manages Gretel API connections and secrets. After setting up a Gretel Airflow Connection, connecting to the Gretel API is as easy as", "For more information about how to configure Airflow connections, please refer to our Github repository ", ".", "The `project` variable in the example above can be used as the main entrypoint for training and running synthetic models using Gretel\u2019s API. For more details, you can check out our ", ".", "Referring back to the booking pipeline, we\u2019ll now review the `generate_synthetic_features` task. This step is responsible for training the synthetic model using the features extracted in the previous task.", "Looking at the method signature, you will see it takes a path, `data_source`. This value points to the S3 features extracted in the previous step. In a later section we\u2019ll walk through how all these inputs and outputs are wired together.", "When creating the model using `project.create_model_obj`, the `model_config` param represents the synthetic model configuration used to generate the model. In this pipeline, we\u2019re using our ", ", but many other ", " are available.", "After the model has been configured, we call `model.submit_cloud()`. This will submit the model for training and record generation using Gretel Cloud. Calling `poll(model)` will block the task until the model has completed training.", "Now that the model has been trained, we\u2019ll use `get_artifact_link` to return a link to download the generated synthetic features.", "This artifact link will be used as an input to the final `upload_synthetic_features` step.", "The original features have been extracted, and a synthetic version has been created. Now it\u2019s time to upload the synthetic features so they can be accessed by downstream consumers. In this example, we\u2019re going to use an S3 bucket as the final destination for the dataset.", "This task is pretty straightforward. The `data_set` input value contains a signed HTTP link to download the synthetic dataset from Gretel\u2019s API. The task will read that file into the Airflow worker, and then use the already configured S3 hook to upload the synthetic feature file to an S3 bucket where downstream consumers or models can access it.", "Over the last three sections we\u2019ve walked through all the code required to extract, synthesize and load a dataset. The last step is to tie each of these tasks together into a single Airflow pipeline. ", "If you\u2019ll recall back to the beginning of this post, we briefly mentioned the concept of a DAG. Using Airflow\u2019s TaskFlow API we can compose these three Python methods into a DAG that defines the inputs, outputs and order each step will be run.", "If you follow the path of these method calls, you will eventually get a graph that looks like our original feature pipeline.", "If you want to run this pipeline, and see it in action, head over to the ", ". There you will find instructions on how to start an Airflow instance and run the pipeline end to end.", "If you\u2019ve made it this far, you\u2019ve seen how Gretel can be integrated into a data pipeline built on Airflow. By combining Gretel\u2019s developer friendly APIs, and Airflow\u2019s powerful system of hooks and operators it\u2019s easy to build ETL pipelines that make data more accessible and safer to use. ", "We also talked about a common feature engineering use case where sensitive data may not be readily accessible. By generating a synthetic version of the dataset, we reduce the risk of exposing any sensitive data, but still retain the utility of the dataset while making it quickly available to those who need it.", "Thinking about the feature pipeline in more abstract terms, we now have a pattern that can be repurposed for any number of new SQL queries. By deploying a new version of the pipeline, and swapping out the initial SQL query, we can front any potentially sensitive query with a synthetic dataset that preserves customer privacy. The only line of code that needs to change is the path to the sql file. No complex data engineering required.", "Send us an email at ", " or come join us in ", " if you have any questions or comments. We\u2019d love to hear how you\u2019re using Airflow and how we can best integrate with your existing data pipelines.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/automating-workflows-with-gretel-and-sagemaker", "page_title": "Synthetic Data, Real Privacy: Automating Secure Workflows with Gretel and Amazon SageMaker", "headers": ["Synthetic Data, Real Privacy: Automating Secure Workflows with Gretel and Amazon SageMaker", "Summary", "Step 1 - Create S3 source and destination buckets", "Step 2 - Create Secret for the Gretel API key", "Step 3 - Create SageMaker notebook", "Step 4 - Add notebook to the SageMaker instance", "Step 5 - Create notebook Lifecycle configuration", "Step 6 - Create the AWS Lambda function", "Step 7 - Attach the right IAM policies to the SageMaker role", "Step 8 - Monitor the workflow in AWS CloudWatch", "Conclusion", "About the authors", "Maarten Van Segbroeck", "Rumi Olsen", "Qiong (Jo) Zhang", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["At Gretel, we\u2019re excited about empowering developers with the tools they need to address data privacy and security risks that often hamper innovation. By partnering with Amazon, we\u2019re making multimodal synthetic data generation tools and evaluation metrics broadly available, so teams everywhere can collaborate and build great products with safe, shareable data that respects personal privacy.", "In this post, we demonstrate how a Gretel job can run seamlessly in the background once a user (or automated process) uploads a CSV file to a particular source S3 bucket with restricted permissions. When set up, the entire pipeline runs autonomously without the need for any manual intervention. The uploaded files could contain sensitive data, so sharing access to this S3 bucket would impose a privacy risk, even across internal teams. Note: For users with stringent security requirements, these processes can be used in ", " to ensure their data never leaves their AWS environment.", "The S3ObjectCreation event in the s3 source bucket will invoke an AWS Lambda function that starts an Amazon SageMaker Notebook instance. The notebook instance will automatically execute the notebook code via the notebook\u2019s lifecycle configuration.", "In the SageMaker notebook, we first retrieve all the uploaded csv files that were added to the restricted bucket (whether this is a single or a bulk upload) and then call the Gretel API to run Gretel Transform and Gretel Synthetics on these files. With Gretel, we first will de-identify the files by removing the PII and subsequently train a synthetic model on the de-identified files, create a Synthetic Data Quality Score (SQS) report and generate a synthetic version of the dataset. The synthetic data and the SQS report will then be uploaded to a destination S3 bucket that could have more permissible access rights.", "Internal teams can retrieve the synthetic data stored in the destination bucket without the need to ever see the production data. They don\u2019t even need to run the synthetic model as this process will automatically run in the background. The synthetic version of the sensitive or production data can then be used by data scientists for data analysis, ML engineers for ML model training, or software engineers for test data management use cases.", "This synthetic data generation process leverages different AWS services to make the workflow possible, including:", "The architectural diagram is depicted below, and you can watch an end-to-end demo of this process ", ".", "We start by creating two S3 buckets:", "We don\u2019t want to expose the Gretel API key or hard-code it in the notebook. So we\u2019ll use AWS Secrets Manager to create a secret Key-Value pair to store the API Key.", "Let\u2019s set the Key name to \u201cgretelApiKey\u201d and grab the Gretel API key from the Gretel Console ", "Finally, we can use the following code snippet in the notebook to retrieve the Gretel API Key to configure the Gretel session.", "Now let\u2019s create the SageMaker notebook instance. In this example, we name the instance ", ". Since we don\u2019t need the notebook to do heavy computations, we can select a cost-efficient instance type. Note that all model training will happen in the Gretel Cloud (in this example) or in Amazon EKS cluster (in case you opt for a Gretel Hybrid deployment).", "Once the notebook instance is created, we start the instance and open JupyterLab to add the notebook.", "The notebook will run the following steps:", "The notebook for this blogpost can be found ", ". We changed the name of the notebook in our workflow to ", ".", "Next, within SageMaker, we\u2019ll create a Lifecycle configuration that allows us to run and automatically stop the notebook. The notebook instance will be stopped when the notebook goes idle after a specified amount of seconds.", "The script to use is the following:", "Once done, we need to attach the lifecycle configuration to the notebook instance. Note that if we want to make any modifications to the notebook, we\u2019ll have to remove the lifecycle from the instance to prevent auto-stopping.", "Now we create a Lambda function with the function name ", "We then add an S3-based event trigger.", "The trigger configuration is set up so the S3 bucket ", " is the event source with \u201cobject create events\u201d as the event types. For this example, we\u2019ll limit file types to CSV.", "\u200d", "Gretel supports other flat data formats, such as json and parquet, and we can always add separate triggers for those file formats. Alternatively, you could also consider using ", " in your automated pipeline with events triggered from Amazon DynamoDB or Amazon DocumentDB.", "Once the Lambda function is configured, we add a few lines of Python code to instruct the function to start the SageMaker notebook instance that we named ", ".", "We want to make sure that the SageMaker role has access rights to our S3 buckets and the Secret that holds the Gretel API key.", "Therefore, we'll create the following policies in IAM and add them to the SageMaker role:\u00a0", "Go to IAM, select Policies and click on \u201cCreate policy\u201d. For each of the policies, add the following JSON file and give it the above names.", "\u200d", "Here, we\u2019ll need to get the ARN from SecretsManager. ", "Next, we\u2019ll add it to the policy.", "Once these policies are created, we\u2019ll add them to the SageMaker role:", "Finally, the role will look as follows:", "Now we\u2019re ready to test out our workflow!", "In AWS CloudWatch, we can retrieve the logs to find out each time the Lambda function was triggered. We can also check the logs produced by the SageMaker notebook, which may be helpful in case of debugging.", "In this post, we walked you through the process of setting up an automated workflow that allows you to create synthetic data using Gretel with AWS services such as SageMaker, Lambda, and Secrets Manager. Once configured, the processes run fully autonomously and can be monitored via Amazon Cloud Watch. If you have more stringent security requirements, these processes can be used in a ", " environment to ensure data never leaves your control. Either setup enables the generation of an unlimited amount of PII-free high quality synthetic data that can be safely shared across your organization and with external stakeholders.\u00a0", "To see an end-to-end demo of the process, check out our webinar recording. The ", ", or you can ", ". ", "You can find Gretel on the ", ". If you have any questions about Gretel\u2019s platform or would like to learn more about how synthetic data can help your business, ", ".", "Maarten Van Segbroeck is a Principal Applied Scientist at Gretel where he helps customers to adopt synthetic data solutions into their use cases. Prior to Gretel, he spent 5 years at Amazon dividing his time between AWS and Amazon Alexa. Maarten holds a PhD in Electrical Engineering from the Catholic University of Leuven and worked as a post-doctoral researcher at the University of California in Los Angeles.\u00a0", "Rumi is a global thought leader in the fields of AI and machine learning, with 18 years of experience in the technology industry. With expertise in AWS cloud, solutions architecture, and hands-on software development, Rumi has consistently achieved remarkable outcomes. Beyond her technical prowess, she is an active blogger and video content creator, crafting original ML content for re:Invent and hosting engaging in-person sessions. As she leads a team of AI/ML Partner Solutions Architects at AWS, Rumi collaborates closely with top machine learning ISV partners, leveraging AWS ML services to elevate their products and spearheading strategic engagements.", "Qiong (Jo) Zhang is a Senior Partner Solutions Architect at Amazon Web Services, specializing in AI/ML. Qiong worked at Fujitsu as a Research Scientist from 2008 - 2021. Qiong received her Ph. D. Degree in Computer Science from The University of Texas at Dallas. She holds 30+ patents and has co-authored 100+ journal/conference papers. She is also the recipient of the Best Paper Award at IEEE NetSoft 2016, IEEE ICC 2011, ONDM 2010, and IEEE GLOBECOM 2005.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/gretel-research", "page_title": "Author: Gretel Research - Gretel.ai", "headers": ["Gretel Research", "What We\u2019re Reading: Trends & Takeaways from the NeurIPS 2021 Conference", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/nick-keune", "page_title": "Author: Nick Keune - Gretel.ai", "headers": ["Nick Keune", "Filling in sparse tables with Gretel\u2019s Tabular LLM", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/johnny-greco", "page_title": "Author: Johnny Greco - Gretel.ai", "headers": ["Johnny Greco", "Nail Synthetic Data Generation Every Time with Gretel Tuner ", "We just streamlined Gretel\u2019s Python SDK ", "Gretel GPT Sentiment Swap", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/transformers", "page_title": "Transform blog posts - Gretel.ai", "headers": ["Transform", "Transforms and Synthetics on Relational Databases", "Auto-anonymize production datasets for development", "Anonymize Data with S3 Object Lambda", "Synthetic Data Configuration Templates", "Introducing Gretel Blueprints", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/install-tensorflow-with-cuda-cdnn-and-gpu-support-in-4-easy-steps", "page_title": "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "headers": ["Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Getting Started", "Step 1 \u2014 Install NVIDIA CUDA Drivers", "Step 2 \u2014 Set up TensorFlow and PyTorch with GPU support", "PyTorch", "TensorFlow", "Step 3 \u2014 Set up Docker with GPU support (optional)", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Latest update:\u00a03/6/2023 - Added support for PyTorch, updated Tensorflow version, and more recent Ubuntu version", "Setting up a deep learning environment with GPU support ", ". In this post, we'll walk through setting up the latest versions of Ubuntu, PyTorch, TensorFlow, and Docker with GPU support to make getting started easier than ever. Prefer video? Check out the", " on Gretel\u2019s Youtube channel.", "Tested with NVIDIA Tesla T4 and RTX 3090 GPUs on GCP, AWS, and Azure. Any NVIDIA CUDA compatible GPU should work.", "These are the baseline drivers that your operating system needs to drive the GPU. NVIDIA recommends using Ubuntu\u2019s package manager to install, but you can install drivers from .run files as well.\u00a0", "Now that you have installed the drivers, reboot your system.", "Log back in and validate that the drivers installed correctly by running NVIDIA\u2019s command line utility.", "Install the Anaconda package manager. Navigate to ", " and download the x86 installer for Linux, or use the command below.", "Sign out and sign back in via SSH or close and re-open your terminal window. Now we\u2019ll set up virtual environments for TensorFlow and PyTorch.", "We\u2019ll start with PyTorch because it\u2019s way less complicated ;-), following PyTorch\u2019s ", ".\u00a0", "\u00a0Now for TensorFlow support. We\u2019ll follow the ", ", and create a dedicated conda environment for TensorFlow to make sure that the library requirements don't have any conflicts.\u00a0", "Sign out and sign back in via SSH or close and re-open your terminal window. Reactivate your conda session.\u00a0", "You\u2019ve done it!", "Now that we have the NVIDIA drivers installed, we\u2019ll install Docker, which will let you run GPU-accelerated containers in your environment. Both ", " and TensorFlow state that containers are the ", ".", "Sign out and sign back in via SSH or close and re-open your terminal window. Now confirm docker is running.", "Now, we\u2019ll enable Docker containers to run with GPU acceleration, following ", ", or just run the commands below.\u00a0\u00a0", "Set up package repository", "Update the package manager", "Validate that you can run Docker containers with GPU acceleration", "Woot! You\u2019re good to go running GPU-accelerated ML containers on your workstation.\u00a0", "Let us know if you have any questions in our ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/security", "page_title": "Security blog posts - Gretel.ai", "headers": ["Security", "Helping Organizations Build Resilient and Trustworthy Information Technology", "What is Data Anonymization?", "Introducing Gretel's Privacy Filters", "Automated Data Exposure Detection with Gretel Outpost", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/prompting-llama-2-at-scale-with-gretel", "page_title": "Prompting Llama-2 at Scale with Gretel", "headers": ["Prompting Llama-2 at Scale with Gretel", "What is Batch Inference?", "Why Llama 2?", "How is this useful?", "How to prompt Llama-2 with Gretel", "1. Setup Development Environment", "2. Load and prepare the dataset", "3. Initialize the LLM", "4. Batch Prompt the LLM", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this blog, we will walk through prompting the 7 billion parameter ", " chat model with a series of 250 questions from ", "- a popular dataset used to assess LLMs ability to solve multi-step problems.\u00a0", "Follow along in a ", ".", "When you\u2019re working with 100s to 100k\u2019s of records, prompting an LLM via a synchronous API, one query at a time can be inefficient. In this post, we'll explore how to leverage ", " to prompt Llama-2 efficiently at scale, for example to complete answers to a large set of prompts, to create additional synthetic text examples, or to label a large corpus of data.", "Meta\u2019s Llama-2 is one of the most advanced open-source large language models (LLMs) available today. With models ranging in size from ", ", it can generate remarkably human-like text across many domains. In this example, we will be using the 7B parameter Llama-2-chat model, which has been fine-tuned and optimized for dialogue applications using Reinforcement Learning Human Feedback (", "), allowing it to achieve comparable performance to ChatGPT across many evaluations. You can read more in the paper ", ".", "Batch prompting unlocks new capabilities for organizations using large language models like Llama-2. By being able to get insights across hundreds or thousands of data points at once, teams can utilize LLMs more efficiently and effectively.", "First, we'll install the Gretel Client and HuggingFace Dataset libraries:", "Import the libraries we need:", "To access the Gretel GPT API for LLM inference we need to log into our Gretel account. You can do this ", " below in a notebook.", "We'll use the ", " (Grade School Math) dataset of 8k real questions to query the LLM. These are problems that take between 2 and 8 steps to solve, primarily performing a sequence of elementary calculations using arithmetic to reach the final answer, creating a good benchmark for assessing LLMs ability to solve everyday real-world multi-step problems.\u00a0", "First we'll load the dataset and extract the first 250 questions:", "Next, we'll format the questions into instructions for Llama-2, adding the ", ":", "Let\u2019s verify our formatting function by printing out a random sample.", "Next, we'll initialize a Gretel synthetic model and configure it to load the Llama 2 chat model:", "With our dataset prepared and Llama-2 model initialized on Gretel, we can now send our batch of questions to prompt the model. On initialization, Gretel's batch APIs load a private container from disk, which can take about 3-5 minutes of processing time to download and initialize the ~13GB model. Therefore, it is most efficient to use the batch APIs when labeling hundreds or thousands of examples. With Gretel operating in either cloud or hybrid mode, you can submit multiple jobs to process datasets in parallel.", "Finally, we can retrieve the results - Llama-2's responses to our batch of questions:", "By leveraging Gretel's platform and Python client library, we can easily prompt large language models like Meta\u2019s Llama 2 Chat model with batches of data for faster, more efficient results compared to prompting query-by-query. Gretel handles scaling out requests across multiple GPUs, monitoring progress, ", " and ", ".", "The full code for this tutorial is ", " and ", ". Try it out yourself to experience prompting LLMs at scale!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel", "page_title": "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "headers": ["How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Key technologies", "What is an Agent in Langchain?", "Generating synthetic tabular data", "Getting started: Installation", "Initializing the Langchain Agent", "Querying the data", "Sample queries", "Importance of privacy: Re-identification attack example", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Agent-based approaches coupled with large language models (LLMs) are quickly transforming how we interact with databases and data warehouses. Combined, these technologies enable natural language queries to data in your application or business, eliminating the need for SQL expertise to interact with data and even facilitating seamless queries across diverse systems.\u00a0", "In this post, we\u2019ll walk through an example of how Langchain, LLMs (whether open-source models like Llama-2, Falcon, or API-based models from OpenAI, Google, Anthropic), and synthetic data from Gretel combine to create a powerful, privacy-preserving solution for natural language data interaction with data in databases and warehouses. We'll introduce key concepts such as Agents, LLM Chains, and synthetic data, then delve into a practical code example to bring these ideas to life.", "Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an \u201cagent\u201d that has access to a suite of tools \u2014 for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.", "Under the hood, the Langchain SQL Agent uses a ", " (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.", "Before diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like ", ". In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.", "To generate your own synthetic data for this example, grab the ", " (or your own) and an API key from ", ". You can run Gretel's ", " or console-based workflow to create a synthetic version of the data.\u00a0", "For this example, I used the Gretel Tabular DP model (", ", ", ") with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (", "), which excels at working with highly dimensional tabular data to enable machine learning and analytics use cases.", "Follow along with our complete ", " or ", ".", "First, install dependencies.", "Note: Please use your OpenAI key for this, which should be kept private.", "Here's the code to initialize the Langchain Agent and connect it to your SQL database.", "Here, we are also importing some sample datasets. We'll use both a real and a synthetic version of the IBM attrition HR dataset. The synthetic version is generated using Gretel's Tabular DP model with an (\u03b5) Epsilon of 5.", "First, we'll create a helper function to compare the outputs of real data and synthetic data.", "The results were quite similar between the synthetic and real datasets, giving us confidence in the synthetic data's reliability.", "Again, the distributions were notably similar between the synthetic and real data sets.", "In this case, we get a perfect match.", "Here, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification \u2014 such as the combination of an attacker who knew someone\u2019s age, gender, and department in the example below.\u00a0", "Synthetic data prevents direct linking of individual information as no record in the output is based on a single user\u2019s data, effectively thwarting re-identification attacks and upholding privacy.", "\u200d", "By using synthetic data, you not only protect privacy but also gain actionable insights\u2014essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.", "This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today's data-centric world, adopting these technologies isn't just an option; it's a must.", "Try synthesizing your own data with this ", "and let us know what you think. ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/training-better-llms-slms-with-diverse-high-quality-synthetic-data", "page_title": "Generate textbook-quality synthetic data for training LLMs and SLMs", "headers": ["Generate textbook-quality synthetic data for training LLMs and SLMs", "Background", "Prerequisites", "Let's get started!", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In the notebook and video walkthrough below, we will leverage ", " to generate new diverse, high-quality training examples building on these techniques to create better LLMs. Our goal with this notebook is to demonstrate how to get started creating high quality synthetic data for LLM training, and facilitate further research into safeguards for completion models.", "Recent research has shown that training small, efficient language models (SLMs) on high-quality, diverse data can achieve state-of-the-art results- even rivaling or surpassing LLMs 5x the size such as Llama2-7b and Falcon-7b on common tasks, as demonstrated by models like Microsoft's \"phi-1.5\" (from their paper \"", "\"), ", ", and ", ". Using similar techniques, we'll demonstrate ways to inject randomness into the prompt in a way that gives rise to the generation of diverse datasets.", "Creating diverse training data is challenging, but vital to reduce overfitting and improve generalization. Techniques like including random word subsets in prompts, as done in ", ", will be used.", "Compared to models trained on web data, \u201c", "\u201d highlights additional advantages from using textbook-like data: \"the model seems to store and access the knowledge more efficiently\" and it has an \"attenuating effect on toxic content generation.\" However, as the authors note, \"although phi-1.5 has a lower propensity for generating toxic content...it is not immune.\" They posit phi-1.5's reliance on synthetic data \"provide[s] a useful platform for exploring these challenges further.\"", "Before diving into the notebook, there are a couple of prerequisites:", "Here's the Colab notebook, and a ", " to guide you.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/gretel-cloud", "page_title": "Gretel Cloud blog posts - Gretel.ai", "headers": ["Gretel Cloud", "What's new in Beta2", "CHANGELOG: Beta2", "Automate Synthetic Data Pipelines with Gretel Workflows", "Introducing Gretel Amplify", "NEW: Integrating with Gretel SDKs just got easier!", "Synthetic Data Configuration Templates", "November 2020 - What\u2019s new in Gretel", "Introducing Gretel Blueprints", "How to use Gretel\u2019s new entity stream", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/bias", "page_title": "Bias blog posts - Gretel.ai", "headers": ["Bias", "Generate time-series data with Gretel\u2019s new DGAN model", "Automatically Reducing AI Bias With Synthetic Data", "Gretel Smart-Seeding is auto-complete for your data", "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/nlp", "page_title": "NLP blog posts - Gretel.ai", "headers": ["NLP", "Synthesizing dialogs for better conversational AI", "Fine-tune a MPT-7B LLM with Gretel GPT", "Conditional Text Generation by Fine Tuning Gretel GPT", "Unlocking Adapted LLMs on Enterprise Data", "Automate Detecting Sensitive Personally Identifiable Information (PII)", "Got text? Use Named Entity Recognition (NER) to label PII in your data", "Innovating With FastText and Table Headers", "Automated Data Exposure Detection with Gretel Outpost", "Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "Exploring NLP Part 1: Why Should a Privacy Engineering Company Care About NLP?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/open-source", "page_title": "Open Source blog posts - Gretel.ai", "headers": ["Open Source", "CHANGELOG: Beta2", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Optuna Your Model Hyperparameters", "We just streamlined Gretel\u2019s Python SDK ", "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Conditional Text Generation by Fine Tuning Gretel GPT", "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "Generate synthetic data in 3 lines of code", "How to use Weights & Biases with Gretel.ai", "Auto-anonymize production datasets for development", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/python", "page_title": "Python blog posts - Gretel.ai", "headers": ["Python", "Contact Tracing: Deep Dive & Simulation", "Anonymize Data with S3 Object Lambda", "How To Create Differentially Private Synthetic Data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/automate-detecting-sensitive-personally-identifiable-information-pii-with-gretel", "page_title": "Automate Detecting Sensitive Personally Identifiable Information (PII)", "headers": ["Automate Detecting Sensitive Personally Identifiable Information (PII)", "Why is continuous detection important?\u00a0", "What kinds of sensitive data can Gretel detect?", "Let\u2019s write some code!\u00a0", "Build a transformation pipeline", "Next steps", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["When you are working directly with customer feedback, application logs, or even public datasets- it is important to identify and protect sensitive information to maintain compliance with standards such as ", " and ", "\u00a0and to earn customer\u2019s trust.\u00a0Many companies discover and label personal data through ", " of their databases and datasets. In today's streaming data pipelines and architectures, the shape and compliance properties of data can change in seconds- whether from a change in data being logged by a developer, to ", " in fields that are not meant for it.", "Gretel\u2019s ", " make it simple to identify, label, and transform sensitive content continuously, ", " and becomes a compliance problem. Let\u2019s get started!\u00a0", "Gretel can currently detect over ", " in both structured and unstructured data including names, addresses, gender, birthday, and credentials. We do this through a combination of regular expression-based detections, custom detectors for entities based on FastText and word embeddings, and support for bringing your own custom named entity recognition models from ", " and", " (coming soon).", "Follow along below, or ", " for a free account and launch our ", " to get started! In this example, we walk through a Blueprint detailing how to label and anonymize free text using Gretel\u2019s NLP and labeling APIs.", "First, connect to Gretel\u2019s API service.", "Next, we need a sample dataset containing PII into Pandas. Let\u2019s use the Enron email dataset from ", "s excellent ", " library.", "Create a temporary project in Gretel for labeling data.", "Now, iterate and display the labeled text stream as HTML.", "After labeling the dataset, we've identified chats that contain PII, such as names and emails. The final step in this blueprint is to build a transformation pipeline that will replace names and other identifying information with fake representations of the data.", "We make a point to replace rather than redact sensitive information. This preservation ensures the dataset remains valuable for downstream use cases such as machine learning, where the structure and contents of the data are essential.", "To learn more about data transformation pipelines with Gretel, check our ", " or ", ".", "Configure the pipeline. FakeConstantConfig will replace any entities configured under labels with a fake version of the entity.", "Run the pipeline to redact any sensitive strings.", "Inspect a transformed email from the dataset.", "Thanks for taking the time to read this article. Hopefully, after following along, you\u2019ve seen how it\u2019s possible to label and anonymize sensitive streams of data in real-time.", "\u200d", " for a free Gretel account to access our premium SDKs and get started in seconds with the code above in Jupyter notebook format below- ", "\u00a0We're in private beta and would love to hear from you! The fastest way to reach our team is via our community ", ", email us at ", " or ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/ner", "page_title": "NER blog posts - Gretel.ai", "headers": ["NER", "Load NER data into Elasticsearch", "Automate Detecting Sensitive Personally Identifiable Information (PII)", "Got text? Use Named Entity Recognition (NER) to label PII in your data", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/helping-organizations-build-resilient-and-trustworthy-information-technology", "page_title": "Helping Organizations Build Resilient and Trustworthy Information Technology", "headers": ["Helping Organizations Build Resilient and Trustworthy Information Technology", "A company philosophy founded on privacy and security", "Why responsible use of data is critical", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["As a multi-modal synthetic data platform that supports the safe scaling of state-of-the-art AI applications in the enterprise, Gretel puts a lot of focus on privacy and security. Our founders have spent much of their careers building information ecosystems that are resilient to cybersecurity and privacy attacks. So it\u2019s no surprise that ensuring the integrity, confidentiality, and safe handling of every one of our partners\u2019 data is a core pillar of our company\u2019s philosophy. Today, as part of this commitment, we\u2019re proud to announce we\u2019ve received our SOC2 Type 2 certification, confirming that our security safeguards can support customers with strict security requirements.\u00a0", " is a set of criteria developed by the American Institute of Certified Public Accountants (AICPA) that defines security standards for service organizations. These standards cover best practices in security, availability, processing integrity, and confidentiality, which are collectively known as trust services criteria. SOC 2 is broken down into two types of reports: Type 1 and Type 2. Type 1 evaluates the design of security measures at a given point in time, while Type 2 reports on the effectiveness of those controls over a period of time. Together, they provide assurances that help build trust and confidence in information systems.\u00a0", "Building trust and confidence among customers, employees, partners, and the public, is critical for modern businesses that are developing data-driven products and services. As more organizations adopt machine learning and AI techniques, security challenges are only becoming more complex, and necessary to address. As our CEO Ali Golshan put it, ", ". In that spirit, Gretel will continue to set the bar for security and privacy protections, so we can support the next generation of safe AI technology.\u00a0", "If you\u2019d like to learn more about Gretel\u2019s security and privacy practices, please visit ", ".", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/drew-newberry", "page_title": "Author: Drew Newberry - Gretel.ai", "headers": ["Drew Newberry", "Build a synthetic data pipeline using Gretel and Apache Airflow", "Auto-anonymize production datasets for development", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/optuna-your-model-hyperparameters", "page_title": "Optuna Your Model Hyperparameters", "headers": ["Optuna Your Model Hyperparameters", "Optuna Functionality", "Optuna Visualization", "Handy Optuna Commands", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In machine learning, hyperparameter tuning is the effort of finding the optimal set of hyperparameter values for your model before the learning process begins. ", " is an awesome open-source framework for automating this task. There are many other popular open-source hyperparameter optimization frameworks out there such as Hyperopt, Scikit-Optimize and Ray-Tune. What I love most about Optuna, though, is its overall ease of use and specifically its easy parallelization. In this article, we walk you through how to tune multiple models and run multiple Optuna trials all in parallel and from the same Jupyter notebook. You can follow along with Gretel.ai\u2019s notebook located ", ".", "Optuna uses the notion of studies and trials.\u00a0 A study contains all the information surrounding the tuning of a specific model. A trial is a specific model training task trying out a specific set of model hyperparameters. Each trial calls the same objective function, which specifies the hyperparameters and their ranges that you\u2019d like to tune and returns a value (such as model accuracy) that you\u2019d like to optimize.", "In our ", ", we tuned multiple Gretel.ai synthetic models and optimized each using our ", " (SQS). We then told Optuna to use an SQLite database (pre-packaged with most operating systems) to enable the running of multiple Optuna trials in parallel. Next, we placed the code to instantiate an Optuna trial in a Python module, which then enabled the Jupyter notebook to execute the module as many times in parallel as we\u2019d\u00a0 like.", "Below is the contents of our objective function:", "Note how at the beginning of the objective function we use the trial.suggest* functions to specify which parameters we\u2019d like to tune and over what values. The trial.suggest_int and trial.suggest_float will let you specify a range and then how you\u2019d like to explore that range.\u00a0 The trial.suggest_categorical will let you specify a list of categorical values for Optuna to try. You can read more about these and other Optuna options ", ". After specifying which hyperparameters you\u2019d like to tune, our objective function then trains a synthetic model, polls for completion and then returns our SQS score. This function exists in our external Python module, so we can run it multiple times.", "In our main Jupyter notebook, we start by reading in the default synthetic configuration file, which contains all the default hyperparameter values needed to train a model:", "We then create a function to instantiate an Optuna study:", "The very first thing we do in the above function is instantiate a new Optuna study using \u201coptuna.create_study\u201d. This function takes as input the study name you\u2019d like to use, a link to your SQLite database and whether you\u2019d like to maximize the output of your objective function or minimize it. You don\u2019t need to worry about creating the SQLite database, if it\u2019s not there Optuna will create it. The function also optionally takes in a specification of which Optuna optimizer you\u2019d like to use. If you\u2019re just starting out, a good rule of thumb is to use the Random optimizer if you have a lot of computing resources, otherwise, the default TPE optimizer is a good option. You can read more about the optimizers available ", ".", "Next, we use Optuna\u2019s \u201cstudy.enqueue_trial\u201d method to tell Optuna to try the hyperparameters in our default configuration first. Oftentimes I\u2019ll also queue up other configurations I\u2019d like Optuna to specifically try as well. You can view Gretel\u2019s recommended configurations for different dataset characteristics ", ". We then use \u201csubprocess.Popen\u201d to execute the Python module that will begin the Optuna trials. We repeat this for the number of trials we want to run in parallel, each time telling it how many trials each process should execute.", "The main processing flow in our Jupyter notebook is then to read in the filenames of the datasets we\u2019d like to tune models on, then call our above \u201ccreate_study\u201d function for each dataset:", "We can then watch for study completion using this snippet of code:", "The Jupyter notebook then contains some handy code for gathering all the Optuna results into one dataframe if you\u2019d like to study the tuning history in-depth. You can refer to our notebook to see how this works.", "Optuna also contains a plethora of ", " to both monitor the progression of your tuning as well as assess the results upon completion. I like to use the \u201cplot_optimization_history(study)\u201d command to watch the optimization history of a study while waiting for it to complete.", "Upon completion, I usually first use the \u201cplot_param_importances(study)\u201d command to study which hyperparameters were most influential in the tuning.", "Another insightful graph is the parallel coordinates one using \u201cplot_parallel_coordinate(study)\u201d.", "You can also use the \u201cplot_slice(study)\u201d command to view how each specific hyperparameter homed in on an optimal value, and the \u201cplot_contour(study)\u201d command to look at the relationship between hyperparameter values.", "There are a variety of handy Optuna commands I like to use either while a study is progressing or once it\u2019s complete. Here are a few of my favorites:", "Tuning a model\u2019s hyperparameters can be a challenging and time-consuming process, yet often necessary if you want to build the most accurate model possible. Optuna is an easy to use and very effective framework for expediting this process. Here at Gretel, we have a series of preset hyperparameter configurations tuned for specific dataset characteristics. If you have an unusual dataset that doesn\u2019t quite fit one of these configurations, or you\u2019d like to make sure you\u2019re building the best synthetic model possible, go ahead and give Optuna a try. It\u2019s simple, fast, and very effective. ", "Please feel free to reach out to ", " at Gretel.ai with any questions.\u00a0 Thank you for reading!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/qiong-zhang", "page_title": "Author: Qiong Zhang - Gretel.ai", "headers": ["Qiong Zhang", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/create-synthetic-time-series-with-doppelganger-and-pytorch", "page_title": "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "headers": ["Create Synthetic Time-series Data with DoppelGANger and PyTorch", "Intro", "DoppelGANger model", "Sample usage", "Results", "Runtime", "Conclusion", "Credits", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Time series data, a sequence of measurements of the same variables across multiple points in time, is ubiquitous in the modern data world. Just as with tabular data, we often want to generate synthetic time series data to protect sensitive information or create more training data when real data is rare. Some applications for synthetic time series data include sensor readings, timestamped log messages, financial market prices, and medical records. The additional dimension of time where trends and correlations across time are just as important as correlations between variables creates added challenges for synthetic data.", "Many models have been published in recent years for synthetic time series. One of our favorites is DoppelGANger, from the excellent paper ", " by Lin et al. Wanting more flexibility in utilizing this model, we created an open-source implementation in PyTorch.", "In this article, we give a brief overview of the DoppelGANger model, provide sample usage of the PyTorch ", ", and demonstrate excellent synthetic data quality on a task synthesizing daily wikipedia web traffic with a ~40x runtime speedup compared to the paper\u2019s TensorFlow 1 ", ".", "DoppelGANger is based on a generative adversarial network (", ") with some modifications to better fit the time series generation task. As a GAN, the model uses an adversarial training scheme to simultaneously optimize the discriminator (or critic) and generator networks by comparing synthetic and real data. Once trained, arbitrary amounts of synthetic time-series data can be created by passing input noise to the generator network.", "In their ", ", Lin et al. review existing synthetic time series approaches and their own observations to identify limitations and propose several specific improvements that make up DoppelGANger. These range from generic GAN improvements, to time-series specific tricks. A few of these key modifications are listed below:", "A small note on terminology and data setup. DoppelGANger requires training data with multiple examples of time series. Each ", " consists of 0 or more ", " values, fixed variables that do not vary over time, and 1 or more ", " that are observed at each time point. When combined into a training data set, the examples look like a 2d array of attributes (example x fixed variable) and a 3d array of features (example x time x time variable). Depending on the task and available data, this setup may require splitting a few long time sequences into shorter chunks that can be used as the examples for training.", "Overall these modifications to a basic GAN provide an expressive time series model that produces high-fidelity synthetic data. We are particularly impressed with DoppelGANger\u2019s ability to learn and generate data with temporal correlations at different scales, such as weekly and yearly trends. For full details on the model, please read the excellent ", " by Lin et. al.", "Our PyTorch implementation supports 2 styles of input (numpy arrays or pandas DataFrame) plus a number of configuration options for the model and we provide an example of both using random training data. For full reference documentation, see ", ".", "First, install the pypi package containing the implementation:", "Once installed, the simplest way to use our model is with your training data in a pandas DataFrame. For this setup, the data must be in a \u201cwide\u201d format where each row is an example, some columns may be attributes, and the remaining columns are the time series values. The following snippet demonstrates training and generating data from a DataFrame.", "If your data isn\u2019t already in this \u201cwide\u201d format, you may be able to use the ", " method to convert it to the expected structure. The DataFrame input is somewhat limited currently, though we have plans to support other ways of accepting of time series data in the future. For the most control and flexibility, you can also pass numpy arrays directly for training (and similarly receive the attribute and feature arrays back when generating data), demonstrated below.", "Runnable versions of these snippets are available at ", ".", "As a new implementation that switches from TensorFlow 1 to PyTorch (with potential differences in underlying components such as optimizers, parameter initialization, etc.), we want to confirm our PyTorch code works as expected. To do this, we\u2019ve replicated a selection of results from the original paper. Since our current implementation only supports fixed-length sequences, we focus on a data set of wikipedia web traffic (WWT).", "The WWT data set, used in Lin et. al. and originally from ", ", contains daily traffic measurements to various wikipedia pages. There are 3 discrete attributes (domain, access type, and agent) associated with each page and a single time series feature of daily page views for 1.5 years (550 days). See Image 1 for a few example time series from the WWT data set.", "Note the page views are log scaled to [-1,1] based on min/max page views across the entire data set. The training data of 50k pages we used in our experiments (already scaled) is available as a ", ".", "We present 3 images showing different aspects of the fidelity of the synthetic data. In each image, we compare the real data with 3 synthetic versions: 1) fast PyTorch implementation with larger batch size and smaller learning rate, 2) PyTorch implementation with original parameters, 3) TensorFlow 1 implementation. In Image 2, we look at the distribution of attributes where the synthetic data is a close match to the real distributions (modeled after Figure 19 from the ", ").", "One of the challenges with the WWT data is that different time series have very different ranges of page views. Some wikipedia pages consistently receive lots of traffic, while others are much less popular, but occasionally get a spike due to some relevant current event, for example, a breaking news story related to the page. Lin et. al. found that DoppelGANger is highly effective at generating time series on different scales (Figure 6 of the original paper). In Image 3, we provide similar plots showing the distribution of time series midpoints. For each example, the midpoint is halfway between the minimum and maximum page views attained over the 550 days. Our PyTorch implementation shows similar fidelity for the midpoints.", "Lastly, traffic to most wikipedia pages exhibits weekly and yearly patterns. To evaluate these patterns, we use autocorrelation, that is, Pearson correlation of page views at different time lags (1 day, 2 days, etc.). Autocorrelation plots for the 3 synthetic versions are shown in Image 4 (similar to Figure 1 of the original paper).", "Both PyTorch versions produce the weekly and yearly trend as observed in the original paper. The TensorFlow 1 results don\u2019t match Figure 1 of Lin et al. exactly as the above plots are from our experiments. We observed somewhat inconsistent training using the original parameters where the model occasionally does not pick up the yearly (or even weekly) pattern. The lower learning rate (1e-4) and larger batch size (1000) used in our fast version makes retrainings more consistent.", "Analysis code to produce the images in this section and to train the 3 models are shared as notebooks on ", ".", "Last but not least, a crucial aspect of more complex models is runtime. An amazing model that takes weeks to train is much more practically limited than one that takes an hour to train. Here, the PyTorch implementation compares extremely well (though as the authors note in their paper, they did not do performance optimization on the TensorFlow 1 code). All models were trained using the GPU and ran on GCP n1-standard-8 instances (8 virtual CPUs, 30 GB RAM) with an NVIDIA Tesla T4. Going from 13 hours to 0.3 hours is crucial for making this impressive model more useful in practice!", "Check out the PyTorch ", " of the DoppelGANger time series model in the open-source ", " library. We showed this implementation produces high-quality synthetic data and is substantially faster (~40x) than the previous TensorFlow 1 implementation. Love the model? Please leave a \u2b50\ufe0f on our ", ", and let us know in ", " if you have any questions!", "Thanks to the authors of the excellent DoppelGANger paper: ", " by Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, Vyas Sekar. And we\u2019re especially grateful to Zinan Lin for responding to questions about the paper and TensorFlow 1 code.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/data-simulation", "page_title": "Data Simulation: Tools, Benefits, and Use Cases", "headers": ["Data Simulation: Tools, Benefits, and Use Cases", "What Is Data Simulation?", "Benefits of Data Simulation", "Data Simulation vs. Data Interpolation", "Data Simulation", "Data Interpolation", "Use Cases for Data Simulation", "Data Science & Research\u00a0", "Software Development", "Oil & Gas", "Manufacturing", "Autonomous Vehicles", "How Do You Simulate Data?", "Form a hypothesis and understand your sample distribution", "Generate Random Data Samples", "3. Create a Histogram and Analyze Your Distribution", "Generate Simulation Data With Gretel.ai", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Data simulation is the process of taking a large amount of data and using it to mimic real-world scenarios or conditions. In technical terms, it could be described as the generation of random numbers or data from a stochastic process which is stated as a distribution equation (e.g., Normal: X~N(\u03bc, \u03c3\u00b2)). It can be used to predict future events, determine the best course of action or validate AI/ML models.", "This post will highlight the many benefits and applications of data simulation as well as how synthetic data can power and improve its utility.\u00a0", "Data simulation has proven highly valuable across nearly every industry and field of study, with business executives, engineers and researchers all using it in their work. Among other benefits, data simulation can:", "To some technical readers, these benefits may sound like those gained from data interpolation, so it\u2019s worth briefly discussing the distinction.\u00a0", "When working with data, it is often necessary to make estimates about what lies outside the scope of the dataset. There are two main methods for doing this: simulation and ", ". Both methods have their advantages and disadvantages, and the best approach depends on the specific situation.", "Simulation can be thought of as an imitation of a real-world process over time. In machine learning, a simulation is an algorithm that mimics a real-world environment that can be used to test different courses of action. While simulations can never be as accurate as their real-world equivalents, the ability to test millions of scenarios against a simulation has been shown to help machine learning models learn quickly, and even out-perform models trained on more limited real world data. However, simulation is only as accurate as the underlying model it is based on, so it is important to have a good understanding of the model before using this approach.", "Data interpolation involves using known data points to estimate values for points in between. This can be done using mathematical functions or by making comparisons with similar datasets. One advantage of interpolation is that it is typically more accurate than simulation since it is based on actual data points. However, interpolation can only be used when there is a close relationship between the variables, so it may not be appropriate in all situations.", "With the advent of high-quality synthetic data generation technology and state-of-the-art ML/AI models, there are some fascinating use cases for data simulation that have emerged in recent years. Here are some specific examples across various fields:", "One of the biggest roadblocks to building better machine learning models is the constant need for more and new data. As an example, let\u2019s say we are working as a data scientist for a local authority and we are asked to find a way to optimize how an emergency evacuation plan should work in the case of a natural disaster (e.g. tsunami or earthquake). Our data would likely be too imbalanced to be useful since there are few similar events that we have data for to use as a comparison.\u00a0", "Here, data simulation could be used to generate synthetic datasets that looked like those of other real-world natural disasters. This would allow our machine learning model to be trained on data that is more representative of these rare real-world events, and thus better inform our evacuation plan.\u00a0", "These methods can be used for studying how technology might impact societies, too. For example, in 2020, Gretel conducted simulations to analyze what results Americans might expect from Apple & Google\u2019s hotly debated ", " proposal, where they\u2019d provide privacy-preserving capabilities that inform people when they have been in close proximity to a COVID-19 infected person. The project code is ", ".\u00a0", "The big takeaway is Apple and Google\u2019s specification substantially lowers privacy risk for contact tracing by not collecting existing data, but by generating new custom data specific to the use case. This privacy preservation happens at the lowest possible level. The handset can compute as much data as possible but share as little as possible in order to provide effective contact tracing while preserving user privacy.", "A key part of developing any software is testing how it will perform under different conditions. By creating data simulations that mimic real-world conditions, developers can put the software through its paces and identify any potential problems. This process can be used to test everything from the user interface to the backend algorithms.\u00a0", "Data simulation is increasingly being used in the oil and gas industry, too. By creating models of reservoirs, geologists can better understand how oil and gas flow through rock and whether they\u2019re present in different geological strata. These models can be used to predict what will happen when new wells are drilled, and they can help engineers design better production facilities, too.\u00a0", "Companies and researchers also study the impact of environmental factors on the industry. By simulating the effects of climate change, researchers gain better understanding of how rising temperatures might affect the production of oil and gas.\u00a0", "Data simulation is also being used to create \u201c", "\u201d which are virtual copies of physical objects, such as a car or production factory. These models enable the study of real-world objects and their operations without ever touching them. Manufacturers can easily identify the most efficient and effective production process for a particular product, and avoid disruptions as they transition to new methods.\u00a0", "And of course, we can\u2019t talk about data simulation without acknowledging its most high-profile use case: the training of self-driving cars, drones and robots. Trying to test and train these systems in the real-world is slow, costly and dangerous. But with synthetic data you can create virtual training environments for improving these emerging technologies.", "Simulations like this give us a glimpse of the future, so we can better prepare for it today. But to make good predictions, simulations must rely on a wealth of past experience (in the form of raw data) to learn patterns. That\u2019s where using synthetic data can help.\u00a0", "In order to simulate data,\u00a0 we need to identify the data patterns and also have information about the data features and how they are distributed.", "In general, there are three main steps for simulating data:\u00a0", "Before you simulate a data distribution, you first need to have a hypothesis regarding your data. This is an idea about what you think might happen when you generate your data simulation. Some distributions don\u2019t need hypothesis testing to know that the data would follow certain paths, but most are necessary.\u00a0", "There are a variety of distributions that can be analyzed when utilizing data simulation, and each type has its own unique characteristics for modeling different types of data. By understanding different distributions, researchers can better utilize data simulation to study a variety of phenomena.", "Below are descriptions of a handful of common distributions:", "Random sampling is a process used to select a group of individuals from a population in which each individual has an equal chance of being selected. Now that we understand our sample distribution we can use that function to produce a random data sample. This is the most common approach. However, there are many ways to generate random simulation data, and the method that is used will depend on the type of research that is being conducted. Let\u2019s look at two popular methods:\u00a0", "Monte Carlo Simulation is done by taking multiple random samplings from a given set of probability distributions. The distribution can be of any type, e.g.: Normal, Exponential, Uniform, etc.\u00a0", "Monte Carlo simulations are used to analyze risk in projects before they're put into practice. It builds on mathematical models, which utilize empirical data from real system input and output values (e.g., number of supplies intake vs production yield). This allows for the early identification of potential threats and what might go wrong that could impact your bottom line or time-to market goal.", "Markov Chain Monte Carlo sampling is a simulation technique from high-dimensional probability distributions. Unlike Monte Carlo sampling methods that are able to draw independent samples from a distribution, Markov Chain Monte Carlo methods draw samples where the next sample is dependent on the existing sample. It means the randomly generated sample only relies on the current state, and so progresses iteratively, as a \u201cchain\u201d of events unfolds.\u00a0", "Gretel used a form of Markov Chain simulation when creating simulations of e-bike location data across cities, in an effort to predict traffic patterns. The chart below is an example of simulated e-bike data. You can read more about this case study and how you can simulate location data in this ", ".\u00a0", "A histogram is a graphical representation of data that shows how often certain values occur. Histograms are used to summarize large data sets, and they are often used in statistical analysis. For example, a histogram of test scores might show you how many students scored in each grade range. The histogram would tell us what the most common score was, how many students scored above average, and so on. Histograms can be helpful in understanding datasets and in making decisions about statistical analysis.\u00a0", "Data simulation is a powerful tool for understanding and solving complex problems. It enables you to better understand how your data behaves and how dynamic systems respond to changing variables. Whether you are trying to forecast the next pandemic or bear market or even train an AI model to walk or drive, you can use data simulation to improve your results.", "If you are interested in learning more about data simulation or want to try it out for yourself, Gretel.ai is here to help. With our platform, you can generate data simulations in minutes and see the results for yourself. Sign up for free and ", "!", "If you have any questions, you can join our Slack community and raise them there, or reach out to us directly at ", ". Thanks for reading!\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/rumi-olsen", "page_title": "Author: Rumi Olsen - Gretel.ai", "headers": ["Rumi Olsen", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/llms", "page_title": "LLMs blog posts - Gretel.ai", "headers": ["LLMs", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "Filling in sparse tables with Gretel\u2019s Tabular LLM", "Nail Synthetic Data Generation Every Time with Gretel Tuner ", "Generate textbook-quality synthetic data for training LLMs and SLMs", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/beta2-privacy-engineering-as-a-service", "page_title": "What's new in Beta2", "headers": ["What's new in Beta2", "Making privacy engineering accessible to anyone", "It's easy to use ", "Privacy built-in by design", "Run your workloads anywhere", "Our blogs and use cases are growing up", "What is going away?", "Are there service limits?", "How can I get started?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Today we are excited to announce launching Beta2 for Gretel to the world on July 14th 2021. Together, we hope to automate privacy engineering as a service for anyone, and early users like you will be critical in defining what Gretel.ai becomes, so please don\u2019t be shy with your feedback, good or bad. Most importantly, thank you for your support, and we hope you stop by to say hi in our ", ". ", "In the meantime, here are some of the new features that we are most excited about!\u00a0", "With Beta2, you can get started creating synthetic data and automating your privacy engineering use cases in minutes. Currently, ", " is completely free, but for the time being is limited to a single workload at a time and workloads are limited to a one hour duration. ", "The ", " (CLI) becomes the primary method for interfacing with Gretel\u2019s APIs- similar to AWS, GitHub, and Terraform. There is no longer any need to write code to label, transform, or synthesize data. Instead, anyone can now edit a template YAML configuration file (or use one of our defaults) and submit tasks directly to Gretel. ", "All you need to do is point to your datasets in CSV format, modify a ", ", and submit your job through either our CLI or web-based Console. Labels and transformations are delivered within seconds, and synthetic models can be trained to generate completely safe and artificial datasets in minutes. ", "Beta2 implements three levels of controls to protect users' privacy in your synthetic data that are enabled by default, with support for optionally enabling ", " during model training for formal, mathematical guarantees around privacy.", "Security and compliance are top of mind, and some customers have regulatory and compliance requirements that the data in their cloud must remain in their cloud. With Beta2, users can choose to scale out their workloads in the Gretel cloud or deploy workers directly into their environment. When running in local mode, your ", ".", "Over the past year, we have published over 30 blogs and code examples for top use cases we see in the space. We identified the most popular use-case blogs below and have promoted them to tutorials on our new ", "! ", "Moving to an API-first approach requires us to deprecate APIs and sunset certain features from Beta1. Here is a comprehensive list of features and deprecation timelines. Do you have any questions about what is being removed? Don\u2019t hesitate to reach out.", "As we continue our commitment to the developer community, we will continue to provide a full-featured freemium offering. Read ", ".", "You can ", ". When Beta2 launches, anyone will be able to install our new CLI tool or start working with our new web console to create safe data. Gretel pricing will be released early next year when we exit our beta period and move to a general availability release. ", "Do you have questions about Beta2 or ideas for how we can improve our services? If so, please reach out to us on Twitter ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/generate-synthetic-taylor-swift-lyrics-using-gretel-gpt", "page_title": "Generate synthetic Taylor Swift-like lyrics using Gretel GPT", "headers": ["Generate synthetic Taylor Swift-like lyrics using Gretel GPT", "Load and preview training data", "Model configuration", "Train the GPT model", "Generate lyrics", "View results", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["As tickets go on sale for ", ", we eagerly await the opportunity to see our favorite artist perform live after four long years. Unlike her previous tours, which centered around a single album, the Eras Tour will range over Swift\u2019s entire catalog of over 200 songs. Despite ", " being released just a few weeks ago, we simply can\u2019t get enough. So, we decided to make some more lyrics by using Gretel GPT to generate synthetic Taylor Swift lyrics.", "Gretel GPT is our language synthetics model designed to enable generation of natural language text. The underlying model is a generative pre-trained transformer (GPT) designed using an open-source implementation of OpenAI\u2019s GPT-3 architecture. GPT can be used to create high quality, coherent text \u2026 and in this case, Taylor Swift lyrics. Since large-scale language models may produce explicit content, we recommend having a human (ideally Taylor herself) curate or filter the outputs, both to censor undesirable content and to improve the quality of the results. Craving more Taylor as you wait for the Eras tour or for her next album release? Follow along with our ", " and generate your own synthetic Taylor Swift lyrics!", "After logging in with your Gretel API key, we\u2019ll preview our training data. GPT accepts a single-column file with natural language data such as reviews, tweets, or conversations. Here, our training data is a single-column file containing the lyrics to Taylor Swift songs. ", "The first row of the training data contains the lyrics to the record-breaking hit \u201cAll Too Well (10 Minute Version) (Taylor\u2019s Version) (From The Vault).\u201d", "Before we get distracted thinking about how Jake Gyllenhal needs to give Taylor her scarf back \u2026 let's move on to model configuration.", "The next step in our synthetic lyric quest is creating a configuration file for our model. The configuration file is a set of instructions that tells the model how to train. Before training on your data, Gretel GPT loads the model specified by the pretrained_model parameter. The pretrained_model can be any valid GPT model in the", ". In this notebook, we\u2019ll load ", " as our pre-trained model and train the model using real Taylor Swift lyrics to generate our synthetic lyrics.", "Creating and submitting our model for training takes just a few lines of code. Our model config is the file we defined above, and our data source is the training data containing real Taylor Swift lyrics.\u00a0", "Now, our GPT model has begun training! We can use the poll(model) function to track its progress.\u00a0", "Once the model finishes training, it's time to generate some Taylor Swift lyrics. Gretel GPT supports both unconditional and conditional generation modes. If you\u2019re interested in learning more about conditional text generation, check out ", "on the topic. For this example, we will use unconditional generation to really give synthetic Taylor full artistic control.", "Once again, we use the poll function to track the model\u2019s progress.\u00a0", "Now that generation has completed successfully, we can take a look at our results.\u00a0", "While clearly not up to the writing skills of Ms. Swift (can anything be?), Gretel GPT produced some Swift-esqe lyrics. The synthetic lyric \u201cMy husband\u2019s a thief and a murderer\u201d gives similar vibes to ", "\u2019s murder mystery song \u201cno body, no crime.\u201d \u201cI\u2019m down here in a pub\u201d reminds me of ", " \u201cLondon Boy.\u201d\u00a0", "Some other lyrics we generated include:", "We\u2019re excited to see your results! Tweet us @gretel_ai with your best synthetic Taylor lyrics and the hashtags #TSTheErasTour and #SyntheticTaylor", "for a chance to win a free Gretel t-shirt.\u00a0", "What should we try next? Perhaps we'll generate Taylor Swift album covers using our upcoming image synthetics. If you\u2019d like to explore other sample notebooks, check out ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/alex-watson?b2715911_page=2", "page_title": "Author: Alex Watson - Gretel.ai", "headers": ["Alex Watson", "Conditional Text Generation by Fine Tuning Gretel GPT", "Unlocking Adapted LLMs on Enterprise Data", "Scale Synthetic Data to Millions of Rows with ACTGAN", "Automate Detecting Sensitive Personally Identifiable Information (PII)", "How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "Creating Synthetic Time Series Data for Global Financial Institutions \u2013 a POC Deep Dive", "Generate synthetic data in 3 lines of code", "Data Is More Valuable When It Can Be Shared", "Conditional data generation in 4 lines of code", "Walkthrough: Create Synthetic Data from any DataFrame or CSV", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/bryan-zimmer", "page_title": "Author: Bryan Zimmer - Gretel.ai", "headers": ["Bryan Zimmer", "Helping Organizations Build Resilient and Trustworthy Information Technology", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/filling-in-sparse-tables-with-gretels-tabular-llm", "page_title": "Filling in sparse tables with Gretel\u2019s Tabular LLM", "headers": ["Filling in sparse tables with Gretel\u2019s Tabular LLM", "Synthesizing Product Listings for E-Commerce", "Prerequisites", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Sparse or incomplete data is one of the most common bottlenecks to scaling effective\u00a0 data pipelines. Often it is not the absolute lack of data, but the lack of consistently high quality data, which impedes data science and ML workflows. The generative capabilities of large language models (LLMs) appear to be a promising solution.\u00a0", "LLMs are trained to create contextually relevant and sometimes even novel information. The rapid rise of LLMs applied for natural language generation has shown models capable of producing high quality responses to contextual prompts. At Gretel, this inspired a question \u2013 can we extend these approaches to solve for the intersections of attention and context that are present in tabular datasets?", "Working with tabular data introduces a unique set of challenges. Tabular datasets often\u00a0 represent a complex set of relationships, with each cell in a table holding multiple relationships with adjacent cells as well as at the row and column-level. Maintaining the table's structured format while reproducing the field and row-level correlations for numerical, categorical, and free text data types is hard to recreate in synthetically generated data.\u00a0 It\u2019s why your ", " (Levin et al., 2022). The task demands a tailormade solution, built from the ground up to work specifically with tabular data. In this blog, we\u2019ll explore one approach using Gretel\u2019s Tabular LLM.\u00a0", "In this ", " and ", ", we will use Gretel\u2019s Tabular LLM to generate contextually relevant values for a sparse tabular dataset. Our dataset will be product listings for shoes, which represents multiple data constraints that we will want reflected in our synthetic insertions. For instance, our generated data should reflect the fact that US shoes come in a small range of whole and half sizes. Product descriptions should be additive and relevant to the given product, so too should any color options listed In our walkthrough we will contextually fill in these missing values, while respecting data validity constraints and providing relevant results that can be validated by a human observer (you!).\u00a0", "For our walkthrough we will work with a row-wise representation of tabular data. Let\u2019s first consider what is needed to effectively complete a missing field. The first constraints are expectations of the data encoded by that column. The column wise constraints will vary based on the type of data in that column. The synthetically generated field will also need to cohere with the observations encoded across its\u2019 row. It will need to make sense with those other observations. When working with Gretel models, we often evaluate how the aggregated results (including synthetic additions) maintain the statistical distribution of the original data set, and the cross-correlations between the fields in the tabular data set. There are column wise, row wise, and distribution wise contexts to maintain and operate within.", "With all this additional context in the sparse tabular use-case, we will get higher quality results from an LLM which was ", ".\u00a0", "Before diving into the notebook, there are a couple of prerequisites:", "Let's get started!", "Here's the ", ", and a ", " to guide you.", "In a tabular context, each of these possible constraints may become a reason to reject our synthetic results. When results don\u2019t meet our expectations, prompt engineering becomes a way to iterate on producing a better result. Prompt engineering is a popular way to drive better results from an LLM, but is often a very human and manually intensive process. For enterprise applications, there needs to be automations within a data pipeline to scale. In a follow up post, we\u2019ll explore the use of ", " techniques that enable users to validate micro batches of data as they go, while still quickly generating and processing a bulk dataset.\u00a0", "We recently made access to Gretel's Tabular LLM early preview ", ", so if you\u2019re interested in testing this example or experimenting with your own applications, you can ", ". If you have specific use cases you\u2019d like us to explore in this series, drop us a note in the Synthetic Data Discord Community or email us at ", "\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/data", "page_title": "Data blog posts - Gretel.ai", "headers": ["Data", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Why Nonprofits Should Care About Synthetic Data", "Comprehensive Data Cleaning for AI and ML", "Gretel and Google Cloud partner on synthetic data", "Generate Synthetic Databases with Gretel Relational", "Transforms and Synthetics on Relational Databases", "Generate synthetic data in 3 lines of code", "Introducing Gretel Amplify", "Conditional data generation in 4 lines of code", "Anonymize Data with S3 Object Lambda", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/amy-steier", "page_title": "Author: Amy Steier - Gretel.ai", "headers": ["Amy Steier", "Machine Learning Accuracy Using Synthetic Data", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Optuna Your Model Hyperparameters", "Comprehensive Data Cleaning for AI and ML", "Transforms and Synthetics on Relational Databases", "How accurate is my synthetic data?", "Automatically Reducing AI Bias With Synthetic Data", "Innovating With FastText and Table Headers", "Transforms and Multi-Table Relational Databases", "Introducing Gretel's Privacy Filters", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/how-to-safely-work-with-another-companys-data", "page_title": "How to safely work with another company's data", "headers": ["How to safely work with another company's data", "Scenario", "Linkage Attack", "The Synthetic Data Defense", "CTGAN Model", "Downstream Machine Learning", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Machine learning drives technologies with wide-ranging impact in the contemporary world and data drives machine learning. Access to data is essential, but complicated given security and privacy. This is especially true in consulting where access to any data requires interfacing with the customer and their systems, and satisfying their security and privacy requirements.\u00a0", "Even with top-of-the-line data security measures in place, clients are regularly concerned that without taking proper privacy preserving precautions, you may accidentally open them up to various attacks that can compromise their sensitive data. One precaution is for clients to provide synthetic data, rather than real data, for consultants to use and analyze. Consultants can even propose this as an option for their clients who are particularly concerned about security and privacy.", "In this blog we describe a hypothetical attack on a customer's private data and showcase how a consultant might use synthetic data to eliminate this risk while still fulfilling the client\u2019s requirements for predictive machine learning models.", "In our scenario, we\u2019re a consultant working with a large grocery store to build a predictive model of customer purchasing behavior. The predictive model will be a classification algorithm that takes information about a customer\u2019s shopping cart and predicts how likely they are to purchase a frozen pizza. The grocery store wants to target select customers, sending them a coupon to encourage them to buy the pizza and increase sales.", "The dataset\u2019s columns have items a customer plans to purchase, and each value is the number of such items in a customer\u2019s cart. In this scenario, we\u2019re tasked to use this data to build a predictive model for the grocery store.\u00a0", "It\u2019s important to note that in addition to the cart information, the data includes three columns that may appear innocuous but are potentially a broken stone in our privacy pyramid.\u00a0", "The columns ", " appear like simple informative features that can help us predict a shopper\u2019s behavior. However, if an attacker with access to the data combined that information with an external dataset of location data they could associate personal information with potentially sensitive shopping information.", "This attack is called a linkage attack, and involves an attacker linking information from one dataset to another while using that link to discover personal information from one or both datasets.", "Fortunately, privacy preserving synthetic data allows us to use this sensitive information for building the predictive model while ", ".\u00a0", "This dataset is sparse (many empty, missing, or zero values) with a large number of categorical columns. For this dataset, we will use the CTGAN model provided through Gretel\u2019s APIs to create an artificial, synthetic version of the real world data. In general, CTGAN is a good choice when the data is high dimensional, meaning it contains many columns. When combined with ", ", CTGAN can provide strong protections against data linkage attacks, even when working with high dimensional datasets. Note that other models, like Gretel\u2019s LSTM, also provide privacy-preserving properties \u2014 including support for differential privacy \u2014 and could be used at the consultant\u2019s discretion. (Update: As of May 8, 2023, Gretel has deprecated DP support in Gretel LSTM in favor of the new ", ".)", "The classification pipeline follows a standard machine learning process, with an additional data synthetics step.", "The original paper ", " introduces the Conditional Tabular Generative Adversarial Network, which enables synthetic data generation by GANs for tabular data. As mentioned before, this particular model is well suited for sparse datasets with many zeros, missing, or empty values, and a larger number of columns.\u00a0", "We train it through the ", " and synthesize 5,000 records with the entire process taking less than 15 minutes. Using the ", " our synthetic data has an Excellent rating, which means it's ready to be used! Sharing and using synthetic data has much less privacy risk than the real data, but the synthetic data has many of the same important statistical qualities, patterns, and relationships necessary for downstream machine learning. Additionally, we can easily generate more synthetic data for our downstream classifier, which can increase the classifier\u2019s performance on pizza prediction.", "We perform the usual classifier training tasks, only this time using synthetic data for training rather than real data. We do a 10-fold cross validation with a logistic regression classifier on the synthetic data to find a hyperparameter configuration that works well. After fitting the model on the synthetic data the client validates the model on real data. The resulting model performs well, with an F1 score of 0.9249.", "One might be curious if better accuracy could be achieved using the real data, and by how much. We find that a classifier trained on the synthetic data achieves performance on the real data very comparable to a classifier trained directly on the real data, with an F1 score of 0.9206. So, with no discernible accuracy change, we nearly eliminated the possibility of a linkage attack because no record in the synthetic data is based on an individual human\u2019s data.", "In consulting, there are many things to track. Putting a client\u2019s mind at ease about the safekeeping of sensitive data is an important part of the process. By using synthetic data, consultants can sidestep the data access problem while creating high-quality machine learning models for their clients. In this scenario, we outlined a scenario where synthetic data can protect sensitive customer data while still allowing consultants to deliver value to their clients.", "If you want to learn more about CTGAN or our synthetic data generation systems, feel free to reach out at hi@gretel.ai or join and share your ideas in our Slack ", ". To get started right away, ", " and create your first synthetic dataset in minutes.", "\u200d", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/downstream-ml-classification-with-gretel-actgan-and-pycaret", "page_title": "Downstream ML classification with Gretel ACTGAN and PyCaret", "headers": ["Downstream ML classification with Gretel ACTGAN and PyCaret", "I\u2019ve generated synthetic data, now what?\u00a0", "Train and evaluate downstream ML models with PyCaret", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In a recent post we discussed ", " by using synthetic data. The synthetic data was used to train a downstream machine learning classifier. Classifiers require accurate and high quality data before being usefully deployed. In this post, we dive into using synthetic data to train a machine learning model to determine if a customer will purchase a certain product.", "When practitioners use the word \u201cdownstream", "\u201d", "they're typically referring to a step in the system that happens after the data has been processed and transformed. For example, if we were to train a machine learning classifier and then use it to make predictions about some customer behavior, those predictions could be used ", " to take further business action on behalf of the customer.\u00a0", "However, we can also refer to the machine learning classifier itself as being ", " if we do a series of transformations to the training data. That's the case here, where we create a synthetic version of the data that we then use to train a downstream classifier which itself may have other downstream effects. We have a notebook where you can follow along.", "The data we use as an example here is our grocery store data. ", "Every food column after the first few metadata columns contains how many of an item a person bought in a single transaction. The column we\u2019ll focus on predicting in this case is `Frozen Pizza`, but we could choose any column of interest.\u00a0", "Since we are going to train both a synthetic data generating model and a downstream classification model, we need to hold out a small validation set. This validation set isn\u2019t seen by the synthetic model or the classification model, and its purpose is to test the eventual classification performance of a classification model trained purely on synthetic data and validated on unseen real data.", "This is an additional step in the traditional ", " and ensures that our classification model trained on synthetic data can be used for real world data without ", ".", "We can use the remaining 95% of the data to train our synthetic model.\u00a0", "In this instance, because we have over 100 columns, we want to use a model that handles high-dimensional data well. Additionally, we want to make sure that our model can handle any columns that might have a large number of purchases. For that reason, we\u2019ll use our newly released ", " model. This model is GAN-based, and highly effective for tabular data generation. Its improved memory usage, speed, and accuracy make it an excellent choice for this use case.", "\u200d", "Once we\u2019ve trained our synthetic model and ", " we can generate records to train our downstream classifier. We generate the same number of records as we had in our training data, which results in a synthetic dataset with high downstream utility.\u00a0", "We split this synthetically generated data in the same way we would split the original data if we were using that directly for our model training.\u00a0", "We also split the 95% data so we can have additional validation for our downstream model\u2019s performance.\u00a0", "A data practitioner would spend a great deal of time selecting a model and validating its performance. These steps can be accelerated using an AutoML tool like ", ", which we do here.", "We first see a table that outlines information about the data and proposed training run. This table shows the size of our data, number of features, and other useful information. A subset of the information is shown here:", "After the 14 models have trained on a 10-fold cross-validation, we see the results. In this case, our best model, as measured by accuracy, is a Random Forest Classifier. The actual numbers will change between runs due to the stochastic nature of synthetic data generation.", "We then evaluate the models on various subsets of the data as we see fit.", ":", ":", "Importantly, we can compare these results to those of downstream models trained on the original data and evaluated on the same test and valid data splits.\u00a0", ":", ":", "We see in this run that models trained on synthetic data get slightly lower performance metrics than models trained on the original data. This result doesn\u2019t hold true in general, and often performance is quite comparable between downstream classifiers trained on synthetic data vs those trained on the original data.\u00a0", "In this post we saw that we can train a downstream classifier on 100% synthetic data and achieve performance comparable to a downstream classifier trained purely on the original data. This is encouraging as it suggests we can reap the benefits of synthetic data (e.g., privacy, volume, cost) and still achieve acceptable performance for downstream machine learning use cases.\u00a0", "Check out our CPO Alex Watson working through this notebook and discussing downstream ML!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/synthetic-data-and-the-data-centric-machine-learning-life-cycle", "page_title": "Synthetic Data and the Data-centric Machine Learning Life Cycle", "headers": ["Synthetic Data and the Data-centric Machine Learning Life Cycle", "The data-centric machine learning life cycle", "1. Data collection", "2. Exploratory data analysis\u00a0", "3. Data preparation and annotation", "4. Model training and evaluation", "5. Model deployment", "6. Model monitoring", "7. Re-training", "What\u2019s next", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this series of posts, we\u2019ll cover how Gretel\u2019s synthetic data platform helps you overcome challenges across the data-centric machine learning life cycle to help you successfully build, deploy, maintain, and realize value from your AI projects.", "The life cycle outlined below is a common framework or workflow process for building machine learning and AI solutions. It\u2019s focused on streamlining the stages necessary to develop machine learning models, deploy them to production, and maintain and monitor them. These steps are a collaborative process, often involving data scientists and DevOps engineers. The process below was inspired by the value chains created by ", ", ", ", ", ", and ", ".", "Below, we dive into specific examples of how synthetic data is increasingly playing a key role in different stages of the machine learning life cycle. In future posts in this series, we'll create and walk through code examples for each use case and stage.\u00a0", "The machine learning life cycle begins with obtaining raw and/or unstructured data. Kaggle\u2019s data science survey of over 13,000 data scientists shows that just ", ". In practice, privacy and regulatory concerns with sensitive training data often cause this bottleneck, though it can be due to a lack of data examples to build with, too. Use ", " to:", "During this stage, data practitioners may filter, model, and visualize data, while searching for insights through a range of tools and methodologies. These insights, such as data distributions and relationships, inform the life cycle, but particularly help data practitioners as they prepare data for labeling, training, and testing machine learning models. Use Gretel\u2019s ", " API and Console interface to:", "Data preparation often includes cleaning, de-duplicating, aggregating, and transforming data so that it can be used effectively for a specific machine learning task and framework. Within many tasks, data is annotated, or labeled, to capture the outcomes that you want your machine learning model to recognize and predict. When the data is imbalanced or insufficiently numerous, the preparation process may entail generating specific annotated data, such as from under-represented classes, to enable models to generalize to previously unseen data and increase accuracy. Use Gretel Synthetics to: ", "{{cta-demo}}", "Model training teaches your model to perform a task. Usually, model training fits into one of four categories: supervised, unsupervised, semi-supervised, and reinforcement learning. Check out NVIDIA\u2019s blog post for an excellent ", ".\u00a0Use Gretel Synthetics and a parameter optimization framework such as ", ", Pycaret, or ", " to: ", "Model deployment places a finished machine learning model into a live environment where it can be used for its intended task as part of an application, whether customer-facing or internal. Models are typically developed to work with carefully prepared test, training, and validation datasets. Few models pass those tests, and typically ", ". Using synthetic data can drastically improve these outcomes. Use Gretel Synthetics to:", "Keeping a close eye on your ML model enables data practitioners and developers to detect problems with model performance. These problems can include data drift (where the data the production model is seeing begins to vary due to a change in the collection process) and ", " (where the behavior in the production shifts due to seasonality or other rare events such as the Covid pandemic and subsequent shift to remote work and social distancing).\u00a0", "Use Gretel Synthetics and ", " to:", "When a model no longer performs at peak levels, the life cycle can be reset. Data practitioners and developers can determine the cause of the drop in performance and retrain the model on an updated training dataset that\u2019s accurate to the current data.\u00a0", "Use Gretel Synthetics to:", "Gretel is a synthetic data platform that simplifies generating, anonymizing, and sharing access to data for thousands of engineers and data science teams around the world. Even if you have no data or limited data, Gretel can help you start building machine learning and AI solutions. Stay tuned, in the next post in this series, we'll dive in with code examples and case studies for stages in the machine learning life cycle.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/product-updates", "page_title": "Product updates blog posts - Gretel.ai", "headers": ["Product updates", "What's new in Beta2", "CHANGELOG: Beta2", "Introducing Gretel Benchmark", "Automate Synthetic Data Pipelines with Gretel Workflows", "Fine-tune a MPT-7B LLM with Gretel GPT", "Gretel releases Beta 2", "How accurate is my synthetic data?", "Data Is More Valuable When It Can Be Shared", "Introducing Gretel Amplify", "NEW: Integrating with Gretel SDKs just got easier!", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/create-a-location-generator-gan", "page_title": "Create a Location Generator GAN", "headers": ["Create a Location Generator GAN", "TL;DR", "Introduction", "Getting Started", "Steps", "Create the training dataset", "Create the test dataset", "Converting synthetic images back to coordinates", "Putting it all together", "Conclusion", "Credit", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Generating realistic location data for users for testing or modeling simulations is a hard problem. Current approaches just create random locations inside a box, placing users in waterways or on top of buildings/etc. The GAN location generator we created can use map data to predict where a human (or e-bike in this case) might be, for any location in the world. ", "In this post, we will explore training a ", " Generative Adversarial Network (GAN) model on map data and public e-bike feeds from cities across the USA. We can then test the model\u2019s ability to learn and generalize by predicting location data sets for cities across the world, including Tokyo. If you\u2019d like to follow along, ", " from GitHub \ud83d\udc47 and create synthetic location data sets for your own city!", "In a ", ", we trained a ", " on precise location data from e-bike feeds, and used the model to generate synthetic and privacy-enhanced datasets for the same regions (e.g. Santa Monica, CA). By framing the problem differently and incorporating map data for context, we can ", ".", "We can model this by encoding e-bike location data as pixels into an image, and then training as an image translation task similar to CycleGAN, Pix2pix, and ", ". For this post, we\u2019ll use the newer ", " (FastCUT) model that was created by the authors of pix2pix and CycleGAN as it\u2019s memory efficient, fast for training (useful for higher-res locations), and generalizes well with minimal parameter tuning. Follow along or create synthetic location data for your own city with the complete end-to-end example on GitHub ", "After installing dependencies, run python -m locations.create_training_data to create pairs of 512x512px map images with and without location data from the e-bike dataset.", "Next, train our model on the dataset- essentially training the FastCUT model to predict where e-bike locations would be given a map.", "The FastCUT model logs data to ", ", which lets us monitor model training. In the plot below, we can see both model losses decreasing during training, and also a preview of the image translation task. The first image is the real DomainA map data, the second is a flipped version of the DomainA image with predicted scooter locations (fake), and the third being the real DomainB locations. We can see that even after 25 epochs, the model is learning to predict what look like reasonable scooter locations- e.g. street corners and along roads.", "The model seemed to overfit when running for the recommended 200 epochs, with predicted scooter locations disappearing from images. For this example, I saw the best performance in earlier epochs (epoch 30). Here are some real world v. synthetic images from San Francisco that were predicted by the model.", "Run the command below to create a training dataset of a 15x15 grid of the map locations in downtown Tokyo, or modify the latitude and longitude parameters to create synthetic locations for any geographic region. Note that with how the FastCUT python code works, we\u2019ll need to copy the map grid images both into the testA and testB directories.", "We can now use our model to process each of the images created for the grid above to predict e-bike locations across Tokyo.", "Looking at the results of individual images:", "Now our task is to take the synthetic e-bike in our images from Tokyo and convert them to real-world coordinates to build our synthetic location dataset. To extract the e-bike locations, we apply an image mask using OpenCV that searches for any groups of magenta-colored pixels in the image. Once the mask is created, we can calculate what the distance is for any pixel in the mask from the center-point latitude and longitude that is encoded in the image file name.", "Note that depending on where the city is in the world, the physical distance between each latitude or longitude degree can vary significantly, and we will need to use an ellipsoid-based model to calculate precise offsets when mapping pixels to locations. Fortunately, the geopy Python library makes this easy.", "The image below uses the cv2.imshow() function to preview the masked image that we then convert back into latitude and longitude coordinates.", "We can now process all images and stitch the synthetic locations into a new dataset for all of Tokyo.", "There are some definite false positives when viewing data across Tokyo, particularly with locations being generated for waterways. Perhaps further model tuning, or providing more negative examples of waterways in the training data (domainA or domainB) would reduce false positives.", "However, results are encouraging (given little model or dataset tuning)- with the model seemingly able to mimic the distributions and locations of the e-bike dataset that was trained on using maps from a different part of the world.", "In this post we have experimented with applying context from the visual domain (e.g. map data) along with tabular data, to create realistic location data for anywhere in the world. Please leave a ", ", or let us know on our ", " if you like this example or have any questions!", "Credits to the authors below for their excellent paper and code on the FastCUT algorithm that we applied for this example.", " by Park, Efros, Zhang, Zhu (2020).", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/andrew-carr", "page_title": "Author: Andrew Carr - Gretel.ai", "headers": ["Andrew Carr", "How to safely work with another company's data", "Teaching large language models to zip their lips", "Bringing AI-generated images to enterprise use cases", "Diffusion models for document synthesis", "Downstream ML classification with Gretel ACTGAN and PyCaret", "Synthetic Image Models for Smart Agriculture", "Evaluating Data Sampling Methods with a Synthetic Quality Score", "What is Model Soup?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/open-source?094d394d_page=2", "page_title": "Open Source blog posts - Gretel.ai", "headers": ["Open Source", "Innovating With FastText and Table Headers", "Instrumenting Kubernetes in AWS with Terraform and FluentBit", "Synthetic Data Configuration Templates", "Reducing AI bias with Synthetic data", "Create a Location Generator GAN", "README.V2", "Introducing Gretel Blueprints", "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://app.vanta.com/gretel/trust/rethkfr2j1a49hfj5bozur", "page_title": "Vanta", "headers": [], "content": []},
{"url": "https://gretel.ai/blog/introducing-gretels-privacy-filters", "page_title": "Introducing Gretel's Privacy Filters", "headers": ["Introducing Gretel's Privacy Filters", "What are the privacy risks surrounding synthetic data?", "Introducing Gretel's Privacy Protection Filters", "Understanding Privacy Protection Levels", "Give it a Try!", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We're excited to announce the release of Gretel Synthetic's new Privacy Protection mechanisms. Now, on top of the privacy inherent in the use of synthetic data, users can choose to add supplemental protection by means of a variety of privacy protection mechanisms (some new, some have long existed). \u00a0The use of these mechanisms helps to ensure that your synthetic data is safe from adversarial attacks.", "Here at Gretel, a priority research area for us is to stay on top of the ever-growing variety of attacks used by adversaries seeking to gain insights into private data. Each attack requires various levels of access to training data, machine learning models, or data created by the models. \u00a0Common examples of adversarial attacks on data include:", "Many of the listed adversarial attacks require access to the model which at Gretel is tightly controlled. Only authenticated members of a project can access and run a synthetic model. To counter the remaining potential attacks, we've studied the nature of these attacks and have been able to isolate weak points in a model or dataset that are commonly exploited. We've countered these weak points with the following privacy protection mechanisms: ", "Synthetic model training and generation are driven by a configuration file. \u00a0Here is an example configuration:", "In our Gretel Synthetic Report, we score your Privacy Protection Level based on the number of privacy mechanisms you've enabled. At the very beginning of the report we provide a graphic showing your Privacy Protection Level:", "We then show you just which privacy mechanisms you have enabled:", "We also provide a handy matrix with the recommended Privacy Protection Levels for a given data sharing use case:", "With Gretel's new Privacy Protection Mechanisms, your data is safer than ever. You can quickly share your data knowing that any sensitive information is well protected. Stay tuned for a part two blog on our Privacy Protection Mechanisms where we\u2019ll step you through a notebook that assesses their impact on Machine Learning accuracy. As always we welcome feedback via email (", ") or you can find us on ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/unlocking-adapted-llms-on-enterprise-data", "page_title": "Unlocking Adapted LLMs on Enterprise Data", "headers": ["Unlocking Adapted LLMs on Enterprise Data", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We are excited to announce a series of updates that will be rolling out over the next few months to the ", ". Gretel GPT is an API for creating synthetic natural language text using Large Language Models (LLMs), which can be used for generating labeled examples for training or testing downstream machine learning models. You can fine-tune the model on your own unique data, or provide a few examples for the model to learn to recreate.\u00a0", "Over the months since launching Gretel GPT in preview, it has grown to be one of our most popular synthetic data models \u2014 last month accounting for nearly 8% of all jobs run in Gretel\u2019s cloud service. Our blueprint example using Gretel GPT to create additional synthetic examples to ", " to better recognize user intents has risen to be in our top five blueprint examples (", "). Today, users are using Gretel GPT for anything from generating labeled examples to help ML models better detect abusive and toxic language, to synthesizing doctors\u2019 notes and patient symptoms from electronic medical records to enable research and data sharing between hospitals while protecting patient privacy.", "The initial Gretel GPT preview was a promising start and it showed great potential in several key areas, notably the ability to run in the ", ", the ability to easily combine Gretel GPT with Gretel\u2019s tabular and time-series models, and simultaneous request handling capabilities through the batch inference API. These features made it simple for developers to quickly iterate and create hundreds or thousands of synthetic examples for training and testing their ML models.", "We are thrilled to reveal the latest enhancements to the Gretel GPT preview, including the ability to fine-tune state-of-the-art LLMs on your enterprise data with the option of operating entirely within your cloud or VPC to meet the strictest security and compliance requirements.", "Key upgrades:", "Get started in minutes by running one of our ", " using the model upgrades \u200c \u2014 such as generating additional labeled examples to ", ", or training the model to ", ". If you\u2019re already using Gretel GPT, you can simply update your ", " to use the new models.", "Gretel GPT ", ":", "For optimal results, we recommend training on up to 2000 examples. Fine-tuning larger datasets with 2000+ examples using default parameters may potentially surpass the default maximum runtime limit of one hour per job on Gretel's developer tier. If you would like to increase your maximum runtime limit, please feel free to reach out to us at support at gretel.ai.", "We're all ears for any thoughts, ideas, or questions you might have, and our ", "is the perfect place for that. Keep an eye out because we've got some pretty cool updates on the horizon. We're working on a bunch of new features that we believe will make it easier for you to trust and feel secure about the privacy and accuracy of LLMs for your enterprise use cases. Can't wait for you to try them out!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/measure-the-quality-of-any-synthetic-dataset-with-gretel-evaluate", "page_title": "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "headers": ["Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "Introduction", "\u200d", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Last year, we ", ", which compares synthetic and real-world datasets when you train a model, and provides a high-level score and evaluation metrics to help you assess the accuracy of your Gretel generated synthetic data. But what about evaluating the quality of existing synthetic data? We\u2019ve heard your feedback and have expanded our tools to allow you to evaluate the quality of any synthetic datasets, regardless of the source.\u00a0", "We\u2019re excited to announce the release of Gretel Evaluate, which generates a ", "ynthetic Data ", "uality ", "core (SQS) report for ", " by comparing it against real-world data. Now, there are no restrictions around evaluating synthetic data only trained with Gretel. The Evaluate API is available via our CLI and SDK and can be run in the Gretel Cloud or locally to generate the Gretel Synthetic Report you know and love. As a fully managed service, you do not need any additional infrastructure to set up or manage.\u00a0", "In this blog, we\u2019ll walk through a simple example of how Gretel Evaluate works in our SDK. If you are interested in using Evaluate with the Gretel CLI, check out our ", ". You can follow along with our Gretel Evaluate demo notebook located ", " or here:", "After inputting our Gretel API key, we start by specifying the real-world and synthetic datasets we want to evaluate. These can be ", ". In this example, our real-world data is a ", " from Kaggle and our synthetic data is the corresponding dataset trained by Gretel Synthetics. Here, we are using Gretel generated synthetic data, but Evaluate can be used on ", "synthetic dataset.", "The notebook then shows a preview of the datasets. Simply looking does not tell us the accuracy of the synthetic data when compared to the real-world data it was trained on. This is where Evaluate comes in.", "Once we have specified our datasets, we use Evaluate to generate a Gretel Synthetic Report. Now, we task a worker running in the Gretel cloud to evaluate our data and produce the quality report using a temporary project. The QualityReport class takes our synthetic data as the data_source parameter and our real-world data as the ref_data parameter. Here, we have not specified a project parameter, so a temporary project is created and deleted after the Evaluate job finishes and delivers our results.", "Evaluate returns the Gretel Synthetic Report as both HTML and a JSON dictionary. Once the Evaluate job is complete, we can use the peek method to see our resulting SQS.", "{'grade': 'Excellent', 'raw_score': 91.42962962962963, 'score': 91}.", "Our Synthetic Data Quality Score (SQS), as shown below, is 91 which is classified as excellent.\u00a0", "This is the same report you see when you train a model using Gretel synthetics, now created as a standalone job. For detailed information on the components of the report, what they mean, and how they are calculated, you can read our ", ".\u00a0", "As demonstrated above, Gretel Evaluate is a simple and powerful tool for analyzing the quality of any ", ". By creating a way to benchmark accuracy of synthetic data from any source, Evaluate empowers you to confidently choose the synthetic tool that best fits your use case. If your Synthetic Data Quality Score isn\u2019t as high as you\u2019d like it to be, check out our ", "and try them out on a new Gretel synthetic model.", "For more information about Gretel Evaluate, check out our ", ".", "We are super excited to see how you use Gretel Evaluate. Feel free to reach out at ", " or join our ", " and share your thoughts there.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/tyler-bray", "page_title": "Author: Tyler Bray - Gretel.ai", "headers": ["Tyler Bray", "Load NER data into Elasticsearch", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/transforms-and-multi-table-relational-databases", "page_title": "Transforms and Multi-Table Relational Databases", "headers": ["Transforms and Multi-Table Relational Databases", "Intro", "Our Database", "Gathering Data Directly From a Database", "Take a Look at the Data", "Define Our Transform Policies", "Model Training and Initial Data Generation", "Transforming Primary/Foreign Key Relationships", "Take a Look at the Final Data", "Load Final Data Back into Database", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The ability to share private, de-identified data is a rapidly growing need. Oftentimes, the original non-private data resides in a multi-table relational database. This blog will walk\u00a0you through how to de-identify a relational database for demo or pre-production testing environments while keeping the referential integrity of primary and foreign keys intact. ", "You can follow along with our Gretel Transform notebook:", "The relational database we'll be using is a mock ecommerce one shown below. The lines between tables represent primary-foreign key relationships. Primary and foreign key relationships are used in relational databases to define many-to-one relationships between tables. To assert referential integrity, a table with a foreign key that references a primary key in another table should be able to be joined with that other table. Below we will demonstrate that referential integrity exists both before and after the data is de-identified.", "After installing the necessary modules and inputting your Gretel API key, we first grab our mock database from S3, and then create an engine using SQLAlchemy:", "This notebook can be run on any database SQLAlchemy supports such as PostgreSQL or MySQL. For example, if you have a PostgreSQL database, simply swap the `sqlite:///` connection string above for a `postgres://` one in the `create_engine` command.", "Next, using SQLAlchemy's reflection extension, we gather the table data.", "We then crawl the schema and produce a list of relationships by table primary key.", "Now let's join the order_items table with the users table using the user_id.", "Below is the output. Note how every record in the order_items table matches a distinct record in the users table. A primary goal of this notebook is to show how we can run transforms on the tables in this database and maintain these relationships.", "Now we need to define a transform policy for any table that contains PII or sensitive information that could be used to re-identify a user. We won't include a transform for any of the primary/foreign key combinations, as we'll be handling those separately. Let's take a look at the transform policy for the users table.", "Within the \"rules\" section, we define each type of transformation we want, each one beginning with \"- name\". We start by replacing any field classified as a person\u2019s name or email address with a fake version. Note, we chose to leave several of the location fields as is, such as \"state\" and \"country,'' since it's public knowledge that this database is about user ecommerce transactions in Arizona. We then transform the \"created_at\" timestamp using a random date shift. And finally, we transform the numeric fields of age, latitude and longitude with a random numeric shift. Note, we did not transform \"id\" because it is a primary key that matches a foreign key. We will have special processing for primary and foreign keys later that ensures referential integrity is maintained.\u00a0", "Each policy should reside in its own yaml file and the locations for each are made known to the notebook as follows:", "We first define some handy functions for training models and generating data using the policies we defined above.", "Now that we have these functions defined, we can easily run all the training and generation in parallel in the Gretel Cloud. You can find the details of how to monitor this process in the notebook code ", ". The key API call for checking a model status is as follows:", "The value of a model status begins with \"created\", then moves to \"pending\" (meaning it\u2019s waiting for a worker to pick it up). Once a worker picks it up, the status becomes \"active\". When the job completes, the status becomes \"completed\". If there was an error at any point along the way, the status becomes \"error\". Similarly, the key API call for checking generation status (all the same valid values) is:", "Note \"model\" is returned by the above \"create_model\" function and \"rh\" (record handler) is returned by the above \"generate_data\" function.", "To ensure referential integrity on each primary key/foreign key table set, we will first fit a scikit-learn ", " on the combined set of unique values in each table. We then run the Label Encoder on the key field in each table in the set. This both de-identifies the keys as well as serves to ensure referential integrity, which means a table with a foreign key that references a primary key in another table should be able to be joined with that other table. The code to accomplish this is shown below.", "We will now show the same join on the order_items and users table ", ", but now on the transformed data.", "Once again, each record in the order_items table matches to a distinct record in the users\u2019 table.", "To wind things up, the last step is to now load the final transformed data back into the database.", "We've shown how easy it is to combine direct access to a relational database with Gretel's Transform API. We\u2019ve also demonstrated how large multi-table databases can be processed in parallel in the Gretel Cloud. And finally, we've demonstrated a technique for ensuring the referential integrity of all primary/foreign key relationships. Coming soon, we'll show you how to accomplish all of this using Gretel Synthetics.\u00a0", "Thank you for reading!\u00a0 Please reach out to me if you have any questions at ", ". ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/how-to-use-weights-biases-with-gretel-ai", "page_title": "How to use Weights & Biases with Gretel.ai", "headers": ["How to use Weights & Biases with Gretel.ai", "\ud83e\uddf9\ud83e\uddf9\ud83e\uddf9", "\u200d", "\u200d", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Last month, I spoke with ", " and demoed Gretel\u2019s SDK with the developer community from WandB. You can watch the full workshop ", ".\u00a0", "In preparation for the event, I had fun experimenting with some of Gretel\u2019s projects using Weights & Biases machine learning tools, specifically their hyperparameter sweeps and visualizations. Below is an example of how you can use their tools to help optimize a synthetic model for a challenging dataset. If you\u2019d like to follow along you can use this ", ".", "Before we dive in, a quick primer on why you need hyperparameter sweeps. Before building any ML model, you want to determine the best variables to use with your learning algorithm, as these determine the model\u2019s structure (e.g. the number of hidden units) and how the model is trained (e.g., learning rate, optimizer type, batch size). These are the knobs that we can control in the machine learning process.", "The rub is that testing for these ideal training values manually gets unwieldy fast. That\u2019s where MLOps tools like Weights and Biases can help. Essentially, WandB automatically \u201csweeps\u201d through hundreds of combinations of hyperparameter values to find the best ones. Once finished, it provides a rich set of visualizations to inform your training process.\u00a0", "With that background, let\u2019s test the sweeps and create some synthetic time-series data! ", "For this example, we will use Weights and Biases\u2019 tools to run a series of hyperparameter sweeps to optimize our model\u2019s training to generate the highest quality synthetic data possible. Follow along with the end-to-end example in this ", "!\u00a0", "First, according to ", ", you ", " by creating a dictionary or a YAML file that specifies the parameters to search through, the search strategy, the optimization metric et al. There are a few search methods to choose from \u2013 grid Search, random Search, or bayesian Search \u2013 each has pros and cons, depending on the project you\u2019re tackling. We\u2019ll stick with a bayesian Search for this example:", "For bayesian sweeps, you also need to tell us a bit about your metric. We need to know its name, so we can find it in the model outputs and we need to know whether your goal is to minimize it (e.g. if it's the squared error) or to maximize it (e.g. if it's the accuracy).", "If you're not running a bayesian Sweep, you don't have to, but it's not a bad idea to include this in your sweep_config anyway, in case you change your mind later. It's also good reproducibility practice to keep note of things like this, in case you, or someone else, come back to your Sweep in 6 months or 6 years and don't know whether val_G_batch is supposed to be high or low.", "Next, you\u2019ll define your hyperparameters: ", "Once that\u2019s done, you\u2019ll ", " with one line of code and pass in the dictionary of sweep configurations: sweep_id = wandb.sweep(sweep_config)", "Finally, you\u2019ll ", " agent, which can also be accomplished with one line of code by calling wandb.agent() and passing the sweep_id to run, along with a function that defines your model architecture and trains it: wandb.agent(sweep_id, function=train). And you\u2019re done!\u00a0", "While there\u2019s a lot going on under the hood, the output is a series of detailed data visuals that tell you everything you need to know \u2013 we\u2019ll highlight two of them:\u00a0", "According to WandB, this plot maps hyperparameter values to model metrics. It\u2019s useful for honing in on combinations of hyperparameters that led to the best model performance. As you can see, using a smaller number of RNN units (256), and character-based tokenization (by using a vocab_size of 0, versus sentence piece for higher) yielded the optimal Synthetic Quality Score. This is most likely due to the relatively small amount of data, where a more complex neural network or vocabulary was not required. ", "This plot surfaces which hyperparameters were the best predictors of your metrics. So depending on the metric we want to focus on (e.g., epoch, accuracy, loss, training time), we get some detailed analysis. For example, here are our best predictors with respect to accuracy:", "Finally, now that we\u2019ve trained a synthetic model with optimized hyperparameters (thanks, WandB!), we can now use the optimal configuration to generate a synthetic dataset for the ML task.", "Based on these charts, we can see that our model tracked closely with the real-world dataset and the synthetic data output is a quality replica of the original model. ", "But before we celebrate, we want to make sure that no real-world data was duplicated i.e., no personally identifiable information was ingested or replayed by our synthetic model. ", " provides some nice summary statistics to answer these questions. It includes an analysis of the field correlation and distribution stability, as well as the overall deep structure of our model.", "The report provides more granular analysis, too, including mappings of correlations that exist between your training and synthetic datasets, and principal component analysis of the individual distributions of elements within those datasets.\u00a0", "It\u2019s exciting to see how other open-source dev tools, like Weights and Biases, can be integrated into the Gretel synthetic data generation process. If you like the WandB visualizations, you should definitely run Gretel\u2019s ", " yourself, as well as watch the ", ".\u00a0", "If you\u2019re new to Gretel, you can ", " and run our APIs to ", ", ", ", or ", " data \u2013 no code required! As always, we\u2019d love to hear from you, either by email at ", " or on our ", ". Thanks for reading!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/alex-watson?b2715911_page=3", "page_title": "Author: Alex Watson - Gretel.ai", "headers": ["Alex Watson", "How to use Weights & Biases with Gretel.ai", "Deep dive on generating synthetic data for Healthcare", "Simplifying Our APIs", "Gretel.ai + Illumina - Using AI to create safe, synthetic datasets for genomics", "How To Create Differentially Private Synthetic Data", "Reducing AI bias with Synthetic data", "What is Synthetic Data?", "What is Privacy Engineering?", "Improving massively imbalanced datasets in machine learning with synthetic data", "Gretel.README", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/scale-synthetic-data-to-millions-of-rows-with-actgan", "page_title": "Scale Synthetic Data to Millions of Rows with ACTGAN", "headers": ["Scale Synthetic Data to Millions of Rows with ACTGAN", "ACTGAN\u2019s improvements for memory and accuracy", "An experiment to examine ACTGAN\u2019s memory improvements", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Whether you\u2019re building better machine learning training data or creating safe pre-production environments for experimentation and testing, synthetic data has become an essential tool for businesses that want to leverage the power of data without risking sensitive information. Companies and developers looking to operationalize synthetic data are faced with the challenge of scaling beyond small sample training data with tens of thousands of rows and tens of columns to datasets that have tens of millions of rows and hundreds of columns.", "Unfortunately, when many developers start training synthetic data at scale, they quickly hit roadblocks due to the scale of their data exceeding the capabilities of generative models that are available. At Gretel, we understand these requirements and have built a ", " to meet the demands of businesses looking to generate synthetic data at scale.\u00a0", "In this post, we'll focus on one of our models called ", " (Anyway Conditional Tabular Generative Adversarial Network), which excels at generating synthetic data for highly dimensional tabular datasets such as those often used in the ads, cyber security, financial, and life sciences verticals.", "About a year ago, we began working with an excellent open-source synthetic data model called CTGAN, which was originally presented at the NeurIPS 2019 conference by the paper titled ", ". While CTGAN works well with datasets with up to hundreds of columns, developers using our platform quickly hit limitations when scaling to datasets with millions of rows. Due to the significant changes required to the model architecture to support this scale, and concerns that the open-source license would change to something more restrictive, we opted to fork a copy into a new model called ", " in the gretel-synthetics library. ACTGAN builds on the foundation of CTGAN and enhances it with new features that make synthetic data generation significantly more efficient and accurate for machine learning use cases.", "It's worth noting that recent changes to the ", " have made it difficult for enterprises to use the model in their products without having to purchase a license, including restrictions for \u201cproductionizing the SDV within an enterprise\u201d or \u201cbuilding and selling software that uses the SDV.\u201d In contrast, ", " is available under a source-available license, which restricts its use only in building competing synthetic data services.\u00a0", " significantly reduces CPU and GPU memory requirements, enabling faster training and larger datasets. We accomplished this reduction through using a new Binary Encoder in addition to one-hot encoding, and a more efficient internal representation of training data. Users can now synthesize much larger datasets without upgrading to higher memory GPUs or experiencing lengthy training times.", "Another significant improvement made to ACTGAN is the automatic detection and transformation of date-times. This feature enables synthetic dates to be sampled from a distribution rather than being treated as categorical. This results in more realistic synthetic data that accurately reflects the underlying distribution of the original dataset.", "Finally, ACTGAN also includes improvements to conditional vector sampling, which greatly enhances the accuracy of conditional data generation. This feature is particularly useful for generating new labeled examples for machine learning datasets.", "{{cta-scale}}", "Let\u2019s take a look at how these memory improvements translate into support for larger datasets and faster, more accurate training. We'll use a ", "from Kansas City, MO\u2019s open data project, which has a mix of categorical and numeric data. ", ".", "First, let\u2019s compare wall-clock time and GPU memory consumption using a sample of 5000 records from the dataset on a Colab instance with 4 cores, 16GB system RAM, and an NVIDIA V100 GPU. In addition to the 6.4x speedup below, ACTGAN requires only 16% of the GPU memory footprint of CTGAN for this dataset (835 MB vs 13,769 MB).", "For the second experiment, we'll run more at scale, processing 100k records of data and increasing the batch size for our GPU. Since we are running 20x more data through, we can reduce the epochs to 25 for this experiment. In this example, ACTGAN scales to a dataset size 20x larger than the first experiment, with only a minimal increase in memory requirements and runtime.", "ACTGAN builds on the foundation of CTGAN and introduces new features that make synthetic data generation scale with the needs of enterprises. Its more permissive licensing also makes it a more accessible choice for businesses that want to generate synthetic data in-house. Check out ", ", or get started in minutes using the fully featured ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/aws-gretel-synthetic-data-accelerator-program", "page_title": "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "headers": ["AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Imagine seamlessly training, testing, and fine-tuning bespoke machine learning models, without the risk of jeopardizing individual privacy or breaching trade secrets?\u00a0", "The path there would require leveraging advanced data security mechanisms and privacy-enhancing technologies, coupled with state of the art deep neural networks for data generation and the right infra to scale. Today, we\u2019re thrilled to provide that comprehensive solution, through a strategic collaboration with AWS, and a new ", " designed to support privacy-centric generative AI development.\u00a0", "This program isn\u2019t merely a step but a giant leap towards developing responsible AI applications. Participants will gain access, insights, and opportunities that include:", "The program is ", " across industries including financial services, healthcare, and the public sector.", "As generative AI applications are rapidly incorporated into services, the demand for safe, accurate, and timely training data has soared. This is where generating synthetic versions of real-world proprietary datasets that maintain the statistical insights but are not linked to any private individual can be a game-changer. Synthetic data enables mitigation of privacy risks, augmentation of limited data supplies, simulation of edge cases, and compliance with regulations like GDPR, CCPA, and HIPAA.\u00a0It is an indispensable tool for modern developers. ", "Industries have already discovered high value applications for synthetic data that highlight its transformative potential. ", " to fine-tune machine learning models, enhancing diagnostic and preventive measures. ", " employ it to amplify the accuracy and reliability of their fraud detection models and for sharing data safely across internal and external teams. The possibilities are endless. ", "However, the shift towards synthetic data comes with its own prerequisites. \"Asking data-driven developers to exchange real-world data for synthetics requires they not only have a deep dedication to privacy, but also access to simple, intuitive solutions that return value immediately. Gretel provides all of the above,\" said Chris Hymes, CISO at Riot Games. This point highlights our dedication to simplifying the process for developers and teams to work with secure data.", "Ultimately, synthetic data is a catalyst for responsible AI innovation. \"The combination of synthetic data generation tools with AWS\u2019s secure infrastructure and flexibility, enables businesses to confidently explore and safely advance their generative AI initiatives,\u201d said Stephen Baker, AWS Director of Generative AI. \u201cThis is a path forward for innovators who seek a modern and more sustainable approach to building with data.\u201d", "Gretel\u2019s collaboration with AWS promises to be a lighthouse for AI engineers and developers who are committed to building with high quality data that is private by design. By enabling teams to safely test, train, and fine-tune proprietary large language models (LLMs) and other AI applications using synthetic data, we can foster an ecosystem of responsible innovation.\u00a0", "Interested in being part of this amazing initiative? ", " for startups and enterprises everywhere. Sign up and let\u2019s build together.\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/alex-watson?b2715911_page=1", "page_title": "Author: Alex Watson - Gretel.ai", "headers": ["Alex Watson", "Differentially Private Synthetic Text Generation with Gretel: Making Data Available at Scale (Part 1)", "What's new in Beta2", "Generate textbook-quality synthetic data for training LLMs and SLMs", "Prompting Llama-2 at Scale with Gretel", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Anonymize tabular data to meet GDPR privacy requirements", "Synthetic Data and the Data-centric Machine Learning Life Cycle", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Fine-tune a MPT-7B LLM with Gretel GPT", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/videos", "page_title": "Video blog posts - Gretel.ai", "headers": ["Video", "Walkthrough: Create Synthetic Data from any DataFrame or CSV", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/machine-learning", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "Data Simulation: Tools, Benefits, and Use Cases", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Machine Learning Accuracy Using Synthetic Data", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Optuna Your Model Hyperparameters", "Test Data Generation: Uses, Benefits, and Tips", "Prompting Llama-2 at Scale with Gretel", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/data?094d394d_page=2", "page_title": "Data blog posts - Gretel.ai", "headers": ["Data", "Gretel Smart-Seeding is auto-complete for your data", "What is Model Soup?", "Fast data cataloging of streaming data for fun and privacy", "Create high quality synthetic data in your cloud with Gretel.ai and Python", "README.V2", "How we accidentally discovered personal data in a popular Kaggle dataset", "A guide to load (almost) anything into a DataFrame", "ML Models: Understanding the Fundamentals", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/test-data-generation", "page_title": "Test Data Generation: Uses, Benefits, and Tips", "headers": ["Test Data Generation: Uses, Benefits, and Tips", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["If you're a developer, data scientist, or machine learning engineer, you know that accurate and timely test data is essential to the success of your projects. But what are the best ways to generate test data? And what are the benefits of using test data generation tools?\u00a0", "In this post, we'll explore those questions and give you some tips on how to get started with test data generation. We\u2019ll also discuss why technical teams are synthesizing test data to meet the ", " for training state-of-the-art AI models and evaluating software applications.\u00a0", "Test data generation is the process of creating new data that mimics aspects of an original real-world dataset, to test applications, develop features, and even train ML/AI models. This fictional data can run the gamut from completely random to being almost statistically identical to the dataset it\u2019s based upon. The data you create depends on your use case, and privacy and security risks, among other factors.\u00a0", "There are almost as many techniques and tools for making test data as there are use cases for it. Before we discuss those, let\u2019s look at why we need to create artificial data at all.\u00a0", "The definition of \u2018test data\u2019 varies across teams in an organization. However, it\u2019s used for two main purposes: evaluating software applications, and improving ML/AI algorithms. Test data generation techniques have changed over the years to keep up with the ever-evolving software development process. For ML/AI engineers though, it has only recently emerged as a powerful tool for unlocking better predictions, capabilities, and model performance.\u00a0", "For software applications and services, test data is most often used in pre-production environments. Quality assurance managers usually take a snapshot of an entire dataset, manually anonymize it and create similar mock data for use as \u2018staging\u2019 test data. This process enables the creation of effective and reliable testing environments for developers and software engineers who are working on projects related to the business's products or services. It also enables the evaluation of expected customer experiences before releasing anything publicly.\u00a0", "But managing this process is a hard, slow, time-consuming, and error-prone grind, and even when successfully implemented the data used becomes outdated fast.\u00a0", "Until recently, it was common to use raw production data that isn\u2019t properly obfuscated or masked in these testing environments. However, modern privacy laws and standards like the ", ", CCPA, and SOC2 have specific language prohibiting these uses and require the data to be effectively anonymized first. If you don\u2019t comply, you risk legal action and heavy fines. You also increase the risk of cyberattacks. Data breaches are a serious threat to customer privacy and when they do occur they can often lead to ", ", too.\u00a0", "\u200d", "\u200d", "As we\u2019ll discuss later, test data can be fully synthetic, anonymized versions of real-world data that are used by engineers for development environments and new features. It can look just like your production environment without the privacy concerns of working with real customer data.\u00a0", "In contrast to software applications, \u00a0there is a different test data technique used by machine learning engineers and data scientists to train algorithms called cross-validation. They take the entire dataset and split it into three to train, validate, then test model performance. The percentages can vary, but the idea is that 80% of all data is used to train the algorithm (train data), and 10% is set aside to validate each iteration during training to make sure the model doesn\u2019t overfit (test data), and the other 10% is used to measure the accuracy of the final model trained on the \"training\" data (validation data).\u00a0", "In addition to the common cross-validation procedures outlined above, machine learning engineers are increasingly using synthetic data to train their ML algorithms. This can be the same test data as software and QA engineers use - assuming the synthetic data is highly accurate and representative of real-world data.\u00a0", "Synthetic data can be less biased and less expensive to acquire than real-world data. For example, using a popular ", " that was skewed almost 2-to-1 towards male patients, we added synthetically generated patient records to a training set to boost female representation in the dataset.\u00a0", "The result: ", ", achieving 96.7% overall accuracy for KNN (up from 88.5%), 93% for Random Forest, and a 13% gain for the Decision Tree classifier against the models trained on the non-synthetic dataset. For a tutorial on how you can do this yourself, ", ".\u00a0", "\u200d", "We demonstrated similar performance improvements when testing Gretel Synthetics on popular Kaggle tabular sets as seen below, which you can read more about in this ", ".\u00a0", "Our results reflect findings in computer vision and speech-trained models, too, where extra data has been proven to improve performance:", "For both software applications and AI models, the test data used must be accurate to be useful. Specifically, it should reflect real-world experiences, like how a customer might interact with your product or service under certain circumstances. Not only must you identify all of these likely customer events for testing, but also the unlikely \u2018black swan\u2019 events, too. These edge cases might not improve your software or model\u2019s overall performance by much, but it\u2019s crucial that you account for them, so you\u2019re prepared if they do occur.\u00a0\u00a0", "When implemented properly, test data can discover bugs in your systems that consumers would otherwise encounter. Here are three common examples of test data surfacing issues:\u00a0", "There are several high-profile examples of companies that didn\u2019t test their models rigorously enough before putting them into production. For instance, ", " had a tool that automatically cropped faces in photos people posted, but for some reason would ignore non-white faces. Similar forms of biases were discovered in ", ", where users would receive different credit limits, based on whether they were male or female.\u00a0", "Test data comes in all shapes and sizes. It all depends on the model or software you\u2019re building. But on a basic level, test data could be charted along a number line, and range from normal to extreme and invalid. To illustrate, consider a range from 1 to 5:", "You can also test a model or software on no data to see how it responds to null values, too. Each type of test data is useful as they all help define the contours and behaviors of the information system you\u2019re designing.\u00a0", "So with that background aside, now all you have to do is make the data!", "Technical teams have come a long way in terms of techniques used for generating test data. Below is a brief walkthrough of this evolution. From the old school \u201cdummy\u201d test data method to what is fast becoming the gold standard today: synthetic test data.\u00a0\u00a0", "Below are several methods teams use when testing models and software. Each should be included as part of a comprehensive testing process:\u00a0", "There are also new technologies for testing the quality of the test data itself. For example, Gretel.ai offers a ", " that provides metrics on the accuracy, utility, and levels of privacy inherent in the test data you are using.\u00a0", "In this post, we walked through the evolution of test data generation techniques, and why high-quality test data is critical to the development of modern software applications and ML/AI model training. Proper testing is often the only way to discover certain bugs, potential crashes, lag and latency issues, and other design problems that could negatively impact your business if released publicly.\u00a0", "While techniques like relying on a subset of your production data may cover the bare minimum privacy requirements under GDPR and other privacy laws, they are insufficient methods for building high-performance state-of-the-art AI/ML models. That requires anywhere from 2-4x the volume of your original production data, not a mere slice of it.\u00a0", "The best way then is to generate new data that captures the full gamut of instances you must test for. That\u2019s why synthetics are becoming the new gold standard for testing. With synthetics, you can simply boost your original dataset and even seed new variations to cover rare events, too. This flexible approach is not only safer, faster, and more cost-effective, but it also opens the door to unlocking other innovations across organizations as well.\u00a0", "At Gretel, we want to ensure that everyone has easy access to safe, compliant, high-quality test data. Whether you\u2019re building the next great software application in your basement or running an AI initiative at a Fortune 500 company, synthetic test data can help power your projects. Speaking of tests, you can ", " by signing up to our Developer tier \u2013 no coding required. Give our console a try, and we promise ", ".\u00a0", "If you have any questions, join our ", " and share your thoughts there, or shoot us an email at ", ". We\u2019d love to hear how your experience is using Gretel. Thanks!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/amy-steier?b2715911_page=2", "page_title": "Author: Amy Steier - Gretel.ai", "headers": ["Amy Steier", "Gretel's New Synthetic Performance Report", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/optimize-the-llama-2-model-with-gretels-text-sqs", "page_title": "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "headers": ["Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this blog, we\u2019ll show how using the ", " can help with quality evaluation of the generated text records as we fine-tune the Llama-2 model.\u00a0", "Fine tuning and generating synthetic text data with LLMs can be quite time consuming.\u00a0 Evaluating the quality of the results manually and updating the parameters according to that can make this cycle even slower. By automating the process of optimization and evaluation using Gretel\u2019s text SQS, developers can focus more on building and uncovering insights.", "We first describe the text dataset and its components, then we\u2019ll observe the text scores as the model is fine-tuned in different steps. At last, we take a sneak peek into the generated records at each step and compare them with the text evaluation score to see how the score relates to a human evaluation. You can try the experiment with your own dataset and monitor the results using ", ".", " is a text summarization dataset which contains a dialogue and its human written summary in English. Text summarization has many use cases including training chatbots for better question answering, as well as analysis of literature, research\u00a0 or\u00a0 legal contracts. In this experiment, we combine the dialogue and summary into a single column for each example since Gretel\u2019s gpt-x model expects each record to consist of a single free text field. We use a ", " with text lengths less than 100 tokens to keep the model fine-tuning duration under 30 minutes.\u00a0", "The following is a snippet of the text training dataset:", "It is vital to generate text dialogues and summaries which are semantically similar to what they are trained with. To do this, we use Gretel\u2019s ", ". This metric is a combination of the semantics and statistics similarity scores between the generated and training dataset, which is monitored as we fine-tune the Llama-2 model with the ", " while changing the \u201csteps\u201d parameter.\u00a0", "As seen in the following figure, gradual increase of steps in lower ranges, improves the learning process, which results in better text SQS. Score values above 80 are considered \u201cexcellent.\u201d After a certain step value, the text quality remains almost the same with no significant improvements.", "Let\u2019s take a look at the first 5 out of the 80 generated records in a few steps:", "The following snippet shows the generated text records when step = 250 and Text SQS = 38. The records are a mix of NLP prompts, code snippets, some non-english texts and punctuations which are completely unrelated to the dialogue-summary training dataset. ", "Let\u2019s now look at the results when the step = 300. There are still some records including unnecessarily punctuations (other words besides \u201cdialogue\u201d and \u201csummary\u201d are surrounded by \u201c**\u201d). Most of the records start with \u201cdialogue\u201d followed by \u201csummary\u201d which shows the model is in the learning process, although the summary is repeated more than once in some examples, so there are still some places for improvements. Text SQS is improved to 66.", "The following shows the generated records when the step =700. \u201cDialogue\u201d is observed in all of the records while \u201csummary\u201d is in more than 90% of them. Also, \u201csummary\u201d is a suitable representation for the \u201cdialogue\u201d and no further punctuations are observed. Text SQS score is also improved to 80.", "When setting the steps to 1500, the text results are pretty much the same as the previous one, and same with the text score(82). We still observe the presence of \u201csummary\u201d in more than 90% of the generated records:", "Note: For the last 2 steps, we can always add an extra evaluation step that only generates\u00a0 the records which include both the\u201d dialogue\u201d and \u201csummary.\u201d Also, considering the model trained with datasets less than 100 tokens, some generated long records are not complete sentences, mostly happening in smaller \u201csteps.\u201d", "Looking at the above results, we can conclude that Gretel\u2019s synthetic text evaluation score is able to measure the quality of the generated records precisely. Also, after certain steps of fine tuning, the text evaluation score and quality of the generated records remains the same, hence we set a default constant number of steps in our gpt-x ", ". Try the experiment with your own dataset and monitor the results using ", ". If you have any questions, reach out to us in our ", " channel. Happy synthesizing!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/open-source?094d394d_page=1", "page_title": "Open Source blog posts - Gretel.ai", "headers": ["Open Source", "CHANGELOG: Beta2", "Veterans Day Reflections: Open source software and evacuation operations, a remarkable combination.", "Optuna Your Model Hyperparameters", "We just streamlined Gretel\u2019s Python SDK ", "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Conditional Text Generation by Fine Tuning Gretel GPT", "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "Generate synthetic data in 3 lines of code", "How to use Weights & Biases with Gretel.ai", "Auto-anonymize production datasets for development", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/what-is-synthetic-data", "page_title": "What is Synthetic Data?", "headers": ["What is Synthetic Data?", "Synthetic Data: Artificial Data, Actual Events", "The privacy bottleneck", "Synthetic data offers faster access to sensitive data", "Synthetic data can augment limited datasets", "Trends driving synthetic data use", "Why is Synthetic Data Important?\u00a0", "Synthetic Data Informs AI/ML Models", "Diverse Use Cases Across Multiple Industries and Business Functions", "How Accurate Is Synthetic Data vs. Real Data?", "Determining Synthetic Data Quality", "The Benefits of Synthetic Data", "Synthetic Data Challenges", "Types of Synthetic Data", "Tabular data", "Time series", "Text", "Images and Video", "Simulation", "Audio", "Gretel\u2019s Advanced Synthetic Data Privacy Guarantees", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["What if you could have instant access to an unlimited supply of high-fidelity data that\u2019s statistically accurate, privacy-protected and safe to share? That\u2019s the promise of synthetic data.\u00a0", "In this blog, we\u2019ll cover what synthetic data is, how it\u2019s made, its various types and benefits, and why developers, data scientists and enterprise teams across industries are eager to use it.", "Synthetic data is commonly used as an alternative to real-world data. More specifically, it is artificially annotated information that is generated by computer algorithms or simulations. Research has shown that synthetic data can be as good or even better than real-world data for data analysis and training AI models; and that it can be ", " in datasets and ", " of any personal data that it\u2019s trained on. With the right tools, synthetic data is also easy to generate, so it is considered a fast, cost-effective data augmentation technique, too.\u00a0", "One of the biggest bottlenecks to innovation that developers and data scientists face today is getting access to data, or creating the data needed to test an idea or build a new product. In fact, in a recent Kaggle survey, 20,000 data scientists listed the \u201cdata gathering\u201d stage as the single most time-consuming part of a typical project, accounting on average for ", "From our experience working at AWS, Google, OpenAI, and with other leaders in the data industry, we know first hand that enabling developers to safely learn and experiment with data is the key to rapid innovation. Developers and data scientists don\u2019t always need - or even want - access to sensitive or personally identifiable information (PII). That\u2019s where synthetic data comes in.\u00a0", "Synthetic data opens up the possibilities of enabling access to artificial and privacy-preserving versions of personal data in minutes, with 95% of the accuracy of the real-world data it was trained on, and without having to wait weeks for manual anonymization and approvals.\u00a0", "Not having enough of the right data is a second bottleneck that limits the utility of data. Whether you are a developer trying to program a chatbot to respond better to a rare question, or you are a medical researcher collecting sensor data from medical devices, data can be your most valuable asset and it may be prohibitively expensive or even impossible to collect more data.\u00a0", "Synthetic data offers a unique solution to this problem. Modern synthetic data uses deep learning models such as language models (LM) and generative adversarial networks (often called GANs) where the models learn to create new, artificial, datasets with the same size and distributions as the data they were trained on. Using techniques such as conditional data generation, synthetic data models can be tuned to generate records with particular attributes\u2014for example, augmenting machine learning training sets to ", " in female patients.", "Privacy by design, and doing more with less\u2026", "In the visual below, Gartner shared a prediction that by 2030 the use of ", ". From what we have heard in customer conversations, this is\u00a0already happening today, and it is driven by several key trends - including increased compliance costs and regulatory risks under new data protection regimes like GDPR and CCPA, record-high cybersecurity attacks and data breaches, privacy concerns related to training algorithms on customer data, high costs of manually annotating data to train machine learning algorithms, and more processing happening directly on devices (therefore requiring less data to be sent to the cloud).", "At Gretel, we\u2019ve seen three core use cases for synthetic data that benefit enterprises, developers, researchers, and applied scientists, too:", "Machine learning algorithms need lots of examples to perform well, and they generalize poorly against data outside what they have been trained on. Consider what it would be like to train a voice assistant such as Alexa to recognize new commands from users. For each new utterance, there are thousands of possible variations of speech, semantics, and slang that the algorithm needs to be able to understand. Traditionally, these algorithms have been trained on large amounts of real-world data that have been manually collected and annotated.", "What if new phrases and combinations could be created automatically by a developer from just a few examples\u2014to include variations in speech, background noises such as dogs barking, or even multiple voices talking at once? The power to create nearly infinite variations of new data is one of the most powerful use cases for synthetic data and helps drive advancements in machine learning.", "Being able to learn from and build with data is one of the fastest contributors to innovation today. As such, there are applications for synthetic data across industries. Here are some common examples:", "Additionally, synthetic data can be used to accelerate different business functions such as:\u00a0", "In contrast to terms often used to describe synthetic data such as \u201cfake data\u201d or \u201cmock data\u201d, quality synthetic data can be nearly as accurate as the real data that it is based on, and in some cases even ", ". This is possible as synthetic data models can generate many more samples from training data that can help downstream machine learning algorithms better generalize. In fact, in testing Gretel\u2019s synthetic data accuracy against the top 8 machine learning datasets on the data science platform Kaggle, synthetic data ", " for downstream classification tasks, with a mean accuracy less than 1% from their real-world equivalents. Even with Gretel\u2019s advanced privacy features enabled on \u201chigh\u201d\u2014which prevents overfitting, detects dangerous similarities and outliers, and adds noise (differential privacy) to the dataset\u2014the mean ", ".\u00a0", "As mentioned above, in some cases synthetic data can even out-perform its real-world counterparts. Consider the example below on the ", ", where models trained on synthetic data outperformed their real-world counterparts with an average accuracy of 78.39% vs 76.56% (+1.25%) in predicting whether a person would default in credit card payments across machine learning classifiers including Random Forest, XGBoost, and SVM.", "There are three core indicators that when combined and weighted accordingly can help assess the quality of a synthetic dataset and correlated use cases for it, if any, that it might be suitable for - field correlation stability, deep structure stability, and field distribution stability. All three metrics are part of ", ", scores range from 0-100 and are provided every time a user generates a synthetic model. We\u2019ll walk through each metric using screenshots from a typical SQS report.\u00a0", " \u2013 the correlation between every pair of fields in the real-world data vs. synthetic data. To aid in the comparison of field correlations, this metric is shown as a heatmap in the synthetic quality report showing the computed difference of correlation values.", "\u200d", " \u2013 To verify the statistical integrity of deeper, multi-field distributions and correlation, Gretel compares a Principal Component Analysis (PCA) computed first on the original data, and then on the synthetic data. Look for a similar size and distribution of elements to verify that the model has not overfitted on the training data.", " \u2013 Field Distribution Stability is a measure of how closely the field distributions in synthetic data mirror those in the original data. For each field, we use a common approach for comparing two distributions referred to as the Jensen-Shannon Distance.", "Understanding a synthetic data model\u2019s quality is about determining its accuracy and the level of privacy it provides. This is vital information when the synthetic data will be used to answer critical questions such as \u201cIs this patient at risk for a stroke?\u201d\u00a0", "Gretel\u2019s Synthetic Data Quality Report is a digestible report that provides you with an overall quality score (from 0-100) and a summary table of the viable use cases you could use the data for, as well as a more detailed analysis of how your synthetic data generation process went. ", "For a deeper dive on quantifying the accuracy of synthetic data for downstream tasks such as business analytics, forecasting, and training machine learning models, check out Gretel\u2019s ", ", where they successfully created a process to enable internal data sharing of synthetic data to replace a legacy and error-prone process of manually de-identifying and redacting datasets.\u00a0", "Here are some of the top benefits of synthetic data:", "Some of the challenges with synthetic data include:", "The main categories of text-based synthetic data include natural language, tabular, and time-series data. Additional categories include image synthetics, audio, video, and simulation.", "Tabular data including tables, databases, and machine learning training sets is a very common use case. A table is an arrangement of information or data, typically in rows and columns, or possibly in a more complex structure. Popular formats for tabular data include comma-separated-value (CSV), JavaScript Object Notation (JSON), and big data formats such as Parquet. Synthetic data models such as language models (LMs) and generative adversarial networks (GANs) are effective at learning and recreating these kinds of datasets.", "Time series data is often tabular data, such as sensor data or financial data, that has a time component that must be learned and maintained across multiple rows. Consider a heart rate sensor on a fitness watch, where each row might be a point in time, and a user\u2019s heart rate should increase gradually as they increase running speed or start going uphill. This requirement requires the synthetic data model to maintain a level of state or memory as it recreates datasets to effectively recreate trends and seasonality in synthetic data. Synthetic data models such as GANs have proven particularly effective at being able to recreate these kinds of time-series components.", "Text or natural language data is another popular use case for synthetic data, and being able to recreate convincing natural language from models trained on just a few examples is a popular use case for developers and data scientists training chatbots, voice assistants, and a multitude of other use cases. Language models such as Transformers have been developed specifically for this task by adding an attention component, with examples such as GPT-3 being trained on 175 Billion parameters (45 TB of text), being able to generate convincing natural language given only a few prompts.", "Generating realistic images including human faces has been a popular research task over the past years and has led to tremendous advances in GANs. Examples such as Nvidia\u2019s ", ", ", ", and now ", " have shown that it is possible to synthesize high-quality images based on only labels and publicly available image datasets. For more information, read our blog on ", ".", "Synthetic data is one of the hottest use cases in the simulation space, where 3D simulations such as computer game engines (see ", ") can be used to train anything from robots, self-driving cars, and other autonomous systems to navigate real-world situations at a scale and speed that is not possible with real-world testing. For additional examples, check out Facebook\u2019s ", " and ", " ", "Technologies for audio and speech synthesis are used heavily in music, and in the voice assistants that many of us use every day such as Siri, Alexa, Google Assistant, etc. The need to train ", " (ASR) algorithms to recognize different kinds of voices, linguistics, and speech has also created the need to synthesize voices and variations of terms, as collecting data from the real world or having speech generated and annotated manually is prohibitively costly.", "In conclusion, whether you need to access or share private data, augment a limited supply of data, or reduce biases in datasets, synthetic data is an efficient and cost-effective way to solve your problem. However, it\u2019s important to remember that when it comes to using synthetic data, ", " a lot.\u00a0", "At Gretel, we\u2019re laser-focused on developing ", " for generating synthetic datasets with privacy-protection mechanisms that immunize them from adversarial attacks. We\u2019re also making sure all of these products and features are accessible via easy-to-use APIs, so developers and data scientists can be confident they are getting high-quality synthetic data whenever and wherever they need it.\u00a0", "Going forward, we will continue to post research, source code, and examples about enabling data sharing and access at scale. ", " and give Gretel a try, you can run Gretel\u2019s APIs to ", ", ", ", or ", " data \u2013 ", ".\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/rob-stark", "page_title": "Author: Rob Stark - Gretel.ai", "headers": ["Rob Stark", "Instrumenting Kubernetes in AWS with Terraform and FluentBit", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/instrumenting-kubernetes-in-aws-with-terraform-and-fluentbit", "page_title": "Instrumenting Kubernetes in AWS with Terraform and FluentBit", "headers": ["Instrumenting Kubernetes in AWS with Terraform and FluentBit", "Setup", "Configuring the Pipeline", "Inputs", "Parsers", "Filters", "Output", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Productionizing Kubernetes presents a lot of challenges. The scale and diversity of jobs it is used to run is massive, which can make operationalization difficult. When (not if) something goes wrong, you need to be able to debug the issue, and fix it in a timely manner. Something that has made a huge difference for us at Gretel is the ", " project.", "FluentBit is an open-source subproject of another project, Fluentd, which is developed and maintained under the ", " (CNCF) umbrella. Fluentd was developed to unify and simplify cloud logging infrastructure. FluentBit has a similar goal, but makes a few sacrifices in order to achieve a lot of the same functionality with a much smaller memory and CPU footprint.", "Gretel utilizes FluentBit to collect logs and publish them to a storage and indexing service. Having all of the logs from our containerized workloads available and searchable makes many aspects of our operations simpler. We can create query based alerts, search for and debug issues, and watch trends in our infrastructure.", "At Gretel, we use Terraform to provision our infrastructure, along with Helm to install and version a lot of our third party Kubernetes cluster tooling, including FluentBit. For the sake of this walkthrough, we will assume you are familiar with all of these. If you are not familiar with these tools, we have provided a little more background information, and a couple pointers, in the ", ".", "To get FluentBit up and running, most of the defaults in its Helm chart work perfectly. By default, the chart will create a ", ", as well as generate the necessary service account and role binding to make it work. We only need to provide basic information.", "We can optionally create the namespace alongside the Helm release instead of specifying it as a separate Kubernetes resource. FluentBit should be deployed in its own namespace to allow for easier network segmentation, so this is a useful option for brevity:", "Since the chart is going to create several resources in the process of deploying FluentBit, we\u2019ll want to make sure we have the \u2018cleanup_on_fail\u2019 flag set in our Helm configuration, otherwise these resources will linger in our environment if FluentBit fails to be created for whatever reason.", "Now, my personal preference is to do most of the modification and configuration of the Helm chart in a template YAML file that I reference from the helm_release resource above. This allows me to separate changes in versioning, or changes that may touch other infrastructure, from changes that are isolated to the application. Changes that are isolated to the application are hardcoded in the YAML, and changes that are not, are passed in for substitution.", "Ok, now we are ready to start configuring the FluentBit application itself. First, we need to take care of a few small bookkeeping items. Namely, we want to set the version of the FluentBit image we want to use, and the Service Account we want the Daemonset to use. Since the image version is a \u2018release\u2019 change, and the service account used may impact other applications, I pass them in from Terraform, this way other infrastructure can be aware.", "Terraform file:", "Template file:", "We now have a valid Helm release of FluentBit that we can deploy to our cluster!", "It doesn\u2019t necessarily do what we want quite yet, but this will run and generate logs (sent off to an elastic search instance that probably doesn\u2019t exist) from a default configuration that FluentBit\u2019s Helm Chart includes. You can check that Pods have been created on all your Nodes just by checking that the count matches your Node count.", "If you notice that you have fewer Pods running than the number of Nodes in your cluster, you likely have Taints preventing them from being scheduled. If you want FluentBit to run on all of your Nodes, you can fix this by adding the \u2018Tolerate Everything\u2019 Toleration to the YAML file.", "When checking the Pod count, there should now be a Pod for every Node. Now that we are able to get instances of FluentBit deployed onto our Nodes, let\u2019s look at how we can configure it to capture the data we need, and send it where we need it.", "FluentBit uses a ", " model to process log data locally and ship it out to various destinations. The maintainers have provided a ", " with the documentation that gives a great birds-eye-view of the stages.", "The stages of a pipeline are organized with ", ". A Tag is a label that is attached to any piece of data, or ", " flowing through the pipeline. All pipeline stages before the Output stage have a `Tag` field, where you set the value for outgoing Records. Conversely, all pipeline stages after the Input stage have a `Match` field, which will only apply the stage to Records with a matching Tag. All Records have Tags, which tell you where the Record came from, enabling selective application of any stage in a given pipeline.", "Both the `Tag` and `Match` fields can optionally make use of the wildcard operator (`*`).If used in the `Match` field, to select incoming data, the wildcard will match any subsequent value. \u00a0If used in the `Tag` field, to set the value for outgoing data, the wildcard is replaced with the `Tag` the data had on its way into the stage.", "Armed with our knowledge of Tags, we can now replace the default pipeline configuration with something more useful.", "The first stage needed in any pipeline is a set of Inputs. FluentBit provides several Input ", ", but, for the sake of this walkthrough, we will focus on the ", " Plugin.", "Tail allows us to read from files that are continuously appended to. At each call, it will read the tail of the file and provide any newly appended line as a Record to the pipeline. The only configuration required is the path(s) on the Node to read, and a Tag labeling our output.", "For Kubernetes clusters using Docker as the container engine (pretty much all Kubernetes clusters), logs written to stdout or stderr by your application will be saved to files on the Node at:", "This is because, by default, Docker uses the \u2018json-file\u2019 logging engine, which outputs container logs to a file under `containers/<container_name>.log`, and the Node\u2019s Kubelet holds onto these log files for the duration of the Pod\u2019s life in the var/log/ directory. Using this to define a Tail Input stage yields:", "This will output lines written to files with the extension `.log` in the `var/log/containers/directory` as Records with the Tag `application.<prior_tag>`. ", "Something to look out for is that many of the Plugins come with pre-baked Tag values. For the Tail plugin, all data is initialized with a Tag specifying it\u2019s filepath. This means that in our above input, all lines propagated to the pipeline as Records will have the tag", "In addition to the basic fields needed to get Tails working, there are many optional fields which can modify its behavior and output. I recommend setting \u00a0`Mem_Buf_Limit` and `Skip_Long_Lines`. `Mem_Buf_Limit` specifies the maximum amount of memory the Input can use when adding data to the pipeline, and `Skip_Long_Lines` tells the Plugin to skip any lines in the file that would run over that buffer size. Together the two options make FluentBit run more resiliently.", "\u200d", " allow us to structure the data produced by our Inputs so that they are more easily interpreted by later stages. FluentBit\u2019s pipeline uses the JSON format to carry and process Records, so Parsers mainly exist to convert non-JSON data to JSON and to reformat and reorganize JSON fields.", "The Parser we will focus on here is provided in the default parser configuration: ", ".", "The docker Parser provided is actually a specialization of the JSON parser, but it sets the time format used by docker, allowing it to be extracted and recognized by later stages. Using the docker parser, the Records output by the Tail plugin will have the log message, source stream, and time all separated into discrete fields of a JSON object.:", "If the contents of your application logs are JSON, and you use the Docker container engine in your Kubernetes cluster, then adding `Parser docker` to your Input specification can improve both the performance of FluentBit (since some fields may be changed to a binary representation, reducing size), and assist in the readability of the logs you produce.", "\u200d", " play a dual role. They assist in formatting logs, making them more compact or more readable, much the same way Parsers do, but they can also append information to logs, providing context. For this walkthrough, I\u2019m going to show an example of each.", "To clean up our logs, we are going to use the ", " Filter. The Grep Filter, like its Unix counterpart, matches patterns. It allows you to ", " or ", " Records that match a regex, but only on a per-record basis. To keep a record that matches the regex in the output of the Filter, use the `Regex` field. To exclude a record that matches the regex in the Filter\u2019s output, use the `Exclude` field.", "Also, since logs are handled as JSON objects, the regex can be selectively applied to a single field in your data:", "The Grep Filter is very handy when you are handling logs that are intended to be human readable. Such logs wind up with lots of extra empty log lines and superfluous syntax that isn\u2019t very helpful when searching for the source of an issue.", "To add some useful context to our logs, we can use the ", " Filter. It allows you to pull information from Kubernetes and append it to all of the data flowing through the pipeline.", "Configuring it is very similar to other pipeline stages, in that we name the Plugin, and provide both `Match` and `Tag` fields, but it additionally requires a way of extracting the Pod information from the Tag, and a source for the Kubernetes metadata.", "The most straightforward way of setting this up, is to connect FluentBit to the Kube API by providing a URL and authentication values.", "The set up of the `Kube_URL`, `Kube_CA_File`, `Kube_Token_File` values match how you would set up access to the Kubernetes API from a Pod ", ". ", "There are two potential pitfalls when using this plugin. The first is the source of information. If you have a large cluster that will be running many many instances of FluentBit, using the Kubernetes API as the source of metadata may overwhelm your network. In that scenario you should look into ", " as the source of metadata.", "The second is, when accessing Kubernetes to get the metadata to append, the plugin uses the Pod information from the Record\u2019s Tag to look it up. If the Tag has the prefix kube then it will by default strip off that prefix and use what follows as the pod information. For instance, if a Record\u2019s Tag is `kube.default.app-xyz` then the plugin will look up ", " in the API and append the resulting information. If you aren\u2019t prefixing your Record Tags with `kube`, or the Pod information does not follow it, you\u2019ll need to set the `Kube_Tag_Prefix` or `Regex_Parser` field so that the plugin can extract the Pod information from the Tag.", "Now that we have a pipeline capable of producing logs we want, we need to exfiltrate those logs to be stored and indexed for later searching. In this guide we will focus on using Amazon\u2019s Cloudwatch service.", "In order to be able to send out logs to cloudwatch we need to be sure of two things. First, that our Nodes have the needed network access, and second, that our Nodes have the needed permission in our target log retention service. Since we are using AWS EKS as our Kubernetes platform, we need to make sure our VPC configuration allows the network traffic we need, and our IAM configuration gives us the application permissions we need.", "Now, it\u2019s nearly impossible for this walkthrough to tell you how to set up your VPC, or in general how to organize and segment your network. That would be an entire book. Instead I\u2019ll just point out that unless you have specifically disable network access from your Nodes, then those permissions should be in place. If you have however instituted network segmentation in your infrastructure, you\u2019ll need to modify it to allow communication from the Nodes to Cloudwatch.", "Fortunately, from the IAM side things are a little more cut and dry. AWS has a walkthrough on ", ". Assuming you have an IAM role for your EKS Nodes, you can simply add a new Policy to the role in the Terraform file. This will give your Node the new permissions :", "Now, the `CloudWatchAgentServerPolicy` attached here, that Amazon provides, may be more permissive than you want, so feel free to create a new policy in IAM that only includes permission for Cloudwatch log operations.", "Once our Node permissions are updated, we can include the `cloudwatch_logs` Output Plugin in our pipeline, and we should see the results of our pipeline in Cloudwatch.", "One thing to pay particular attention to is that the `Region` in the config matches the region where your EKS cluster is running. Cross region network traffic can be expensive, so here I\u2019ve templated out the region field to match the region specified in the Terraform file, ensuring we won\u2019t be eating a bunch of network costs:", "Now we have everything in place to collect logs from applications operating in our Kubernetes cluster, tagged with all of the source information you may need to debug problems. In the ", " we\u2019ve gone a step further and set this up on an AWS EKS cluster so you can watch it in action. Head on over there and check it out.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/load-ner-data-into-elasticsearch", "page_title": "Load NER data into Elasticsearch", "headers": ["Load NER data into Elasticsearch", "Getting the Blueprint", "Running Elasticsearch, beyond the blog", "Structure of a Gretel Record", "Record Metadata", "But wait there's more", "Record ETL", "Exploring the records in Kibana", "Viewing records programmatically", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The Gretel Console provides several views and tools to help explore your data. \u00a0Another option that may work well with common existing workflows is to load the data into Elasticsearch and use Kibana for exploration and reporting. ", "This blog post walks through the structure of Gretel records, including NER results, and shows a simple workflow for loading that data into an Elasticsearch cluster", "The blueprint accompanying this blog post uses docker to stand up a local Elasticsearch instance. \u00a0For this reason we recommend downloading the blueprint and running it locally. \u00a0To do this, first check out the ", ". \u00a0Using the command line and assuming you have git and jupyter-notebook installed, here is one way to do it:", "You can now navigate to the blueprint in a browser. \u00a0The very last cell of the blueprint will delete our project and shut down the docker containers. \u00a0If you are going through the blueprint and this blog at the same time, or if you want to interact with Kibana before tearing everything down, please run one cell at a time.", "You will also need your API Key to complete the blueprint. \u00a0You can find it in the Gretel Console at ", ".", "As mentioned above, in this blueprint we use docker to start up a local Elasticsearch cluster. \u00a0The docker-compose.yaml in this directory was gratefully borrowed from ", "Another option is to install the necessary Elasticsearch packages directly using a package manager such as Homebrew or yum. \u00a0Elasticsearch is also available as a managed service offered by cloud providers if you want to set up a production service. \u00a0Using a cloud hosted Elasticsearch cluster may affect how you create the Elasticsearch client in code. \u00a0It may also give you other options for writing the data to the cluster (AWS Kinesis Firehose, for example). \u00a0The code for creating a Gretel client and using it to pull records would still be mostly the same (but see comments about \"forward\" and \"backward\" directions below).", "Gretel makes a few different sample data sets available. \u00a0We access one of these, a set of bike order data, using the Gretel python client and use this to create a new project. \u00a0We can then pull back a record from this new project to show its structure and highlight different features. \u00a0To do this, insert a cell in the notebook below the one where we call project.send_bulk. \u00a0Add the following code to the cell and try running it. \u00a0You should see a Gretel record in all its glory.", "The first thing to note is that the original record is retained and available under the \"record\" key. \u00a0By default, the record will be flattened; however, Elasticsearch will reject key names with periods in them. \u00a0In the notebook we use the params={\"flatten\": \"no\"} kwarg to preserve nesting in the record. \u00a0The original record embedded in the Gretel record will look something like this:", "Gretel writes NER findings to the metadata section, organized both by field and by entity. In the fields section, each field has a list of matching labels. Within each label object the label is the entity type. Other fields in the object give more details about the matching snippet.", "Under the entities key, the detected entities are first organized by high/med/low score. Following this are all the record fields grouped by entity.", "As seen above, the record also has a top level \"ingest_time\" field and a \"metadata.received_at\" field, both UTC timestamps.", "In the blueprint we ingest our project records as one static batch, but note that iter_records can also block and wait for new records to arrive (handy for a long running process) or resume scanning for records from a checkpoint (nice if you have a Lambda function being invoked periodically). This forward direction is the default behavior. \u00a0See the ", " for details.", "For this example we keep just the score lists from the NER metadata and then bulk index the records. Note that the Gretel python client includes a collection of record transformers that you can use to redact, fake or encrypt fields if you wish to include these operations in your ETL workflow. These are also covered in the ", ".", "By only saving these lists and not the lists of labels organized by fields we dodge the issue of using nested vs. object data types. Nested types save you from some search gotchas if you are querying across lists of objects but they must be declared in advance in your mapping. Consider your use cases and look at the ", " for more details.", "Another detail to note is our index name. For this example we have a simple name. If you plan on loading streaming data into Elasticsearch, a common practice is to append the date to the index name. This allows you to age off old data more easily by dropping older indexes. Kibana supports specifying index patterns with a trailing * wildcard, which plays nicely with this approach.", "Finally, you can specify a mapping for the fields in your index in advance. This lets you deal with things like the nested vs. object issue but also increases operational overhead. Your expected queries can help inform your decisions about specifying mappings in advance vs. letting Elasticsearch dynamically type fields.", "We can now take a look at our records. Go to `http://localhost:5601/app/discover#/`", "You will need to do some setup to initialize index patterns. Click the Create index pattern button.", "For step one, use gretel_ner_blueprint as the Index pattern name. Click Next step.", "For Step 2, choose ingest_time as the primary time field for use with the global time filter. Because we loaded all of our records in one batch, this is not the most exciting time filter. If you have streaming data that you then index into Elasticsearch a time filter becomes more interesting and useful for detecting changing patterns in your data over time. Click Create index pattern.", "You will see a screen with a list of detected fields. Return to `http://localhost:5601/app/discover#/`. You may need to adjust the time range to see your records. Showing 1 day ago to present will suffice.", "At this point you can do various queries within Kibana to filter results. As a somewhat contrived example, you can look for that rare Londoner who owns 3 or more cars and still needs to get a bike. Put record.NumberCarsOwned >= 3 and record.City: \"London\" into the search bar and hit the Refresh button. You should have a much smaller list of records.", "Clicking on one of the fields in the left hand column will expand it to show the counts for the top values for that field.", "In addition to free form exploration, Kibana supports a rich set of dashboards. See the ", " for getting started with this.", "You can also make a raw query to pull back records if, for example, you need to generate regular reports from a script. Here is the same query (Londoners with 3 or more cars). We cap the number of records returned for space and clarity, but you can see the total number of records returned. We add an aggregation showing the breakdown of the number of children across the results, mimicking the bucketing shown on the left hand side of the discover view in Kibana.", "As a slight twist, we also specify that we only want records that have the NER tag \"location.\" In this data set, the records are homogeneous enough that this doesn't affect the results. If there were a tag that you expected to see infrequently, this would help catch those rare events. \u00a0A classic example of this is the \"Thank you\" note in a gift order that contains an SSN.", "Note that if you try to use the score_high field for aggregations you will get an error! Because we have not specified an explicit type mapping up front, the field is typed as \"text.\" You can specify \"fielddata\": \"true\" as part of the mapping for this field to make it available for aggregations. See ", " for details on aggregations and ", " for details on the text type. \u00a0Here is the query we use in the blueprint:", "Elasticsearch and Kibana provide industry standard tools for the flexible exploration of data. \u00a0Combined with Gretel's NER analysis, users have a powerful and simple means to monitor trends in their data over time or spot suspicious outliers.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/red-teaming-synthetic-data-models", "page_title": "Red Teaming Synthetic Data Models", "headers": ["Red Teaming Synthetic Data Models", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["May 8, 2023 update: The DP Model referenced in this blog post is Gretel LSTM with differential privacy (DP). Since this post, we've released ", ", a fast and powerful model with differential privacy built-in, and have deprecated the DP support in Gretel LSTM in favor of this new model.", "Data privacy is one of the biggest concerns when it comes to ML models learning features from sensitive information. At Gretel, to prevent these risks, we use synthetic data models to generate artificial datasets that have very similar insights and distributions as the real-world data, but include enhanced privacy protections. In this post, we will implement a practical attack on synthetic data models that was described in the ", " by Carlini et. al. We will use this attack to demonstrate the synthetic model\u2019s ability to protect sensitive data. At the end, we compare the results of various model parameter settings, including differential privacy, and find some fascinating results!\u00a0", "You can follow along with our notebook here:", "In this example, we\u2019ll work with a ", ". It includes 1.3 million legitimate and fraudulent transactions from simulated credit card numbers.", "We select four informative yet sensitive features from the dataset: the last four digits of the credit card number, gender, and the first and last name of the card owner. We consider a random sample of 28,000 rows to limit runtime of the experiments. Why did we choose these features?", "Credit card numbers contain sensitive information about the owner. We simplify the credit card numbers to the last four digits since it is commonly used and there is a mixed distribution of the credit card number length.", "There are more than 860 unique credit card numbers in the training dataset. Including the first name, last name, and gender of the card owners can reduce the risk of generating the same exact records. There\u2019s a small probability of generating the same credit card number, but it is even less probable in a synthetic model to replay the same credit card number with the same user's first and last name.", "The following is the sample of the dataset with all features: ", "Here\u2019s a sample of 28,000 records with selected features from the above dataset. We use this as the training dataset: ", "A practical method for testing if the synthetic data is protected from adversarial attacks is by measuring the model\u2019s memory when generating secret values, i.e., \u201ccanaries\u201d. The more canaries generated, the better the model is at memorizing sensitive information and the more prone the dataset is to adversarial attacks. You can read more about various types of attacks in ", "\u200d", "Let\u2019s start with generating some canaries with various counts. The secret values are unique and in the same format as the training dataset. We randomly sample the training set and insert the canaries. Here is a sample of the code we used:", "Let\u2019s then look at the canary values and their count in the 28,000 records of the training dataset:", "Now, we run our experiments. We use four different neural network and privacy settings on the synthetic model to generate the dataset. Each model\u2019s performance is evaluated on overall prediction accuracy and given a synthetic quality score (SQS) from 0-100. You can read ", " for more info on how we determine synthetic data quality. Let\u2019s look at the models and how they did:", "For more consistency, we visualized the results with box-plots showing 5-95 percentile from 100 trials for each canary value and model. In this figure, wider box-plots depict more variability in the generated number of canaries, resulting in less preserving the private data.", "Since the count of canaries #1 and #2 in the training data is relatively small, all 4 models (almost) did not generate them in the synthetic data.", "While for canary #3 and #4, the distribution is mostly dense around the median for all models except the \u201cFiltered DP Model\u201d. The \u201cDP Model\u201d and\u00a0 \u201cNoisy DP Model\u201d do a pretty good job of not replaying canaries counted ~38. In this case, depending on the user\u2019s expectation from the model on either performance or generating a low epsilon, we choose each in order.\u00a0", "In the last row, comparing the models in memorizing relatively high counted canaries, the \u201cNoisy DP Model\u201d does not generate the secret values while the \u201cFiltered DP Model\u201d replays the maximum count among all models. As mentioned before, it\u2019s always about balancing the privacy guarantees of the secret data with the overall model accuracy. Here, the best choice for maximizing accuracy and privacy protection is the \u201cDP Model\u201d.\u00a0", "Now, we would like to define the \u201cmaximum value\u201d, which indicates the maximum canary counts in the training dataset that the synthetic model is able to not generate with a small number of canaries leaking in the generated dataset (error). In the following, we set the error value between (1, 3). The higher the maximum value, the better the model is in not memorizing and replaying the canaries.", "We consider the median for each model upon all canaries (rows) from the figure above when comparing the maximum values:", "As demonstrated above, models with various parameters and privacy settings can be used depending on the user's priorities\u00a0 in protecting the secret values and achieving the desirable performance. In the scenario where the model's performance is extremely more important than protecting the sensitive values, we use the \u201cVanilla Model\u201d, while in the reverse scenario we use the \u201cNoisy DP Model\u201d. There are also cases where the performance and privacy are both equally important when we use the \u201cDP model\u201d. If this is exciting to you, feel free to join and share your ideas in our ", " community, or reach out at ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/using-generative-differentially-private-models-to-build-privacy-enhancing-synthetic-datasets-from-real-data", "page_title": "Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "headers": ["Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "Introduction", "Anonymizing data", "Diving into the ride share dataset", "The privacy challenge", "Build a generative model", "Design the model", "Train the model", "Generate synthetic records", "Examining Synthetic Data", "Why Differential Privacy", "Applying Differential Privacy to our model", "Visualizing the Synthetic Datasets", "Quantifying the Privacy of Synthetic Data", "Thoughts", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Open datasets are incredibly valuable for democratizing knowledge, and enabling developers and data scientists to experiment with and use data in new ways. Whether the datasets contain location data, imagery (such as security camera video), or even movie reviews, the underlying data used to create these datasets often contains and is based on individual data.", "The cornerstones of modern privacy regulations (such as Europe\u2019s GDPR, and California\u2019s CCPA), and also privacy-enhancing technologies such as differential privacy and k-anonymity, are centered on protecting the privacy of individual users within a dataset.", "To release datasets to the world, developers and data scientists have traditionally employed a set of techniques to maintain the statistical insights of data, while reducing the risk of individual users personal data being revealed. These techniques are applicable anywhere from protecting highly sensitive data, such as an individual user\u2019s medical condition in a medical research dataset, to being able to track an individual users locations in an advertising dataset. Common techniques include data anonymization (for example, masking or replacing sensitive customer information in a dataset), generalizing data (bucketing a distinguishing value such as age, or satisfaction scores into less distinct ranges), or perturbing data (inserting random noise into data such as dates to prevent joining with another dataset). More advanced and cutting edge techniques, such as differential privacy, offer formal guarantees that no individual user\u2019s data will affect the output of the overall model.", "Let\u2019s explore whether we can apply a combination of machine learning and differential privacy to further enhance individual user privacy. This approach has the potential benefit of being less prone to human error than the manual techniques described above. It also increases flexibility with the ability to generate new datasets of any size, and offers formal mathematical guarantees around privacy.", "To test our theory, we\u2019ll train a generative, probabilistic neural network on a public ride-sharing dataset, and then attempt to create both synthetic, and differentially private synthetic datasets of the same size and distribution. The goals is to see if synthetic (artificially generated) data can provide nearly the same usefulness and statistical insights as the original dataset, with enhanced protection for individual privacy. In part 2, we\u2019ll dive into baking off our newly generated synthetic and private datasets against the source data on several data science challenges.", "We\u2019re going to train, and build our synthetic dataset off of a real-time public feed of e-bike ride-share data called the ", " (General Bike-share Feed Specification). This data is often shared with municipalities to help with city planning and understanding traffic patterns.", "GBFS feeds are real-time, so you can only see data for the past minute or so. For this experiment, we collected an entire day of data. Let\u2019s take a look at a full day\u2019s ride-share data for Los Angeles to better understand it and see where we might be able to improve on some privacy guarantees. You can snag the raw version of the dataset we\u2019re working with on our ", ".", "Initial observations are that there are over 6M location updates sent each day. Also, the GPS locations are quite precise (.000001 degrees = .111 meters!). Realistically, the GPS devices in scooters are most likely only accurate to a few meters.", "The dataset contains 3,393 scooters in the LA area, each reporting its location on average 2,755 times per day (equates to reporting every~31 seconds at the 75th percentile).", "This is all great, but what\u2019s really interesting is ", "data like this can help anywhere from civic planning, to thinking about where to put your new bar, restaurant, or even even where to advertise your business. To make this simple- we are working with only times that scooters reported location updates more than 100m from their previous position. We logged these updates over the course of one day, and you can download the dataset on our Github. Here\u2019s the format:", "While the ride share data is anonymized in a traditional sense and only tracking scooter locations over time, the combination of the precision of the location updates (.111m), the frequency of the updates (logging location by the start and end of each trip), and the ability to uniquely track scooters by name makes this data, in theory, open to re-identification attacks. In a re-identification attack, an attacker has some knowledge about an individual user, or a second dataset that can be joined with an anonymized dataset that can compromise individual privacy. Some widely known examples include researchers combining IMDB movie reviews and timestamps with the anonymized Netflix Challenge dataset to de-anonymize movie reviews.", "Working with the scooter dataset in Los Angeles, the concern would be that if a bad actor (Alice) knew something about another individual\u2019s location (let\u2019s say Bob), such as where they worked, and that Bob often rode scooters home from work around 4pm, it\u2019s theoretically possible that Alice could identify Bob leaving work on a scooter, and then track where he was traveling to.", "Our goal for this post is to use machine learning to create an artificial, synthetic, dataset with almost all of the utility of the original dataset, but with enhancing the privacy for any individuals in the dataset.", "We will begin by training a generative neural network on the real scooter data, and then use the neural network to create a synthetic dataset that, hopefully, contains many of the same statistical insights and applications as the original dataset, but with enhanced user privacy so that a bad actor, such as Alice (in the example above) would no longer have confidence that a particular record in the dataset is real.", "There has been incredible progress in using Recurrent Neural Networks (RNN) and Generative Adversarial Networks (GAN) to generate realistic, but synthetic text (check out OpenAI\u2019s ", " to generate realistic text) and image data (check out Facebook\u2019s ", " to generate realistic environments). We will use a similar approach, using an RNN to learn and generate both a synthetic, and differentially private synthetic ride share datasets from real data.", "First, we\u2019ll load our entire CSV training corpus of scooter data as a text string. The next step is mapping strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters.", "In the training dataset above, we discovered 66 different unique characters and mapped them to integer values. Here is an example of what the mapping looks like on the first line of input data:", "Next, we set up the prediction task and training dataset. We are training our model to predict the most probable next character. The input to our RNN model will be a sequence of characters, and we train the model to predict the output \u2014 the following character at each time step.", "In the example above, `split_input_target(\u201cSo hot right now\u201d)` returns `(\u2018So hot right now\u2019, \u2018o hot right now.\u2019)`.", "To build our model, we will use tf.keras.Sequential to define the next-char-prediction model, with atf.keras.layers.Embedding as the input layer to map numbers of each character to a vector with 256 dimensions. For classification, we\u2019ll use two hidden tf.keras.layers.LSTM layers with 256 memory units, three tf.keras.layers.Dropout layers to help with generalization, and a tf.keras.layers.Dense layer for our output. The Dense layer acts as our output layer, using a softmax activation function to output a probability prediction for the next character for each of the 66 characters observed in our training set (between 0 and 1).", "During training, our goal is to predict the next character in the dataset, while optimizing the log loss (cross_entropy). TensorFlow provides multiple choices for optimization, we\u2019ll use the ADAM algorithm for its balance of performance and speed.", "When generating a synthetic dataset, we are are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that recreates the training set by perfectly predicting each character. Instead we are seeking a balance between generalization and overfitting that lets us create useful synthetic data containing the statistical insights of the original data, but that minimizes memorization of data and removes statistical outliers that impact user privacy.", "history = model.fit(dataset, epochs=15, callbacks [checkpoint_callback])", "Now, we\u2019re ready to use our model to generate synthetic records!", "Even after training on a limited amount of input data, we can see that the RNN has learned the structure of the data and is generating realistic-looking data.", "You can run through our sample code yourself here. For our privacy experiment, we\u2019ll generate a synthetic dataset equal in size to the original training set, based on ~5800 scooter trips for 30 epochs. You should be able to generate useful data with a CPU if you\u2019re patient- but I\u2019d recommend a GPU- we\u2019ll use an Nvidia Tesla T4.", "Let\u2019s plot our synthetic data vs original data to see if our model was able to generate useful data. Note that with limited input data, the output format is not perfect- expect to do some light-weight filtering for missing field values/etc!", "Our 5,800 ride dataset is gathered from Uber ride shares all over the United States, and we are diving into the Los Angeles region to visualize some of the micro patterns that emerge in our model that would not be apparent zoomed out at the country level. At first glance, our synthetically generated dataset looks pretty good. There are a few scooters in the ocean, and a few scooters up North, but we have the benefits of what appears to be a statistically similar dataset (we\u2019ll dive into this in the Part 2), and as the data is effectively generated by our ML model, we hope to have have greatly reduced the chance that a single ride will be memorized by our model, and that a bad actor will be able to use it to compromise individual user privacy.", "What if we were releasing a dataset to the world, and wanted to provide mathematical guarantees that no individual trip in our training set (in our example earlier, Bob taking the scooter home from work) would be memorized by our model, even at the expense of some accuracy?", "Differential privacy is also promising for maintaining machine learning models in an environment with GDPR, CCPA, or HIPAA. Many companies building machine learning models from customer data face the challenge that when a customer wishes to have their individual data deleted, that any models based on their individual data need to be recomputed as well. Differentially private learning helps prove that one individual\u2019s data will not be memorized by a machine learning model, and thus you may not to retrain your models. But, this privacy comes at the expense of a hit to accuracy. Below, we can see that differential privacy becomes usable at a much smaller and more accessible scale than you might think.", "The TensorFlow team has taken on a lot of the heavy lifting of implementing and released ", ", an extension to ", " that allows differentially private learning.", "Let\u2019s build support for differentially private learning into our character-RNN generative model, and see if we can still use it to create realistic and usable data, while maintaining an acceptable privacy-loss (epsilon) value.", "In the code above, we make changes to the our existing model- specifically gradient clipping and noising in the training optimizer. First, we need to limit how much each individual training point sampled in a mini-batch can can possibly impact model parameters, and the resulting gradient computation. Second, we need to randomize the algorithm\u2019s behavior to make it statistically impossible to know whether or not a particular point was included in the training set. We achieve this through sampling random noise and adding it to the clipped gradients. This is where ", " kicks in: it provides code that wraps an existing TF optimizer that performs both of these steps needed to obtain differential privacy.", "Next, we need to tune our hyper parameter selections to support private learning with a minimal epsilon value. Intuitively, we\u2019ll want to lower the rnn_unit size to minimize complexity of our network. Tuning the other parameters to get usable results from our differentially private algorithm is not so easy. Fortunately, in the \u201c", ", by ", ", ", ", ", ", ", ", ", ", the researchers point out some useful parameters for configuring differentially private learning. Most notably, replacing the plain Differentially Private SGD optimizer with a DP RMSProp version provided significantly higher accuracy than the default implementation. We saw gains here as well. Using the RNN and differential privacy learning settings below, we were able to achieve an epsilon of 0.666 and delta of 6.53e-07 with a 0.96 categorical cross-entropy loss on 27,000 bike share trips.", "Phew! Now for the moment we\u2019ve been waiting for, let\u2019s graph the original ride share data, synthetically generated (non-private) version, and finally the differentially private synthetic datasets and see how the DP model performs.", "To quantify the privacy of our newly generated synthetic dataset, there is one question that immediately comes to mind", "To test this, we\u2019ll take two approaches. First, we will see if any of the training data was memorized and replayed exactly by our synthetic model (e.g. duplicate records). Second, we will insert \u201ccanaries\u201d, which are fake, but less-than-random scooter rides into the training dataset. Finally, we will train models using a variety of settings, and generate synthetic datasets for each configuration. We can then search for the existence of any training data (which may have repetitions and overlaps in locations resulting in memorization), and canaries (more randomized, but still realistic) in our synthetic datasets.", "To make the canaries, we generated a bounding box around the Los Angeles area, interpolated a 30x30 grid, inserted random noise to both the source and destinations to keep our model from memorizing any similarities, and selected 100 data points at random for the canaries (canaries account for 1.72% of overall training dataset).", "Below are the results of three models trained with different settings, including \u201cC3\u201d, which utilizes differentially private learning. C2 uses 1024 LSTM units and 100 epochs to intentionally overfit the data.", "The 12,000 record synthetic dataset generated by our C1 and C3 models do not contain a single record that\u2019s identical to the training or canary data, indicating that the model selection parameters are performing well towards of goal of capturing statistical insights, but not memorizing data. While C2 performs very well (82% accuracy) and did not memorize any of our artificial canaries, it did perfectly replay 30 out of 5800 training records in the 12,000 synthetic records that we generated. Configuration C3 utilizes differentially private learning, with a DP epsilon of 0.66 over 30 epochs, with mathematical guarantees around privacy with only a 3% drop in accuracy vs C1.", "The DP dataset is clearly still generating statistically relevant and useful locations (even if there are now some scooters in the ocean), ", ", and that\u2019s pretty cool.", "Check out the code at ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/product-updates?094d394d_page=2", "page_title": "Product updates blog posts - Gretel.ai", "headers": ["Product updates", "Simplifying Our APIs", "Introducing Gretel's Privacy Filters", "November 2020 - What\u2019s new in Gretel", "How to use Gretel\u2019s new entity stream", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/ali-golshan", "page_title": "Author: Ali Golshan - Gretel.ai", "headers": ["Ali Golshan", "Gretel is now available in the AWS Marketplace", "Gretel is live on Google Cloud Marketplace \ud83c\udf89", "Gretel and Google Cloud partner on synthetic data", "Why privacy by design matters more than ever", "Why I Joined Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/exploring-nlp-part-1-why-should-a-privacy-engineering-company-care-about-nlp", "page_title": "Exploring NLP Part 1: Why Should a Privacy Engineering Company Care About NLP?", "headers": ["Exploring NLP Part 1: Why Should a Privacy Engineering Company Care About NLP?", "TL;DR", "Here\u2019s what we were reading:", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Large language models present immense opportunities for synthetic text generation. Researchers and practitioners can use these pre-trained models to generate close to human level essays and answers to complex questions. We at Gretel are actively looking at innovations in NLP, so we can learn more about synthetic text. But we are doing so with a critical lens. Researchers are starting to question whether the advances in large language models are worth the risks. These models tend to pick up on systemic societal biases present in their training data. Moreover, training these models has huge financial and environmental costs. Although these models could offer Gretel higher quality synthetic text, we want to acknowledge the ethical challenges and use these models responsibly (if we choose to use them at all).", "Over the past decade, natural language processing has undergone a revolution. Practitioners now have a choice from a variety of models, ranging from ", " to ", " and everything in between. Multiple libraries have robust support for advanced language models, such as ", ", ", ", ", ", and ", " (just to name a few!). ", "In this blog series, we will explore", "In these blog posts, we assume some familiarity with pre-trained models and fine-tuning. ", " for a crash course.", "We at Gretel care as much about the quality of our generated synthetic data as we care about its privacy. When thinking about large language models, we want to make sure that the data being produced will not compromise the privacy of an individual. ", " discovered that out of distribution text in the training data, which they call \u201csecrets\u201d, can get memorized and replayed by the model quite easily. One of the benefits of pre-trained transformer models is that it is pre-trained! So, even with fine-tuning, the model would most likely replay the original training data, if any data at all.", "But we want to do better than that. ", " has shown that large language models have a multitude of ethical issues. Many models contain systematic societal biases, such as racism and sexism, that can affect the synthetic data that is generated. This is due to both the explicit and implicit biases that are ubiquitously present in books, the internet, and other sources of text. Because of this, we at Gretel have thought about curating our own datasets and pre-training our own language models. However, curating this type of data set correctly is still an open research problem.", "Even if we could create an unbiased dataset (which is quite a lofty assumption!), we\u2019d then have to contend with the environmental implications of training our own language model. ", " described that one training run of a BERT base model (approximately 110 million parameters) produced as much CO2 as a trans-American flight. ", " showed that even when researchers are trying to be environmentally conscious with a 200 billion parameter language model, the experiments and training of such a model can produce even more CO2 than the lifetime of a car. Until renewable energy sources are ubiquitous amongst cloud compute machines and training time goes drastically down, the tradeoff for a possibly unbiased model may be too large.", "None of this is to say that we shouldn\u2019t try to use large language models. One of our goals at Gretel is to democratize data. By providing an easy and seamless way for our users to generate text from large language models, small companies, nonprofits, and even individuals may have better access to these innovations. If we use a pre-trained model in our backend, we can utilize the previously trained parameters. Thus, using a ", " for fine-tuning or even as little as a few examples for ", ", many of these pre-trained language models can provide state-of-the-art synthetic text for a much lower cost to our users.", "But we first want to know what is the best solution. So we are asking questions like:", "In the next blog post of this series, we will explore the ideas behind text metrics and propose a new idea inspired by current research in the field of NLP. By defining what we mean by quality synthetic text, we\u2019ll be one step closer to answering the above questions.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/reducing-ai-bias-with-synthetic-data", "page_title": "Reducing AI bias with Synthetic data", "headers": ["Reducing AI bias with Synthetic data", "Train a synthetic data model", "Run the experiment", "Results", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this post, we are going to explore using ", " to augment a popular health dataset on Kaggle, and then train AI and ML models that perform and generalize better, while reducing ", ".", "The ", " published by ", " is one of the top 5 datasets on the data science competition site ", ", with 9 data science tasks listed and 1,014+ notebook kernels created by data scientists. It is a series of health 14 attributes and is labeled with whether the patient had a heart disease or not, making it a great dataset for prediction.", "A quick look at the dataset shows that male patient records account for 68% of the overall dataset, with female patient records at only 32%. With a 2 to 1 ratio of male to female patients, this could result in algorithms trained on the dataset over-indexing on male symptoms and performing poor diagnoses for female patients. There is no substitute for having an equal representation of groups in training data, especially with Healthcare. In absence of that, how do we reduce biases in our input data as much as possible?", "To test our thesis, we will use ", "\u2019s open source ", "to generate additional female patient records to attempt to compensate for the biased training data. Our hope is that this will help the classifiers improve ", ", and generalize better to unknown data. We can then run the synthetic dataset through ML algorithms on Kaggle to compare results vs. the training set.", "A top data science notebook on Kaggle (by forks, linked below) runs a series of 6 classification algorithms on the UCI dataset and compares the resulting model accuracies. The notebook splits the original dataset into a train (80%) and test (20%) split, which we save to disk as ", " and ", ". We will use train.csv to train our synthetic model, and test.csv to validate the results with the 6 classification algorithms from the notebook.", "To generate your own synthetic records, launch Gretel-synthetics via ", ", or check out the notebook directly on our ", ". Click \u201cRun all\u201d in Colaboratory to download the training dataset exported from Kaggle, train a model, and generate new patient records to augment the original training data.", "Configure the following settings for your synthetic data model- note that we found a good balance of generalization vs model accuracy with 15 epochs of training and 256 hidden units. Note that in this configuration, training with differential privacy is not necessary as the dataset has already been de-identified.", "To test the theory about algorithmic bias, we added a very simple custom validator to the notebook that only accepts Female records generated by our synthetic model (column 1-Gender is equal to 0).", "We are now ready to train a synthetic data model on our input data, and use it to generate 111 female patient data records to augment our training set.", "The synthetic model quickly learned the semantics of the data, and trained to 95%+ accuracy within 10 epochs. Next, download the generated dataset and let\u2019s run it on Kaggle!", "Now, let\u2019s go to Kaggle and run the classification notebook (with minimal edits to allow both the original models and our augmented models to run). To make this easy, you can load the modified notebook here. By default, it will run with our generated test set. To use your own, upload the generated dataset from the previous step to the Kaggle notebook.", "As we can see below, creating and adding synthetically generated patient records to the training set increased accuracy in 5 out of 6 classification algorithms, achieving ", ", 93% for Random Forest, and 13% gains for the Decision Tree classifier against the models trained on the non-synthetic dataset. It is possible that Naive Bayes accuracy dropped as the algorithm makes a strong assumption that all features are independent (hence Naive), and the synthetic data model likely learned and replayed correlations in the training data.", "Finally, looking at the model accuracy results by gender, the original model accuracy average for Females was 84.57%. By augmenting female patient data records and training the same models, accuracy improved to 90.74%. Interestingly, male patient data prediction accuracy improved as well from 86.61% to 90.71%.", "At ", " we are super excited about the possibility of using synthetic data to augment training sets to create ML and AI models that generalize better against unknown data and with reduced algorithmic biases. We\u2019d love to hear about your use cases- feel free to reach out to us for a more in-depth discussion in the comments, ", ", or ", ". Follow us to keep up on the latest trends with synthetic data!", "Interested in training on your own dataset? ", " is free and open source, and you can start experimenting in seconds via ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/alex-watson?b2715911_page=4", "page_title": "Author: Alex Watson - Gretel.ai", "headers": ["Alex Watson", "Create high quality synthetic data in your cloud with Gretel.ai and Python", "Create a Location Generator GAN", "Creating synthetic time series data", "README.V2", "Create artificial data with Gretel Synthetics and Google Colaboratory", "Gretel Synthetics Frequently Asked Questions (FAQs)", "Gretel.ai Raises $12 Million in Series A to Safely Share, Build with Data", "Practical Privacy with Synthetic Data", "Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "Workshop: Generating Synthetic Data for Healthcare & Life Sciences", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/videos/build-a-synthetic-data-pipeline-using-gretel-and-apache-airflow", "page_title": "Build a synthetic data pipeline using Gretel and Apache Airflow - Video", "headers": ["Build a synthetic data pipeline using Gretel and Apache Airflow", "Video description", "Read the blog post", "Build a synthetic data pipeline using Gretel and Apache Airflow", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["During this webinar, we\u2019ll build an ETL pipeline that generates synthetic data from a PostgreSQL database using Gretel\u2019s Synthetic Data APIs and Apache Airflow.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/generating-synthetic-data-for-healthcare-life-sciences", "page_title": "Generating Synthetic Data for Healthcare & Life Sciences - Video", "headers": ["Generating Synthetic Data for Healthcare & Life Sciences", "Video description", "Read the blog post", "Deep dive on generating synthetic data for Healthcare", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Enabling faster access to data for medical research with statistically accurate, equitable and private synthetic datasets.", "Alex (00:02):", "Hey, my name is Alex Watson. I am a co-founder at Gretel AI. Today we're going to talk about synthetic data and how we see it being used by data scientists and software developers in the healthcare and life sciences spaces. We'll start with some of the use cases that bring certain life sciences or healthcare organizations to say I'm interested in checking out synthetic data. Actually the use case on the bottom is probably the most popular one that we've seen, enabling faster access to data. Often getting access to medical research data sets can take four to six months to go through the necessary approval processes to meet compliance obligations. The question that we ask with synthetic data is can we create an artificial data set based on this real world data that doesn't link back to any actual patients? And can we use that data set to enable a doctor, for example, to learn about a rare disease without learning about the patient?", "Alex (00:55):", "So really exciting use case there we see with a fair amount of our customers really diving it on. Other use cases are around making better data sets. Both reducing bias, and then also generating more samples from limited data sets are around kind of the same goal where you have access to some data, but you don't have enough of the right data. So a question is can we boost the representation of a minority class in a data set or change some, engineer some feature of the data to create a more equitably, fair or balanced data set. Some of the examples that we've worked with and I'll link to them in the blogs before are working with the UCI data science team on an openly published heart disease detection data set, where there was a two to one representation of males to females. We had really encouraging results. Essentially, we boosted the representation of female patients in that data where it's prohibitively expensive to gather more data and then ran some of the same top models from Kaggle across that data and noticed both a 6% increase in overall accuracy for the data set for female detection and a 2% in overall accuracy.", "Alex (02:03):", "We've talked about some of the use cases around synthetic data. Now let's take a chance to talk about what synthetic data is. Here's a definition I like quite much actually came from the Invidia folks. Synthetic data is an annotated information that computer simulations or algorithms generate as an alternative to real world data. So synthetic data is nothing new. It's been something that's been around since the advent of simulations and algorithms to create data, given a certain representation or distribution that it's seen. What has changed in the past couple years is this massive advancement in deep learning. Today we'll talk about the techniques that are used in this industry, really focusing on one of the core algorithms that Gretel uses, actually, which essentially trains a language model or a sequential model on a customer, a very sensitive data set. It trains it while imposing a certain level of privacy parameters and, and preventing the model's ability to memorize data it shouldn't.", "Alex (02:59):", "What the model then outputs actually is a new artificial data set that has many of the same insights and distributions. In fact, in some cases, even better accuracy than the original data. I'll talk about that in a minute, but then actually isn't based on any real world person object or thing. What's new with synthetic data, here we see, it's not often I include a Gartner chart in a data science talk, but I think this is a really interesting one to talk about. What we see as a change in the way that organizations are accessing and being able to work with data in the sense that as we expect more and more from a machine learning algorithms, as we have more and more devices that are gathering data, there's a couple big things that are changing. One due to sensitivity of data it's harder to enable access.", "Alex (03:46):", "There's a risk of a model memorizing data that it shouldn't. That's challenge number one. Number two, in the IOT space, one of the things that we see is that less data is actually being sent back to the cloud. As we've advanced on technology I think a really simple is example to give is the Alexa or the, the Google assistant devices we have in our home. When they're actually able to push machine learning models out to those devices, they can make decisions much more quickly. Self-driving cars, same thing. The decision's being made like right there on the device. That means the data's not going back to the cloud. So if you're trying to train a model to become really good at a certain type of detecting a certain type of scene or object or voice utterance, at some point to recreate all of the possibilities that might exist or might not exist in the world, a simulation or a synthetic version actually becomes a much more scalable approach. And we're starting to see that both across our customers and how they're building with machine learning and also reflected in the industries you see here.", "Alex (04:48):", "So we can talk about synthetic data all day long. One of the things I think actually would make this fun and interactive was going through and, and actually using CTO code, building our own synthetic data model. Diving right in this approach for synthetic data is really very similar to open AI's, GBT models or other language models that we've seen transformers, things like that in the industry. And we're going to go through a very simple example here using a recurrent neural network, LSTM, to learn, to recognize and replay certain characters that exist in a text stream. Where open AI has built incredible models for processing natural language Gretel and other synthetic data approaches actually have applied very similar approach, but to learning either loosely structured or structured data, this could be CSV. It could be JSON. It could be something that you've pulled out of your data warehouse.", "Alex (05:42):", "Essentially it learns the structure of a CSV. As we can see here, we've got three columns, learns that as if it's its own language and then replays it. The first step of what we do this is we need to take these arbitrary character sequences, which can be characters, it can be commas it can be emojis, it can be whatever you want and map that to some sort of integer representation. So this is called tokenization in the LP space, or vectorization. Here where you can see the character three is being mapped to the character eights. We see a space as the third character, which has gotten the representation one. We see another space like later, see that reflected right here. So very simple, just mapping that we're doing here of each character to an integer representation that we can feed into a network.", "Alex (06:27):", "Step two, how does it take these individual characters and turn this into a model that can recreate language? The answer is surprisingly similar, simple, actually looking at it and going through here. All we're doing is saying, given a certain set of input text that goes into the neural network, can you predict the next character? And then networks are capable of doing this at such a scale that they can recreate entire language stories, things like that, just given input data. So here we've got a very simple function and all it does is take an input data set such as here we see so hot right now and returns the next character. So you can see it chopped off the S at the beginning, and introduced a period here at the end. What the network is learning here is that given the inputs so hot right now, most likely character distribution we're going to see it for the next character is going to be a period. If we do this enough, can we learn to learn and replay an entire language?", "Alex (07:22):", "Step three, here, we're going to define our neural network model. We'll link code to the bottom so there's no need to, to cut and paste here. We've got simple GitHub repository you can use to run through the entire example. But here we're going to use a popular machine learning framework called TensorFlow. And we're going to use Caris to make simple, to build the model. And we're going to go ahead and create a sequential model that we can use to predict the next character. We're doing some fancy things here. You see actually two LSTM layers being used here. This helps the network learn more complex embeddings and representations in the data. And we see dropout layers, which is being used to help the model generalize better. You can read up on any of these at the end of the day, we're configuring a neural network here that's going to treat our input text and learn to predict the next character.", "Alex (08:13):", "After the model's been created, we run our test data through it. So here we can see we're training for 15 epochs. Really what we're doing is for every single possible next character shifted one by one that exists in our original data set. Can we train it to predict the next character? What we keep a look on here very closely is the loss. So here we're looking at prediction loss, how good is the model predicting the loss, the next character based on our cross entropy loss. We see that going down when that stops improving. We know that the model is reached a good kind of plateau, and we can go ahead and start to use it for prediction.", "Alex (08:51):", "Now, for the fun part. Now that our model's been trained on our existing input text, we can tell the model to create as much data as we want. There's two ways that we can do this one. We can bootstrap the model. We can essentially prompt the model with some level of text. For example, if we were looking at a biased data set and when to create more female records, we could start by inputting to the model that this record is female. The age distribution is within 30 to 50 years old and have it complete the rest of the record based on what is learned on the input data. Alternatively, we can just let the model run by itself. In which case, the model will over time create a new data set that has a very similar size and distribution to the original data.", "Alex (09:36):", "Here's a fun example of the bias reduction use case that we mentioned earlier, working with the UCI data set. The original question was can we increase the representation of female records in this data set to be on parody with the male representation. What would that do for the overall accuracy of models built on this data set? Had very positive results. So that here we see the increase going up, as I said, nearly 6% across the female population here, and 2% accuracy across the overall data set. So here's an example. What did we do working with the UCI folks? We took the original data set from Kaggle. We created a simple synthetic model and told it to train and boost the representation. As you can see there very briefly to, to build an equal representation of male and female attributes. A note that you can actually go as arbitrarily complex, as you want to here, if you wanted to bucket age range and ethnicity and gender, all of those things can be done. This is just a very simple example, boosting one simple minority class in the data.", "Alex (10:46):", "After we created our synthetic data set, we took one of the top notebooks that we see on the Kaggle platform and ran against... Here we see six different classification techniques here. So anywhere from very simple techniques, such as a simple decision tree, that's being trained on the network naive phase, for example, to more complex, random forest, support vector machines, things like that. And we looked at the overall results. Here you see pretty awesome increase here and, and actually some interesting insights in the data that may tell us a little bit about how a synthetic model works. What I see here looking across is an increase in accuracy for actually every single model with the exception of one. It's worth diving in here and often these models can be very difficult to explain why naive based for example, the overall accuracy of the model did not improve after using the out of balance data set. A theory about this one that is worth diving into is that naive base assumes that there is no co-dependence or correlation between the different variables we're training.", "Alex (11:52):", "From practical experience, it does seem that the synthetic models are very good at learning deeply correlated things inside of the data. So it may have replayed certain types of correlations. For example, if you were over six feet tall, you might weigh over 200 pounds as an example. It may have replayed this which aided the other algorithms in their decision process, leading to a better result than the overall data set. Naive base, which doesn't take this into account may not have been able to take advantage of it. Earlier we mentioned the possibility of something more complex than addressing a single imbalance. Another example we've got right here, and then you can actually run through the source code as well is balancing both race and income bracket and gender to create a better version of a U.S. census data set to use for ML prediction.", "Alex (12:47):", "A final question we get often is were I as an organization, for example, to take just my super sensitive data set, create a completely artificial synthetic version of that data set. What is the loss or hit to utility or accuracy that I might see? We saw in the previous slide, that when you are able to use synthetic data to engineer your data, so to create a less biased or more fair data set to work with, we saw that there's a potential for even increasing accuracy for the different tasks that you want to apply in this machine learning for. In this case, what we're doing is a very simple substitution, where in purple you see is the original data. In blue here we see a completely synthetic version of that data set so completely artificial using none of the original records.", "Alex (13:33):", "We tick the top six or seven different data sets that saw on the co data set platform. And we applied the different tasks, so classification tasks that these were intended to be used for. Does a user have heart disease? Was this person a successful hire or not? Things like that. And then here, you can see the results from running essentially a default configuration of Gretel synthetic data on a machine learning task against the original data. So often very similar. In some cases you do see a hit. In this HR attrition prediction we see a nearly 6% drop in accuracy, but for many cases where you're concerned about here, for example, like individual employee records or things like that, being memorized by a model and becoming identifiable, becoming a security or compliance risk for your organization. It's an acceptable trade off to work with and that's one of the things that we're always working to improve. That's it for today. Thank you for your time. And don't hesitate to reach out if you have any questions.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/machine-learning-frameworks-weights-biases", "page_title": "Machine Learning Frameworks - Weights & Biases - Video", "headers": ["Machine Learning Frameworks - Weights & Biases", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel.ai's co-founder & CPO Alexander Watson for an ML Framework talks with Sanyam Bhutani.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/nvidia-gtc-spring-2022-workshop-outperforming-real-world-data-with-synthetic-data", "page_title": "NVIDIA GTC Spring 2022 Workshop: Outperforming Real-World Data With Synthetic Data - Video", "headers": ["NVIDIA GTC Spring 2022 Workshop: Outperforming Real-World Data With Synthetic Data", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Alex Watson, Co-founder and Chief Product Officer at Gretel.ai hosts a workshop on how to generate synthetic data that is even better than the real thing. ", "(00:03):", "Hi, my name is Alex Watson. I'm a co-founder at Gretel.ai.", "(00:06):", "Today, we're going to start with an overview of synthetic data: what it is, some of the popular use cases that we're seeing around synthetic data, and also from our research, how synthetic data is capable of outperforming real world data for some machine learning tasks.", "(00:23):", "For a little background on Gretel, we are a startup that helps developers and data scientists build with synthetic data.", "(00:31):", "Starting at the very top. What is synthetic data? I like this definition from Nvidia: \"Synthetic data is annotated information that computer simulations or algorithms generate as an alternative to real world data.\"", "(00:44):", "The concept of synthetic data is nothing that's new. It's been around for tens of years, really, since the first time that someone had to create an artificial data set, or one that didn't exist, to demonstrate a solution to a problem.", "(00:56):", "What is fascinating about synthetic data are the recent advancements we've had in machine learning, whether we're dealing with language models, whether we're dealing with generative adversarial networks or other types of models, we've created the ability for machine-learning models to learn the insights and the distributions, and to be able to recreate those insights and distributions from real world data sets.", "(01:22):", "Talking about some of the use cases that we see for synthetic data. One is with synthetic data, you have a unique ability to influence the output of that model. One of the things that we can enforce during the time that a model is being trained is privacy. We can make sure the model doesn't memorize rare secrets that might exist in the data, or that the model outputs data that could never be linked back to the original data that it was generated from.", "(01:49):", "This helps address some of the biggest challenges that we have around data access and sharing today. And highlights the first use case that we see here.", "(01:58):", "Number one, making data, private data, accessible. One of the patterns we've seen talking with tens or hundreds of customers at this point, are increasingly decentralized development teams that are building in their own environments and incredibly empowered to build what they need to build. However, this naturally creates data silos and it makes it difficult for data sharing to be possible, even data sharing within the same team. For example, if you have a production application that has sensitive customer information, you might want to build a dev test or staging environment that you can use to simulate scale with your service. You can use to test your service. However, one of the big concerns you have is actually copying that sensitive production data into that pre-production environment.", "(02:44):", "So one of these cases that we can dive in here, is it possible to train a machine learning model on our production data, create a new anonymized version of that production data could never be linked back to the individuals that the data was based on, but it would simulate the same type of dynamism and insights as the original production data that is trained on?", "(03:06):", "Number two, generating samples from limited data sets. In the age of big data, it's surprising that no one seems to have enough of the right type of data. So here we talk about this pattern of starting from just a few examples. For example, I'm a machine-learning researcher and I'm training a chatbot on utterances or commands that might come from our users. And anytime there's a new type of utterance or command, perhaps the model hasn't seen that well before, is having a hard time with it. So they need to start from just a couple examples, but then generate a multitude of examples from that, is a real challenge.", "(03:43):", "An evolution of that thinking too, is when you look across the typical data science project and how long it takes to get access to the original data you need, sometimes having that data doesn't exist at all.", "(03:56):", "So another neat use case for synthetic data are models that are trained on massive amounts of public information. This could be on public location data. It could be on shopping cart behavior. It could be even examples of open source, heart rate, or EKG type monitors, and allowing you to compose your own data sets. So you can quickly test your ideas on realistic synthetic data.", "(04:21):", "Number three, this ability to influence a model and the type of output that it has has allowed us to address some of the bigger challenges with machine learning today. So essentially, what we've done is created a toolkit around synthetic data that you can use to influence the output of a model. And when you can do that, this is something we'll dive into in our examples today, you have the ability to correct bias. You have the ability to influence the distribution of data, which has implications anywhere from AI ethics to fairness.", "(04:56):", "So how big is synthetic data and what's new with it. And it's rare that I put a Gartner slide inside my presentations, but I think this is one that seems to highlight some of the potential for synthetic data very well. Here on the diagram in front of us, we see that by the year 2030 Gartner predicts that AI models in the future will be completely overshadowed. Synthetic data will completely overshadow the use of real data inside of the model. How is this possible? And what we're seeing today is we all, I think we all know that the advanced machine learning models require a lot of data to work well. And when we get into some of the more advanced applications of machine learning today, whether it is speech recognition, whether it's self-driving cars, whether it's even kind of face recognition or things like that, there are infinite examples that exist in the real world.", "(05:50):", "And the training set that can be used to train a machine learning model is based on real world examples that have been annotated. This process can get very expensive, especially as you're working on the long tail, trying to improve your machine learning algorithm. So how do you account for different, if you're creating something to recognize people's faces, how do you account for different shades, backgrounds, things like that. If you're working with voice, how do you account for background noises? How do you account or different tonal variations in speech?", "(06:21):", "The promise in synthetic data is the ability to work from just a few examples or an idea, and to create a multitude, almost unlimited amount of examples of different permutations of that data that can be used to train machine learning algorithms. So when it comes to the generalization problem, how do algorithms learn to recognize data they've never seen before, synthetic data is a very promising approach.", "(06:43):", "A second potential trend that we're seeing too is with the trends that are happening in devices. So more work is being done on the computer, on your iPhone, for example, or on your Alexa, than ever before. And what that means is less data is being sent to the cloud. So the traditional approach of annotating data in the cloud and using that to train your models isn't always possible because less data is being sent to the cloud. Some really positive privacy benefits here, and really good things for consumers. But it's created a challenge for people building applications that, for example, need to understand your voice and need to work from there. Whereas the synthetic data will allow you to take advantage of a small amount of samples that you might have, an increasingly small amount of examples, but create algorithms that will generalize the new inputs extremely well.", "(07:36):", "So here, we're going to pivot a little bit and start talking about the APIs that Gretel has built for synthetic data. And we're going to use these APIs in the next couple slides to work on popular data sets. And we're going to take one particular example and address it both from an accuracy and a fairness perspective.", "(07:52):", "At Gretel, we have three core APIs. On the far left here you see the area we spend the most time and our research efforts around synthetic data. So here we talk about a couple of the advantages, but what Gretel is really trying to do is make data, synthetic data, APIs, not complicated, not scary, make it available to any developer, any person that wants to sign it and use it.", "(08:15):", "So here on the left, you see synthetics. This can be language models. This can be gans. We really believe there is no single bullet approach to creating synthetic data.", "(08:24):", "However, there is a lot that you can do to make it accessible. So anyone working with the data set, how do we create that to be part of the pipeline they have for training the machine learning model, if they are trying to enable access to a data warehouse or to a data set for another team, how do we allow you to really quickly de-identify? So that remove the known variables, names, addresses, things like that, and then synthesize, create a model that can be used to create unlimited amounts of data, either the same size in the shape as original data or 10 times as many records. For example, if you wanted to have additional variations with different levels of privacy. And how do we help you find that right balance between privacy and accuracy for your downstream use cases?", "(09:09):", "The two APIs here are on the right transforms and the data classification, the labeling are a pre-processing step that we use with synthetic data. As I mentioned a second ago, it's really important to identify the no knowns inside of a data set. For example, if you are training a chat bot on customer service or customer reviews, for example, you want that chat bot to learn from semantics of the data. You don't want to simply redact names though. And that would kind of send the chat bot down the wrong direction. So one of the options here are using the data classification APIs to identify PII names, addresses, credit cards, things like that that should never find their way into machine learning model transforms lie to create a simple policy, to replace that with a fake version to encrypt it in place, to redact it, to drop the record. Really, you have a complete building block of different options here.", "(10:03):", "And then finally, synthetics, which really gives you the finite control over what you went into with that data. One of the top questions that we get around synthetic data is how accurate is it? And the use case being I'm trying to enable data access inside of my organization, and I want teams to be able to share data, I want teams to be able to access our awesome data warehouse. How do I make that possible? Question number one. Synthetic data's a promising approach for that, where you can create a synthetic twin of your original data set that has increased privacy guarantees and you as a developer inside your business, instead of waiting two weeks, four weeks, even six months, in some cases, as we've seen in the genomics world, to get access to data sets. What if I could get access to a data set right now that had 97, 98% the accuracy of the real world data? As a developer, as a data scientist, do I always need to see real names? Do I always need to see real addresses? The answer is no. Often we don't.", "(11:04):", "Traditional de-identification, that would be replacing names and addresses, but keeping the rest of the records the same, has been proven over and over again in the privacy space to have to be inadequate, really, to protect the privacy of the users that it's based on. Simplest example I could give would be the Netflix challenge, for example. Where a couple years ago, Netflix listed a competition on data science platform Kaggle, and they de-identified a hundred million different movie reviews and they did an excellent job de-identifying this data set. It had only a movie ID and a user ID and a date and a number of stars that they gave for the movie. They ended up having to pull down the competition because some of the competition teams realized that just that combination of the date review was done and a movie ID and a user was identifying enough that they could unmask the users by joining it with movie reviews that they had seen, for example, on IMDB.", "(12:01):", "So this is a real challenge. This is called a data linkage or re-identification attack. Synthetic data, how it helps get around this is it trains on the overall Corpus of data. And it creates a new Corpus of data where none of those individual records are linked to a real record that might exist in a database somewhere else. So it's one of the real promises of synthetic data. The question is, how accurate is this new Corpus of data that I created? And we wanted to go with some examples we thought would be right in line with data scientists and developers are working with here. We took the eight most popular data sets on Kaggle. So just looking at the hotness or the relevance metric on Kaggle and used completely default parameters for Gretel synthetics and created another data set of the exact same size and shape as the original real world data that is trained on.", "(12:53):", "Then we took, as you can see here, and we can dive into the details on how this worked, we ran each one of these on a downstream use case or task that was associated with this. So we took the top notebooks on Kaggle that were running on real world data. And then we ran it on our synthetic version as well. And we compared the results. And really excited results here. You see for just complete replacement, not augmentation, complete replacement of data, here you see for stroke prediction, which we will dive into, in later example, we actually saw an improvement here using the synthetic data. That means the model must have keyed or learned on something that helps with the downstream analysis. In some cases you see a slight degradation performance. Here we can see on the data science job candidates use case and things like that. But really important thing is overall just using our default parameters here, we had only a 2.58% decrease in accuracy between the real world data and the synthetic data that was created. So pretty exciting initial results here.", "(13:55):", "So if we can get within 2.58% using our standard data classification or sorry, our standard synthetic data libraries, can we make it better? And can we improve on real world data? And what types of problems are we seeing in this space right now anyways? Here, I wanted to highlight just how powerful, and I think many people in this audience understand just how powerful machine learning models are in our lives and how much they influence things increasingly day to day that are really important to us. These are based on data sets and often those data sets, I think we all know are limited. They may not have the right distribution of people that they're based on. They might be out of date. All sorts of different things that can happen that impact from a fairness perspective or an accuracy perspective, impact us in a very real way.", "(14:44):", "A couple of the examples to call out here, you know, whether you're getting hired for a job or not, right? So organizations increasingly creating bots that will scan resumes and help with that really manually intensive process of scanning resumes and selecting candidates that would be selected for a following phone screen. We've seen through examples, there was a famous kind of Amazon example where those models were largely focusing in on the data that they were trained on, which was largely male candidates. And they were actually disqualifying different terms that female candidates used inside of their resumes. So really dangerous example there. Medical use cases, diagnosing heart disease, things like that. Even the ability for us with a slightly different voice pattern, inflections or slang to talk to the devices around us, how do we make sure we have a good representation across all different possibilities of people and demographics and things like that to ensure that these algorithms are going to give us a fair response?", "(15:47):", "Well, there is no bullet, but we have tools. And that's what we're going to dive into here and talk about how we might be able to make this better. One of the times this was described to me really well by one of our customers who is running a major data science team for a really large gaming company said, \"If there are biases in the real world, in the virtual world, they're often magnified.\" We want to make sure and provide a set of tools to help developers and data scientists here is to help them influence this, to minimize new biases being introduced and to create essentially the most fair experience possible.", "(16:27):", "So I think it's time now to jump into a real use case. So we will provide some code. You're welcome to follow along and run this yourself. But, starting at one of those data sets we saw earlier, an extremely popular data set, one of the top five most popular data sets on data science platform, Kaggle is this heart failure prediction data set. It was published by the folks at UCI. So really incredible data set. And it's been one of the canonical examples for data classification techniques on this data science platform for quite some time. The question is, what is the distribution of the data that this was trained on? Where did this come from? Are there any gaps in this data that we might want to address? And if we can address those gaps, can we make a better data set? And better, we use the definition here, could be either more accurate, so better overall performance for the data set or more fair. When you look at some of the different categories inside of this data set, for example, we look at age distribution, we look at here we use the term sex for male or female. If we look at the location the user came from, things like that, how do we make sure that those demographics are evenly balanced inside of the data set? If we do that, what impact does that have and the overall accuracy of our data?", "(17:45):", "So what we're going to do is we are going to train a synthetic model on this data set, and we're going to create essentially another version of this, where we will balance out one of the attributes that stood out really quickly here. And here we see 32% female records over 68% male records. So a theory here is that an algorithm trained on this data set would possibly over index on being really great at male heart disease detection, and pay less attention to minority classes, for example, different age groups, different sexes, genders, things like that that might exist in the data. And if we balance this out, so essentially help the algorithm to create a more fair response by boosting the representation in this case, just a single attribute, we're going to try boosting the female representation. How does that affect overall performance?", "(18:40):", "So diving right in, we'll provide links to run all this code yourself. One of the areas I like to do first is just run a parameter suite. So what we did is we took this data set, loaded it in Gretel through a Python notebook, which we can walk through here. And we tried a set of different parameters. Really our goal here is maximum accuracy and maximum fairness. So how do we get there? You can use the default settings as we've shown earlier, which work quite well. Or we can try a bunch of different parameters. We can try our downstream use case here. So I ran a standard random forest classifier on the results and tried to predict heart disease detection with the purely synthetic data set. And here we can get a good feeling for which different sets of APIs and configurations here work best.", "(19:25):", "So we'll jump right over to the actual parameter sweep here, and we could take a look. So here's one, we had the best classification accuracy here for the pure synthetic version here. And we can see batch size, relatively small number of epox, a pretty conservative learning rate. Since this is a small data set, this is kind of interesting here. A smaller number of RNN units for this synthetic data task and a vocabulary sized. So essentially allowing, we use something out of the hood called sentence piece, which is built by Google to find tokens within the input data. So using that versus pure character based tokenization seemed to work pretty well. So this is the configuration we're going to start with essentially using this to train the neural network. We will have the neural network essentially output additional records so we can balance out the male and female class. And then we'll compare essentially the pure real world data to the real world data plus augmenting it with a balanced number of female and male records. Let's jump right over to that use case.", "(20:37):", "Let's jump right into our example. So we'll go to docs.gretel.ai. From there, click on STK notebooks. And go down to the bottom. And you can see an example called \"Improve accuracy, heart disease\". So we're going to go ahead and click that. This is going to open up a CoLab notebook that will walk us through our entire experiment. So here we see it loading up in the free Jupyter notebook experience with CoLab. We'll go ahead and clear out the outputs that exist. Click run all. So we're going to go ahead and run this. We'll run the entire experiment using the parameters that we selected in the previous parameter sweep. While it's installing dependencies, we're going to go ahead and log into the Gretel console service here. We need to access the API key. So the notebook knows how to talk to our cloud service. Go ahead and sign in with my Gmail ID.", "(21:29):", "It's just about finished installing dependencies. Next, it's going to go ahead and ask for my API key, which we can go ahead and enter in and copy that. As we could see here, the next step is loading up the train and test sets. So pulling down the example heart disease data set from Kaggle, we did the 70/30 split, created two different sets. What we're going to be doing here is comparing their real world performance of the training set versus a train set that's been augmented with synthetic data. So that's what we're going to do for the rest of this notebook. So go ahead and get it, go ahead and let it run.", "(22:10):", "So similarly to what we saw in the original graph that we just showed in the previous slide show here, we see a big skew in distribution between male and female records. And what we're going to do here is create a synthetic model train on the real world data set, tell it to boost the representation, in this case of female records. And we're going to see how that data set that performs against the original real world data. Next, we have a really important part. So what's happening here is we are training a synthetic model on the real world data set. So here you can see we're bringing in the Kaggle client. We pull back the default configuration parameter, we'll make a few updates to match the parameters that we had from our parameters suite.", "(22:55):", "And here we're going to use that conditional data generation task. This is a really kind of neat task with more advanced machine learning models, where I'm defining a single field, in this case, sex. You could define multiple fields. You could build something for a particular age range, plus sex or different heart rate. You know, really any of the different features you wish. In this case, as we said, we're just be balancing out this sex attribute. We're going to define this as a seed task. This is the same thing as conditional model generation. Essentially, we are telling the model what type of record we want it to produce. And as we indicated earlier, we'll be telling it to produce additional female records to balance out the quantity of male versus female records in the training set. Why would we do this? I think is an important question. Why balance this out? Why not just gather more data? Often the steps necessary to reproduce and experiment and get the same results become prohibitively expensive, if not impossible to recreate.", "(23:51):", "So often we are limited with the data that we have. And in this case, there was no way to go back and recreate additional heart patient experiments with the same methods and procedures that were used before. So this leaves us with the option of boosting the representation across that data set. In this initial run, we're going to tell it to generate 500 records. We can use that to look at the quality of the synthetic model, essentially generate data, throw the kitchen sink from a statistical perspective at the real world data versus synthetic. And we can compare the two. You can see here also we are turning off the default privacy filters. We're working with the real small data set here. We want to make sure we capture every insight. So in this case where privacy filters often give very little actual real world accuracy hit, when we're going from maximum performance on a really small data set like this. In this case, we've made the decision here to remove this and laity the outlier filtering.", "(24:45):", "This will create a new project in Gretel called UCI heart disease. And we'll take a look at that in just a minute. And we're going to go ahead and kick off training. So here we can see, training is starting. What's happening is behind the scenes here, the cloud is firing up a container with access to a GPU. It's going to start processing this data set and it's going to iterate over the data set until the model has effectively learned the parameters of the underlying data set. Let's go ahead and let that start.", "(25:19):", "So here we can see it's loaded it up. It has started creating the data set. It's going to start creating the validators here in a minute. Essentially what validators do is they ensure that the output of the neural network matches the same types of distributions as the original real world data. So one of the downsides of neural networks is they can output anything. When you're looking for high quality synthetic data, one of the things that we can do to make sure that it works better is to enforce that the somewhat random output of interval network makes sense for your particular use case based on what we saw in the training data. Here, we can see it trained for about 44 epox. I sped this up a few minutes just so you guys didn't have to wait through it. Here we see 44 epox we see very good accuracy. So we're at 90% prediction accuracy. Loss is very small. So you want to see accuracy going up. You want to see loss going down. We're looking good on that. We had to generate 500 records. So it's just starting that record generation process now. This invalid count, which you're seeing here, is some of these re records didn't pass validation. So something about that record, whether a floating point number was too high or a new category was invented that didn't exist, something like that occasionally happens, we'll see that being dropped.", "(26:32):", "As that data set's being generated here, we're going to go back to the Gretel console and we can see this model and let's go ahead and take a look at the performance of this model and how it's doing. So the model train pretty quickly. We can see the records being generated, just like what you're seeing in the Python notebook over there.", "(26:51):", "And record generation is complete. At the end, we generate a synthetic quality score. You can see here is very good. So we're happy with our ability to reproduce the insights and distributions to the original data. Privacy protection level is much lower since we turned off the default privacy mechanisms. Looking through here, we see a quick summary of the privacy statistics and then diving right into the accuracy, which is so important to us. This is my favorite graphs to look at. What we see here are the correlation graphs that exist between the training data. So the correlations that sat in the original training set and the synthetic data set, and what we're looking for is how closely is it able to match that. You don't want to recreate it exactly, you want to match the correlations. So here, where we see stronger correlations, for example, between these two different sensor readings here, we want to see them be as close as possible. See 0.41 in the training data looks like synthetic correlation may have been slightly stronger in this case, but still very close at a 0.45.", "(27:50):", "This is this second one of my favorite graphs to look at. Principal component analysis. This really helps you understand that whether the model overfit on a few features in the data, or if it was able to kind of recreate some favorite in a data science toolkit, whether it was able to recreate the same type of dimensionality as the original data. So we're looking for distributions and shape between these two different features to be very similar. Looking at that, even for such a small data set that we're working with here with only a little bit over a thousand rows, we see it learned it pretty well. We see the distribution shape being just about the same. So it's another kind of sense of confidence that it learned the distribution fairly well.", "(28:28):", "Finally, when we're looking at per field or per column distributions, we've got the field distribution plots. This shows you for every value in the synthetic data set we created versus the data that was trained on, how closely are we matching the distributions? You don't want this to match exactly. What you're looking for are patterns that are close, but not the same. You see a nice kind of [inaudible 00:28:50] look on some of these distributions, which is very good. So we see things being close, but not quite the same. The intent of this report is really to give you a sense of confidence that yes, like this data set, this synthetic model, learned the nuances of my data and is able to recreate it. If we task the model to create enough records, so we created a total 2000 records here that we're using to augment our original training set, that we would create an even distribution between male and female records here.", "(29:17):", "So here's our augmented data set, synthetic plus training data. We'll take a look at it very close within one. So it was a math error on my part. And next thing we're going to do is run a set of data classification tasks on there. So we are comparing in this case, we're comparing real world data versus our augmented synthetic data set. And we're comparing the accuracy for a set of downstream classifiers. So does this patient, particular patient in the test data set that we isolated at the very beginning, have heart disease or not? Really encouraging results that you see here as we go through here. So we took six or in this case five, the more popular classifiers that are out there, random forest, decision trees, XG boost, SVM, and we ran the classification task as you can see here for each one of these. On the left, you see real world data. The accuracy for predicting heart disease here in this case with random forest was 95% on this data set.", "(30:13):", "So pretty good accuracy. The really cool and inspiring thing here is we actually see an increase in synthetic data accuracy. So when we're using our synthetic data set on four out of five use cases here. So support vector machine, for some reason, the accuracy from our synthetic model was slightly lower. But in many of these cases, using this boosted data set with an equal number of male and female patient records, we're able to achieve just as good, if not better, accuracy that we did with the real world data.", "(30:47):", "Going on a little bit more. Now we're going to take a look at the distributions, not just at accuracy, but let's take a look at fairness. So essentially we're going to break down some of those minority classes. In this case, we're going to take a look at the sex attribute and see for males and for females across synthetic data and real world data, which one performed the best. We'll jump back over to the slide so we can see this a little bit better.", "(31:14):", "So we'll jump back into the slides here and take a look at the results from our notebook. So pretty exciting results. As we saw here just a second ago, we see improvements in four out of five classifiers working with the augmented data set versus the real world data and pretty close here. So about a 1.3% improvement in accuracy, which is pretty substantial coming from an 88.97% start. So pretty exciting. Looking at the class distribution here. And this is where you start looking at fairness. How well did the algorithm work for different, in this case, we're looking at the sex attribute inside of the data set, which was defined as male and female. How well do we perform here? So in the light colored blue on the left here, we see real world male performance, heart disease using the real world data set versus the augmented data set.", "(32:04):", "So we see, for example, a random forest, see a 1% improvement here. And we see with random forest female heart disease detection staying very consistent. However, across the rest of the algorithms here, you see sometimes really substantial improvements here. SBM is up nearly 15% in heart disease detection for females, XG boost, which is I think widely regarded is a really excellent classification algorithm, the accuracy went from 96.63% to 100% for female with heart disease detection using this augmented synthetic data set. So really encouraging results across the board for females. As you can see here at the top, we had a 4.49% improvement in heart disease. And 1.25% improvement for males resulting in an average 1.3% improvement across the entire data set. So really exciting and something that many of the customers we're working with right now are working on. How do we open it and build better data sets? How do we automate this process and create kind of the most fair and inclusive responses that we can?", "(33:13):", "If you'd like to run through any of this stuff yourself, feel free. We've got tons of open source examples. It's free to use Gretel. We have a developer here. So you've got several hours of compute that you can use for free, or you can deploy Gretel and run inside your own environment as well. So here's a link to YouTube channel. I will drop links to our notebooks and our docs below, but docs.gretel.ai works as well. If you have any questions and would like to follow up on your use case, ask any questions and have a discussion. Either join us on our community slack, where many of us spend a lot of our time. You can get to that at Gretel.ai/slackinvite or reach out to us at hi@gretel.ai. Thank you very much for your time. Bye.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/category/machine-learning?094d394d_page=2", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "Gretel GPT Sentiment Swap", "Install TensorFlow and PyTorch with CUDA, cUDNN, and GPU Support in 3 Easy Steps", "Synthetic Data and the Data-centric Machine Learning Life Cycle", "Comprehensive Data Cleaning for AI and ML", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Conditional Text Generation by Fine Tuning Gretel GPT", "Unlocking Adapted LLMs on Enterprise Data", "How to safely work with another company's data", "Red Teaming Synthetic Data Models", "Scale Synthetic Data to Millions of Rows with ACTGAN", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/videos/odsc-east-2022-workshop", "page_title": "ODSC East 2022 Workshop: Open-source Tools for Generating Synthetic Data on Demand - Video", "headers": ["ODSC East 2022 Workshop: Open-source Tools for Generating Synthetic Data on Demand", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel Senior Applied Scientist Lipika Ramaswamy hosted a workshop at this year\u2019s Open Data Science Conference on how to use our open-source tools to democratize access to sensitive datasets. ", "Hi, everyone. Welcome to this session. It's going to be a workshop and we're going to use some open-source tools to generate synthetic data.", "So a little bit about me and just a quick intro. I work at a startup called Gretel and we help developers and data scientists build with synthetic data. I specifically focus on the privacy side of the house. So figuring out how deals like synthetic data, which is one among many, can help to solve privacy problems and enable wider and better data sharing.", "All right. So today we're going to cover many things, but the broad topics are the what, how and why of synthetic data. So we're all sort of starting on the same page. Then we'll follow with two examples. One is to generate synthetic Census data and I'll motivate the problem and make sure to give you all enough background around why I think it's important and why I've selected it. The second example is around time-series data, which tends to be very popular out there in the wild. So we'll look at generating synthetic time-series data and we'll use web traffic data as an example. And finally, just towards the end of the session, it's going to be 90 minutes, give and take some, we might need a beverage at the end of it. So let's see what we can do with that in the synthetic world.", "All right. So I wanted to take some polls before we get started. Okay, so the focus of these polls is I just want to get a sense for sort of what industry you work in, what your level is, are you a student, are you directly a data scientist, whatnot? So trying to understand what you work in, and then also your level of comfort with machine learning, Python, just synthetic data in general and generative modeling. And so that'll give me a good sense for how to sort of communicate with everybody through the rest of this tutorial workshop. So I'm going to make some educated guesses, or maybe not so educated guesses here. I'll try to explain things in certain amount of detail, but I'll assume that folks have sort of a foundational knowledge of data science and machine learning. But we'll certainly [inaudible 00:02:27] through motivation for why I'm suggesting we do things the way that we do them.", "All right. So the big question, what is synthetic data? Maybe you've heard of it before, maybe you haven't, maybe you think of synthetic data as just sort of fake data that sometimes does not make sense. So what is synthetic data? There is a definition from NVIDIA that I really like, which is, \"Synthetic data is annotated information that computer simulations or algorithms generate as an alternative to real-world data.\" And so this is not something new, it's been around for many decades. In fact, the first time that somebody built a dataset that didn't exist before in an artificial way to solve some problem, that's synthetic data. What's really awesome about synthetic data now in the recent, let's say decade or two decades, is the advances in machine learning, which allow us to build really convincing synthetic data.", "And so how do we make synthetic data? So a very high level is you take a sensitive dataset or you take any dataset that you have and you apply some machine learning models to them. So that could be a language model. It could be a generative adversarial network, which is a type of generative model. It could be a model that says, \"Hey, okay, I have some data. I'm going to see what the distribution of each variable is within this dataset. And then based on those distributions, I'm going try to sample new records.\" And so implicit here is my assumption of your understanding of the generative models. And I'll take a step back and maybe define that a little bit. What do I mean by a generative model? It's a model that takes its input data and also produce it as output data. So there's some process in the middle that takes data as an input and produces data as an output. That's in general what a generative model does, but how it does that is by attempting to learn sort of the underlying distribution of the data.", "So if you assume that there's some generating process, there's some data generating process out there, that's sort of ground truth. So for example, there's some, let's say we're talking about some law of physics, you're collecting data to support that law. There's that law that governs what the relationships are between variables. And so that exists. There is a process. It's known. And that's sort of the best piece where you know what the process is. In most cases, we don't know what the data generating process is. We don't know what the ground truth of this data is. We've only just absorbed instances it. So after absorbing instances, we're trying to understand where does this data come from? What is the data generating process? Can we write it in equations? Or can we have, for example, a neural network, learn how this is done?", "Maybe that went over your head, maybe you understood it, maybe you know a lot more about this. So hopefully that helped, but it's not super essential. Just knowing that what we're looking for is data as an outcome or as an output from a model. That's what a generative model is and that's what we use to make synthetic data.", "So you might be thinking, \"Well, why is there a need for synthetic data? Why do we even care about this problem?\" There are a few different reasons. It depends on sort of what stage you're at. Are you a student? Are you working in a large organization? Are you working at a startup? And so your motivation for seeking synthetic data might be different, but let's cover a few of them. So one very common one is to make sensitive data accessible. So for example, let's say you're a student and you are trying to build some sort of prediction model on healthcare data. So you might go to Kaggle, the open source, sorry, the data science platform Kaggle. And you might say, \"Hey, I'm looking for datasets in the healthcare field. Let me go see what I can find.\" And so that's data that's openly available, but it may not actually fit the problem that you're trying to solve, right? Maybe you're trying to find a dataset on a population with a rare disease, and maybe that doesn't exist in Kaggle. Maybe you can't find it in data world and it doesn't exist.", "So synthetic data would be really useful in this case where, let's say a hospital has a population of people with a certain rare disease and they're able to produce a synthetic dataset and give you access as a student to build some model. And so you don't actually need the real data. You don't need patient information. You're just trying to learn what the relationships are, underline that. In that case, something like synthetic data would be really useful. Another really common example of this is, for example, in many large organizations, right? There can be a lot of different verticals. There can be a lot of different data silos. So you have to have sort of permissions to view certain types of datasets, right? Maybe they're collected from customers. And there are only a certain set of people who are sort of authorized to work with that data.", "And maybe you are a software developer and you want to build some sort of application that uses that data. Or you're trying to sort of test an algorithm that's supposed to work in production, but you can't actually get access to the data upon which it's supposed to work. So something like synthetic data could be extremely useful in that case as well where you don't really need access to the exact data. You just need something that looks and feels like it and does the job. So you know it's going to work in production. So that's one really, really common example, but how do we just get over all these access issues and get some data that's going to help us accomplish our task?", "Another one is to augment limited datasets. All right, it can be the case where you're trying to collect data and it's very, very expensive to collect this data. Maybe it is... You have a certain budget and you need to collect EEG data because you have to do a certain type of analysis. And that can get expensive really quickly. Really medical measurements get expensive. Even if you're trying to build an application, maybe you're very industrious and you started a startup and you're trying to build some sort of software tool and you need data. You can only collect a certain amount. How do you get more, right? Because there's an expense to that. So that's another example where you really need to augment limited datasets and they may be limited for any number of reasons.", "Final one that's very important is the ability to balance data to reduce bias. And in many cases, improve predictions from models. And this is the one that we'll be focusing on today. So we'll go a lot more in depth into this particular point and we'll see how we can actually get better predictions from models by augmenting our dataset with synthetic samples.", "Okay. So that's sort of the groundwork that I've laid for this session. Let's talk about the nuts and bolts, right? So today we're going to use Google Colab. We'll take about the next hour to do a few examples to run through them. I'll be sort of live coding with you all, showing you all the tools and resources that I use to build synthetic data. So we'll be using Google Colab. You will need a Google account to execute code. And the links to the notebooks are just on the next slide. So you'll all have it. The second point is we'll be using, for part of this tutorial, we'll be using Gretel Cloud. So a little bit of background. So the company that I'm at, Gretel, we build tools for open-source synthetic data generation. And our console is a place where you can submit a dataset and it sort of gets processed on our Cloud and we provide you with the synthetic dataset.", "So this is not exactly fully open-source because there's a whole console or element to it, but all the underlying elements are open-source and we'll use those sources. Okay. So please sign up for a free account on console.gretel.cloud. The third point is if you really don't want to code in Python, you haven't done that in years, you don't feel like doing that today, that's totally be fine. Please just follow along with what I'm doing, but there will be a no-code example for which we'll leverage the Gretel Console. So stick around for that. It's going to be long, right? It's roughly 60 minutes from now. So we'll take a few three-minute breaks. Feel free to ask me questions during that. Please use the questions panel on the go to webinar and ask me any questions. We can about that during the breaks, or really anytime if you're super stuck. There are other folks on here. Some of my colleagues are on here who can help with that as well.", "So let's get some breaks in, stretch your legs, but also, I want to make sure that you all have momentum to sort of execute a task from start to finish. So feel free to jump in with questions. And if some of this doesn't make sense or you think things are going too fast or you miss a step, don't worry. In a few days, we'll publish all these notebooks and have this all available to you so you could use it. All right.", "So here are some links. These are the ones that you'll need. We'll start with example one, which is we're going to use Census data to classify individuals based on income. So you all might be familiar with... There's a dataset that's called UCI Adult. And it took data from, I want to say the [inaudible 00:12:57]. So some Census data and a researcher took this data and basically processed it and was trying to create a prediction task to predict whether an individual has an income of greater than $50,000 or less than $50,000. So the classification task for which this dataset was produced. What we're going to be using is a little update to that dataset that was published last year. It uses more recent data, so data from 2018. And we'll use it via a tool called Folktables that some researchers published.", "So if you take this URL, I will try to drop in the chat. Oh, there we go. All right. That is the first one. And you should have access to this page. So I would highly recommend because I've authored this, I would recommend in Google Colab if you go and save a copy in your own drive. You'll be able to execute the code and save your notebooks and there wouldn't be any sort of version control issues. All right.", "So first thing we'll do is we'll install all the things that we need here. I'm going to do the same. I'm going to take my advice and save a copy in my drive.", "And let me do something here. Maybe this time URL doesn't work. So here's a direct link to the notebook. All right. Hopefully everyone has access. Just run the first cell.", "Hopefully you should be able to connect with a GPU by the way, but we'll run through that. All right. Let me know if you're not able to access this notebook. I'm getting a lot of email notifications, which is totally fine, but just click on this link. And it's the one in the chat from 11:39 AM or 12:39 AM Eastern. All right. So the first few cells should be populated. And the first thing you'll do is download and store data from Folktables. And so what this is doing is it's going to the American community survey data. And for the survey of 2018 for each individual and for sort of a one year horizon. I mean, these means some things. You can look up the documentation for folk tables. Basically, we'll go and get all this survey data for 2018 and we'll process it in such a way that it has the same format as the UCI Adult dataset. So there's a certain set of variables like age, work class education, marital status, occupation, whatnot. And finally, the income bracket, which is a variable that's 0 or 1. And that's what this process is linked to.", "I'm also choosing this state of Kansas to get this data for. So you can feel free to choose any state that you'd like. The reason I chose Kansas is because there's sort of class and balance that I want to demonstrate.", "All right. So this is, if I look in the files here, this is downloaded, data for 2018. There's one-year data in here is the big CSV. And then this new USAdultIncome.CSV is the one that I'm going to use. And that's the one that we created to sort of fit this classification task. All right. So let's look at this, right? Everything is coded. It's all [inaudible 00:17:20] because that's how the ACS survey is coded. As you'll see, there's age. And then a lot of these are categorical. We want to go into what the categories mean, but there's sort of a code book that you can look up pretty easily. And I think I've provided links to where that can be found in this cell. All right. So here's the dataset and our goal is to create a synthetic version of this dataset. Before we go any further, I just want to make sure that everybody has access. So if you don't, please feel free to add a note in the chat or add a question and folks will help you.", "And so everybody should have viewing privileges and you can just make a copy for yourself and your own drive. Okay, great. It looks like things are working. Thank you everyone for chiming in. Appreciate that. Little bumpy getting started, but hopefully things should go smoothly now since you all have access. All right. So while that data is running, let's talk about what we're going to do first. So we're going to use something called Gretel Synthetics. The link to the documentation is here in one of these cells. So we're going to use Gretel Synthetics, which is an open-source library that has tools to regenerate synthetic data. So as you can see here, you sort of have to provide a configuration. There's a tokenization element, and then you specify a model, you train it, you use it to generate records and then there's some utilities to sort of make things run faster.", "Okay. So here is Gretel Synthetics. This is the GitHub page. I just got here by clicking edit on GitHub and going to the main [inaudible 00:19:25]. And so you can see everything that's in here. So if you're interested in the internals of the algorithm, you can find that within the source file and you can see basically everything that we do in order to generate synthetic data. And so the idea is... Sort of at a high level of what this model is is a next open prediction model. So it's a language model that uses an LSDM cell, which is sort of like a recorded neural network. And what it does is learn sort of temporal correlations in the data. And so if you treat the entire dataset, so the whole CSV, treat it as a string of text. The task is simply, given what you saw last, what's the next thing that you're going to see?", "So in this dataset right here, given that I saw one, can I predict that the next character is nine? So that's a very basic explanation of what's going on here. But the way to interact with Gretel Synthetics is through a configuration file.", "So let's go look for the configuration here. I'm using something called Local Config. Just basically a TensorFlow configuration if you're familiar with TensorFlow. And we can go through and set up basically everything that's required for training this model. So I know my input dataset is going to be this one up here that I've defined that's called NewUSAdultIncomeKansas.CSV. And then I'm going to define all the things that need to go in. So let's start this. We'll see our config is Local Config, and then we'll specify a ton of things. So you'll see in the doc string here, there's a lot that you can supply that's also available in the docs. But we'll add in a few things and I am going to copy paste some stuff. So it's a little bit quicker and I can doc through it, as you all are able to type this as well.", "So the first one is called maximum line length, and that basically measures the number of characters in each line of the CSV. And here I'm saying that the line length can't be greater than 2048 when actually I can make it much smaller because these lines are going to be a lot shorter, right? This becomes important when you have three text and stuff in your data, because it does relate to performance slightly. Okay. The next variable is vocab size and that is sort of defining what type of tokenizer we'll use. So in any language model, like if you're looking at text, you can't just take text and pass it to a machine learning model because machine learning models are used to seeing some numeric values, right? So the whole idea of tokenization is can you take your entire text and can you identify, for example, words or maybe even alphabets? Very simple, which is denoted by zero here is character-based organization.", "And it basically says, \"Well, we're going to map each number and each letter and all punctuation to a certain index.\" So let's say the letter A gets indexed to 0, B, 1, 2 and so on. And that's what we call tokenizing. And that's important for this type of model to work.", "The next is field delimiter. So I'm specifying that this is a CSV. So it's comma separated, I'm adding that. You could add a [inaudible 00:23:03] separated. Overwrite, just a functionality. If we're going to train this model many times, right? Do you want to just overwrite each checkpoint because we are training a model using TensorFlow? So it outputs TensorFlow checkpoints. Then there's the learning rate, which is sort of the rate at which grading updates are made during the training process of a neuro network. Dropout rate, which is a regularization parameter. We use dropout layers in our model architecture. If these things mean something to you, that's great and feel free to play around with them. If they don't, these are just generally good defaults and you can think of it as all sort of ways to get models that don't overfit. Gen temp, this is something that's sort of related to the diversity of examples that are generated by the model after its trained. So if you set a higher temperature, you get sort of more wacky kind of examples. If you set a lower temperature, it's the opposite.", "And epochs is the number of passes over the data. So effectively, how long do you want to train this for? Well, let's keep 50 for now. I think this should go pretty quickly. Of course then there's the input data path, which is, it should just be this file right here and the checkpoint directory. All right, I'm going to run that.", "Great. So I have my configuration stored and training a model is super simple. I'm just going to use this function called train R&N. I'm going to pass this config that we defined up here to do that and run it. And you'll see, it'll start outputting a bunch of stuff. So it's saying that it's using a character tokenizer. You're using the right input dataset. It's doing a bunch of stuff. It's shuffling the input dataset. It's creating validation dataset. It's doing all the things that we wanted to do. If you're interested, here is the architecture of the model. So it's super simple, right?", "You might be looking at this and thinking, \"That's simple. What can it do?\" But you'd be surprised. LSDMs, I mean, they get a very bad rep these days, especially with the advent of transformers, but this still works pretty well for a variety of examples. So we'll start here. My training might be much slower just because I've been using Colab a lot trying to prep these notebooks. So you might get much faster results. So to do that, you might want to change your run time type to use a GPU. I'm apparently already using a GPU, but I'm out my credits. So, yeah, that's fine. I'll just train on PPU and it'll be okay.", "So you can see what this model is doing. It's basically telling you the accuracy of next token prediction. And even with, let's say one epoch, I'm getting, and it hasn't even completed, I'm at about 36% accuracy. So things are moving along and we'll let this rest go. But in the meanwhile, we can sort of do the same thing, but in a slightly different way. So if you scroll down just a little bit, you'll find this section called create a model to generate synthetic data using Gretel Console and Gretel Client. And what we'll do is we'll use Gretel Console to do sort of a point and click way of generating synthetic data.", "All right, I'm just reading through the questions now. Okay. So the first question that Madou has is, can we say it as a pre-trained model for creating synthetic data? This model here that we're training is not pre-trained at all. So I'm initializing it with random weights. There was no pre-training done on this model and it's simply trained from scratch. So it's only going to launch what's in the dataset that I provided. Now, can you use a pre-trained model? Yes, you can. So for example, if you're familiar with large language models, you could use, for example, GPD 2 or you could use GPDE or any of the ones on Hugging Face, right? Those are pre-trained models and you could find tune them on the dataset that you have in mind. So that is certainly a way of producing synthetic data when you have fewer examples. Yep.", "Okay. The next question from Madou is, is there a way to identify which model is the best fit for a test data creation? So here we're only looking at one model, right? This is just a simple LSCM that seems to work well for a lot of used cases. Are there other models? There certainly are. So there are things like tabular GANs, there are simple sort of marginals-based methods that you can use to produce synthetic data. There's patient networks. There's a lot you can do. So the model universe is very vast, right? There's obviously no single model can produce the best data all the time. And so that's really great question. How do you identify which model is going to work the best? Sometimes there are heuristics. So for text data, you probably want to use something that has a recurrent nature. For entirely tabular numeric data, maybe you're actually, and maybe it doesn't have that many columns, maybe you're better off going with the marginals-based method.", "But there is no sort of formula for it I would say. There are also trade offs, right? So there are some architectures that are very, very good for producing a certain type of synthetic data, but you might want to think about what is the compute cost of it, right? Is it going to be a very complex model to run? Is it going to take a lot of resources? So those are sort of trade offs that I would encourage you to think about when looking to identify a model that's the best for the particular dataset that you have.", "Okay. I will [inaudible 00:29:39] back here for Max. So the config file, you use Local Config. You can also use TensorFlow Config, but I would suggest you sticking with Local Config and that wasn't the documentation. You can find this here in this code block. So these are all the things that I have specified, but really it comes from the documentation. Okay. I'm also going to change my screen slightly so we can see the documentation at the same time. So hold on. All right. So I've got the documentation pulled up here. Yeah. Honestly, when using anything, I always look up documentation because you don't know what updates people have [inaudible 00:30:39] and how old a notebook is and stuff. So would've gotten to always use documentation even for the best-maintained open source things. All right. So this model that I started, it's still training. It's at about 70% accuracy or it's getting there.", "Okay. So question from on Arne about reading resources or reading resources for how to select models. You could start with, honestly, I would just do a Google search for generative models. So look at generative models for the specific type of data that you're interested in, because of course there's a whole universe for image data, for audio data, video data. Any other modality that is not represented in a tabular format, even text data, there's so much out there. So if you figure out what type of data you want to create a synthetic version of, I would honestly do a Google search and read papers. So I would read things that have been published in any sort of computer science, machine learning conference.", "Even medical journals have a lot because a lot of this is motivated by problems in the medical field. So I would recommend just doing a Google search. I can sort of recommend the top things that I know for tabular data generation. And those are the ones that I talk through. So conditional tabular, again. There is of course GPD and all its variants. So you can look at the models and Hugging Face. There's diffusion models for images, and there's just regular GANs and variational autoencoders for images. So there's a lot out there.", "All right. I'm going to keep moving and we'll leave my model to train at very, very slow pace. And here we go. All right. So let's move on to using Gretel Console. And for those of you who are really not wanting to code in Python, this is your time to shine. So if you head on over to Gretel Console, I'm going to go full screen again, you can see that I'm logged in. I have been working on Gretel Console before, so I have some projects, but you likely will not. So your interface might look slightly different, but let's go to projects and hit new project. Okay. We'll call this, let's see.", "Okay. So I gave it a name. And the important thing to see here is in this interface, you even create models. You can sort of use our cloud offering to do kind of basically what we've done in this notebook, but there's a lot that's taken care of. So there's sort of a lot of output that you won't see. There's a lot of validation and stuff that you won't see as well, but it all uses the underlying open source library. And we'll do two things here.", "So let's go to the files in Google Colab, and let's download this NewAdultIncomeKansas.CSV file. We'll go back here and go to data sources. We'll upload a data source, which is going to be this file. Wait for that to get up there. So this is all getting uploaded to Gretel's Cloud. There's an option to run basically all of this in local mode as well. And we can talk about that in a bit, but the distinction is that the thing that we did before with the Local Config and train R&N, that's fully open sourced. You just clone the GitHub repo or you Pip install it and that's all you need.", "In this case, we are sort of using a little bit more abstraction on top of that. Okay. So I've got this file in here. I'm going to go back to models and I'm going to create a new model. So here, you'll see there are a few things that you can do, and we'll be focusing on generating synthetic data. But sometimes we use these other do in [inaudible 00:35:35] with generating synthetic data.", "The first one is classify and label, and that uses named entity recognition to identify within a dataset if you have sensitive fields. So for example, you might be looking for whether maybe there's a free text field and there's an email address or social security number or a [inaudible 00:35:55] date in there and you wanted to add that. So that's what this thing does. Transform data does more like plastic data anonymization. So for example, if you had a bunch of names and you wanted to replace them with fake names, you could do that in here. There's a lot of sort of sophisticated things that you can do with Transform as well. But we want to focus on those for today. We'll select generate synthetic data and we'll go to data source and select this file, which is NewUSAdultIncomeKansas. Continue.", "Okay. So the next thing it's doing is figuring out model configuration. And that is basically that same Local Config, but there are a lot more things in here, right? So let's go through this. This is the configuration file, anD it's just sort of like a YAML or pretty printed JSON representation of what we had in the Jupyter notebook. And all right, so we can see here we've got this default configuration, right? We have some sort of recommended configurations that you all can use. And those are... You can look at it in the documentation that's linked here. There's a lot of information in this documentation as well, but let's go through this page, right? So there's some epochs, there's batch size, vocab size. These are sort of all the things related to model training, things that we looked at before, like learning rate, dropout and whatnot.", "The additional thing that you'll see here, which is actually also something that you can do in open source with that Local Config, is to use differential privacy. So a little bit, I'll take a step back. And what differential privacy enables us to do with models is to add some mathematical guarantees around privacy. And the basic idea is if you take a dataset and you add or remove one row, or if you change one row, differential privacy guarantees that if you were to, let's say, I don't know, do some sort of aggregate on it, right? So the aggregate can be account. It can be a sum. It can be even training a machine learning model or training a model like this that we're looking at. The guarantee is that the outputs don't change by all that much if you change inputs by one row. And that sort of provides this level of privacy or a guarantee of privacy that you're learning general things about the data. You're not learning anything too specific about one record off the data.", "And so that's something that you can turn on. We have a ton of blogs on differential privacy. There's also a lot of literature out there on it. So if that's of interest to be able to generate synthetic data with differential privacy guarantees, I would recommend that. Some of the sort of state-of-the-arts methods are marginals-based methods. So that's where sort of you create a 2D histogram or an ND histogram, and try to sample from it. And that whole process to make it differentially private, it's a lot, maybe not easier, but it's a lot better documented how to do that and how to do it well than it is with neural networks. But you can certainly try it with this neural network. So you can say, \"Okay, set differential privacy to true and select some type of parameters for this.\" All of this is explained in our documentation if that's something that's of interest to you. We won't be using it for now though.", "All right. So that's on sort of the parameters for the model itself. The next thing is validators. And that becomes really, really important. So you can train any type of generative model, but I don't think for most cases you can guarantee that all the data that comes out of it is going to be good quality, it's all consistently going to be good. And you can't always guarantee that because models can sometimes produce really terrible outputs. It just depends on where they sample from. So what these validators do is they provide sort of like an additional level on top of the generation step to see, \"Hey, does this make sense as an output for the model or doesn't it?\"", "So in set count basically says, \"Okay, for example, let's say in this dataset, I have gender. And maybe it's just male and female and that's it. Or male, female unknown. Or male, female, not defined.\" If I set in set count to 10 here, or if I set it, for example, to three, it'll say, \"Okay, so for that column, I'm going to check what the total possible options are in the original dataset. And I'm only going to provide you with rows where those options are provided.\" So either it's male, female, unknown or 0, 1, 2, if there's a 3 generated in that, it'll throw the sample out and generate another one. So that's sort of what this in set count does is for categorical variables. If you set it to a large enough number, for any categorical variable with the number of categories less than or equal to this number, it'll validate that you've got the right categories in there.", "So for example, gender can't have a category from race. That's an example. Pattern count is kind of similar. So if we were working with date style columns that are formatted as MM/DD/YYYY, it'll identify what that pattern is and it'll make sure that when you're generating a date column, that's the pattern that's generated and that's what's returned. So that's an example. There's a lot more about this in the documentation, but usually these defaults work pretty good if you have a lot of categories. So for example, if you have a state column, you might want to make this 55 or 60 just to capture everything in there and American states I mean. But, yeah, that's how you use these things.", "And the final few things I'll talk about that are generally important are, one is generate. And that basically says, \"Okay, once the model is trained, let's generate 5,000 new records and that's going to be our synthetic dataset.\" And the model has to do that to know if the model was trained well. So you can train a model to really high accuracy and next token prediction. But there's some guarantee that the dataset that comes out is actually going to be valid. The model could be getting really, really good on doing certain things and sort of accuracy is just one measure of model performance. So once the model is trained, there's some generation process that needs to occur. And what we allow for is something called max invalid. And that basically says like, \"Oh, this is the maximum number of invalid rows that's allowed when generating a new sample.\" So if you're only producing invalid rows, it probably means the model doesn't work. So let's just start afresh.", "Okay. The final thing is privacy filters. And that's something that we add on top of the generated model. So one is outlier filters. And that basically says if something looks like an outlier in the generated dataset, then we'll remove that from the generated dataset. The similarity filter says if something looks too similar to a record in the original dataset, then let's remove that. And there are some algorithms that we use to figure these things out. All right. So we'll begin training here and you'll see it sort of goes through the thing. It'll print out a bunch of logs. It's printed out our config file here. There's telling us it started a worker up in the Cloud and so on. Okay. Let's check in on this model. I think. Well, I'm hoping you all have models that have trained much further than mine has. Well, this is unfortunate, but I'm going to pause training here. It'll take a few seconds.", "Okay. So I stopped training. It saved a checkpoint. This is not going to be very good quality data, or at least I don't think it is. But let's see what we're going to do next to generate the text. And I apologize for the sound not being clear when I talk, I'll try to [inaudible 00:45:15] and be louder. All right. So to generate the text, we're talking about validating, whether a record actually fits with the dataset that we're trying to produce. And we'll use a function called validate record here that I've written. And basically what it does is it says, \"Okay, for every line that's generated, right?\" So you're telling this model, \"Okay, start with the new line tag, which is [inaudible 00:45:47] and generate an entire line for me until you see that tag again.\"", "And in that line that's generated, I want to validate whether it's actually doing the right thing. So I want to validate that it's of length 11, which means that there are 11 columns in there. And for each of those, I want to do some additional things. So one of the things I want to check is that everything is an integer, right? Because in that original dataset, all the columns were integers. So I'm going to check that. The other thing I'm going to check, which we talked about in Gretel Cloud, is checking whether the values that are generated, so any of these values that are integers, whether they actually belong to the original set. So for example, again, that gender example. If there's only 0, 1 and 2 or there's only 0 and 1, maybe it shouldn't produce a 3 so we'll kick any records like that out. And if those don't happen, then we'll raise an exception and say that the record is not 11 parts. All right. So let's try to just generate one line. Hopefully you all have better luck with this than I do. It's producing... Oh.", "Hold on one second. The greater thing about live coding I don't know if this is entirely right. Okay. Oh, hold on. So when I do this, I just need to make sure that for each column, I'm checking that they're integers and that I'm checking that they're in the right set. All right. They should work now. Okay. So I had quite a few invalid records that were originally produced. And finally I got one that's valid, right? It has 11 parts, everything's an integer and everything falls in the correct range. Okay. So I've got that.", "The next thing I might want to do is generate a large number of records. So maybe I want to generate 10,000 lines. Let's see what happens with that. And hopefully you all are also at this stage. Please feel free to stop me if you're not and we can take a little break. Actually looking at the time, this is a good time to take a break. So why don't we let this run wherever your model is. Just let it keep going. We'll take just a little break here. Three minutes. We'll be back at the [inaudible 00:48:54]. Well, thanks for your question, Methyl. So Methyl asked, what are the requirements for the training dataset in terms of size? Is there a better alternative to LSDMs if training data size is small? Yeah, that's exactly right. So for an LSDM, I mean, you probably want at least a few thousand examples. There are some configurations of our architecture that work with small dataset sizes.", "So you try those and you should be able to find those in our blueprints, which is linked in the chat. As far as other architectures, yeah, Transformers are definitely the way to go if you are training on a limited dataset. So, again, those are pre-trained models that you can use. There are pre-trained models you can use. You can also train from scratch with the Transformer, architecture though I don't really recommend that just given the number of pre-trained models that are available. Again, it's important with pre-trained models, in my opinion, to find one that sort of fits the [inaudible 00:49:58], right? So you need to know what type of dataset they were trained on, right?", "You're trying to produce code. You probably want you the model that's trained to work to generate code like [inaudible 00:50:13] or maybe you're just looking for a quick sort of BOC style thing. And for that, you use GPD 2, because that's open source. So it really depends, but I would highly recommend sort of being very careful about which one you choose. Of course there's no substitute for experimentations. So you could try literally everything. The only cost is compute, but you could certainly try it.", "All right. So for [inaudible 00:50:44], hopefully your model ran and you were able to generate some new text. I tried to get 10,000 records and it looks like I was able to. I'm going to review the data distributions now, because I didn't train this model for very long, I don't expect this to be fantastic. But let's see.", "Okay. So here's the comparison of data distributions for all the different variables. This is not beautifully formatted, but that's fine. You can see there's some overlap. It's not doing particularly well on this first one, but that's okay. I suspect if you train it for about 50 epochs, you should get pretty good results. So this is one example. We got that here. All right. And let's go back to Gretel Console and you'll see hopefully everything has trained, it's generated records and we have some results. So in my case, using the configuration that I did, I have a dataset 5,000 records, and I also got a quality report on this. So there's something called a synthetic quality score, which is at a 95. And this is a composite of a few different things. So let's look at it. I'm going to download the synthetic report", "And let's take a look. All right. So it's showing me a few things. It's showing me scores of these three individual statistics. One is field correlation, which is other correlations in the original data maintained in the synthetic data. This deep structure stability, which looks at sort of if you were to look at the same dataset in a reduced dimensionality space to doing something like PCA, what does it look like between the original and the synthetic? And the final is distribution. Field distribution stability, which is you compare histograms of all the variables. So it's like a univariate comparison.", "And here you can see we're doing pretty well. So on the data correlations, there's a little difference in correlations for OCCP. I forget what that is. I think it's occupation. And the rest of the variables. But it seems okay. Looking at the PCA plots. I mean, it looks roughly good. And then looking at the distributions, you can see it's better maintained than the other one that we ran. Of course, that could just be because my model didn't train very long. But, yeah, so we have a comparison of distributions.", "We have something that we... Gretel Synthetic report says, \"Okay, you have a quality score of 95. The other thing that I'm not spending a lot of time on is this privacy protection level. And it's just sort of a scale, right? So we did that outlier filter that said remove things that look like outliers. We said remove things that are too similar to your records and the original dataset. And the thing that we don't have is differential privacy enabled. That type of training takes, I think a little bit more parameter tweaking. It's a little finicky. So you can certainly try. Right. There's a question from Gary. Is my config variable from GitHub... Could you specify which one? Okay. Oh, we're looking at the one in Colab? So the config that I specified was imported from Gretel Synthetics.config. And this is the fully open source library Gretel Synthetics.", "So if you go to Gretel Synthetics, I'm just going to go to master. If you look at the examples, just look at synthetic records. That's a very basic example, and you'll see some of this stuff in here. So I'll post a link to that. That's for a fully open source.", "Okay. So maybe the next thing we should do is I was going to walk you all through all through also the same thing that we did in this point and click sort of way right here. We can do that programmatically. And we can do that by installing something called Gretel Client, which allows for interface with sort of our client offering. And that goes through and you can build the same startup synthetic dataset, run through it. I'll share what needs to be done there, but maybe we can ignore that for right now. Just know that that's an option. You can do this all programmatically as well.", "And let's move down to performance on a machine learning task. And so this is something that I talked about earlier, which was how can we use synthetic data to reduce bias or to balance variables? So one thing I'm doing here is importing everything from the start. So hopefully you all look here. I have a function here that just prints performance of a model. We're going to use XGBoost. And all I'm doing is reading in the original data and the synthetic data, just in case you didn't run any of the other stuff. Hopefully you have some synthetic records already and you can use those. But if you don't, you can wait for everything to complete and still follow along and just see what I'm doing. So here we go.", "I've got the original data and the synthetic data. I'm just getting the variable names to look like what they look like in Kaggle. Really just for giggles. I think mostly. And the next thing we'll do is a train test split. So really not much there. I'm doing a train size of 80% or 0.8. And I'm going to stratify it by income bracket, which is now the target variable. So if you recall this dataset, I'm just going to run this and let's take a look at it.", "Okay. So if you recall in the original dataset, there's this income bracket, 0 means that of person owns below $50,000. And 1 means the person owns over $50,000. And then of course, there's all the other variables, age, work class, education, so on after race. Okay. So I've done a trade test split. I've got a training dataset, a test dataset that I'm going to treat as totally out of sample. We'll see how performance of the model is on that. And let's also take a look at the class imbalance.", "So in the original dataset, right? The reason I'm suggesting that we augmented is because I believe there is class imbalance. So you'll see at some 11,000 examples for folks who earned under $50,000 and 5,000 examples for folks who earn over. And so the idea here is, can we use a synthetic dataset to augment this dataset? And as a result, do better on the minority class, which is the class that we're trying to predict, right? So the reason this could be important is this is very contrived example and sort of used a lot as a dataset or as a classification task that you do as an intro data science problem. But the reason it's important is because maybe you're trying to predict whether somebody has an income higher than a certain amount or a projected income higher than a certain amount. And you're trying to suggest some sort of policy intervention, or you're trying to suggest some sort of [inaudible 00:59:41] intervention or things like that. And that can become really important. So you want to make sure that you minimize or maximize the precision and recall. Let's use those as metrics for today.", "So we're going to create and actually boost classifier. Sean, just to make sure that you can import it, hopefully it installed correctly. And let's define a classifier. So really simple. I think somebody is off mute. Take a look at your audio, please.", "All right. So let's define a classifier. All right. That's fine, we'll leave all the defaults. No need to sort of over-engineer this and let's decide to fit it. So I think for XGBoost, things need to be in as [inaudible 01:00:44] arrays. So we'll do that. I basically taken everything. So this is X and this is Y. Everything up until the last column is the X. The last column is the Y. And that's it. Cool? So that's train and I'm going to look at the performance of this model. Okay. So here's what I'm looking at. I'm going to look at the test set performance [inaudible 01:01:17]. And I'm looking at an accuracy of 80%, but as you can see, I'm doing really badly on the minority class, right? I have an F1 score of 0.64. Really, really terrible recall. It's 60% recall. That is not very good. And you can see the support is already... It's not that large and it's certainly not balanced.", "So let's see what we can do with the augmented dataset. Okay. So I'm going to make an augmented dataset, just going to concatenate these two datasets. The original training dataset, and then I'm going to look at the synthetic dataset. And I'm only going to select the samples in the minority class just as an example. You could use the whole thing, but the goal here is more to augment the dataset. Okay. So let's augment the dataset.", "Let's look at income bracket. Okay. So we're getting closer to having balanced data. You could put the optimizers to have an equal number of examples per class. That's totally fine. And I'm going to do the same thing. So I realize this is a little fast, but I'm just going to copy paste this and drop in the correct dataset. So I'll call this classifier 2. I'm going to use the augmented dataset. Okay. Going to train this classifier. Cool. It's trained and let's see the performance. So I'm going to print performance. I'm going to provide the classifier, which is the model. The dataset and the test set.", "Okay. So here I seem to be doing a little bit better on recall. I'm looking at the test set performance. I'll give it a minute so everyone can get here. Hopefully you'll have a dataset you're working with, and hopefully your results look better than mine do. All right. So what I did was I augmented the dataset, the original dataset, with minority class examples. And that was from the synthetic dataset that I produced. And what I was able to do was increase the recall. So I believe it was 59% and now I've reached a recall of 75% and that's quite an improvement, right? So I'm actually measuring that the model is correctly identifying through positives here at a much higher rate, right? That's recall. And so depending on sort of the used case here or what the downstream is of such a model is, you might want to optimize the different things and perhaps further balance the dataset or generate better quality synthetic data to be able to do this.", "For example, another check would be, if you just download the 5,000 records from here that were produced in Gretel Console and drop that in here, seeing how performance changes, you might get results that are a lot better.", "So that's sort of this example from start to finish. There are some steps that we didn't cover, but I'll publish the notebook with all the code that you can run through to sort of accomplish this in a programmatic fashion using Gretel Console as well. So just to recap, what we got through here was using a fully open source library, which is Gretel Synthetics right here. We generated synthetic Census data records to be able to augment a dataset and produce a machine learning model that's able to have higher recall in predicting whether somebody has an income of $50,000 or more. So that's the example that we went through. There was some back and forth. Hopefully, it was clear enough that you were able to follow along. But if not, please just look out for this notebook. We'll publish it. We'll add a lot of pros to it. So you should be able to run it from start to finish.", "Okay. So that's one example. I'm going to break for about two to three minutes. We'll be back at the 28 minute mark. And we'll take about five minutes to run through really quickly a very, very simple notebook. And while we're taking a break, I'm going to update it. So, yeah. Feel free to stretch your legs. Ask questions. Yamin from my team is here as well to answer things. Thanks, Yamin. Appreciate your support. So please ask questions, take a break, whatever you need. We're almost there. And this is a really, really fun part that's coming up. I'm so excited about it. So please stick around. We take a few minutes to talk about synthetic time-series data. This kind of goes back to the question about what is a good model for... How do you choose a good model for a specific dataset type?", "And for time theory data, things become very, very complicated because so much data out there, right? Is collected sort of on a time step basis. So if you have an application, you're collecting information about users, you'll have that information for a lot of different time steps. So this type of data is ubiquitous. And models that can handle time-series data are really important to get right. And it's important to have good ones for that because a simple LSCM often does not do it. So that end, we have this notebook that I just dropped in the chat. And hopefully you all can access it. I'm going to open it up here.", "All right. I've added basically all the code in, so we're not going to go through and live code any of this. But I'm just going to walk you through what's going on here. So if you install Gretel Synthetics straight from GitHub, so not using Pip, just download the repo and then install it and install certain few other libraries. Again the dataset... Oops. It's fine. All right. This dataset is web traffic dataset from Wikipedia. And so what it has is for many different pages on Wikipedia, across different domains, different access [inaudible 01:08:31] agents. It's got the number of page views and that's across 550 times steps or 550 days. So once we're fully installed, let me show you the dataset itself. It's a time-series dataset, and there's a lot of data. There's a lot in here.", "Okay. Hopefully they should install install soon. All right, so basically what we're doing here is we're using a new type of model. So I'll go in here and show you all where it is in the GitHub repo. It's called time-series DGAN. And this is something called DoppelGANger. It was published in 2020. There's a TensorFlow 1 implementation of it. But for anyone who uses TensorFlow a lot, I'm guessing you don't use TensorFlow 1 anymore. It's kind of difficult to wrangle with and doesn't always catch on easily that you have a GPU, which is pretty essential for models to train models with a lot of data, right? So what we did at Gretel was we re-wrote the whole thing in PyTorch. And so this is the repo that contains everything. So this is the entire model architecture in PyTorch and the notebook, oops, the notebook here basically runs through and it trains a model or this particular model type to produce synthetic time-series data.", "Right. So we'll just look at what the dataset looks like. And we can talk about why it's so difficult to do this with an LSDM, but we can also just run through this. You could honestly run this notebook. It'll probably take a while, but you'll get some synthetic data that looks kind of like this. I guess there are still some outputs here, but you'll see for page number, let's say 24992. Here's how page views go across time. So this is from 2015 to 2016 and there's some variation in the deck... In the number of page views. Sorry. So anyhow, this is taking a while, but basically if you go through this, super exciting. This is a model that a lot of folks have been requesting and updated implementation off and we have it and we're going to publish some blog posts on it and how to use it.", "But this is a very, very easy example to get started with. So if you were to just run this notebook from start to finish, you should be able to see this. So I'm doing a few things here. I'm just encoding the data. We have some categories here, so you'll see the domain, access an agent. I'm just encoding that ordinally, so it's not one [inaudible 01:11:27] encoded, but just as numbers. I've created a training input dataset, and I'm going to specify a few things, namely that there are some things that vary with time, which are these page views. There are some things that don't vary with time, which is the domain of the page, the access and the agent. So I'm going to specify that these three things are discreet. So time-series observations are continuous and here we go. I'm just formatting these as [inaudible 01:12:00] raised to epochs into the model. There's honestly nothing [inaudible 01:12:03] going on here.", "And I specify another configuration file. I specify some normalization for the data as well. And then I go ahead and train this model. And this is likely going to take a while. I've suggested 400 epochs and yesterday that took me about an R and something. So you can go through and train this. But what's really cool, and sure, maybe I will. I'll just pull up what I have out here. I train this model for about 200 epochs and I generated some synthetic data from it. And you can see it's not perfect, but for the time in variant attributes, it's produced this thought of distribution.", "I don't think I have any examples printed out of the synthetic sample itself, but let's just try this. Hold on. Nevermind. It won't let me run things, but basically let's not complicate life. If you run through this notebook and you run through all the way to the end, you should have a way to evaluate how the dataset was produced, what types of time-series you're getting, what types of distributions of the time in variant things you are getting and how does that compare? And there's a lot of tweaking that needs to be done because this is a generative adversarial network and they tend to suffer [inaudible 01:13:45]. And they're just very [inaudible 01:13:47]. So I would recommend having some patience with this, but we will be publishing a lot of guidance and how to do this well. And it'll soon be available as well to do it in a point of quick fashion like we did over here.", "Okay. So there's been a lot here. I appreciate 69 if you hanging out with me for so long, thank you for joining. One of the fun things that we do here at Gretel is use our models for kind of silly things. And one thing that we did a while ago, that's unfortunately not still operational, is we took some cocktail recipes or at least a list of ingredients and then we trained our model. So it's that LSDM. We trained it to generate cocktail recipes. We made a Twitter bot for that. So that's one fun thing that we did. You could try it out. You could try making synthetic Pok\u00e9mon, you could try making synthetic Harry Potter spells. You could try literally any type of text data, try it out. There's plenty of applications for this. A lot of really cool used cases that we've seen in the last few years.", "We are really excited to share this with the open source community, because we are at the core of it, open source. Everything's out there for folks to take a look at, for folks to use and give us feedback on. So hopefully this was helpful. I believe this workshop is recorded and, yeah, that's all I got. We have about four minutes. So I'm going to go through the questions and see if there's anything that I haven't answered, but please feel free to send more questions. Ask anything and you can always shoot me an email or send anyone at Gretel an email. Use this email address. All right.", "A question from Mikael: Would it be correct to say that the generated data in most cases would have more false results, which have to be filtered out? Honestly, that depends. It depends on the structure of the data that you're trying to learn. So, in this case, whoops, here we go. So I'll take this example here, right? The thing that I trained. I trained it for 11 epochs, just slightly over 11 or not even 11 epochs, sorry. Just slightly under 11 epochs. I had this validation accuracy of 72% and I went forth and generated synthetic data, right? By any means, I wouldn't treat this as the best possible model, but I did go through and I was able to still generate some valid results, right? The reason that this has more false results is because I just didn't train the model very well.", "I was limited by compute and time. And that was a choice that I made and sure we are. But you could, in theory, train this model for 50 epochs and you'll find [inaudible 01:16:58] and you can calculate this pretty easily. How many invalid records were generated while you were trying to generate your 10,000 sample or 10,000 size sample? And so you can see all that and you should see that if you train a model to a higher accuracy for next token prediction, that you'll get more valid samples just right off the bat. You wouldn't have to do so much validation and checking and discarding of records. Sorry.", "Ah, okay. I don't know if you answered this, Yamini, but Arne's question on, whether there is a documentation link for more background information on the metrics used in the quality report? Yes, there is. So if you look at... So this is our documentation page docs.gretel.ai. We also publish a lot on a blog. So if you go to the blog, there should be search for quality. Okay. Maybe I'm just not finding it right now. It's somewhere. I will find it [inaudible 01:18:31]. But we have a lot of information on the quality score that we use. It should be somewhere here, but I'll find it and be sure to add some information on that for you. But, yeah, if you-", "Sorry, I did drop in a link in the chat to the docs, which breaks down all the metrics and so on. So that is there...Also, the thing I was thinking of was privacy filters and you can also have more information on that in the blog.", "Okay. With that, I think we're at time. Thank you everybody for sticking around for 90 minutes. This is a very long time, but I hope that was helpful. Please reach out to me with any questions. We'll be publishing basically all these materials completed from start to finish. So you can run through the examples and hopefully you have a dataset or a problem that you want to apply this to and try it out. If you have any issues, reach out to us. Feel free to use the fully open source offering, feel free to use our Console or use our Client in Python as well. So many ways of accessing this. I really appreciate your time. And with that, I think we'll call this workshop.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/podcasts/data-accessibility-privacy-engineering-with-danielle-beringer-of-gretel-ai", "page_title": "Podcast - Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "headers": ["Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Show Notes:", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Danielle Beringer is Director of Technical Partnerships at ", ", an open-source, privacy engineering as a service allowing you to synthesize and transform your data quickly and securely. Gretel just closed a $50M B round and Danielle shares how they are delivering great outcomes with synthetics while reshaping how people are working with sensitive data and PII datasets. Listen in to hear how Gretel is tackling the challenges of data sharing and accessibility to data, and what they are planning for the future. "]},
{"url": "https://gretel.ai/podcasts/software-engineering-daily-privacy-engineering-with-alex-watson", "page_title": "Podcast - Software Engineering Daily - Privacy Engineering with Alex Watson", "headers": ["Software Engineering Daily - Privacy Engineering with Alex Watson", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Protecting your customers begins with best practices for securely capturing, storing, and protecting the data you collect for or about them. \u00a0When an organization has a large enough dataset, needs typically arise for doing analytical workloads or training machine learning models on this data. \u00a0If you use random or mock data to generate a report or train a model, you arrive at an output that doesn\u2019t reflect the true use case of the organization. \u00a0Success on tasks like this seems to require production data.", "Alternatively, perhaps production-like data is good enough. \u00a0In this episode, I interview Alex Watson, co-founder and chief product officer at gretel. \u00a0We discuss their solution for privacy preserving synthetic data that remains representative of the underlying dataset."]},
{"url": "https://gretel.ai/podcasts/using-synthetic-data-to-power-machine-learning-while-protecting-user-privacy", "page_title": "Podcast - Using synthetic data to power machine learning while protecting user privacy", "headers": ["Using synthetic data to power machine learning while protecting user privacy", "Transcript", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["On this episode, we talk to ", ", CTO and cofounder of Gretel. The company provides users with synthetic data that can be used in machine learning models, generating results comparable to the real data, but without exposing personally identifiable information (PII). We talk about how data outliers can identify individuals, demo data that feels real but isn\u2019t, and skewing patterns by skewing dates.", " uses machine learning to create statistically similar data that contains no personally identifiable information (PII). ", "Think your commits are anonymous? Think again: DefCon researchers figured out how to ", ". ", "We published an article about the importance of including privacy in your SDLC: ", "Our Lifeboat badge shoutout goes to 1983 (the year Ben was born) for their answer to ", "John Myers I'm the only person in the Philadelphia area in my company. And if you look at our Google call logs, and you took out my name, and my email address, you're like this person is calling in from Philadelphia, like, that's John, like there he is. Bow do you mitigate that? That becomes a really challenging problem. And so what our synthetic data capability does is goes in and builds a machine learning model on the original data, at which point you can throw out the original data. And then you can use that model to create records that look and feel like the original records. And then we have a bunch of post-processing that removes outliers or overly similar records, and what we call privacy filtering. So it's kind of like the easy button, although it's not so easy. But we want to make it easy for the developers to use.", "[intro music]", "Ben Popper Stop overpaying for Big Tech cloud - Try Vultr Instead! Vultr offers powerful cloud compute and bare metal at a fraction of the cost. Visit Vultr-V-U-L-T-R-dot-com-slash-stack to redeem $100 in credit today!", "BP Hello everybody, welcome to the Stack Overflow Podcast, a place to talk all things, software and technology. I am Ben Popper, the Director of Content here at Stack Overflow. And I am joined today, as I often am by my terrific co-hosts, Cassidy Williams and Ryan Donovan. How's it going y'all? ", "Cassidy Williams Glad to be here.", "Ryan Donovan Hey Ben, how you doing?", "BP So let's put this out there, before today, before we set up for this podcast, either. Have you heard of privacy engineering?", "CW I have heard of it. I've heard some talks given around it. Not so much privacy engineering as it is, like figuring out how to privatize your code, because everybody has a fingerprint on the internet and fingerprint way of doing things. And I saw a really interesting talk once about how someone was able to determine who was coding a particular code sample based on the style of code. The simple example is like, do they put spaces around parentheses or put a curly bracket on another line? But it got much more deep into coding styles. And her research was all around how to anonymize your code a bit more.", "RD Yeah, we published a blog post a while back about why your privacy should shift left into the software engineering cycle.", "BP Yeah, it makes a lot of sense to me, I feel like we've had a bunch of conversations on the podcast about how these days, even at sort of almost the MVP level, you are worrying about things to do with consumer privacy and data, especially around GDPR, and how you help people auth-in or log in and keep a password or share some of their data with you, without compromising your users in some way. And having that come back to bite you. Alright, well, let's get into the meat of the episode. Our guest today is John Myers, who is CTO and co- founder at Gretel.ai, an organization, a company that specializes in privacy engineering. John, welcome to the show.", "JM Hey, thanks for having me, excited to be here.", "BP So how would you define at a high level what privacy engineering is all about? And tell us a little bit about how you ended up in the the gig you're at right now?", "JM Sure, yeah. So to summarize what I see privacy engineering, as you know, I came from a background of doing a lot of data engineering. And I view it as kind of an extension of that, where as part of doing what you'd normally do in data engineering, whether it's like ETL, or doing some type of enrichment or modification to the data, you need to really consider what types of steps you need to take to make the data safe. From a privacy perspective. You know, I've spent my entire career building a variety of data pipelines mostly for you know, some end state where you're trying to do some analysis on it. And oftentimes, I found myself spending more time figuring out how to make the data safe to insert into some downstream data storage that was going to be queried by, you know, a whole different user base. I would say most of the people that Gretel have a similar story. And, you know, as an engineer, myself, I never really found tools that could do it. Or if I did find tools, they were pretty scattered across different places, right? Like, I would have to go use some library to do one thing and another library to do another thing, and nothing was really unified together. So when we launched Gretel, we said, what if we took like, all these capabilities, like put them behind a singular set of APIs with like a very, you know, similar developer experience so that you don't have to go reach out to six different vendors to do what is basically a related task of taking data and making it safe to share.", "RD How do you make it safe to share? ", "JM Yeah, so we have kind of a buffet of capabilities. So I would say we start off with a couple of tools that we consider to be table stakes. So some of the capabilities we have focus around the ability to detect sensitive information and data sets. And then after that, you can detect it you can apply various transforms to it. Those capabilities are out there. That's basically an area known as like data loss prevention. There's a lot of API's that are baked into different cloud vendors for that, like AWS has won Google has won Azure has won basically have to be in their ecosystem to use them. And then there's a bunch of vendors standalone that you can install in your ecosystem. But they're really designed to work at a high level, like they're either doing some type of like data governance type of function, where they're trying to identify and create pointers to where you have sensitive information, we're really we wanted to do was kind of bring that down to a lower level and enable developers to actually execute those types of tasks directly in their workflows, whether it's the ICD or some type of ETL process. So that's one of the big capabilities that we have. And that gives you a lot of flexibility to identify what I would call like discrete privacy risks, right, you have phone numbers, or email addresses or addresses, stuff like that, you can detect those. And then you can apply a variety of transforms to it. Tokenization and encryption, you can do partial redaction, we have a capability to do like a surrogate replacement with a fake value that maintains consistent across the data set, you could do a lot of that relative to other foreign keys on the data. And then we kind of move up in complexity from that into what we call synthetic data, where you might not know all the risks that are within your data set. So outside of having those types of discrete risks, where you have all those identifiers to individuals, there's a whole myriad of risks around like re-identification attacks and aggregate risks and being able to you know, identify someone just because I'm the only person in the Philadelphia area in my company. And if you look at our Google call logs, and you took out my name, and my email address, you're like this person is calling in from Philadelphia, like, that's John, like there he is, how do you mitigate that, that becomes a really challenging problem. And so our what our synthetic data capability does is goes in and builds a machine learning model on the original data, at which point you can throw out the original data. And then you can use that model to create records that look and feel like the original records. And then we have a bunch of post processing that removes outliers or overly similar records, and what we call privacy filtering. So it's kind of like the the easy button, although it's not so easy. But we want to make it easy for the developers to use. But under the hood, we have a whole bunch of complex things going on that been developed by our ML team. And then we've, you know, put it into our actual API ecosystem. And that's really kind of where we're going with it is, how do you make it so developers don't necessarily have to go through this, like discrete identification process and know what's exactly what's in their data and have to be like experts in the schema.", "CW That's amazing. And I feel like it's something that when you describe it, it totally makes sense that something like this needs to exist for developers to maintain that level of privacy. But it feels like it's something that not enough people are aware of.", "RD This is a really cool application of machine learning. But I would suspect there's going to be some folks who look at this and say, oh, this is a over engineered solution, right? For data privacy.", "JM Maybe, yeah, I put an extension on my deck. And they said it was over engineered. I was like, what does that mean, they're like, well I won't ever fall down. ", "So, I don't know if that's a problem.", "BP Right. It cost you a little money, but won't ever break on you. ", "RD And I think the fact that you're creating statistically similar data that is usable. That's fascinating. And I think there was one of the things you sent over talked about a particular attack that can be performed on either the hash data or real data.", "JM Yeah, it might be like a, I don't know, if it was like one of the model inference attacks or just a re-identification attack that you can run on, on data. Both of those are, you know, basically, using different properties of the data set to piece, you basically take two or three other columns, and you basically are recreating the same statistical relevance as my name in the dataset, right? There's a study out there, I can't remember who it was from, but you know, they say given like, you know, even like zip code, and gender or some other characteristic, you can get close to 80%, of just identifying a singular person to that. So, that's one of the things that it's really hard to figure that out, like, from a brute force attempts on a data because if you're highly dimensional data that's really big, you're going to run out of resources, trying to find all the permutations of columns that allow you to do that type of inference. So on the synthetic data side, you know, we have different privacy levers, you can pull from differential privacy, to privacy filtering, to injecting additional types of noise into the data that kind of takes care of that for you without having to run those exhaustive types of like, searches on the dataset.", "BP And let me ask the dumb questions. I guess before I do that, let me say one thing, which is Cassidy reminded me is talking about spaces versus tabs and a curly bracket, you know, that would identify some of these code. And what you were just saying about how you could de-anonymize somebody with a few simple traits that you might appear at a zip code a gender and an age. Ah, and from there, you're going to work back and figure out that person's name and social security number. I remember reading a story once about this. And it was about things that I don't usually think about. But that make a lot of sense. It was like, they look at what browser you use and what size your screen is. And then, you know, they know that you end up at this GPS point and that GPS point everyday because you're going to and from work, and from there, boom, it's like trivial to own you and have everything that, you know, they can they know who you are, and they can probably even open up some of your less well protected accounts, right? Because they've got your name, your address, and bunch of other stuff. So yeah, this stupid question I wanted to ask was, if you create synthetic data, you mentioned you can throw away the other stuff. But if you need the original data, how do you work backwards when you want to, you know, like, reach out to the customer with some of that identifying stuff, their phone number, their address, like this synthetic data, I can understand how would let you identify them as a person and therefore work with them on a bunch of different, you know, areas, but how can you throw out the original, and then just work from the synthetic. Are there not things in the original that are a value to you, that the customer or the user has given?", "JM So what I meant when I throw it out, meaning you don't need the original data to then go ahead and create the new safe versions of it, because the model is going to take care of that for you. ", "BP Gotcha. ", "JM What we recommend and to kind of, you know, bleed into a little bit how you talk about operationalization of this and see ICD data pipelines that we recommend is you basically want to get as closest to the data creation as possible, right? So if argument's sake, let's say you have a stream of records, and that stream is like, you know, the ground truth, right, that's like the first stop, where let's say, a transaction record falls into, at that point, you want to probably start capturing some of that real data and start creating your safe version of it, whether you're gonna do transforms or synthetics, or whatever. And then you can keep a copy of that somewhere. And that repository of that synthetic data is probably the one you want to start forking the data from when people start coming to you and be like, Hey, can you dump a table from two weeks ago, because we want to send it to the biz analyst to do something with right, that's you just want to pull from that table, because you know, it's ready safe, and it's good to go from there, the original data should stay behind some, you know, really big vault, so to speak with a key and only have access to who needs it. Because most and a lot of other industries are going to require that for compliance or legal or FBI comes knocking for one reason or another, like you can't just be like, cool, here's a bunch of synthetic data, they were they will need the real data. But for the use case, where you want to just share that data for collaboration, or you want to share it especially over with teams that are doing like machine learning and data science on it. Right most of the time that that is more valuable in aggregate than it is knowing I need to know who made this purchase? Exactly, because they're gonna go to jail, right? Like most companies aren't doing that's an edge case that you have to still retain that original data for.", "BP Right, I'm on the data privacy team, the privacy engineering team, we've got a lot of customers, and they've shared with us their credit card and address and phone number. But we've also got other user metrics, and we want to send that over to a team that's going to look at that user behavior and use that to make good product decisions. We synthesize the stuff we don't want need them to see we send over the stuff they do want to see. Now they get the value without the privacy risk.", "JM Right. Yeah. And in our experiments and our customers have given us is actually back to, you know, we've had folks that have taken whatever their data is the created a machine learning model on it, because they want to predict a user behavior, they want to predict something. And they have whatever accuracy of that model. And they put that same original data set through Gretel to take the synthetic data set train is same exact machine learning model, same outcome, right. So really, what you can do there is you can, you can build a machine learning model on synthetic data, take that model, put it back over like that safe, like firewall, so to speak, and then start running on your production data, you get the same output, because most times the data owners are not the ones that are doing that heavy machine learning work, it's usually got to get that data over to an ML team, sometimes even a third party if you're a small business, right? Like you're going to bring on a company that has expertise of doing machine learning. And so do you really want to give them all that sensitive information that they don't even need access to? Right? That's one of the biggest use cases we see for the ML side of the house.", "CW When you're generating all of this synthetic data, how do you measure the quality of it? How are you confident to be able to say like this dataset gives you the same value as the original?", "JM Yeah, so when the data gets created, when the model gets created, we generate a sample of data, we use that that immediately compare it against the training set. And we have this whole synthetic quality report that comes out. And it shows you the breakdown of how well the distributions in a field hold up, the correlations across the field. We do other checks like principal component analysis to kind of look at like underlying structures. And then we also have a bunch of things that run that do semantic validation on the dataset. Those are kind of guarantees you get that don't have to be analyzed. So like a whole column of names, you know, will identify what the character set is right like only these letters use these numbers are used. We'll make sure that those only come out so you don't get like the oh, John, zero, right? Like you don't want that to happen. So we enforce that type of thing. And so that report comes out. And then they can basically look at that report. And they'll know, okay, based off of the report, we kind of have a spectrum and we say, Well, if your score is this good and excellent, you can probably use this data for the exactly exact same thing you're going to do with the original data, like train a machine learning model of it's kind of like, the highest level of like, proof that the data was good quality. Now, if you start turning on, like the privacy knobs, and that's the thing is you have utility versus privacy. And that's kind of a trade off, right? If you if you say I want maximum privacy, we're going to filter out all the outliers, we're going to filter out similar records, or we're going to do it like very aggressively, you might not get the utility you're looking for. But some companies are just looking for the data to kind of look the same, because they want to use it for demo data. If you're testing like a user interface, right? Like, I don't care if it's statistically sound, but I just need the first names to look like first names, you know, and in the Styles, berries to taste like sounds, berries. And but again, if you're going to share that data for a demo, you really just want to turn up the privacy filter as much as you can. So you have those capabilities before you start training. And then the report will tell you, here's what you should use the data for it, here's what you shouldn't use the data for.", "BP So once you learn how to do this, if you saw great results, could you then build me some sort of procedurally generated engine that just creates new fake customers, which are equally believable as the existing customers?", "JM Yeah, so that's kind of like on our roadmap for this year is now that we have that ability do we have the ability to go from a cold start, basically, is, can someone come in and be like, I'm a bank, right? And all banks kind of here, their transactions look, the same doesn't matter. If you're like a Visa or MasterCard, right? They're all had the same like, schema. Can you just generate me transactions that are common in the northeast side of the United States around the holidays, we should have that ability, then to basically generate those from scratch, either from a pre-learned model, or some kind of underlying semantics we've learned from dataset. So that's actually a roadmap for this year to be able to kind of do that. So people can come in and just describe data they want, and then they get it back.", "BP Oh, great. Well, you should hire me to be on the r&d team because I thought of that just now.", "CW Aha! So when it comes to the creation of the data, kind of blows my mind, like, it makes sense from a theoretical perspective, but you've talked about measuring the quality of it, how do you ensure that like, privacy is still maintained now that the data has been created and is so high quality that it could be used the same way? Like, what are the best practices around that?", "JM Yeah, so there's a couple of different capabilities we have around that, what we're finding more and more is that customers are actually combining a lot of our capabilities to start with. So you know, previously I mentioned, we have this capability to do PI`I detection, and what I would call like more discrete transforms on the data. And sometimes it's really safe just to go ahead and make take a pass with that before we even train a synthetic model. So for example, if you have like time series data, right, and it's heavily dependent on, you know, some point in time and some measured value, you could go ahead and say, Hey, before we even train a synthetic model, go ahead and apply, we have like a date shift transform, that will apply like some date shift within some window, call it plus or minus five days, go ahead and do that first. Because then when the model learns on that, it's going to learn the kind of semantics of those shifted dates. And so it reduces the risk of it, memorizing, you know, some dates that were in the training model, you know, go ahead and take all the names and replace them with fake names before we, before you train this synthetic model, you basically helps reduce the risk of the model memorizing anything. And meanwhile, all the numerical data, all the correlated that is still held when you do it that way. And there's a whole bunch of hyper parameters on the machine learning model that you can tune as well, on how much it should like learn. And then that will kind of control like how much I'll say like variants gets injected when you start creating it. So there's, like on the underlying machine learning model, which, you know, I'm not the expert in, right, I'm, I'm kind of on the service delivery side, but you can turn a lot of those knobs that kind of control the data output. And then finally, there's another stage that we call privacy filtering that you can set to a bunch of different settings that specifically look for outliers in the data set. So when the synthetic data gets created, there's any outliers you by default, you kind of want to throw them out, right? So if you have outliers in your training set, it's going to learn those and recreate those. And so, you know, it still might create some records for someone who's in Philadelphia, and that's me. Sometimes you just got to remove those records from the dataset because there's no other way to avoid that. You know, there's only one person in this remote location. So all those knobs can be tuned by the customer based off of what like what level of privacy they need, and that kind of maps to like, what level of sharing you're going to kind of do with it, right? Like hey, you just really got to share this inside of your organization. Are you actually going to try to like send this outside of your organization because you're sharing it to a third party or vendor? You know, we've worked with like volunteer organizations that are getting data from companies, if you're going to share it with them, you probably want to make, you know, err on the side of precaution and use like a higher privacy setting there.", "CW That makes sense. ", "RD So, you talked about transforming the data. So does this all happen within the sort of ETL pipeline or happen, kind of, at various stages?", "JM We would recommend that would happen like right, as part of like the Gretel stage in your in your pipeline. So Gretel would be kind of like a bump in the line of your data pipeline, or your CI/CD process, where a job based system so that's kind of the way the system works. So it would just be like two Gretel jobs, the first job would do some transform the output from that would go into the synthetic job. And then the output from that would be your synthetic data.", "BP And can you tell us a little bit about sort of like what the tech stack is that you work with? ", "CW I was going to ask that too. I'm so curious.", "JM So we run as a service, so a SaaS model. So the entire stack is running on AWS. And it's a job based system. So if there's like an Apache Spark, or something like that, you're you know, you're basically starting a job. And then that job consumes an input dataset, a configuration file, does its thing, and then kicks out a bunch of artifacts, right? The synthetic data, the report that you get all that. Our API and our console, or CLI or like, kind of the interfaces into the system. You know, through our API, it's more of an orchestration API, so you request that a job can get started. And then once you get like authorization to run that job, can start the job either in our cloud or you can start the job yourself. If you start the job yourself can actually just run a Docker container that takes in all your data, that Docker container still sends information to our API, just basically like when the job started, when it stopped, are there any errors that so we can kind of use it as like a billing and metering mechanism? If you choose to run the job in our cloud, we basically do the same thing. We have a managed Kubernetes cluster that runs all the containers for you, our containers don't know, like where they're living, they just know that they're starting up and are getting some metadata for your API, then they start their work. And they're saying, hey, when you're done, drop your output data, either on a volume out, like if I'm running in, like my own VPC, we're upload those artifacts back into Gretl cloud, and a customer can download them. Yeah, so tech stacks on AWS, we pretty much rely on a lot of the serverless capabilities from AWS on the back end to do all that we manage a big Kubernetes cluster, which does our cloud providers for our our customers if they want to run on our compute, which is helpful because on the synthetic side, you have to run with GPU support. And even just getting like a deep learning setup, like configured is a royal pain. So we kind of take care of that for the customers. For customers like enterprises that run on prem, all they really need to do is configure some compute, you know, that can run Docker containers. And you can use our CLI and our CLI to launch the containers for you on your behalf and all that. But our cloud system was really just like this big troll plane that manages the jobs.", "RD But how do you protect the data, whether it's in transit or on your servers?", "JM Yeah, so I mean, all remote communications through our cloud is TLS, 1.2. and above, any kind of data that gets sent to us by customer to process is encrypted in transit, and then encrypted at rest, and only accessible through that customer via their API key to upload and download. We don't really hold on to much, we're kind of in a femoral system like that. And that out, we expect the users to download the data out of our cloud, we don't, we don't recommend that they store it up there. And that's really just that helps minimize like cost, you know, we're not a remote s3 service for them. So basically, if you upload a dataset to us, we process it if you use a lot of our SDKs. And our tutorials will end up just deleting the data you sent as soon as it's done that we expect or recommend that you download your results right through it. Really, our cloud is just a way for you not to have to like manage your own compute and scale your own compute.", "[music]", "BP Alright, everybody, it is that time of the show. I'm going to shout out the winner of a lifeboat badge who came on Stack Overflow and helped rescue a question from the dustbin of history. awarded five hours ago to 1983. Same year I was born. 'Why can't I use the new with the arrow function in JavaScript ES6?' Alright, we've got an answer for you. And we have people on Stack Overflow been answering this question in a duplicate for the last five years, so appreciate your help there. I'm Ben Popper, the Director of Content at Stack Overflow. You can always find me on Twitter @BenPopper, email us podcast@stackoverflow.com. And if you'd like to show, leave us a rating and a review, really helps.", "RD I'm Ryan Donovan. I edit the blog here at Stack Overflow. I'm on Twitter @RThorDonovan. And if you have a great idea for a blog post, please email me at pitches@stackoverflow.com", "CW I'm Cassidy Williams, I'm head of developer experience in education at remote you can find me @cassidoo on most things.", "BP And John, tell us who you are, where you can be found. If you like to be found. If you want to find the synthetic John, that's also okay, the real John or the synthetic John. And then if folks are interested in Gretel, where should they go to check it out?", "JM , thanks for having me on the show. And I'm John Meyers, I can be found on Twitter, sometimes, JTM_tech. Other than that, you know, I'm easily reachable at Gretel, anyone can ever always reach out to us check out our homepage at gretel.ai. We are hiring like crazy for all roles imaginable. Engineering, machine learning, marketing, product, sales, you name it. Or you can always reach out to us at hi@gretel.ai. Yes, that's what comes to me or the other co-founders, so chances are one of us will answer you and it will not be a synthetic John who answers. ", "BP Maybe. Alright everybody, thanks for listening, and we'll talk to you soon.", "CW Bye!"]},
{"url": "https://gretel.ai/podcasts/code-story", "page_title": "Podcast - Code Story | E6: John Myers, Gretel.ai", "headers": ["Code Story | E6: John Myers, Gretel.ai", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["John Myers comes from a big military family. He was convinced he was going to be a pilot his whole life. When he was a junior in High School, he started doing C++ programing and he got hooked. After graduation, he got accepted to RPI in update New York, and started suiting computer science. He eventually joined the Air Force, and focused on information systems. Through an interesting turn of events, John got accepted to the NSA, while a good friend of his took his deployment spot to Iraq. This job launched him into the world of cyber security, which took him to Afghanistan doing engineering for the military. When he came back to the states, he jumped head first into the startup world. John is married, and just bought a fixer upper house with his wife. He is into exercise, likes to bike in the summer and ski in the winter. Mostly, he likes to do simple things and decompress with his friends and family.", "Post acquisition of his first startup, John stared working on a bunch of projects, one of them requiring him to aggregate a large amount of data and anonymize it. This took a lot of work, and there wasn\u2019t anything out on the market that provided this type of functionality.", "This is the creation story of Gretel. Listen to the episode ", ". ", "\u200d"]},
{"url": "https://gretel.ai/blog/why-i-joined-gretel", "page_title": "Why I Joined Gretel", "headers": ["Why I Joined Gretel", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Joining Gretel was the easiest decision in my career. John, Alex and I, as founders, have been in the security industry for almost two decades. \u00a0What has become clear to us is that every time there is a shift in a technology paradigm, there is a wave of new tools and companies to follow. It's as if you needed a new surfboard for every wave. We have taken a fundamentally different approach; developing tools that help build the fundamental building blocks of data privacy, so you can adapt and to evolve with new architectures and emerging technologies. At Gretel, we don\u2019t want to solve for form-factors, we are creating tools that help build a foundation for data privacy.", "To address fundamental data privacy issues, we believe the solution must be built into the development workflows. This means creating the right tools for the right jobs. Our tools are targeted for developers, engineers, scientists, and anyone who wants to work and build with data. Our privacy engineering tools address challenges that teams face at various maturity stages of their data journey. Gretel's simple APIs for privacy engineering let you leverage the knowledge and capabilities of the world's best privacy engineering teams, giving you superpowers. Gretel makes it easy for teams to tap into the power of thousands of privacy engineers, through just a few APIs.", "We are working to make data safe, so everyone can build with it. Data privacy and compliance is already a difficult problem and as we transition more towards an immersive world where every signal from us can be collected, it's becoming orders of magnitude more complex. The technology problem is complicated, but the more complicated problem is how do you design a business model that aligns users and organizations to trust each other? As we transition further into this immersive world, we will expose our more vulnerable selves. The right to retain and use that data needs to come with trust. Companies hoarding and monetizing this data today have not earned our trust. At Gretel, we believe there is a better way, and we want to enable a world where any company can find, share, and publish safe synthetic data.", "Most value will be created via interactions with data. The faster, easier and safer those interactions, the higher the advantage and value. Currently, teams have to deal with what we call the \u201ctwo out of three problems.\u201d \u00a0Teams can build fast (time), build safely (trust), or build with real data (accuracy), but they only get to pick two out of the three. Gretel enables teams to build using all three. This is at the core of what we call Privacy Engineering as a Service.", "One element that became apparent to me over the last 12 years of building startups has been how you structure the team is paramount. Not just the quality of the team, but complementary skills, and an environment where individuals can truly trust each other. At Gretel, these qualities come through from every member of the team. Further, the team is passionately aligned on the problem we are trying to solve: making data safe. This trust and alignment is necessary for us at Gretel because of the magnitude of the problem we have chosen to tackle. \u00a0This enables our team to operate with more ownership and autonomy. \u00a0", "This is a difficult problem we are dedicated to solving, and the largest challenges are still ahead of us. However, the opportunity is enormous and most of the innovations and fun are ahead of us. I am incredibly excited to go on this journey with the entire Gretel team and community, and if this is something that interests you drop us a line, as we would love for you to join us.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/ner-sensitive-yelp-reviews", "page_title": "Got text? Use Named Entity Recognition (NER) to label PII in your data", "headers": ["Got text? Use Named Entity Recognition (NER) to label PII in your data", "Is your unstructured data safe?\u00a0", "Let\u2019s get started!\u00a0", "Did you know?", "What do you think?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": [", and yet most companies are unprepared when it comes to labeling and securing it. Those that do have processes in place often use ", " to secure their data, which is neither reliable nor scalable. ", "Gretel\u2019s ", " make it simple to identify and label sensitive content in both structured and unstructured data. Gretel can currently detect over", " including names, addresses, gender, birthday, and credentials. We do this through a combination of regular expression-based detections, custom detectors for entities based on FastText, and word embeddings. And we now also support", " industrial strength Natural Language Processing (NLP) for identifying person names and locations.", " for a free account using the Gretel Console, and then follow along below. Or use our ", " to get started! In this example, we'll walk you through labeling free text in a dataset of restaurant reviews using Gretel\u2019s NLP and labeling APIs.", "First, let's install all our python dependencies and configure the Gretel client. ", "Next, we'll connect to Gretel\u2019s API service. You'll need your ", " for this step. ", "We can now load a sample dataset containing PII into Pandas. Let\u2019s use the ", " dataset from ", "s ", " library.", "Next, let's create a project in Gretel for labeling our data.", "We'll use a very simple configuration file. NLP can be enabled in a single line. ", "The next step is to create the classification model and verify it to make sure it has been configured correctly.", "Once the model has been configured and verified, all we have to do is run the full dataset through it.", "Here's a sample of the labeled reviews. If you're interested in how we color-coded the labels, check out our ", " or ", ". You can also use this blueprint to label your own datasets using NLP.", "In addition to classifying sensitive data in unstructured text, you can also transform it. Gretel includes support for replacing names and addresses with fake versions, or securely hashing PII (personally identifiable information) to meet compliance requirements. Here's a", " on redacting PII from a dataset using the Gretel CLI, or", ". Don\u2019t forget to set ", " in your config file.", "As with all of Gretel\u2019s APIs, you can get started quickly with SaaS, or deploy our NLP into your own cloud or VPC. ", " to get your API key and see for yourself how easy it is.", "Did you find this blog post interesting?\u00a0Have any use cases you'd like us to write about? ", ", ", ", or ping us on ", ". We'd love to hear from you!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/auto-anonymize-production-datasets-for-development", "page_title": "Auto-anonymize production datasets for development", "headers": ["Auto-anonymize production datasets for development", "Getting started", "An overview of the pipeline", "Labeling and Discovery", "Rules Evaluation", "Record Transformations", "Next steps", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Developers want to work with data that closely mirrors what\u2019s in production. This makes it easier to catch bugs during testing and it gives teams confidence that each release remains stable.", "A common practice is to replicate either a subset or the entirety of a production database into a pre-production environment such as staging. This maximizes data parity between the two environments, but introduces challenges when ensuring customer privacy isn\u2019t compromised.", "To prevent customer details from leaking into the dataset, a developer might write a pipeline that defines specific anonymizing transformations per table and per column. While this process is effective, there\u2019s friction. It requires a deep understanding of the underlying dataset and its statistical properties. It requires an understanding of various anonymization techniques. It requires ongoing maintenance as the upstream schemas evolve. If these factors aren\u2019t properly managed, there\u2019s risk customer details might leak.", "In this blog post, we walk through an ", " that details how to build a data pipeline to auto-anonymize streaming data using Gretel.ai\u2019s SDKs. You can easily modify the blueprint for your own anonymization project, making the process easy and automatic while reducing risk.", "In this blueprint we\u2019ll be anonymizing a dataset containing customer bike orders with full names and addresses. ", "1. Sign into the Gretel console with your Gmail or Github account at ", ".", "\u200d", "2. Create a new data project. Name the project whatever you like.", "\u200d", "3. From the \u201cMore\u201d menu next to \u201cUpload Files\u201d, select \u201cLoad test dataset\u201d, select \u201ccustomer-bike-orders\u201d and then \u201cContinue\u201d.", "4. After the dataset has loaded, click the \u201cIntegration\u201d tab, and copy the project URI to the clipboard. You will need this to allow the Python notebook to connect to the \u201cBike Orders\u201d project in Gretel.", "\u200d", "5. Click \u201cTransform\u201d, and then launch the \u201cAuto-anonymize production datasets for development\u201d blueprint. This will launch the notebook in Google Colab\u2019s free notebook environment. Alternatively, you can download the notebook from Colab and run in your own environment.", "\u200d", "6. Click \u201cRun all\u201d to run the blueprint code, which uses the URI key to access your Gretel project, generate a ruleset for anonymization, and automatically anonymize the data. Details below! ", "\u200d", "Let\u2019s start with some code. The snippet below is adapted from the ", " and represents an end to end pipeline. We start with a sensitive dataset represented as a stream of customer bike orders, sample_records, and produce an anonymized copy, xf_records.", "Next, we\u2019ll break the code snippet down into three core components and dive into the details.", "Using send_batch, inbound source data is passed through Gretel\u2019s ", " where entities such as names, emails and addresses are identified. Gretel will also calculate various statistical properties of the dataset such as cardinality, uniqueness and distribution. These properties will help determine an optimal transformation pipeline.", "At the heart of the pipeline is a rules engine. build_pipeline accepts a Gretel project and streams down the enriched source dataset for evaluation against a managed set of rules. These rules determine what transformations are applied to the dataset.", "For example, if a field is identified as containing a person\u2019s name, a rule will be matched that automatically replaces the contents of each column\u2019s field with a fake name.", "Gretel ships a convenience function in ", ", rule_inspector, that helps us evaluate the results of a run.", "Looking at the screenshot we see all the fields, rules and transformations that the pipeline was able to detect. Applying these rules will give us an anonymized dataset.", "Note: In this demonstration, we run the rules engine against a batched set of \u201ctraining\u201d data. Depending on the use case the engine can be configured to run \u201conline\u201d against streaming data.", "After a set of rules have been matched, we generate a transformation pipeline from those rules using Gretel\u2019s ", ". With the pipeline now built, pipeline.xf_project will process each project record through the transformation pipeline for anonymization.", "Referring back to the code snippet, xf_records now contains an anonymized copy of our source dataset. Using another helper function ", ", df_diff, we can perform a row-wise comparison between an original and transformed record.", "Notice the \u201cTransformed\u201d column contains an anonymized version of the record. These transformed records are now ready to be pushed into a downstream pre-production environment without risk of leaking customer details.", "Hopefully after reading this you\u2019ve seen how it\u2019s possible to take a dataset with an unknown schema and compose together several Gretel services in order to create an anonymized version of the dataset. Thanks for following along!", "Interested in building your own pipeline for anonymizing datasets? ", " for a free Gretel account to access our premium SDKs and get started in seconds with our blueprint, ", ".", "If you have feedback or questions we\u2019d love to hear from you! Email us at ", " or ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/podcasts/synthetic-data-as-a-service-for-simplifying-privacy-engineering-with-gretel", "page_title": "Podcast - Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "headers": ["Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Interview", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Any time that you are storing data about people there are a number of privacy and security considerations that come with it. Privacy engineering is a growing field in data management that focuses on how to protect attributes of personal data so that the containing datasets can be shared safely. In this episode Gretel co-founder and CTO John Myers explains how they are building tools for data engineers and analysts to incorporate privacy engineering techniques into their workflows and validate the safety of their data against re-identification attacks.", "\u200d"]},
{"url": "https://gretel.ai/category/synthetics", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Filling in sparse tables with Gretel\u2019s Tabular LLM", "Data Simulation: Tools, Benefits, and Use Cases", "Gretel Demo Day: Exploring the Future of Synthetic Data", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Machine Learning Accuracy Using Synthetic Data", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Why Nonprofits Should Care About Synthetic Data", "What is Data Anonymization?", "Generate time-series data with Gretel\u2019s new DGAN model", "Optuna Your Model Hyperparameters", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/podcasts/towards-data-science", "page_title": "Podcast - Alex Watson - Synthetic data could change everything", "headers": ["Alex Watson - Synthetic data could change everything", "Chapters:", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["There\u2019s a website called ", ". When you visit it, you\u2019re confronted by a high-resolution, photorealistic AI-generated picture of a human face. As the website\u2019s name suggests, there\u2019s no human being on the face of the earth who looks quite like the person staring back at you on the page.", "Each of those generated pictures are a piece of data that captures so much of the essence of what it means to look like a human being. And yet they do so without telling you anything whatsoever about any particular person. In that sense, it\u2019s ", ".", "That\u2019s impressive enough, and it speaks to how far generative image models have come over the last decade. But what if we could do the same for any kind of data?", "What if I could generate an anonymized set of medical records or financial transaction data that captures all of the latent relationships buried in a private dataset, without the risk of leaking sensitive information about real people? That\u2019s the mission of Alex Watson, the Chief Product Officer and co-founder of Gretel AI, where he works on unlocking value hidden in sensitive datasets in ways that preserve privacy.", "What I realized talking to Alex was that synthetic data is about much more than ensuring privacy. As you\u2019ll see over the course of the conversation, we may well be heading for a world where most data can benefit from augmentation via data synthesis \u2014 where synthetic data brings privacy value almost as a side-effect of enriching ground truth data with context imported from the wider world.", "Alex joined me to talk about data privacy, data synthesis, and what could be the very strange future of the data lifecycle on this episode of the TDS podcast. Here were some of my favourite take-homes from the conversation:", "You can ", ", or ", ". You can also find some links that you might like to check out if you found the conversation interesting, below.", "\u200d"]},
{"url": "https://gretel.ai/podcasts/making-data-work", "page_title": "Podcast - Making Data Work", "headers": ["Making Data Work", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In today\u2019s largely cloud-native world, having access to real world data is critical in order for organizations to derive meaning that drives innovation.", "Yet most organizations are still contending with inadequate tools that enable them to safely share, collaborate, and build with data. What\u2019s more, many businesses lack the engineering resources to develop workarounds in the form of anonymized data sets or synthetic datasets that can be used to run powerful tests.", "Ali Golshan, the CEO and co-founder of ", " says that dynamic has put the majority of businesses and organizations at a disadvantage when it comes to innovation, which is exactly where Gretel.ai comes in. The company, which was founded in 2019, aims to be the \u201cGitHub of data,\u201d stepping into to provide tools for data similar to the way the open platform effectively democratized building with code. Greylock has been an investor ", ", and ", " is on the company\u2019s board.", "\u201cYou have better tools, you get better data, you have more data, you can build better tools,\u201d says Golshan. \u201cConsidering the immersive world we\u2019re venturing into beyond the traditional web 2.0, and more sensors, IoT data collection with different types of methods, we felt like that the larger community had the right to be able to not only have as great of set of tools, but to also be able to share and collaborate with that data.\u201d", "Gretel.ai works with a wide range of organizations across industry sectors including gaming platforms and healthcare and biotech organizations including genomic sequencing giant Illumina. Initially developed for heavily data-focused personas such as ML/AI engineers and data scientists, the platform became generally available for any user in February.", "Golshan sat down with Greylock head of content and editorial Heather Mack on the Greymatter podcast to discuss the current data-powered business landscape and how Gretel.ai is enabling innovation. You can listen to the podcast at the link below, ", ", or ", "."]},
{"url": "https://gretel.ai/podcasts/the-data-stack-show", "page_title": "Podcast - The Data Stack Show", "headers": ["The Data Stack Show", "Transcript", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This week on The Data Stack Show, Eric and Kostas chat with Alex Watson, Co-Founder and Chief Product Officer at Gretel.ai. During the episode, Alex shares his journey in data and how working with the NSA impacted his career in space. The conversation also includes synthetic data, the evolution of machine learning models, boundaries between synthetic and prediction models, and more.", "Highlights from this week\u2019s conversation include:", " ", " is a weekly podcast powered by RudderStack, the CDP for developers. Each week we\u2019ll talk to data engineers, analysts, and data scientists about their experience around building and maintaining data infrastructure, delivering data and data products, and driving better outcomes across their businesses with data.", "RudderStack helps businesses make the most out of their customer data while ensuring data privacy and security. To learn more about RudderStack visit ", ".", "\u200d", "Eric Dodds 00:05", "Welcome to The Data Stack Show. Each week we explore the world of data by talking to the people shaping its future, you\u2019ll learn about new data technology and trends and how data teams and processes are run the top companies, The Data Stack Show is brought to you buy RudderStack, the CDP for developers, you can learn more@rudderstack.com Welcome back to The Data Stack Show. Today, we are going to talk with Alex, the Chief Product Officer at Gretel gretel.ai. And we actually have been talking about them for a while. Kostas, it\u2019s a really interesting company. They do a number of things, but the primary thing they talk about on their website is synthetic data. And today, we\u2019re going to talk all about machine learning models and training synthetic data on real data in all the interesting use cases. So this sounds basic, but I want to ask Alex, what their definition of synthetic data is, I mean, you can create synthetic data, you know, in a spreadsheet, you know, in Excel, right, but their flavor of synthetic data is, you know, is pretty specific. And I think it\u2019s really powerful. So that\u2019s what I\u2019m going to ask is for him to define it in, in Grebel terms, if you will.", "Kostas Pardalis 01:25", "Yeah. And so I want to get a little bit deeper into what it means to generate synthetic data from multiple data sets, like what\u2019s how we can reason about things like accuracy? Like what kind of, let\u2019s say characteristics of the original data sets, we want to recreate. So that\u2019s like something that I\u2019m super curious to learn more about. I think we\u2019d have the right person to do that today, so let\u2019s go and do it.", "Eric Dodds 01:58", "Let\u2019s do it. Alex, welcome to The Data Stack Show. Thanks, Eric. We wanted to talk about actually, we\u2019ve talked about Gradel Akasa, for some time and synthetic data. So just super excited to actually cover that topic on the show. Been on the list for a while. Let\u2019s start where we always do give us your background. And what led you to grip metal.", "Alex Watson 02:21", "Yeah, sure, give the two minute version of it here. I started my academic career in computer science, moved out to the East Coast right after September 11. And actually joined the NSA and I was working there for about seven years. Awesome experience, I got to dive in on, you know, early applications of machine learning. And also security, which has influenced my career quite a bit. Over the years since then. 2013 ish, I moved out to San Diego where I am now. I started my first company, a company called Harvest AI. We were helping large companies that were starting at the time to transition to use SAS applications like Google Suite, office 365, Salesforce, AWS, things like that. Help them to identify where important data was inside their environment and protect it. It was a really cool experience there. We built that for about two years. We were at that point, when I offered a series a raise, had some interest around acquisition and actually got acquired by AWS. And I went on to spend the next four years of my career at AWS as the General Manager, launching their first security service for AWS, which was our product at harvest called Macy. It\u2019s a service that customers use today, within the AWS world to identify and protect important data in the cloud. And through and happy to kind of dive in on, you know, that process here, but I think through, you know, both the incredible access that we had inside the walls to data at AWS and then also talking to customers and realizing how difficult it was for them to enable access to the sometimes really incredible datasets that they had they, you know, to enable decisions inside of their business was really some of the that led to the initial pieces that we have with Gretel and synthetic data today.", "Eric Dodds 04:07", "Very cool. So much to dive into there. i Can I ask one question about working for the NSA because, you know, the government likes working in sort of intelligence type stuff for the government? I think a lot of times probably because of Hollywood, you have like two views of it. It\u2019s either extremely advanced and very scary like Big Brother, or it\u2019s like well, it\u2019s the government and they move slowly. And so maybe the technology is quite good. Like, where on that spectrum was the actual experience of working with the NSA if you can tell us something", "Alex Watson 04:42", "you had to there is both so my you know, my first job was programming credit supercomputers actually when I started so I got a chance to work with cutting edge multimillion dollar machines and really cool on so you know the scale with which they were working Also, the caliber of the people there are, you know, almost unlike any other place I\u2019ve ever worked well, also really incredible. Also, it\u2019s the government so things don\u2019t move quite as quickly as you would hope. But, yeah, Greta Lou Spencer.", "Eric Dodds 05:14", "Yeah, very cool. Thanks for indulging me. Okay, let\u2019s talk about how to harvest. So you were going out for a Series A, and then you get, you know, sort of ingested into, you know, a company that, you know, provides, you know, more maybe more data infrastructure than any other company in the world. Right? What was that? Like? Because it may say you work with data at a huge scale. So can you just talk about that experience a little bit? And maybe, you know, especially as Macy\u2019s sort of grew to be a, you know, a very widely used product? What were some of the lessons that you learned, working at AWS scale providing a data service via AWS?", "Alex Watson 05:56", "Yeah, yeah. You know, with scale, I think one of the things you learn really fast is the details matter. So that\u2019s one thing that really stands out: those things that you only have a couple customers and your, you know, even, you know, large scale customers, but those things that are okay to let slide, yeah, come really big issues when you have 1000s of customers. And that\u2019s really what we needed to prepare for, I think on kind of cool experiences or things that I learned, even during my time there, I think, dealing with the scale, like how do we do natural language processing NLP at the scale of terabytes, or petabytes of data that customers have in the cloud was really fascinating. I also think the experience of taking, you know, the time of single tenant software that we\u2019d written that would run inside of VPC, per customer, to a multi tenant, they needed to support, you know, 1000s to 10s of 1000s of customers in the first month, was quite the experience, what happened had some, some some pretty cool learnings during that process. One of the stories maybe just didn\u2019t cover it really quickly, that like really stood out to me, and just kind of helped shape how I think about building software today was there\u2019s most people know like, at AWS like, everything revolves around reinvent and New York Summit, those two launches, right. And those are the two times that you want to service and we were hitting the ground running and having really good traction, I think with a couple customers, and we were getting ready to launch. Macy to the world, and then fully multi-tenant version that may see. And one of the kinds of challenges that we ran into was we had not enough time to completely finish multi-tenancy before we launched. So our choice was either delay six months and launch it at reinvent or launch at New York Summit, we really wanted to launch, you know, and how could we get there? What can we do? And one of our product managers had a really kind of ingenious idea and said, \u201cWhat if we launched the whole back end as a multi-tenant, we launched the front end as a single tenant?\u201d So what that meant is that each customer would have their own unique box in the cloud, that would be running our complete user interface stack. And since it\u2019s AWS, it\u2019s never just one box, you have three regions, you know, per zone. Sorry, you have three, three zones per region. So you have high availability within there. So for each region, we need to have three boxes per customer, we forecasted we would have about 6000 customers at launch at the bar window, you know, that\u2019s somewhere on there. So that meant that for us to launch on time, we needed to run 18,000 virtual machines, just out of the user interface for these customers that might sign up. It was such an incredible experience, it was wild, we almost broke CloudFormation doing a deployment at the time, I\u2019m sure it can handle it quite easily now. But at the time, that was pretty new. And we forecasted that if we could finish the multi-tenant version of the UI within 45 days and shut it down there, we would actually have a pretty conservative amount of cost for running all these user interfaces. So that was one of my more wild experiences. 45 days into the launch, we were able to turn 18,000 machines into nine machines. And I\u2019m sure a data center kind of collectively cooled down at that point, but that wasn\u2019t a neat experience and without a hitch. So it\u2019s one of those things like just taking a step back and asking how you can do something when you\u2019re trying to, you know, hit a deadline or do something like that. And, you know, being data driven and decisions you made. We felt like we could get there and we did. That was a really cool experience.", "Eric Dodds 09:29", "Wow, what a story. What a great story that is so great.", "Alex Watson 09:37", "There\u2019s a lot of stress in there. It sounds like it\u2019s scary now.", "Eric Dodds 09:42", "I can only imagine, right? It\u2019s the pendulum swinging between like, this is gonna be awesome. We can pull it off and are we completely crazy? Totally. Well, tell us about the rattle. When did you decide to start it and then Give us an overview of the problems that you solve.", "Alex Watson 10:02", "So we started Gretel with this thesis. And the thesis was that it was really difficult, as we saw, and I saw as I saw running Macy and talking to, you know, our big customers that are trying to figure out whether all their important data is in the cloud and protect it and figure out if it\u2019s exposed to the world or, you know, answer those questions like how difficult of a problem it is to enable access to data inside of business. And usually, that revolves around privacy, right. So like a contract that you have with your customers for your brand, or sometimes legally enforced, you know, with things like GDPR. And a feeling that we had that kind of the existing methods that are like, Oh, build a wall around your data, build a perimeter, build a better perimeter or VPC, like those things are effective tools, but they don\u2019t work at some point, they\u2019re gonna break. And that\u2019s what kind of leads to breaches happening. And our initial thesis for Gretel was saying, What if we could train a generative AI model? So, you know, very similar technology under the hood to what you see as open AI with the TBT model? On data instead of natural language text? And what if we could get that model to recreate another data set that looks just like your sensitive data set? Except it\u2019s not based on actual people, objects, things? And what effect did that have on privacy? And in theory, if you could pull it off, it wouldn\u2019t matter if someone\u2019s, you know, computer got lifted to Starbucks, and it got picked up and it had, you know, a lot of sensitive information on it. And when possible, maybe we could unlock new ways to share data. It\u2019s evolved quite a bit since then, we\u2019ve got a couple of use cases. But I think that\u2019s still one of the primary ones that we see today is how to address privacy, and how to essentially use these generative models to anonymize data. Yeah, super", "Eric Dodds 11:49", "interesting. You mentioned a few tools that companies, you know, turn to, in order to mitigate concerns around, you know, privacy and security. You know, you mentioned VPC, for example, those seem to be pretty pervasive. I mean, those are sort of like the default set. Would you agree with that? Is that the most common pattern that you see? I think perimeter is a great term, right? I mean, you know, nothing is obviously not an option for many companies. But to your point, like, there\u2019s data breaches in the news every single week.", "Alex Watson 12:24", "Yeah. The various levels, you see customers need some customers keeping data within, like, within their own kind of perimeter or their walls, their private cloud, you see other customers using the cloud that will, you know, embrace technologies, which are awesome, in my opinion, like, you know, V PCs, and, and using role based access to things instead of passwords and things like that. So really good patterns all around. But, you know, access control still leads to the chance and the risk of raw data finding its way out. So it\u2019s one of the things like just to, you know, I would say I applaud the effort that a lot of companies put in making that work really difficult, like you start seeing permissions when you\u2019re trying to set up a VPC or an S3 group. And often a developer just makes the change. And they say, Hey, I\u2019m gonna do this real fast, and see if it works. And I\u2019ll fix it later, they forget, you start seeing issues like that. So there\u2019s a whole new class of tools that are being built to address problems like that. But I\u2019ve been in a security role long enough that you start to see, you know, the repeated patterns. And that repeated pattern of a better way to build perimeter around data is one that sounds good. And it works in some cases, but it\u2019s not the, you know, a long term answer. Sure.", "Eric Dodds 13:42", "Yeah. I mean, all best practices, for sure, right, that don\u2019t necessarily get to the root of the problem. One thing to be helpful, I think, and especially to give context to the rest of the conversation, as we get more technical here. Could you define synthetic data? As Gretel sees it, because it\u2019s, you know, creating synthetic data is, you know, concept has been around for, you know, for a very long time. I guess we could argue how far back in history, but especially as it relates to technology, you know, people have been creating datasets synthetically for, you know, for decades and decades. So, could you help orient us around the term, you know, as it relates to specifically, Gretel does? Yeah. So,", "Alex Watson 14:33", "it\u2019d be I\u2019ll start with a really broad term, you know, we were describing the, you know, 1970s, someone sitting at a, like a DOS terminal writing up like their own or Unix terminal writing up with, you know, a CSV file, right, of data that you would use to test your program. That\u2019s synthetic data. So, you know, broadly speaking, I would define synthetic data as a computer simulation or algorithm that can simulate real world events. Some objects, activities, things like that. So it could be a spreadsheet, it could be a mathematical formula, it could be a computer program that just spits out random temperatures, ages, things like that for people. So it can be that simple. You also hear the term, a lot of times like fake data, or things like that you have some kind of mock data that might make sense for testing a user interface or something like that. But you wouldn\u2019t want to ever query that data, or ask it questions. In the Gretel context, we use synthetic data to define data generated by a set of deep learning algorithms. So similar, once again, to use that analogy with open AI as GPT models or chat GPT or stable diffusion per image. Essentially, we have models that learn to recreate data like what they\u2019ve been trained on. And you can either create another data set that looks just like it once again, with artificial, you know, people, places, things like that. Or you can prompt the model to create a new class or to boost the representation of class in your dataset where you want to see more examples. And we see a lot of that, too. So maybe summarizing here, like, the deep learning approach allows you to use the data for a lot more use cases, whether you want to use that data to train machine learning models to power data science use cases inside your business or information exchanges, or things like that, we see a lot of this in the life sciences world, where you\u2019ve got companies that are trying to share broadly, you know, research about COVID, or about genetic diseases or things like that. It\u2019s here while preserving privacy. So that is when we talk about, you know, synthetic data in the global context, it\u2019s data that can be used that has the same quality and accuracy as the original data was based on.", "Eric Dodds 16:49", "Fascinating. Okay, can we talk about similarities to Chad GBT has been making its way, you know, across Hacker News and Twitter and, you know, all over our internal Slack channels, with people doing interesting stuff with it. And you have a lot of experience with natural language processing and algorithms that run on language. Could you explain the differences in that flavor of deep learning as compared with running deep learning on data itself? Right. I mean, that\u2019s an interesting concept to consider in general, right? Can you just explain the difference and sort of the even the ergonomics of how you would approach deep learning on data versus natural language, right? Because there\u2019s, it\u2019s just a really, it seems like a very different paradigm. But it sounds like they\u2019re actually pretty close.", "Alex Watson 17:41", "Yeah, they are, I think, the underlying technology, maybe to talk about that first, and then talk about the interface for how people interact with it. Yeah, the underlying technology is using a class of machine learning models called language models, or large language models for both open AI and what we\u2019re doing in Gretel, and that we came out of realization really on our part that a dataset is a language and its own kind of right, that makes sense to computers, that is harder for, you know, kind of humans to assimilate. But under the hood, the technology is very similar. We\u2019re using language models, we use a recent class of language models called transformers, that have a great ability to learn data from wide, wide collections of datasets and be able to apply it to whatever context you\u2019re asking about. So you can essentially augment your data with better examples. So I think the open AI GPT examples is very close to what we do credal chat GPT is a layer on top of GPT-3. And it has a slightly different mechanism that\u2019s used for training. And this is, you know, kind of wild to think about, and you really have to dive in here. But under the hood, it\u2019s hard to believe that it works at this scale, but under the hood GPT-3. Or, you know, rattle at this time, is really predicting the next field, if I have a user that\u2019s from if you\u2019ve got a movie review dataset, and you\u2019ve got, you know, people that have consistently rated a movie at this level, you have a new user being generated, probably going to generate a rating within a certain range. So really, it\u2019s just saying, Okay, if I have alteration, what\u2019s the next most logical thing for me to do? Had GPT put a layer on top of that, and did two things that I think we\u2019re really significant. One essentially uses this concept called like human based reinforcement learning where you have humans that are kind of, instead of just getting an algorithm that is the single best thing at predicting what the next token in a sequence is going to be they, it takes a look at the whole result and says, Is this the result that I want as a human or not? There\u2019s human laborers that are looking at it in their sayings, so I asked them to create a list of to-do items for today, like do these make sense to me and a human reviewer at the opening . I will look at it and I\u2019ll say that is the best answer. And then we\u2019ll use that to feed back into The algorithm and come up with better results. So two things that have, I think, that are really significant are kind of happening right now. One, we\u2019re orienting machine learning algorithms to have responsibility, but humans want to see, which is good, right? And the robots will be rising up against us if we\u2019re teaching them to do the things we want them to do.", "Eric Dodds 20:18", "And we have a say, in the uprising, we have we get a say on the", "Alex Watson 20:24", "right, all it takes is one person to train in a different way. But fortunately, our training has got the right direction. So that part is really neat. I think the other part that I love about it that I\u2019m really excited to see in the synthetic data world is this natural language interface. It used to talk to models, right? So instead of GPT, two, if we were to rewind back when major GPT version, right, and look at it, and you would give it a couple of examples, you give it examples of tweets or blogs, and it would create new tweets or blogs, like what it was trained on with chatty btw. And increasingly with the other TBT models, you can just say, like, brainstorm a list of to do topics for me to look at today or something like that. Yeah, you have this natural language interface similar to stable diffusion with images, right, where you can say, yep, what a unicorn on a surfboard on Mars, right, and it\u2019ll generate it. So really excited to see this way that we interact with data becoming more based on natural language than SQL queries, or, you know, data engineering that we all have to do to get that kind of answer right now.", "Eric Dodds 21:28", "Yeah, absolutely. Fascinating. All right. Well, I\u2019m going to stop myself, because I have a little wave of questions backed up, Costas, please jump in here, because I know you have a ton of questions as well. Yeah.", "Kostas Pardalis 21:43", "Thank you, Eric. So Alex, let\u2019s look a little bit more about synthetic data. And you mentioned it because Eric asked us, like what synthetic data is. And I\u2019d like to get a little bit more detailed on what it means from a datasets to generate more data that are synthetic artificial rights. And they are similar, like they serve the same properties. i What are these properties? How can how we can how should we think about", "Alex Watson 22:21", "about that? A synthetic data set, and we use the term like if you were to query it, right. And so if you were to issue a, build a dashboard off of this dataset, or just send it to SQL query that you would get a very similar response for an aggregate statistic. So what is the average age of a person who likes to buy this product? Or when I have this spike in activity, things like that will be very similar between synthetic data sets and the real world dataset. There\u2019s a couple of ways we measure this, you know, at first you create your first synthetic data set and look at it like that looks great. I don\u2019t know how it\u2019s gonna work for me. And that\u2019s the first question that, you know, we always hear from our users, the dataset looks awesome. You know, like, when I look at it, it looks fine. But I don\u2019t know how accurate it is. How do I measure that? And so what we try to do, we have both like opinionated ways to measure the quality or the accuracy of synthetic data and opinionated ways, the unimpeded ways make the most sense, when you\u2019re just trying to create an artificial version of a data set, you don\u2019t know how it\u2019s going to be used. So we don\u2019t know what type of machine learning tasks are gonna be used for. So we can\u2019t measure that. So what we do is we look at, I\u2019m giving you a couple of examples, we look at the correlations that exist between each pair of records and the original data set and the correlations that exist in the synthetic data set. So if I knew that, to go back to the movie review data set, right that like, you know, movies with Keanu Reeves usually had really high ratings, right? Like, I would expect another movie and a synthetic data set with catteries to have high ratings and kind of it goes on much more deep than just kind of that two level correlation. But that\u2019s the first thing that you know, one of the things that we look at. Another really helpful, you know, helpful tool we have is called field distribution stability. So we\u2019re looking at this and this is just a pretty common data science tactic. A lot of people will do this, we just automate it, where you might have a dataset that\u2019s got 100 rows in it, or 100 columns. Essentially, we plot that it\u2019s something called PCA, principal component analysis. Now we plot it in a 2d plane. And we look at the difference between the plots or the synthetic in the real world dataset. And we say, when we map these 56, or 100, column data set down to two dimensions, how similar these distributions look, and that gives you that insight as to whether the model is overfitting. And it\u2019s just repeating a couple things inside of there, or if it\u2019s capturing the whole distribution. And the third thing is the most probably intuitive way to think about it. And it\u2019s just looking at the parochial distributions, right? If you\u2019ve got admission times where people in an EHR data set or you have you\u2019re looking at a financial data set Do you have open high, low, close? Yeah, type data, do the distributions of each one of those match what you\u2019re expecting to see. And that\u2019s something we try to automate the whole process and give you a single score that helps you reason about how well the model is working.", "Kostas Pardalis 25:15", "All right, and this is the opinion 80s ways, what are the opinionated ways,", "Alex Watson 25:20", "depending on the ways when you know how you\u2019re going to use the dataset, if you\u2019re gonna use it for downstream classification, training? Regression, you\u2019re using it for forecasting, we see this quite a bit in the financial space, right? Where do you want to use time sensitive data, but to forecast a stock price, that\u2019s what that\u2019s going to be, things like that. When you know how you\u2019re going to do that, you can actually simulate running the synthetic data on the same downstream forecasting use case as the real world data and compare the two. Now, there are some really great tools out there that make this easy. So there\u2019s a framework in Python called PI care, a lot of our customers like to use that quite a bit. Essentially, it simplifies this process of testing, how your synthetic data works on classification tasks, or QA question answering tasks and stuff like that, versus the real world data is based on.", "Kostas Pardalis 26:13", "Okay, that\u2019s super interesting. Like, I can\u2019t feel my soul, like, I have to ask you that, like, where is like, the boundary between synthetic data and prediction of the future of the right, because, yeah, and I am asking that, because you are mentioning is like, okay, like financial models, for example, where you\u2019re using, like synthetic data to go and like, run some models and do the whatever they want to do there. But. And, also, we have, like the conversation earlier about, like, Saudi Putin, like all these things about trying to predict what should be the next, right. Part of the text. So prediction is part of the whole thing that we are doing here, right? So what\u2019s the boundary there between trying to predict what is going to happen, like, you know, at some point in the future, for example, like or like in some kind of like data set, and actually just creating, let\u2019s say, data, that\u2019s they serve some common characteristics, but at the same time, they don\u2019t represent reality, right.", "Alex Watson 27:24", "I think we\u2019re machine learning models in general, and have a really hard time dealing with data that they\u2019ve never seen before. Oh, when there is a market event to go back to the financial world. Yep, never happened before, it\u2019s unlikely that your machine learning model is going to be proficient at detecting it. But that said, history repeats itself. So one of the really popular use cases that we see in the financial space is when you have rare events, for example, the Gamestop events, or the two that happened, were significant market changes happen due to something that has, you know, happened for the first time, crypto market crashes, things like that, when you want to train your machine learning models to be good at detecting this, and you can only pass that a single example. Once again, it\u2019s not going to do well. So that is an area where I think synthetic data can really help. Today\u2019s synthetic data can really help is that you can give it an example of saying like, hey, look what happened with GameStop, I want you to create another 50 or 100 examples of something like that happening. So better at detecting that if that happens in the future. So those are artificial, they, they\u2019re based on real world data. And they\u2019re based of, you know, kind of learning off what happened in that one example. But they\u2019re not perfect. But in many cases, that actually really helps we see. And that\u2019s kind of one of the neat kind of patterns, we\u2019re starting to see with our journey, you know, kind of building synthetic data, I think the you know, we\u2019re in year three now at Gretel. And first year was like, does it work on my dataset, right? And the next year was like, Okay, but how does it work against my real data, and then now we\u2019re starting to see this kind of tipping point where people are realizing that machine learning models are data hungry, there will always be classes that you\u2019re not good at. So this idea of augmenting your real world dataset with additional synthetic examples that are perhaps trained off public data, so has the world team is before and can I incorporate some of that knowledge into my own dataset helps you build a better data set that can have better accuracy than then than you would have all by itself?", "Kostas Pardalis 29:36", "That makes sense all right. So in talking about data, what are the types of data we are talking about here because we can have a synthetic picture we can harvest synthetic audio files we can have as synthetics are all in the daybook. So what are like the most common use cases that use see out there like synthetic data is important today. And by the way, I know that because we hear you mentioned many times like the natural language processing part of these shirts, probably more textual. But we have other things that we have like time series data we have structured versus unstructured data. So yeah, great. Well, tell me more about that. Yeah.", "Alex Watson 30:27", "I\u2019m going to come across to what, you know, probably a little bias here, because, like, I would say, Gradel would be the, one of the leading companies, if not the leading company, and working with tabular formats of data. That\u2019s really where we got our start. That\u2019s, you know, where we built on that said, like, our vision, and I think the vision you described person, DAG data is much bigger than like one type of data. So maybe to talk about the types of data that we see being used for synthetics quite often, that can even give some examples for different types, but", "Kostas Pardalis 30:56", "you have tabular data to", "Alex Watson 30:59", "start out. So the stuff that you haven\u2019t said, a data warehouse, a database, things like that time series data, which helps a person in each category of tabular data, then until you realize, like 50% of the world\u2019s data sets, the time is such an important component, that it\u2019s one we actually treat differently. Text to natural language, text, or different languages. And image synthetics are really big, right? So people are using images quite often to train models for self-driving cars, or to recognize problems in a manufacturing line or things like that. So a lot of use cases around that. And, you know, I\u2019d say increasingly getting into video and audio. So some of the new technologies that came out recently, like stable diffusion are really showing the ability to create new variations or artificial versions of images and videos, were the companies that are trying to build, let\u2019s say, you\u2019re an insurance company, and you\u2019re trying to build something to give somebody a better insurance quote, for their house, and you want to look at the quality of the materials that they have. And does it look like they have fire extinguishers and things like that around the house, just from a set of pictures, you never have enough data to start with. So this idea of augmenting images with, you might have a room and you will see a room with really fancy furniture or blondes with more like something that a college student might have things like that. So we\u2019re seeing a lot of use cases there. And maybe the last place to touch on would be the simulation space. So we\u2019re talking about today, we\u2019ve talked a lot about generative models, machine learning models, neural networks that create new examples of things. But there is, you know, in parallel to that there is a simulation space where you might use something like a computer game engine. So Unity would be a good example of this, and Vidya. Nvidia has a neat product called the omniverse, as well, where essentially, they have created a 3d world using a game engine that you can use to create and test these different kinds of simulation based outcomes.", "Kostas Pardalis 33:02", "Wow, that\u2019s super interesting. Okay, let\u2019s focus on tabular data. What do we mean by tabular data? tabular data is", "Alex Watson 33:11", "any type of values the term here in the left do you think on it to like any type of structured or semi structured data format, so it could be anything from a CSV file to use the format\u2019s, again, JSON, where you don\u2019t necessarily have the same level of structure, but you can have arbitrary levels of nesting, more advanced data formats, like Parquet, that are really efficient at encoding large amounts of data, or just data that\u2019s inside a database or data warehouse.", "Kostas Pardalis 33:37", "Okay. And when we\u2019re talking about creating, like synthetic data here, what will they do? The most common approach that you see out there is like, Okay, I have, let\u2019s say, a user table, right, with like, 1 million users. And I\u2019d like to see like, 2 million of these users having like, let\u2019s say, okay, similar characteristics, or like the distribution of like the users like similar, let\u2019s say in terms of like, the age or the geography or like, whatever, what kind of information we capture are already on this table. Is this like something that \u2018s like the most common use case that you see out there or like people that are actually coming in, they\u2019re like, Okay, that\u2019s my database here, right? Like I have users and the users have, I don\u2019t know, products, not they procured at some points. And I also have, let\u2019s say, my inventory. And I also have, like, let\u2019s say, we represent the whole domain of like, what the company is dealing with, or the user is dealing with, which can be like, quite complex, right? And they want like to synthesize the whole database that is other", "Alex Watson 34:50", "relational components. So not just capturing the relationships that are inside a single table like your users table, but capturing relationships between users and then Inventory table is a really cool challenge in tennis and dedicated space. Yeah, popular. So you know, to answer your question when we have users come in and use our platform, often, especially if you\u2019re doing pre production testing, like a really big use case, we haven\u2019t talked about yet, as much as that you are trying to build a version of your production environment that you might use inside a development or a staging. You don\u2019t want to have real world data, but you want to have it reflect what\u2019s happening in your production system. So this allows any of your developers to use it in a Hammermill way to investigate different records without worrying about privacy or things getting compromised or anything like that. So in this use case, we have customers often that will create a twin version, a diverse staging test version of a production database. Essentially, they\u2019ll queue it up to depending on how recent they need to keep it once an hour, once a day, will run the job will bring in all the new records or records that have changed, train this model on that data and create another, essentially create another database that sits inside of your near test to your staging environment. The really neat thing is not just the database, you\u2019re getting here getting a model. And this model can be used to either subset that data. So if you have, you know, 2 billion records inside your production database, and you can run that in your dev staging environment without having insane, you know, DynamoDB costs, you can create a smaller data set that captures as many variations as possible. So it\u2019s much more efficient than just taking a slice of that data set. It\u2019s more native to it. Or and I think what\u2019s another really neat use case for scale testing. Yep. You\u2019re, you want to test the ability of your application to handle 10 or 100 times the amount of data you might encounter on a typical day, without just repeating the same records over and over again, you can use that same model to generate new variations of the data that you can use the test is", "Kostas Pardalis 37:00", "not super interesting, and how CDC and this processes kind of can take off like through like, to the developer experience, let\u2019s say I\u2019m like a developer, I have like my database to date. And I want to go and try my best, the limits of my production environment, right? What am I going to be doing and how am I going to be using glide results to do that? Yeah.", "Alex Watson 37:26", "So this process with Gretel is two stages, you\u2019ve got your production database that\u2019s sitting, let\u2019s say it\u2019s a Postgres database, or it\u2019s an atlas database, hosted or not hosted really doesn\u2019t matter. You want to create a version of it or your lower production version, there\u2019s two steps you need to do. One, you don\u2019t want the synthetic data model to memorize important information, like customer information names, customer IDs, and things like that. So we have two steps. These are both powered by a cloud API. So you can either run in the cloud, sometimes customers have really sensitive data requirements. So they need to run inside their own cloud. So you can deploy these workers as containers to your own cloud. But the two steps are one scan, and use NLP to identify for example, sensitive data, customer IDs, names, things like that. From there, you have a policy that says, Whenever I see this, I\u2019m going to redact it, I\u2019m going to replace it with a fake version of it, I\u2019m going to encrypt it in place or whatever your company feels is appropriate. Often, we see people using fake data because it is just the name, for example, my user name. I might, you know, replace it with another artificial name. Yep, just make sure the model doesn\u2019t learn it. You have a risk there that even when you do that traditional D identification, the other attributes of your dataset that by themselves aren\u2019t, aren\u2019t identifying, for example, like my age, my location, a lot of times advertising via this precise location will put you right at somebody\u2019s house, right become a very identifying when you put those together. And the real power of these synthetic models is that they will create artificial versions of those things. So you remove or replace the names inside your data set, you create new artificial locations, shopping cart activity, like whatever you have inside of your data set. So the second stage is the data center where you train a model, and then you tell that model, I want to generate 10 times as much data or I want to take 1/5 as much data and you take the outputs and essentially put that right back into your database. So you create a twin database that you can use for testing.", "Kostas Pardalis 39:33", "Okay, and how long that process takes, how long it takes to dream this model.", "Alex Watson 39:41", "That varies, and it varies based on what your use cases are. And so we have this kind of belief that there\u2019s no one machine learning model to rule them all. And each one has different advantages. So if you are going we\u2019re a machine learning use case and you care about accuracy. You\u2019d want to use a deep learning generative model Like Carlo, which gives you the best performance of anything. Alternatively, you have GaNS. So generative adversarial networks, which don\u2019t offer quite the performance of our language models, but they\u2019re pretty fast after training. And, and we have a built in really working with customers that when they have, like, tremendous scale, they need to run out, we built statistical models. So these are based on copulas. So instead of using a deep learning technique, they use a mathematical, really neat kind of technique to learn and recreate distributions in data. So, essentially, based on your use case, like do I care about accuracy in my training a machine learning model on this, use a generative algorithm that might take an hour to five or six hours to train on a data set, depending on the size of the data set. If you want speed, and you want to generate data at 100. Meg\u2019s per second, so I can create 40 billion records to test my dataset. That\u2019s where, you know, I really suggest using, we call it our amplify model, but the statistical model and on a, you know, 32 core machine, we\u2019ve clocked it at about 100. Meg\u2019s per second it can generate so if you\u2019re generating billions of records, it\u2019s entirely possible to do that within a day, instead of having to wait a month", "Kostas Pardalis 41:15", "to bottle to do it. Yeah, that makes total sense. And like how that\u2019s very interesting, actually, like, there isn\u2019t like this trade off between, like, let\u2019s say, fidelity and time that you spent, like training the model, right? Again, going back to what it means to represent with accuracy, like the characteristics of, of the data that you have, like initially, what does this mean, like from the user perspective? Like, how can I reason as a user about that stuff, because it\u2019s on a high level, it\u2019s easy to understand. But I think that when you start, like working with a real example, and you have, you know, like your own data are there, things are like, much harder like to figure out how you can reason about these things.", "Alex Watson 42:10", "Often, it\u2019s somewhat, at the end of the day, depending on the domain. And like the use case, you\u2019re going after, you know, we see a lot of repeated domains that we talked to you, we\u2019ve got a discord chat channel, where we talk about things, from life sciences to things that we see in the ad advertising space, another area that\u2019s really kind of picking up on that data. So we can reason about those, I\u2019d say between our top end, most capable language models and GaNS versus our statistical models, you\u2019ll see about a 10% decrease in accuracy. So were you to train that as a classifier, that downstream dataset model with, you know, using the statistical method would be about 10%, less accurate, on average, than this and one of the things, I can link to the link to you guys, after the show here, we run all of our models against about 50 different datasets, and then compare the results in the accuracy of each one. And you can kind of see how the you know, state of the art language model performs versus a state of the art again, versus the, the statistical model and kind of make your own decision there. We also are realizing that so many of our users don\u2019t have time to make this decision. So you know, we\u2019re introducing these things called Auto params, that are on by default with many systems now. And it just looks at the size of the data set, and it says, Are you trying to generate, you know, to use that example, again, 40 billion records? If you are in use, it will just pick the right algorithm to do this for you. You know, increasingly like me I think all of our vision is that six months from now, people don\u2019t have to worry about What model to choose for this use case, we just pick it based on what we\u2019ve observed with the data.", "Kostas Pardalis 43:52", "Okay. All right. And one last question for me. Because we\u2019re getting closer to the buzzer here, as Eric usually says, and I want to give him some time to ask any questions that he has. If I\u2019m new into the likes, synthetic data worlds, where should I look to learn more, and play around like, with technologies or like tools or like anything else out there that exists right now?", "Alex Watson 44:31", "Yeah, so in this world, I would, I mean, of course, first would recommend starting with Gretel. So just a quick thing on that, and then I\u2019ll mention a couple other platforms to check out as well. Our underlying models and code are all open source, so Brettell, synthetics on GitHub, so you can see how they work. You can introspect how we do privacy, things like that. Our service has a free tier, so all you need is a G email or GitHub to sign up and we have an example data set. So we have these local On interfaces where you can just say I\u2019m trying to balance a dataset, I\u2019m trying to classify a data center and create a synthetic version of the CSV that I have. You don\u2019t have to write a single line of code, you can do it yourself. And that\u2019s where I always recommend starting, because it just makes so much more sense after you\u2019ve tried it. So that part is free. I would also definitely recommend open AI has a really great playground for the chat GPT and open AI GPT models, just trying some prompts or trying to send some data in and tell it to summarize something for you or create a list for something I think is that kind of gives you a feel for where models are today and where they\u2019re going. I mean, other things too, as well. Awesome. Thank you. So", "Kostas Pardalis 45:40", "WODs Eric\u2019s microphone is yours again.", "Eric Dodds 45:46", "Oh, wow. So I feel so empowered. Yeah, this has been such a fascinating conversation. Alex, I want to pick your brain here in the last couple of minutes on your thoughts on sort of the impact that these technologies will have? You know, I think, as we think about Gretel, you know, one example we talked about, as we were prepping for the show, as you know, hospitals, being able to share records around, you know, a particular disease, right, in order to help researchers and medical professionals, you know, solve a problem, you know, and help treat that disease or even cure the disease, which is really incredible. And then you have, you know, sort of, I would say, things that are in a little bit more of a gray area with like, stable diffusion, right, here, even chat GB t where the uses can vary, you know, widely, right, and can even be used for things that, you know, people would consider unethical, you know, sort of depending on what you\u2019re talking about. And you have really deep experience, I was thinking about this, as Costas was talking, and you were explaining a lot of the stuff I mean, you have experience with, you know, intelligence, the government and building AI technologies, solving privacy problems. Maybe a good way to frame my question would be, do you think about stewardship of these deep learning models as an artificial intelligence in general? And if so, what are the things that are top of mind for you, as we break new ground with deep learning and the ability to produce all these novel outputs? Yeah.", "Alex Watson 47:38", "Great question. Maybe that\u2019s, you know, kind of two parts of the question, you know, where could this be transformational? Or what are we going to see across these different technologies, both for kind of sharing data, or creating data? And then what are the ethical implications we need to think about around that, or where this is going in the potential of it? There is a very good chance that, you know, these, this kind of, kind of back this up in a second, we start something a little bit more bold, but like this will be the biggest innovation issue that has happened since cloud computing. And the reason, I believe so, is because these models give you the ability to distill and disseminate information or intelligence in a way that has never been possible before, right, natural language interface, you can query. So I think that\u2019s huge. And we\u2019re just starting to see the use cases for it. Speaking of the data sharing use case, for example, like with life sciences, institutions, things like that, right, like so, data driven healthcare and medicine is like, you know, that anyone in the space would say, that is like the biggest potential for helping health, you know, that they can see, the biggest limitation that they have, is that often that data is siloed within a particular region. Sure. If you\u2019re trying to create a cure, that\u2019s going to work for people across the world, but you only have access to one demographic, right? For example, the UK Biobank right? How do you know that it\u2019s not just you created or you found a signal that is then a population that will work everywhere. So the power of this and we did a really cool study with Illumina working on genomic data was showing that we could in fact, synthesize the one of the most complex data sets it\u2019s ever been created. Sure, I started with mice, which was kind of funny. So you know, but even the mice had about 100,000 columns of attributes. So we\u2019re only gathering and what we showed is that we could synthesize that dataset, create a totally artificial version of it, but then recreate the results of a popular research paper that have been created using that data, which was cool. So a lot of work left to be done. They\u2019re both on, you know, the sheer scale of human genomic data. And then also the privacy but the potential there is that a researcher anywhere in the world that had an idea on how If you cure rare disease could test that against every hospital in the world, which would just be. So a really exciting example there, on the chat GPT and open AI approach, I hear a lot, particularly stable diffusion that is just for creative use cases, it\u2019s just for kind of like messing around. And I would challenge that and say, That\u2019s just where it is today, it\u2019s not going to be there for long. Yeah, and what I think is missing right now is the confidence that you have, that model is going to output what you\u2019re looking for. Right? So you could say, like, you know, generate a picture of me standing on a mountain, like drinking a coffee or something like that, right. And like, maybe the first time I\u2019ll do it, second time, third time, it won\u2019t. And in the data world, in the conversations we have with our users, right? Like, there are tons of applications for machine learning, training machine learning models based on being able to generate new images, but you have to have confidence that what the model is outputting meets your expectations. So I think that\u2019s going to be the next big, you know, kind of big thing there. But I do think that these models are going to, you know, in one way or another, they\u2019re going to be everywhere, right? So training, creating more training data for models, whether it\u2019s summarizing a meeting that you had automatically for you at the end, or things like that, you\u2019re gonna see these models by a bit. And the last part on ethics and pet stewardship, where this goes, it\u2019s an interesting question, particularly how you kind of phrased it with the background around, you know, intelligence and things like that. And when technologies exist, when they get created, they will inevitably, by some level, be abused. So that will happen. And so I would personally vector a lot more towards openness, and, you know, relying on society to solve these problems together than having the risk of, you know, trying to control it, but then essentially just creating a small set of, you know, governments and rich companies that have access to this technology. So I really kind of applaud the open source movement here. All open source publishing, and research and things like that. That approach, I think, is working well. Things start to, you know, I think historically have gotten more problematic, when that gets closed off or limited. And then you don\u2019t have the kind of the ability for a community to look at something and give you an opinion on whether it\u2019s ethically correct, or we should do something about it.", "Eric Dodds 52:16", "Sure. Such insightful answers, I will be considering these things, definitely for the rest of this week. And probably long after Alex, this has been an unbelievably thought provoking show, and we\u2019ve learned a ton. So thank you so much for appreciating it. I think my big takeaway from this show, Costas is that Alex is number one, so approachable as a person, but number two, has such a variety of deep experience in the space, you know, from government intelligence to startups to, you know, delivering things at scale on a crazy timeline, you know, within sight of, you know, within AWS. And so I just grew more and more to respect his opinion throughout the show, which made his final thoughts on where these types of deep learning technologies are going, I think, even more poignant, for me, and I really agree with him, I think he it was a really fresh, honest take not to say, Well, you shouldn\u2019t use it for this, or you should use it for this. I mean, he acknowledged outright that, you know, these new technologies are always used in ways that, you know, humanity probably shouldn\u2019t use. And doing things in the open, is, you know, a really healthy antidote to that. And so I really appreciated his perspective on that. It sounded simple, but I think it was very powerful. And something that I\u2019ll definitely keep from the show.", "Kostas Pardalis 53:57", "Yeah, 100% I totally agree with that. I mean, I think at the end, especially when you\u2019re talking about technologies, or knowledge in general look, I\u2019m like, I don\u2019t know, like, changing the very foundational way, like, the way that we operate as humans. Yeah, it might be scary. Like, obviously, we can make mistakes and use technology in the wrong way. But in the end, like that\u2019s how I don\u2019t like human models to make Congress right. Another thing that we can change and I don\u2019t think that there\u2019s that much value at the end in not taking the risk of having access to these new tools or like this new knowledge and again, the best way to protect humanity is to make these things available to everyone. So I totally agree. I think we\u2019re going to hear more about these technologies, and okay, there\u2019s a lot of, let\u2019s say, also like Guy The Hype right now. And we\u2019re still just scratching the surface of what can be done with these technologies. But I have a feeling like in the next couple of months, we will see much more practical and interesting uses with these technologies. And we\u2019ll have more people on the show also to talk about that stuff.", "Eric Dodds 55:23", "Absolutely, no, it was. It was a great episode. And we want to have them back on. But like we said earlier, when we were wrapping up the year, we wanted to talk more about some of these emerging technologies like Chuck GPT and gravel that are forging new ground. So thank you for joining us. A subscription if you haven\u2019t told a friend and we\u2019ll catch you on the next one. We hope you enjoyed this episode of The Data Stack Show. Be sure to subscribe to your favorite podcast app to get notified about new episodes every week. We\u2019d also love your feedback. You can email me Eric DODDS at Eric at data stack show.com. That\u2019s e r i see that data stack show.com. This show is brought to you by RudderStack. The CDP for developers learn how to build CDP on your data warehouse@rudderstack.com", "\u200d"]},
{"url": "https://gretel.ai/blog/why-privacy-by-design-matters-more-than-ever", "page_title": "Why privacy by design matters more than ever", "headers": ["Why privacy by design matters more than ever", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Earlier today we ", " that Gretel raised $50 million in funding to help us advance our mission to bring privacy to all developers. At Gretel, we are a privacy-first company and that means we are not tied to a specific tool, workflow, or framework. Our mission is to solve data privacy problems for our users in the most elegant and extensible way possible. We believe there is an entire community of developers and innovators that want to embrace privacy, and our pursuit is to build great tools to help the community decisively reshape data privacy.", "Companies today approach data privacy as a risk-management challenge which from what we\u2019ve seen over the years, consistently result in platforms, processes and products that are difficult to implement, lack effective results and create substantial friction in development velocity including:", "These friction points are intertwined, and there are no flexible solutions for developers to easily integrate data privacy into their unique workflows and services to enable \u201cprivacy-by-design\u201d.", "Engineers and developers face the same friction points with data privacy that we\u2019ve solved for throughout our careers \u2013 at Gretel, we\u2019re building an engineering-driven solution to these friction points. In today\u2019s world where data is integrated into our lives; the products and services we use, and throughout our digital experiences, \u201cprivacy by design\u201d is critical to the development process and requires developer-native tools to enable it.", "Our goal is to equip developers with the right tools to collectively change how privacy is built into engineering and data workflows. Gretel\u2019s Privacy Engineering as a Service tools offer easy-to-use developer APIs and a suite of tools that provide the highest quality results through data operations to label and classify, transform and anonymize, and generate synthetic data.", "Gretel\u2019s privacy-by-design tools can transform privacy from a bottleneck to an accelerant. Thousands of developers in the Gretel community and early enterprise customers are running our services with success. Our work with our customers shows synthetic data can have less bias and better accuracy ", " than real-world data based on actual objects, events or people. Always-on ", " built to defend against known privacy attacks can help teams collaborate quickly and freely with data, too. ", "Trust and transparency is a top priority for Gretel. Gretel tools are based on an open-source core. The need seems clear; we\u2019ve seen over 70k downloads of our synthetics library, and 500% jump in the number of users over the last year. Collectively, alongside the developer community we can solve for data privacy bottlenecks together. ", "Thank you to all the developers and supporters who have helped us prove you can learn from data without compromising privacy. We\u2019re building this for developers like you and will continue to build and share our research and learnings with the ", ". You can ", ", and please don\u2019t be shy with your feedback.", "Gretel is hiring! If you have a passion for privacy and technology for developers check out our ", "!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/product-updates?094d394d_page=1", "page_title": "Product updates blog posts - Gretel.ai", "headers": ["Product updates", "What's new in Beta2", "CHANGELOG: Beta2", "Introducing Gretel Benchmark", "Automate Synthetic Data Pipelines with Gretel Workflows", "Fine-tune a MPT-7B LLM with Gretel GPT", "Gretel releases Beta 2", "How accurate is my synthetic data?", "Data Is More Valuable When It Can Be Shared", "Introducing Gretel Amplify", "NEW: Integrating with Gretel SDKs just got easier!", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/podcasts/the-shifting-privacy-left-podcast", "page_title": "Podcast - Shifting Privacy Left Podcast", "headers": ["Shifting Privacy Left Podcast", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This week, we welcome ", ", Senior Applied Scientist at ", ", a privacy tech company that makes it simple to generate anonymized and safe synthetic data via APIs. Previously, Lipika worked as a Data Scientist at LeapYear Technologies, and was the Machine Learning Researcher at ", ".", "Lipika\u2019s interest in both machine learning and privacy comes from her love of math and things that can be defined with equations. Her interest was piqued in grad school and accidentally walked into a classroom holding a lecture on Applying Differential Privacy for Data Science. The intersection of data combined with the privacy guarantees that we have available today has kept her hooked ever since.", "There's a lot to unpack when it comes to synthetic data & privacy guarantees, as she takes listeners on a deep dive of these compelling topics. Lipika finds elegant how privacy assurances like differential privacy revolve around math and statistics at their core. Essentially, she loves building things with 'usable privacy' & security that people can easily use. We also delve into the metrics tracked in the Gretel Synthetic Data Report, which assesses both 'statistical integrity' & 'privacy levels' of a customer's training data.", ":", ":", ":"]},
{"url": "https://gretel.ai/blog/workshop-generating-synthetic-data-for-healthcare-life-sciences", "page_title": "Workshop: Generating Synthetic Data for Healthcare & Life Sciences", "headers": ["Workshop: Generating Synthetic Data for Healthcare & Life Sciences", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["At Gretel when it comes to data we care a lot about ethical, equitable and fair practices. Gretel\u2019s CPO Alex Watson gave a workshop to a data science working group at Emory University on how you can address these practices, and how to leverage tools from Gretel to create statistically accurate synthetic data for health and life sciences research. You can ", " or read the full transcript below, and then ", " for free with Gretel. ", "Alex (00:02):", "Hey, my name is Alex Watson. I am a co-founder at Gretel AI. Today we're going to talk about synthetic data and how we see it being used by data scientists and software developers in the healthcare and life sciences spaces. We'll start with some of the use cases that bring certain life sciences or healthcare organizations to say I'm interested in checking out synthetic data. Actually the use case on the bottom is probably the most popular one that we've seen, enabling faster access to data. Often getting access to medical research data sets can take four to six months to go through the necessary approval processes to meet compliance obligations. The question that we ask with synthetic data is can we create an artificial data set based on this real world data that doesn't link back to any actual patients? And can we use that data set to enable a doctor, for example, to learn about a rare disease without learning about the patient?", "Alex (00:55):", "So really exciting use case there we see with a fair amount of our customers really diving it on. Other use cases are around making better data sets. Both reducing bias, and then also generating more samples from limited data sets are around kind of the same goal where you have access to some data, but you don't have enough of the right data. So a question is can we boost the representation of a minority class in a data set or change some, engineer some feature of the data to create a more equitably, fair or balanced data set. Some of the examples that we've worked with and I'll link to them in the blogs before are working with the UCI data science team on an openly published heart disease detection data set, where there was a two to one representation of males to females. We had really encouraging results. Essentially, we boosted the representation of female patients in that data where it's prohibitively expensive to gather more data and then ran some of the same top models from Kaggle across that data and noticed both a 6% increase in overall accuracy for the data set for female detection and a 2% in overall accuracy.", "Alex (02:03):", "We've talked about some of the use cases around synthetic data. Now let's take a chance to talk about what synthetic data is. Here's a definition I like quite much actually came from the Invidia folks. Synthetic data is an annotated information that computer simulations or algorithms generate as an alternative to real world data. So synthetic data is nothing new. It's been something that's been around since the advent of simulations and algorithms to create data, given a certain representation or distribution that it's seen. What has changed in the past couple years is this massive advancement in deep learning. Today we'll talk about the techniques that are used in this industry, really focusing on one of the core algorithms that Gretel uses, actually, which essentially trains a language model or a sequential model on a customer, a very sensitive data set. It trains it while imposing a certain level of privacy parameters and, and preventing the model's ability to memorize data it shouldn't.", "Alex (02:59):", "What the model then outputs actually is a new artificial data set that has many of the same insights and distributions. In fact, in some cases, even better accuracy than the original data. I'll talk about that in a minute, but then actually isn't based on any real world person object or thing. What's new with synthetic data, here we see, it's not often I include a Gartner chart in a data science talk, but I think this is a really interesting one to talk about. What we see as a change in the way that organizations are accessing and being able to work with data in the sense that as we expect more and more from a machine learning algorithms, as we have more and more devices that are gathering data, there's a couple big things that are changing. One due to sensitivity of data it's harder to enable access.", "Alex (03:46):", "There's a risk of a model memorizing data that it shouldn't. That's challenge number one. Number two, in the IOT space, one of the things that we see is that less data is actually being sent back to the cloud. As we've advanced on technology I think a really simple is example to give is the Alexa or the, the Google assistant devices we have in our home. When they're actually able to push machine learning models out to those devices, they can make decisions much more quickly. Self-driving cars, same thing. The decision's being made like right there on the device. That means the data's not going back to the cloud. So if you're trying to train a model to become really good at a certain type of detecting a certain type of scene or object or voice utterance, at some point to recreate all of the possibilities that might exist or might not exist in the world, a simulation or a synthetic version actually becomes a much more scalable approach. And we're starting to see that both across our customers and how they're building with machine learning and also reflected in the industries you see here.", "Alex (04:48):", "So we can talk about synthetic data all day long. One of the things I think actually would make this fun and interactive was going through and, and actually using CTO code, building our own synthetic data model. Diving right in this approach for synthetic data is really very similar to open AI's, GBT models or other language models that we've seen transformers, things like that in the industry. And we're going to go through a very simple example here using a recurrent neural network, LSTM, to learn, to recognize and replay certain characters that exist in a text stream. Where open AI has built incredible models for processing natural language Gretel and other synthetic data approaches actually have applied very similar approach, but to learning either loosely structured or structured data, this could be CSV. It could be JSON. It could be something that you've pulled out of your data warehouse.", "Alex (05:42):", "Essentially it learns the structure of a CSV. As we can see here, we've got three columns, learns that as if it's its own language and then replays it. The first step of what we do this is we need to take these arbitrary character sequences, which can be characters, it can be commas it can be emojis, it can be whatever you want and map that to some sort of integer representation. So this is called tokenization in the LP space, or vectorization. Here where you can see the character three is being mapped to the character eights. We see a space as the third character, which has gotten the representation one. We see another space like later, see that reflected right here. So very simple, just mapping that we're doing here of each character to an integer representation that we can feed into a network.", "Alex (06:27):", "Step two, how does it take these individual characters and turn this into a model that can recreate language? The answer is surprisingly similar, simple, actually looking at it and going through here. All we're doing is saying, given a certain set of input text that goes into the neural network, can you predict the next character? And then networks are capable of doing this at such a scale that they can recreate entire language stories, things like that, just given input data. So here we've got a very simple function and all it does is take an input data set such as here we see so hot right now and returns the next character. So you can see it chopped off the S at the beginning, and introduced a period here at the end. What the network is learning here is that given the inputs so hot right now, most likely character distribution we're going to see it for the next character is going to be a period. If we do this enough, can we learn to learn and replay an entire language?", "Alex (07:22):", "Step three, here, we're going to define our neural network model. We'll link code to the bottom so there's no need to, to cut and paste here. We've got simple GitHub repository you can use to run through the entire example. But here we're going to use a popular machine learning framework called TensorFlow. And we're going to use Caris to make simple, to build the model. And we're going to go ahead and create a sequential model that we can use to predict the next character. We're doing some fancy things here. You see actually two LSTM layers being used here. This helps the network learn more complex embeddings and representations in the data. And we see dropout layers, which is being used to help the model generalize better. You can read up on any of these at the end of the day, we're configuring a neural network here that's going to treat our input text and learn to predict the next character.", "Alex (08:13):", "After the model's been created, we run our test data through it. So here we can see we're training for 15 epochs. Really what we're doing is for every single possible next character shifted one by one that exists in our original data set. Can we train it to predict the next character? What we keep a look on here very closely is the loss. So here we're looking at prediction loss, how good is the model predicting the loss, the next character based on our cross entropy loss. We see that going down when that stops improving. We know that the model is reached a good kind of plateau, and we can go ahead and start to use it for prediction.", "Alex (08:51):", "Now, for the fun part. Now that our model's been trained on our existing input text, we can tell the model to create as much data as we want. There's two ways that we can do this one. We can bootstrap the model. We can essentially prompt the model with some level of text. For example, if we were looking at a biased data set and when to create more female records, we could start by inputting to the model that this record is female. The age distribution is within 30 to 50 years old and have it complete the rest of the record based on what is learned on the input data. Alternatively, we can just let the model run by itself. In which case, the model will over time create a new data set that has a very similar size and distribution to the original data.", "Alex (09:36):", "Here's a fun example of the bias reduction use case that we mentioned earlier, working with the UCI data set. The original question was can we increase the representation of female records in this data set to be on parody with the male representation. What would that do for the overall accuracy of models built on this data set? Had very positive results. So that here we see the increase going up, as I said, nearly 6% across the female population here, and 2% accuracy across the overall data set. So here's an example. What did we do working with the UCI folks? We took the original data set from Kaggle. We created a simple synthetic model and told it to train and boost the representation. As you can see there very briefly to, to build an equal representation of male and female attributes. A note that you can actually go as arbitrarily complex, as you want to here, if you wanted to bucket age range and ethnicity and gender, all of those things can be done. This is just a very simple example, boosting one simple minority class in the data.", "Alex (10:46):", "After we created our synthetic data set, we took one of the top notebooks that we see on the Kaggle platform and ran against... Here we see six different classification techniques here. So anywhere from very simple techniques, such as a simple decision tree, that's being trained on the network naive phase, for example, to more complex, random forest, support vector machines, things like that. And we looked at the overall results. Here you see pretty awesome increase here and, and actually some interesting insights in the data that may tell us a little bit about how a synthetic model works. What I see here looking across is an increase in accuracy for actually every single model with the exception of one. It's worth diving in here and often these models can be very difficult to explain why naive based for example, the overall accuracy of the model did not improve after using the out of balance data set. A theory about this one that is worth diving into is that naive base assumes that there is no co-dependence or correlation between the different variables we're training.", "Alex (11:52):", "From practical experience, it does seem that the synthetic models are very good at learning deeply correlated things inside of the data. So it may have replayed certain types of correlations. For example, if you were over six feet tall, you might weigh over 200 pounds as an example. It may have replayed this which aided the other algorithms in their decision process, leading to a better result than the overall data set. Naive base, which doesn't take this into account may not have been able to take advantage of it. Earlier we mentioned the possibility of something more complex than addressing a single imbalance. Another example we've got right here, and then you can actually run through the source code as well is balancing both race and income bracket and gender to create a better version of a U.S. census data set to use for ML prediction.", "Alex (12:47):", "A final question we get often is were I as an organization, for example, to take just my super sensitive data set, create a completely artificial synthetic version of that data set. What is the loss or hit to utility or accuracy that I might see? We saw in the previous slide, that when you are able to use synthetic data to engineer your data, so to create a less biased or more fair data set to work with, we saw that there's a potential for even increasing accuracy for the different tasks that you want to apply in this machine learning for. In this case, what we're doing is a very simple substitution, where in purple you see is the original data. In blue here we see a completely synthetic version of that data set so completely artificial using none of the original records.", "Alex (13:33):", "We tick the top six or seven different data sets that saw on the co data set platform. And we applied the different tasks, so classification tasks that these were intended to be used for. Does a user have heart disease? Was this person a successful hire or not? Things like that. And then here, you can see the results from running essentially a default configuration of Gretel synthetic data on a machine learning task against the original data. So often very similar. In some cases you do see a hit. In this HR attrition prediction we see a nearly 6% drop in accuracy, but for many cases where you're concerned about here, for example, like individual employee records or things like that, being memorized by a model and becoming identifiable, becoming a security or compliance risk for your organization. It's an acceptable trade off to work with and that's one of the things that we're always working to improve. That's it for today. Thank you for your time. And don't hesitate to reach out if you have any questions. [End]", "If you have questions, join our ", " to continue the conversation. ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/podcasts/startup-field-guide", "page_title": "Podcast - How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "headers": ["How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "TL;DR", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["\u200d"]},
{"url": "https://gretel.ai/videos/conditional-text-generation-with-gretel-gpt", "page_title": "Conditional Text Generation with Gretel GPT - Video", "headers": ["Conditional Text Generation with Gretel GPT", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In this video, Alex walks through an example of how to use a large language model to create additional text that you can use to augment a machine learning dataset.", "Alex: Hey, today we're going to walk through an example of using a large language model to create additional text, which we can use to augment a machine learning data set.", "Alex: Go ahead and take a look here. First, let's start with the data set. We're loading a data set here called Banking77. Go hover and look at this at the hugging face data sets repository. We see these examples of a label which could be card arrival, or card is late, or things like that. And the corresponding text that's part of this data set.", "Alex: So let's say we don't have enough text examples for a given intent or label, like what you have here. This is where we can use models that are trained, like this model that we're using today, trained on millions of documents to create realistic, additional examples we can use to add to that data set in a much more cost effective manner than having a human generate and label these data sets themselves.", "Alex: So here we go ahead. We log into Gretel. We configure our project. Grab an API key is really all you need to do. Go back to your notebook and click run all. Goes through. It creates our project. Look at our model configuration. You can read more about these parameters on docs.Gretel.ai. But fundamentally, we're selecting a language model here to run on. We're having it run for single epoch with a relatively low learning rate. So what we're trying to do is just very, very finely tune this model on the new data set that we're working with.", "Alex: The GPT models don't really understand tabular data. So what we're going to have to do here is combine these two fields, card arrival and am I waiting on my card, into a single field. So here we've got intent, comma, and then the text that we're seeking to generate. This is what we're going to train the model on right here.", "Alex: And the next step we call the Gretel APIs. We save this fine tuning data set to a CSV file, and we tell the model, the cloud, essentially to fine tune this language model on our particular data set. What that's going to do is basically teach that language model about the structure that we're looking for. And see a train there? It took about five minutes to run through. Next, we have our newly trained and fine tuned model. What we need to do is prompt the model to generate more records of a certain class. And this is one of the harder things to get language models to do but incredibly powerful in practice when you get it working. What we're going to do is we are going to create a prompt. So essentially we are going to seed or bootstrap the model with some text that looks just like what we're looking for.", "Alex: So it can continue generating meaningful and realistic patterns that match that text. To do that, we're taking our original data frame. We're going to grab 25 records from it for the given intent, which is card arrival. And we'll use that to create a single string. We can see that below. So we create the prompt. We submit it to our model. Can see this. So essentially as the worker loads up the model, we're going to prompt that model with a bunch of examples of card arrival. And we're going to ask that model to generate more examples for us to work with. So it's completed here, it's prompted the model. The model will return another text string. So just like we compressed two columns into one, now we need to separate those two columns out. We have a feature here called get intense and what it does, this splits by the intent and separator, and then pulls out examples.", "Alex: So here we can see this newly generated synthetic text. How do I track my card? Will I be able to see my new card from my sites? I'm still waiting on my card. Can you track it? So these are examples that are being generated by our model that are labeled with a given intent. Sometimes it takes a level of tuning here. Sometimes you see some repetitive data being created by the transformer based model. That's very normal. We have parameters that you can tune to minimize that or generate more examples. Hopefully this has been helpful.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/category/data?094d394d_page=1", "page_title": "Data blog posts - Gretel.ai", "headers": ["Data", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "Why Nonprofits Should Care About Synthetic Data", "Comprehensive Data Cleaning for AI and ML", "Gretel and Google Cloud partner on synthetic data", "Generate Synthetic Databases with Gretel Relational", "Transforms and Synthetics on Relational Databases", "Generate synthetic data in 3 lines of code", "Introducing Gretel Amplify", "Conditional data generation in 4 lines of code", "Anonymize Data with S3 Object Lambda", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/podcasts/generative-ai-x-synthetic-data-ali-golshan-cofounder-and-ceo-of-gretel-ai", "page_title": "Podcast - Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "headers": ["Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "More Podcasts", "Generative AI x Synthetic data | Ali Golshan, cofounder and CEO of Gretel AI", "How Gretel found product-market fit: Ali Golshan on unlocking synthetic data for developers", "Shifting Privacy Left Podcast", "The Data Stack Show", "Alex Watson - Synthetic data could change everything", "Making Data Work", "Synthetic Data As A Service For Simplifying Privacy Engineering With Gretel", "Code Story | E6: John Myers, Gretel.ai", "Using synthetic data to power machine learning while protecting user privacy", "Software Engineering Daily - Privacy Engineering with Alex Watson", "Data Accessibility & Privacy Engineering with Danielle Beringer of Gretel.ai", "Al and Alex Watson discuss Gretel, security, and privacy issues around synthetic data", "Diving Deep into Synthetic Data with Alex Watson of Gretel.ai", "Cooking up synthetic data with Gretel", "Protecting Data Privacy Within Databases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Ali Golshan is the cofounder and CEO of Gretel AI, a synthetic data platform for ML developers. They have raised $65M in funding so far from investors such as Greylock and Anthos. He was previously the cofounder of StackRox, which was acquired by Red Hat for about $100M. Prior to that, he was the cofounder of Cyphort, which was acquired by Juniper Networks. ", "In this episode, we cover a range of topics including: ", "Ali's favorite books: "]},
{"url": "https://gretel.ai/videos/deep-dive-on-synthetic-time-series-data-with-gretel-ai", "page_title": "Deep Dive on Synthetic Time-Series Data with Gretel.ai - Video", "headers": ["Deep Dive on Synthetic Time-Series Data with Gretel.ai", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The Gretel.ai team takes a deep dive into using #GANs to generate synthetic time-series data. ", "Mason: Good afternoon, or evening, or morning from wherever you may be joining us, and welcome to another Gretel live event. My name is Mason Egger, and I am one of the developer advocates here at Gretel. Joining me today is Alex Watson. For those of you that have ever watched me stream, I can't point in reverse. So he's over here on my side-", "Alex: You're right.", "Mason: Our glorious CPO. And then we have Kendrick Boyd at the bottom, one of our principal machine learning scientists. So, it's great to have you two here today.", "Mason: Just a quick rundown of events. Today, we're going to be talking about DoppelGANger, our new time-series synthetic data model. We'll have a couple of other people join us later, and we're going to have a live Q&A at the end. So if you have any questions during the presentation, drop them in the chat, and we will try to answer as many of, if not all of them, at the end of the presentation.", "Mason: Thank you for being here. So great to have you here with our community and we're dedicated to learning in public. So hopefully you're here to learn with us and have a great time.", "Mason: So I'm going to go ahead and I'm going to leave and turn it over. Alex and Kendrick are going to take you through the presentation. I'll see you all at the end, and Alex and Kendrick, take it away.", "Alex: Thanks, Mason. I'm Alex Watson. I'm one of the co-founders here at Gretel, and I'm going to start with a quick overview of synthetic data and some of the use cases and pain points that we're addressing for customers. And then Kendrick will take us through an implementation of the DoppelGANger GAN, time-series data, and answer any technical questions that you might have around working with time-series data and synthetic data.", "Alex: To start with the pain point that we see from our customers, the users of our platform, that they're using synthetic data to address. And it really centers around this bottleneck with data. There is an awesome survey that data science platform Kaggle puts out every year, and in recent surveys, one of the things that really stood out to us, at least, was that for most developers, whether you're a developer, you're a data scientist, that initial bootstrapping of data, just getting access to it or creating the data if it doesn't exist, can take up to 35% of your total project time, really making it one of the biggest bottlenecks that people face today when they're building new products.", "Alex: Synthetic data is kind of an old idea. It's been around for a while, but has been made possible by advances in machine learning for us to create realistic data sets that are created by computer algorithms or models that are really a simulation of real-world data. And the benefits you have here can be increased privacy; it can be the ability to create data that doesn't exist by using models that are trained on large amounts of public data; or even addressing AI fairness and bias and things like that inside data sets, so really exciting use case.", "Alex: And then down at the bottom here, we talk about three of the biggest pain points we see customers going after. When they're using synthetic data, what are you using it for? It can start with, at the top here, we're talking about meeting compliance concerns: whether you're GDPR or you're CCPA, the need to have anonymized data that doesn't point to a real human or one of your customers is really important. It's also traditionally very difficult to do. Synthetic data gives you a unique way to create a whole new data set that if you look at it, looks just the same. You can carry it, you can ask it questions, but it's not based on a real human's data. So this makes things like data sharing and access much easier.", "Alex: Second, and this is a really neat one, we've even done a couple talks on it, is addressing bias and data sets. So often you have data, but just not enough of the right data. This could be a medical data set. We had a really neat example, working with a heart disease data set that was largely biased, unfortunately, for male patients. And the users that were building with their data set were asking if it was possible to balance out that data, and what effect did that have for downstream machine learning use cases? So really exciting results we had there.", "Alex: And finally, not having any data at all, which is I think a problem that we're all familiar with, and the idea of using pre-trained models that have been trained on large amounts of public data to compose a new data set that meets your requirements is a really neat and pretty recent advancement with synthetic data. Next slide.", "Alex: We started with this already. So talking a little bit about what synthetic data is, and here's some pretty nice definitions here. It's artificially annotated information generated by computer algorithms or simulations, used often as an alternative to real world data. So when we say something annotated, what does that mean? It means often that data is labeled. So you might have a text, utterance is pointing to a label for it, saying, \"This is the sentiment associated with it,\" or things like that, which means that it's somewhat-structured data that you can reason about, because it gives you a little bit of context about the data. I'm going to let you take it from here, Kendrick. Go ahead. I think you're on mute, Kendrick.", "Kendrick: That is correct. I had muted myself. I can't get through a zoom meeting or a presentation these days without a muting issue. So within that framework of synthetic models that Alex was just talking about, Gretel has a number of different models that we use or are in development for different, specific tasks.", "Kendrick: A quick overview of those here: we have the Gretel LSTM, which is our main workhorse model that generally works pretty well on lots and lots of different data types. We have a recently released model, specifically dedicated for NLP model for text, that's the Gretel GPT. And then two upcoming models, one optimized for tabular data, CTGAN, that's upcoming. And then the one we're going to be talking about today is this DGAN or DoppelGANger model, and this is a dedicated time-series model.", "Kendrick: So first off, a little bit of background, why a dedicated time-series model, if you followed Gretel's blog posts over the last year or so, we've had some existing blog posts talking about how to use the Gretel LSTM for time-series. But time-series data is everywhere these days in the world, from sensor data to medical records and everything in between. And with that really varied ranges of data, we want to be able to have different models for different data sets.", "Kendrick: We want to provide options because different models excel in different areas; and overall, time-series data set synthesis is quite challenging, because you have to worry about both correlations between variables as you would in a standard tabular setup. If you want tech stocks, if you're thinking about synthesizing price data from the stock market, text stocks should generally move together, or other sectors should generally move together each day. But you also have correlations across time that you have to worry about that your model needs to be able to produce accurately. So that could be weekly and yearly trends that might happen due to patterns of work and such and electricity usage, or many other data sets.", "Kendrick: And so that's this little bit of background about why we want to have this dedicated time-series model. And the model that we've chosen to work with here is this DoppelGANger model. There's a brief snapshot from the original paper that was published in 2019 that the paper titled, \"Using GANs for Sharing Network Time-series Data Challenges, Initial Promise, and Open Questions.\" It's a really great paper. We loved reading about it and chose this paper for a couple different reasons. One, it's quite expressive, it's a very flexible model, can generalize to lots of different types of data, different data sets. And also, it's very good and effective at periodic data, data that has weekly or daily or hourly patterns to it. And that's a place where our current model, Gretel LSTM, can have some more difficulty. And so this is a great model to help fill in the gaps within our suite of models.", "Kendrick: As part of this work, though, one of the things is that we've implemented a new PyTorch and implementation of this model, and it's now open sourced as part of our Gretel Synthetics library. And so we can talk a little bit later, you'll have to ask some questions about why we did that and all that, but we wanted to have something in a little bit more modern setup than the TF1 code that was published with the paper. But the TF1 code was really helpful as a reference implementation that we can compare against.", "Kendrick: So before we get into too many more details about how to use the model, we want to talk a little bit about, for the purposes of this talk and this particular model here, what do we mean by a time-series? What's the data that we're going to be working with? The synthetic data that the model is going to be producing? What does that look like?", "Kendrick: So first of all, we might start off with just one variable. Say we have some smart home data: you might have a temperature sensor that's in your kitchen collecting a temperature over time. So here we have three days of data there. That's the first basic step. You have one sensor that you have data from.", "Kendrick: And then you might have multiple sensors. You are collecting temperature and humidity, now, from both your kitchen and your bathroom. So we have a multivariate time-series here. But here, this is still just one home, one collection of sensors. But for these neural network models, the GAN is the Generative Adversarial Network, we need to have lots of different examples in order to effectively train this model.", "Kendrick: And so then, you might imagine that you can obtain multiple samples by either looking at different houses, putting lots of sensors in different houses across a particular country or the world, or you could take a look at different weeks, different time ranges in order to produce different examples like this.", "Alex: Kendrick, could you touch maybe on some of the difficulties around multi-variate time-series, like what we're looking at here versus a single variable?", "Kendrick: Sure. Let's see... Okay, I do have my mouse here as a pointer. Looking across these three temperatures, there's a couple different things going on here. One, there are patterns that happen each day, that there's generally it's warmer inside the house during the daytime. I don't know if the exact hours line up here because of times zones and such. This is data sampled from a house, actually, in Belgium. So there's daily, weekly patterns and things going on here. But then also, the individual variables need to be correlated, that if there's a spike in... I think this is the humidity here in the bathroom, so maybe someone took a bath or a shower at this time, you also see a spike in the temperature, as well, the way that this system happens to work.", "Kendrick: And if you see the full data set here that's available with this, there are sensors in a number of other rooms, and so you can also see that rooms that are nearby in the house, physically, those temperatures tend to be correlated more, those humidities tend to be correlated more. The bathroom might open into the living room, and so those temperatures should be correlated more so than the bathroom versus the garage, which might be further away. And so all of those dynamics are things that you're wanting to capture within your synthetic model, and be able to reproduce accurately with new synthetic data.", "Kendrick: Then another aspect and another thing that we really liked about the DoppelGANger model is that it also supports, essentially, metadata on each of these time-series examples, so that you can annotate and say, \"Oh, this particular data set here is from maybe the season, it's from the winter, and the particular country it's from, it's from Belgium.\" And then you might have other examples that are from the spring or from different countries, for example.", "Kendrick: And so this allows you to represent in your data and for the model to learn that, \"Oh, if it's winter in some country near the equator, for example, maybe temperatures aren't that much different from the summer. But if it's winter in Canada, then that should have much colder outdoor temperatures, and in general, even your indoor temperatures would probably be a little bit lower.\" And so that's a way for the model to capture those correlations dependencies and learn about them.", "Kendrick: All right. So with that... Oh, then finally, bringing that all together as a summary here. The types of data that DoppelGANger is going to be able to work with, be able to train on, that we need to have multiple examples. Now, certainly there are situations where you don't really have multiple examples naturally: you only have sensors in one house, or maybe you're looking at the history of something. There's one history of the full prices on the New York Stock Exchange, for example. So in those situations you can break up a long sequence into many smaller subsequences in order to have examples for the model to train with.", "Kendrick: And then the other components of this input data are these fixed variables, attributes that don't train over time. And so for the training setup, this looks like a two-dimensional array of data like a standard tabular data set, but then you also have associated, linked with that, these time variables or features and that these are the ones that change over time. That training input you can think of as a three-dimensional array where the first dimension is the different examples that you have, the second dimension is time, and the third dimension is your multivariate, the different variables that you've measured. And so throughout this talk and throughout documentation in the paper, we refer to these, that the fixed variables are attributes and the time variables are features. Those two arrays constitute the main input to the DoppelGANger model.", "Kendrick: So now we've talked a little bit about background of what types of data, what does the data look like for this model? We're going to jump into talking a bit about our PyTorch implementation. We've implemented this new version of this in PyTorch. It's available at the link there, and it you can accept Pandas Dataframes, or non-Py arrays directly as the input. And we've also done some extensive testing of this that's been just been recently... Today, actually... Published in a KDNuggets article, showing how this new implementation produces similar results from the original TF1 code. We've replicated several figures from the original paper in that article.", "Kendrick: And another really nice aspect of this PyTorch version is that it's quite a bit faster for training. These training times here are on the data set from the original paper. It's a larger dataset, but the TF1 code took about 13 hours to train on an Nvidia T4, and with the PyTorch implementation, and tweaking the parameters to increase the batch size a bit, we can bring that training time down to about 20 minutes or so, so it's about a 40x speed up. That's really nice and able to improve how quickly you can iterate and try out different parameters, try out different setups for your data.", "Kendrick: All right. And so with that, I'm going to jump into a little bit of a case study, work through what this might look like to set up this DoppelGANger model for a simple little data set here. All of this, the code for this is all available on a jupyter notebook that you can find at the grtl.ai/dgan-code link, if you would like to follow along here. I'll talk through this at a high level, and then I'm happy to answer any detailed questions that you'd like to add to the comments at the end.", "Kendrick: So again, this is the same sensor data that I was talking about before. This is a data set that's up on the UCI repository, and it contains about four and a half months of observations from the one house in Belgium. These are sampled at 10-minute intervals, so there's really quite a bit of data here. And you can see at the top right there, there's a full view of the data. There's, I don't know, 30 or 40 columns here. There's lots of different measurements that were made both in the house and some stuff around the area.", "Kendrick: And so for the purpose of this example, we're going to use four temperature columns: temperature from three different rooms within the house, and then another temperature that's measured from the nearby weather station, the outside temperature. And so on the bottom right, we can see those four temperatures graphed over the four and a half months. And that constitutes our main data.", "Alex: Kendrick, when we go through this process and synthesize this data set, are we effectively creating another synthetic house and all the sensor readings for it? Or how do you think about that?", "Kendrick: Yeah, that's a good question, and comes into this next step. Here we just have basically one example. In order to train the model, though, we're going to need to have multiple examples. And so the ways that we can do this, we could get sensor data from many other homes. That's going to be expensive, time consuming and so on; or we can split this four and a half months into smaller chunks. And so the choice that I've made here for this particular setup is to use one day of observations. That's 144 observations, 10 minutes each.", "Kendrick: And so what the synthetic model is learning and what the synthetic model will produce at the end is one day of observations of these four temperature readings from this house. You could think of that either as generalizing to, \"Oh, what might the temperature readings look like next year?\" Or you could also think about it generalized to, \"What might the temperature readings look like at a similar house that's nearby in the area?\" And certainly if you collected other data from other homes or from other countries or things, you could think about it generally, being able to generalize more broadly.", "Alex: This gets to the bootstrapping use case we were talking about earlier, where, let's say you're building an app to make a smart home type device or sensor, but you're limited by the initial data you have, you could use something like this to create additional examples for your algorithms?", "Kendrick: Yes, that's correct.", "Kendrick: Yeah. In this particular example here, then we have the attributes array that I was talking about before. There is no attributes array that we're using here, although you could certainly think about maybe adding something like that, maybe based on the time of year based on this data. And then for the features, we have this three-dimensional array, and there's 137 examples because we have data for 137 days. There's 144 time points for each example, and there's the four variables for the four temperature sensors that we are working with.", "Kendrick: So with that data pre-processing finished out here, then we can go ahead and set up and train the model. There's the little snippet of the code that you can use to create the model and train the model on the right here. All the config details and such are documented at our documentation link, synthetics.docs.gretel.ai, and also within the Gretel synthetics repo code, as well.", "Kendrick: In this particular case, because this is a pretty small data set, 137 examples and about 150 time points, it trains very quickly in 64 seconds. The other training set that I was talking about before had about 50,000 examples and 550 time points. In general, this DoppelGANger model... GAN models, I think, in general work best with more examples, so this is probably on the smaller end of where the model would really excel, but it's still a good example, and we can see what sort of data we get out of that.", "Kendrick: All right, so now that we've trained the model, we can generate as many one-day snippets of temperatures as we would like to. That's the beauty of the GAN model, that you can pass in some noise and it gives back data that should look like the original data. So as a quick comparison here, the two plots on the right are two different examples that we have synthesized from our model. We can compare what that looks like compared to one of the original days that we're looking at here. There's some good things going on here, and there's some not-so-great things going on here. Some good things that are happening, the model's recognizing that the indoor temperatures are usually in 20 to 25 degrees Celsius, where people generally like their houses to be. And the outside temperature can vary; it's often quite a bit lower than that. And we also see that there are some changes in the outside temperature over the day. Not as many changes in the inside temperature, but still maybe sometimes seeing some, like this one room got a little bit colder during this day.", "Kendrick: On the other hand, there's some other challenging things about this, as well, that the data doesn't look very smooth, and this is a little bit surprising because the input data was quite smooth. This is an aspect of something you could look into exploring, different ways of setting up the model, some of those different parameters that I mentioned that we have documented at synthetics.docs.gretel.ai, and possibly some different training with an appropriate learning rate might help with this. It's hard to trade that off when you have such a small data set to work with.", "Kendrick: This is some visual inspection. We can also look at very high level, a couple of more numeric-based evaluation approaches we could take. And we can talk more in detail about this, but at a high level, the left side here is s looking at autocorrelation. If you look at one time point, how similar is one time point to 10 minutes before versus 20 minutes before, 30 minutes before and so on; and how does that correlation change over time?", "Kendrick: In general, you want the real and synthetic lines here to be similar. For one of the temperatures, I think the temperature in the living room here, it did a pretty good job here with maybe tailing off with the long time ranges, not having quite as great a correlation, but then for another temp sensor that we had, the model didn't do particularly well. And so again, that's something that you could try out some different parameter settings to help improve this, or finding some more sensor data so that the model has more data to train from.", "Kendrick: And then on the right hand side, this is a simple thing of looking at how well these values change together. Does T1, T2, and T3, the temperatures from inside the house... Generally are those close together? Do those correlate more? Whereas the temperature outside is further away. Here we have a correlation matrix for these four variables, and so in general, you want these values to be similar between what you had from the real data and what you had from the synthetic data. And these numbers are reasonably close, although there's certainly some room that perhaps a better model could do.", "Kendrick: That's a high-level look at how you might apply this DGAN model to some sensor data. And with that, coming to the end of the presentation here, what are some of the next steps here for time-series at Gretel? First off, we have some further improvements for the open-source model that we have that are in progress here. We'd like to add variable-length time series support to that. This was described in the original paper and it's part of a TF1 code, but getting an initial version out, we had skipped that support. We can talk in more in detail about how exactly that happens, but we'd like to add that to it. And then there's also some other PRs that are in progress that should show up in the GitHub repo probably in the next week or two to allow more robust, more flexible support for different input types, input and data frames that you can pass in.", "Kendrick: And then finally, we're working towards getting DoppelGANger available as something that you can use through the Gretel SDK and through the console that you can use the Gretel APIs with all the various features around that and running these models in the Gretel cloud. Look for an announcement for that probably in the next month or so.", "Kendrick: But right now you can use the open source code yourself, and you can start out from these two example notebooks: one, the home sensor data that I walked through today, or the Wikipedia page view data that was from the original paper. And hopefully we can get some links to those in the chat if you'd like to go check those out.", "Kendrick: And so with that, I think we're ready to switch over to the question section. I'm happy to talk about questions about this particular data set, about the model, about the PyTorch implementation, or just generically about Gretel. And I think we have a couple other people who'll be joining us for the Q and A session, so there may be a few moments as we're getting situated here.", "Mason: Yes, we are back. Well, that was a great presentation. Thank you for that, Kendrick. Now we're moving into the Q and A session. We have some predefined questions that I'm going to let Lipika ask and discuss with Kendrick and Alex, but also if you have any questions that you want, please drop them in the comments, either on YouTube or LinkedIn. We'll definitely try to get to all your questions today. So I think what we're going to do is I'm going to bring up the question, and then Lipika, I'll let you take it from there. I'll bring it up on the little shashasha thing.", "Lipika: Awesome. Thanks, Mason. Hey everyone, I'm Lipika, I'm a machine learning scientist here, yet another one, and we'll jump into questions. When should you use DoppelGANger versus Gretel Synthetics? I imagine by Gretel Synthetics, we mean the LSTM model, our workhorse model. I can provide a one-liner, which is when you have data that's collected many different series, so if you had data from houses all across Belgium, Egypt, Canada, the US, and you had a lot of data collected from all these houses and multiple series, DoppelGANger would be a very good set because that's a lot of underlying distributions to learn, that DoppelGANger does well in the time-series realm. Whereas if you simply had, I don't know, information on stock prices for one or two stocks, using something like Gretel Synthetics... The LSDM in Gretel Synthetics is a good bet because there is some upward trend, downward trend, long term, those types of fluctuations that Gretel Synthetics can handle really well. And so the underlying model is an LSTM, which is a recorder [inaudible 00:29:52], so that handles that type of thing really well. Any other examples you all can think of, Kendrick, Alex?", "Alex: Maybe for a couple examples that I've seen as we've been building out the time series work here, and then financial markets that Kendrick alluded to earlier, like the open, high, low, close data, we have so much financial information, but there's only one New York Stock Exchange. So one of the things that we see often with financial customers is they're building models to predict market changes like the 2008 housing crash or the GameStop thing that happened earlier this year, or even like the Bitcoin stuff that's happening right now.", "Alex: This is a case where you can train synthetic models to create additional examples that will help your machine learning to generalize a little bit better. That's a pretty exciting one. We also see it quite often with medical devices where it's so expensive to test with patients and things like that, but you want to build a machine learning algorithm once again to detect some sort of medical condition or something like that. So this is another really useful area where you can use DoppelGANger like this to consume the input of the sensors and create another data set.", "Kendrick: Yeah. I guess to add one other example that maybe might be a better fit for Gretel Synthetics, the Gretel LSTM right now is anything that you have more free text or higher cardinality discrete features that the DoppelGANger is very focused and is very effective for numeric data or discrete categorical data that has a relatively few number of different values, like maybe 20 or 50 values. But of something where you want to have a bit more flexibility as a free-form text field that might be associated with medical records or something, it's possible that the Gretel Synthetics model might do better for that.", "Alex: Last thing... Go ahead, Lipika.", "Lipika: Ah, go on. I was going to say that the one thing they both do really well is handle a data set pretty much as you provide it. So you don't need to encode features, you don't need to do sort of all that plumbing to get things to work with a model that's very low cardinality, categorical, and numeric-feature first. DoppelGANger, you can just pass it in and all the plumbing is in there, so you don't have to worry about it, just pass in a Pandas Dataframe and call it a day and see what comes out. Not to say that you might not run into errors, but just to say that we've handled a lot of things.", "Alex: That's what I was going to suggest, is try both. Why not?See which one works better for your use case.", "Lipika: All right. I think we're ready to move to the next question. Are there particular types of time series datasets that DGAN does exceptionally well on? I will open the floor as I think about this.", "Alex: From what I've seen, maybe talking about this, Kendrick alluded to the multivariate nature of time series. What that means is you have multiple inputs or sensors that worked, or stock prices that are all correlated to each other. And in our testing, that's where they approach the DoppelGANger. It worked really well, is learning to recreate that correlation that when your outside temperature heats up, that your air conditioning would flip on, things like that across these different sensors is one thing that it does super well.", "Lipika: Yeah. I've also found that the shorter the series, the better DGAN does, because implicitly, if you're trying to learn a longer time series, let's say an entire year's worth of data versus just a week's worth of data or a month's worth of data and the patterns of that, anyone would be able to do that. Even you and I would be able to do that much better for shorter time periods. And so that's a natural occurrence in such models.", "Kendrick: Another place where I think it really excels is when you have sensor readings or other readings that all the values fall within a particular range. So ECGs or temperatures, that sort of thing, that the way that the DoppelGANger model is structured, it handles that very well, whereas the Greta LSTM or other models might be better for things like prices or population or something that might just sort of continue growing without bound, depending on what the inflation is doing lately.", "Lipika: Oh, and sorry. One of the things that we totally forgot to mention was obviously if you have data that has attributes or things that don't change with time, DoppelGANger is definitely the best bet with that. Especially when you have these other features like multivariate series or longer series, more complications, things like that. Certainly if you have attributes and many of them, DoppelGANger does quite well, and actually does incredibly well, went to it correctly, in maintaining the distribution of the attributes between the original and synthetic data set. That's a really neat feature.", "Lipika: All right. I think we can move to the next question. \"Does Gretel have a library of produced synthetic data that people can look at?\" That's a great question. We have a lot of examples on our Blueprints GitHub repo, and we publish a lot of blogs where we use our models, so different types of data sets like Alex had mentioned. The UCI heart disease data set... Kendrick of course used the Wikipedia web traffic data set in his latest blog. And we keep publishing new data sets on Blueprints and in UpLogs. That's what I can think of. Alex?", "Alex: Yeah. Most of our examples, I think as you highlighted, require you to generate the data set yourself, but you could go to docs.gretel.ai and click on the synthetic data notebooks, and it will walk through creating... We've probably got 25 different examples for creating different kinds of data.", "Lipika: Awesome. Thanks for that question. \"How are times series of varying length handled?\" Kendrick, you talked about this a bit earlier. Would you like to discuss this in detail?", "Kendrick: Sure. Yeah. Time series of varying lengths, there's still going to be a maximum length that the model can produce. And then effectively, we add an additional feature, an additional Boolean feature over time that is one until the end of the time of that particular time sequence, and then it turns to zero and is zero the rest of the time. And we just feed that into the model and let the model, model that as just an additional feature. That works really surprisingly well.", "Kendrick: The original paper had a couple of different data sets with these varying lengths and the DoppelGANger model did better than any of the other models that they compared with to replicate the actual distributions of time, of the sequences that comes out of that. And so that's something that we are looking to implement and add to our open-source version here in PyTorch here in the next month or so, I think.", "Lipika: That's awesome. Yeah. That's basically an additional time series. You're extending your multivariate series by one more and having it modeled on that, as well.", "Kendrick: Right. You have an extra sensor, an extra measurement that is this time series; is this particular series still active, if you will?", "Lipika: And that raises another point, which is that if you only had multivariate series that are Boolean, for example, you could model that with DoppelGANger, as well.", "Kendrick: Right. The model does support discrete variables that are part of the time series.", "Lipika: Yeah. For example, \"Did you test positive or negative for COVID?\" And we collected that information from everybody here across the many months since March 2020. All right. We can answer the next question now, I think. Why did you choose PyTorch versus TensorFlow 2 to implement DoppelGANger?", "Alex: Insert loaded question.", "Kendrick: I guess I can take this. I think there's lots and lots of great tools and a great, huge ecosystem around PyTorch in the same way that there is around TensorFlow. And I think we were really interested in learning more about the PyTorch ecosystem and really building up Gretel's knowledge and experience so that we are familiar with both pie torch and TensorFlow. The other models right now that are in our Gretel synthetics library are in TensorFlow, but now we have both PyTorch and TensorFlow, we have experience with both of those. And I think we can continue going forward that for whatever particular data set or our task that comes up, we can be able to utilize the best models, the best tools, from either TensorFlow 2 or PyTorch and not be forced into choosing only from one. And it was also just fun to jump in and learn a new way of thinking about neural networks and implementing them.", "Lipika: And a new way of dealing with Cuda, as well, on the GPU.", "Kendrick: Yes. And trying to make sure that everything, all your Python stuff works properly with Cuda.", "Lipika: Yeah. Okay. I think we can move to the next question, if we have any. Ah. \"What does the input to DGAN look like?\" I think Kendrick had shown a little snippet of a data frame that was passed in.", "Kendrick: Let's see here. So there's this... Is this the snippet you were thinking about?", "Lipika: Oh, no. I was thinking about the one with just the raw data set.", "Kendrick: Oh, back here.", "Lipika: Yeah. In that data set, you can see time going down and measurements going across, so the different variables going across. The input to DGAN, Kendrick, doesn't like 2D and 3D. There's a lot of things that somewhat need to happen, but one of them is that if you don't have any attributes and you're just looking at features and you're interested in, for example, just T1 or temperature in whichever room, you can basically... And if you had multiple of these series for different homes, you could basically flip this for each of those homes and that would comprise your input to DoppelGANger.", "Lipika: Now, Kendrick did do some sorcery here to get just this one series to look like multiple time series so we had more examples, and you can see that in the notebook that's linked here in the public research repo. I think everyone should have a link to that, as well.", "Kendrick: Yeah. And I think for common use cases like that, some of that data processing, we are looking at how to generalize that and include that directly in the interface, if you're doing something that's a typical transformation like that. But we're still iterating a little bit on exactly what that should look like, and if you have thoughts about what would be most useful, most natural for you as an interface, we'd love to hear about that either on GitHub as an issue, or you can join our Slack community and chat there.", "Lipika: Cool. Yeah. And one more thing to mention is that if you look at Kendrick's recently-published blog on the Wikipedia web traffic data set, the input to the model for that data set is exactly as it's provided on Gaggle, or maybe with a few minor tweaks, but it's basically a data set that already has time going across the top or something like that. Or maybe I misspoke, but if you have time going across the top and you have multiple examples going down, that's perfect. You can just pass that in as a data frame and see what we do with that.", "Lipika: Cool. Okay. I think we can move. \"How long does training typically take?\" There was the comparison that Kendrick had of 0.3 hours when we changed the configuration a little bit so it runs faster. I think there we increased the batch size, is that right?", "Kendrick: Yes, that's correct.", "Lipika: Okay. And so if we didn't increase the batch size and we used... What is it? This batch size was a thousand, the previous one was... No, this batch size would've been just the size of the data set.", "Kendrick: For the home sensor data, yeah. The batch size was the entire data set. And it's possible that maybe why maybe the model isn't doing quite as well, because we're not really doing stochastic, gradient descent. We're not getting the stochasticity of that. But that's another sort of parameter that we might tweak that could help provide better results.", "Lipika: Yeah. That makes sense, I think. I recall you said that training 2000 E-box was less than a minute on a T4 for this particular data set, but then I think the times that are listed here are for the Wikipedia data set that was of size, I want to say, 50,000, and each series had 500 time points.", "Lipika: Cool. Yeah. The best way to figure out how long training something takes is to train it, obviously, but we'd highly recommend using a GPU for this. It really reduces training time. In fact, I just took Kendrick's notebook and I moved it to Colab. I ran that in and I changed the run time to GPU. I ran that in again, maybe like a minute or two, depends on what type of GBU you get allocated when you start that up. But it was very quick for this data set, which is super small and purely for demonstration.", "Lipika: Cool. All right. Do we have another question? Ah. \"Does DoppelGANger only work for continuous time series data?\" I think we answered that during the rest of the questions, but the simple answer is, it works for discrete data as well. If you have a categorical variable with limited cardinality or small cardinality, rather, or if you have a Boolean variable, DoppelGANger works for that, as well. You're on mute, Mason.", "Mason: Welcome to world of mute. Yes. I think that's all the questions we have today. Awesome. That was a lot of fun. Really enjoyed that. For those of you that are here and made it all the way to the end or anyone who's watching this video after the fact, and you want to get some swag, you can go to the link on the screen and I'll also drop a link in the comments. It's grtl.ai. It's a short little cute Bitly link, /dgan/live. And we will send you a Gretel swag pack with a bunch of the stickers that we're currently having made. If you made it all the way this far, get some stickers. They're going to be fun stickers. You're going to like these stickers. Been spending a lot of time working on them. They are going to be... I'm not going to say they're going to be the best stickers ever, but they're going to be close. I like stickers. My laptop looks like I'm a three year old with stickers. So... Awesome. Well, does anyone have any final remarks or anything you want to say before we wrap it up?", "Alex: Nope. It was a fun discussion. Love the questions. And maybe as two quick followups, one, we're going to have more examples, and some rooted in popular use cases we're seeing today, like financial data, coming soon. And second thing is if you have any questions, join our community Slack. We'll drop a link below and you can ask us questions there.", "Mason: Fantastic. Well, thank you so much, Alex, Kendrick and Lipika for giving us a great presentation today on synthetic time series data with DoppelGANger. Keep your eyes open for more live events coming. If you're not already subscribed to our YouTube channel, go ahead and subscribe. We're going to start doing these technical presentations, deep dives, tech talks, AMAs at a much more frequent basis. So if you're interested in learning more from us, feel free to come back, and we hope that we'll see you next time.", "Kendrick: Thank you.", "Mason: Bye everyone.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/developer-workshop-synthetic-text-generation-with-gretel-gpt", "page_title": "Developer Workshop: Synthetic Text Generation with Gretel GPT - Video", "headers": ["Developer Workshop: Synthetic Text Generation with Gretel GPT", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Learn how to use Synthetic Data to augment AI/ML datasets to train chatbots and sentiment classifiers by generating your own text examples from scratch.", "Mason:", "And hello, everyone. Welcome to another Gretel workshop. My name is Mason Egger, and I'm the lead developer advocate here. And joining me today is Andrew Carr. How's it going, Andrew?", "Andrew:", "It's going awesome. Yeah, I'm a senior scientist here at Gretel and we're pretty excited to talk to you about what we have cooked up with text data. Yeah, it's going to be good.", "Mason:", "Yeah. I'm super excited for it. So today, for those of you just tuning in, we are going to be covering synthetic text data generation using one of our most recently released models, Gretel GPT. So if you're here and you're in chat, go ahead and post in chat. Let us know where you're coming from. We always like saying hi to everyone. But if we ... Yay, LinkedIn is being cranky.", "Mason:", "Oh, well, we'll deal with that later. Anyway, watch it on YouTube and, yeah, I don't know why I said that, but we'll go from there. So let us know if you're here in the chat. If not, we'll go ahead and get started on our presentation day, talking about text data. So text data and synthetic text data.", "Mason:", "So Andrew, why don't you go ahead and tell us why ... Well, no, obviously, text data is data that is text, but why is it important? Why do developers and data scientists need it?", "Andrew:", "Yeah, that's a great question. So something like 90% of all business data is tabular in some sense. So you'll see that in Excel document or you'll see that in some CSVs, but a lot of the data in those tabular datasets is text. So you have messages from tech support or you have Jira tickets or customer descriptions. There's all kinds of stuff, product descriptions. And so text is everywhere and it's how we interact with the world.", "Andrew:", "And so to be able to properly synthesize text data is just paramount to getting business value out of your already existing tabular data and maybe your other kinds of data.", "Mason:", "Fantastic. Okay. So yeah, tabular data, text data, that makes a lot of sense. But what ... So I know a lot of people talk about how difficult it is sometimes to work with or just to get valuable text data. But it seems like the entire world around us is surrounded by text data. I feel like it's the number one type of data that humans consume.", "Andrew:", "Yeah.", "Mason:", "So what makes text data so hard?", "Andrew:", "Yeah. So you could imagine just arbitrary text, right? Just random words that you put onto a page. And that would probably be pretty easy to work with because it's random and they're just words. But language has a lot of structure and it has a lot of irregularity. So there's grammar and there's parts of speech and there's just all of this stuff that comes from the linguistics side of things. Just hard. I mean, I don't know, me as a human, I've been speaking most of my life and I still mess up all the time.", "Andrew:", "And so to expect a machine to figure that out, it's tough to do. But then not only is there that structure and there's that grammar, we have a lot of interpretive ability with our language. I mean, we could say whatever we want and most of the time the meaning still comes across, which makes things challenging for machines that are expecting the same thing every single time. So a great example, if I were to write a blog about how cool I think machine learning is, I could write this thing and it'd be great, but then you ask a hundred other scientists to write blogs about how cool they think machine learning is, every blog will be very different, but they will all convey the same idea. And so how do you tackle that variation all mapping to the same idea of machine learning being cool?", "Mason:", "Awesome. Yeah. It's like that weird ... You said about we've been speaking our whole lives and the text data can be difficult. It's like that weird thing that I get whenever I type one word too many times and suddenly I question that that word is actually spelled like that. So I think I got stuck up on the word school the other day.", "Andrew:", "Yeah.", "Mason:", "I had just sat there staring at it. I was like, \"There's no way this is how you actually spell the word school.\" It doesn't make any sense for that to be [inaudible 00:04:26].", "Andrew:", "This happens to me all the time with the word oven. For whatever forever reason, oven just trips me up sometimes. I'm like, \"There's no way OV, what is even happening?\"", "Mason:", "Yeah. That is so weird. And if it trips us up, I can only imagine how computers must feel if they could feel.", "Andrew:", "Yeah.", "Mason:", "Or do they feel? That's a question for the ages. Cool. So what are some of the pain points that we're trying to solve with synthetic text data? I guess the obvious answer to me would be, we're just trying to help you create more text data that's more accessible to you, but is there something deeper to that?", "Andrew:", "Yeah. Good question. So text data is everywhere, but it's hard to generate at times. So you may need to have an army of highly trained writers typing continuously to get a bunch of high-quality text. And that's expensive and prohibitive and, I don't know, it's not very creative for the writers. There's lots of obstacles to that.", "Andrew:", "And so if you could just generate a bunch of text data, even if it's not the same quality as human-generated, it's close, that would just be awesome. Huge value add. So you can automate customer support, you can explore different aspects of sentiment. A really great example is you have reviews on your website for products and you want to make a review a little bit more positive, a little bit more negative, just to see how your machine learning models would respond. You can generate synthetic positive reviews. You can generate synthetic negative reviews that still have the same grounded idea as the original text. And that's just a superpower in some sense for your business.", "Mason:", "Yeah. It almost sounds like it's the thousand monkeys with a thousand typewriters kind of thing.", "Andrew:", "Yeah. So for those who are unaware, there's this infinite monkey theorem that essentially says if you have infinite monkeys typing on infinite keyboards, one of them will randomly produce Shakespeare just because infinite is really big. Luckily, in this case, we have finite computers, just a few GPUs and they're pretty good at reproducing Shakespeare-type stuff.", "Mason:", "Awesome. Okay. So I guess the last question about just text data in general before we move on is what industries use text data? And I think I know the answer to this, but what industries would you say would be some of the bigger consumers of text data?", "Andrew:", "Yeah. So the obvious answer that I would probably skip over is everyone uses text, right? It's ubiquitous, but I think there are a few where it's really valuable, and we've touched on some of them. So e-commerce with customer reviews and product descriptions. Then we have finance. So there's lots of banking statements or documents describing some company's performance, legal documents, and then technical field. So you have customer support, you have bug tickets, there's all sorts of things in that tech realm as well.", "Mason:", "Awesome. Yeah, lots of stuff. Imagine all these chatbots that I'm forced to interact with on a daily basis seem to probably use text data as well.", "Andrew:", "Yeah, that's right.", "Mason:", "Great. Okay. So we've talked about text data. So now, let's talk about the new model that we have here at Gretel called Gretel GPT. And GPTs, I think most people maybe have somewhat have heard about it. The acronym GPT is relatively popular in the programmer zeitgeist right now because I think it wound up on hacker [inaudible 00:07:51].", "Andrew:", "Well, I think I may have lost you [inaudible 00:08:04].", "Mason:", "Okay. Can you hear me now?", "Andrew:", "I can hear you now. Yup, we're good.", "Mason:", "Okay. So ... Yeah, I noticed that. Okay. So what is GPT and what has Gretel done with GPT?", "Andrew:", "Awesome, great question. So GPT stands for Generative Pre-Training and it's a type of models that was built on the transformer architecture, and it's pretty new. So transformers are recent as of 2017 and GPT just a few years later. And it's a pretty simple idea that's really hard to get right. So we take all of the internet. You could at least take all of the internet and push it through GPT. What does that mean? You say, \"GPT, here's the sentence.\" I go to the, and you say, \"What is the next letter?\" And so ... Or the next word, maybe.", "Andrew:", "And so GPT will guess and it will say, \"Cat. I go to the cat.\" That doesn't make any sense. We say, \"No, no, I go to the store,\" or \"I go to the movies,\" right? That's more likely. And so the data exists to show GPT. And so then it will try it again. And we'll say, \"I love my ball,\" or something, right? And whatever data exists on the internet. And eventually, GPT will get really, really good at predicting the next word in a sentence. That's all we're doing. So we're pre-training GPT by having it generate stuff.", "Andrew:", "Well, now, that's awesome because we don't have to label any data. We can just gather text data or we can get customer data. So say you have chat records from customer support or you have reviews on your website, you can just put that data into the training process and GPT will learn how to predict the next word. And eventually, it can just generate new stuff. So you could ask it, generate a review with five stars, and it will do a five-star review based on the product category or whatever you want. And so that's the overall idea.", "Andrew:", "And what we have done is we have taken a GPT model trained on a large portion of the internet, cleaned internet data. And we have wrapped it up in Gretel's console cloud, SDK, and given you access. So now, you can either use it as is or you can train it on your own data and get pretty good performance for whatever your text use case is.", "Mason:", "Okay, cool. So the way that you described it to me sounds like whenever I play with my phone and auto-correct.", "Andrew:", "Yeah.", "Mason:", "Yeah, auto-correct. So just ... I know this is completely off topic, but do phones or do any of those predictive text models on any technology device we're using, are those utilizing a form of GPT or is it another model?", "Andrew:", "So you could easily replicate that functionality with GPT. I don't know if your Android phone or your Apple keyboard is using GPT under the hood. They easily could and they probably are. But, yeah, I mean, you can think of it as just the world's most fancy auto complete in some ways.", "Mason:", "Okay. Yeah. That sounds really good.", "Andrew:", "Sometimes in Gmail or in Google Docs, you get suggestions or whatever, that is the exact same thing that's happening with GPT. We just let it keep running. And so it'll write for you in some ways.", "Mason:", "Oh, that makes a lot of sense now. That makes a lot of sense with some of the data because sometimes it gets a little bit wonky towards the end, it's because we just left it running and it didn't know where to go from there. Okay. It's always fun whenever you answer my questions.", "Andrew:", "There we go. So there's a few drawbacks. The ... Because it's trained on the internet, there's some concerns that maybe you will ... The internet's not a great place sometimes. Some corners of the internet, you want to stay away from. And so what if you trained on the wrong data? Or what if you leak private data that existed on the internet? And so that's something that we and the community at large take pretty seriously and are working to avoid, but it comes with the territory.", "Andrew:", "There's a funny xkcd comic about the Google document auto-complete, where they put in as the prompt, meet at the warehouse at, and then let it complete. And the model says 3:00 p.m. behind the docks or whatever. So you're finding out about the secret plan just from auto-complete. And ... Yeah, risk, but hopefully mitigated and we're starting to work towards that.", "Mason:", "Oh, okay. So it's ... Because I know that Randall Munroe is usually pretty awesome about letting people use his stuff for educational. So it's this one.", "Andrew:", "This is one, yep.", "Mason:", "The ... Okay. Long live the revolution, our next week will be at. Okay. And then it auto-completes at aha, found them. That is hilarious.", "Andrew:", "So this is exactly what happened. And Randall Munroe is describing here, essentially, GPT architecture type prediction, where in the light gray, you see what's completed and in the dark black, you see the original data. In reality, this happens much less often. And if you train on customer data, it happens very rarely. And we have privacy filters in protections in place in Gretel, which is, I think, one of the benefits. You can ... There's GPT models out there that you can use, but we have privacy filters in place that at least give me peace of mind when I use models like this.", "Mason:", "Awesome. Yes. I ... There is an xkcd for almost everything and it's one of my favorite things to use to teach. I think that ... I've also bought his books if you've never bought his books on, what if you tried to do the high jump possible? And it's like, well, if you have the proper updraft and large enough synthetic wing span, you could totally catch the updraft and jump 60 feet in the air. And that is not what anybody was talking about, but I'm glad that we did the math here to prove that that was possible.", "Andrew:", "Amazing.", "Mason:", "So it's absolutely great. Cool. So how is ... So GPT is predictive text model. So how is it ... What do you say are probably its most common use cases? Where do you think people are getting the most use out of GPT right now?", "Andrew:", "Yeah. I mean, the possibilities are pretty amazing with GPT. So not with Gretel GPT specifically, but with GPT models across the industry, people use them often for assisted copywriting. So you were trying to write a blog and you maybe get stuck. You don't know what to write next. People use GPT as an idea generation tool. You can also use it to correct your own grammar. So it's in the internet, hopefully, it's learned what good and bad grammar is, and you maybe write something wrong and you can use it to correct.", "Andrew:", "People use GPT for summarization. So you give a large piece of text and a summary of it. And it learns almost to translate from the large text to the summary. If you've seen the code generation work, people use it to write code. I mean, you can do all kinds of stuff with it.", "Andrew:", "As far as what people use it for at Gretel, we have people in the financial services using it for customer service type calls. Today, we're going to be doing some fun chatbot stuff and we'll be doing some lyric generation stuff. So writing music, writing songs, and you can do all kinds of stuff with it.", "Mason:", "Fantastic. So let's go ahead and get into that. I think we've been jibber-jabbering for long enough. Now, let's actually pull out the code and have some fun.", "Andrew:", "All right.", "Mason:", "So we have two different notebooks that we're going to cover with you today. We're going to show you. And this is the ... I think one of the great benefits of this is that these are going to be completely different use cases. We're going to do some lyric generation, and then we're also going to do some chatbot text, that word, synthesization.", "Andrew:", "Got it.", "Mason:", "There should not be that many syllables. I disagree. Someone get me in touch with the person who created English. But I think the ... One of the things that you're really going to notice here in this code sample is that the code is probably 99% identical. Whenever you want to swap out what text you want to generate, the core setup for training the GPT model is almost the same.", "Mason:", "Now, there will be some fine-tuning and stuff that Andrew's going to help us with that will probably vary on a case-by-case basis. But the core setup for all of this is going to be almost identical. So you're going to see a lot of the same code, but it's going to do drastically different things, which is pretty awesome.", "Mason:", "So we're going to start off with our lyric generator. And we basically took a data set that we found that has a bunch of pop song lyrics from a handful of artists. I think it's like 6,000 rows in this data set. And we're going to throw it into GPT and we're going to see what happens whenever we ask it to generate song lyrics for us. Now, I've been playing with this for the last day. And let me tell you, Lady Gaga really messes up the GPT model because of all her vocalized sounds. So it's funny. It makes me laugh so much. I never thought that Lady Gaga would be hurting my model training. And that's not a sentence I ever thought I would say in my life, but here we are. So we're going to move forward with that.", "Mason:", "So the first thing we're going to do is we're going to go ahead and in our notebook, we're going to install the Gretel client. While we're waiting on that to go forward, I'm going to show you how to use the Gretel cloud to get an API key. And you'll see that these are really straightforward. So I'm just going to go ahead and log in with Google. And I'm actually going to delete my old projects because you can see I've been playing with this a lot. So let's click on this. We're going to delete these. So that way, we start from the ground up. I don't like having ... And I think chatbot text generator was the other one that I had.", "Mason:", "So these are my models and I'm just going to delete them, clear them out. But whenever you sign up for Gretel, you'll see something here. You won't have any base projects, but you'll have dashboard and you won't see projects here, but it'll be about the same. And then you'll have this API key. So you just come over here and copy your API key. And this is how you would access and use all of ... Well, basically, all of Gretel's model stuff, how you use the API key, whether or not you're using it in the CLI or the SDK or the rest of the API. That's obviously a little bit different, but it's the same key regardless.", "Mason:", "So what we're going to do is we have a data set stored in S3, which is clean lyrics, which Andrew went through. And as you can tell, pop songs sometimes, like he said about the internet earlier, not the nicest place. Pop lyrics, also not always the nicest things to say about people. So we've cleaned up the lyrics for it. And then we're going to do a couple of imports. We're going to import JSON, Pandas. Everybody loves Pandas. And then from our library, our configure session, which basically allows us to authenticate. Poll, which allows us to keep track of how our model training is going. Create or get unique project. I love long method names, basically. Create the project if it's there or create the project or get it if it's already there and then get project. So we're just going to go ahead and run that relatively straightforward.", "Mason:", "And then now, we come down here to our login to Gretel. As you can see, my previous results are here. We basically configure the session. We could supply the API key as an environment variable, but we actually can say prompt, so it will prompt you for it, which I really like. It's actually one of my more favorite features about this because if I'm using it in a notebook ... I've never tried to use environment variables in notebooks. And I don't know how. So the prompt is nice. Are you going to cash the credentials? What endpoint? Validate, clear.", "Mason:", "And then we're going to create a new project called lyric generator. So we do that. And then we paste in our key and it says we've logged in. So relatively straightforward. Now, we have a log-in authenticated notebook within Gretel, and it's indicated session, I should say.", "Mason:", "Okay. So this is where I'm going to turn it back over to Andrew. And Andrew's going to tell us a little bit about the model configuration here.", "Andrew:", "Yeah. So here at Gretel, we use this idea of configs. And so you can have a lot of flexibility with how you set up your models. Additionally, we choose same defaults. And so if you don't want to mess with it, you don't have to. And I think our defaults are actually pretty good. And we have defaults for different use cases.", "Andrew:", "In this case, we're going to be training the GPT model. So you can see here, we just have a dictionary. We can also do this via YAML and pull it in, whatever you like. We set up the type of model, it's called GPTX. And the pre-trained model we're using is GPT-Neo 125 million parameters. And so this is, excuse me, pre-trained by EleutherAI on the pile, which is a huge data set. And you can see the model card about this if you're interested in some of the technical details, drawbacks, what have you. And then you get to choose now what machine learning parameters, hyperparameters you want to set up.", "Andrew:", "So in this case, we're going to do batch size of one and epochs of one. So the GPU that we have on our cloud supports batch size one, batch size two, it depends on how big your data is. You can run this locally. If you have a super beefy GPU, and we're in the process of getting beefy or GPUs as well, and then there's all kinds of stuff you can set up, these are pretty good defaults, [inaudible 00:21:58] warmups, learning rate, scheduler, and the learning rate.", "Andrew:", "Mason, I'm curious, do you want me to talk through some of these because we can get in the weeds if we want to?", "Mason:", "So I actually ... Top level on them, I actually have already talked about doing another one of these workshops where we just talk about all of these because I feel like I understand very little about it and I'm loving learning about it. So brush over them and then we'll come back in a couple of months and we'll do a deeper dive into all of these.", "Andrew:", "Okay, perfect. We'll start from the bottom and up. So learning rate is a parameter that determines how much of the current data you show GPT it absorbs. And so we train these things via gradient descent and we use Adam in this case, and it just says essentially for each step, how much do you pull in. The learning rate scheduler, you can change the learning rate over time. So in this case, you can think of a cosine wave just wiggling. The learning rate oscillates up and down, and that's to keep the model from overfitting too much, keep it from memorizing the data.", "Andrew:", "We do a hundred warmup steps because this model's pre-trained. If we start right away doing gradient steps, then things just get confused. And then weight decay, again, because this model's pre-trained and we're not training on a lot of data, we don't want it to overfit. And so we need to clamp the weights just a little bit. So these are all things that are a function of the model being pre-trained already.", "Mason:", "Awesome. So in this part of the code, though, what we do is we basically set up a dictionary with all these in here. As Andrew said, you can use JSON or you can use YAML. I feel like that's the new Vim or Emacs and JSON is the correct answer.", "Mason:", "As someone who loves Python, for some reason, I loathe space deliminated config files. I always mess them up. I can't write YAML. It's awful. It's a bad day. So we'll basically create this JSON file here. And then we have this function down here, which allows us to load and preview the training set. So one of the things you'll see is, we basically create the data set and what we do is we can create a combined column of artist, title, and then the clean lyrics. And then we truncate it by 256.", "Mason:", "So I'm going to let Andrew explain this because I think he'll explain it better. So Andrew, why do we need to do this concatenation and then truncation?", "Andrew:", "Yeah, that's right. So the GPT model operates purely on text. And so we need to treat our data as if it were a sentence. And so the separator is actually just a comma. And so we've said that at the top. And so to train GPT on a CSV, that's a lot of acronyms, to train this text model on tabular data, we need to first make it look like a sentence. And then we truncate just to train faster and space on our GPU. So 256 characters is pretty good truncation. You'll get to see the first verse and then the chorus and maybe the bridge. And that's about all we need to verify that our model is working.", "Mason:", "Yes. And then we have this dropna, which I had to learn about Googling because, apparently, there was some random non-data in here and the model does not like non-data.", "Andrew:", "Yeah.", "Mason:", "So I had to figure out, get rid of any field that had non-data in it, which is fine. It wasn't that many. And with how big the data set is, missing a couple of them, wasn't going to be a problem.", "Andrew:", "Exactly.", "Mason:", "So then we create the data set and then it basically will create a local copy of this data set called finetune.csv. It just creates a ... We can call it whatever we want to call it. We call it kittens if we want to, but it creates just a CSV for us here locally. That way, we can submit this to the Gretel model.", "Mason:", "So all this code did right here is just go through and get a song, take the lyrics, and then create this ... We created this combined ... Whoop [inaudible 00:25:59], that's ... I'm still learning how to scroll in notebook. Sometimes I scroll in the data set. Sometimes I scroll up here and I lose my place.", "Mason:", "Well, as you see, we've created this combined column and that's what's going to get submitted to the model. So there we go again, scrolling in the data. It's 6,000 records, that'll take a while to get through.", "Mason:", "So the next thing we're going to do is we're actually going to train the model. So we've ... These first steps, what we've done so far is we've logged in and we've basically made the data look the way it needs to for the model so we can use it. And now, we're going to train it. And this is actually probably my favorite part about Gretel and about everything in general. Whenever I hear someone say, \"Train a model,\" I think this Herculean task, we're going up Everest. And as you can see, it's like four lines of code.", "Mason:", "So all we have to do is create a model object with our configuration. We're going to tell it what the data source is, tell it what the name is, and then we're just going to run, submit it to the cloud. And it's going to do the rest for us. This is the advantage you get of GPT with Gretel, is that, yes, there are other GPT models in the ecosystem. You get the ease of use of our APIs to use it, along with all of the other models. So if you're trying to do ... While we're waiting on this, because these things take some time, I would love for Andrew to talk about generating data with multiple models.", "Andrew:", "Yeah. So there's no one size fits all solution for generative modeling or for synthetic data generation, which means you need to have the model fit for your use case. So here, we have GPT that's really good at text. We have a couple of other Gretel models. Some of them are good for numerical data. Some of them are good for sparse data, small data, big data, whatever. And so our documents were always a work in progress, but they're pretty good at outlining which models are good for which tasks.", "Andrew:", "I mean, honestly, it's about as simple as swapping out the name in your config file and you can just train a new model, which I think is pretty cool.", "Mason:", "That is really cool. And I think right now, we currently have two models that are available to the general public, which is our GPT model that we're talking about today. And then our LSTM. Is that long short-term memory? Is that what that's ... I can never ... Okay, good. Acronyms. I thought I had all the acronyms whenever I was in the DevOps space and now I'm in AIML and all my acronyms have changed. So there's two current models with a lot more coming, which is really exciting.", "Andrew:", "Yeah.", "Mason:", "And now, we're just waiting on this model to train. So is there anything else you want to just chat about while we wait? This usually only takes about three to five minutes.", "Andrew:", "Yeah. I mean ... Yeah, we won't be here for very long. We have ... Our engineering team has set up a pretty nice training infrastructure in the background. So these go quick.", "Andrew:", "Yeah. So there's all stuff we can talk about here. One thing that I think is interesting to talk about is that while the GPT model is really good at text, it's also really good at things that are not text, surprisingly enough. So if instead of lyrics and whatever, you had numbers in there, or you had dates in there, or you accidentally had, I don't know, something else, names, latitudes, whatever you want. The model is general purpose enough that it actually wouldn't do a bad job and you could generate whatever you want, which there's some danger there, but I've always been just impressed and somewhat shocked.", "Mason:", "Yeah.", "Andrew:", "[inaudible 00:29:30]. So there's a question in the chat.", "Mason:", "Yes. We have a question from Saeid who asks, \"What if we cannot share our raw data due to privacy?\"", "Andrew:", "Awesome question. So you can run all of this on your own infrastructure. And so we provide a Gretel worker, the Gretel client and everything, and the code you write would look the same, but the endpoint that you point at would not be our cloud. It would be whatever your local cloud is. So, yeah, you can do totally private internal training.", "Mason:", "Yes. You still will need a Gretel account and API key because it still ties into the API. It doesn't submit any of the data, but it sounds like telemetry data and usage data, but you 100% can generate synthetic data, both with GPT and with LSTM, and I think probably with every one of our models going forward on your own infrastructure.", "Mason:", "Now, I assume you'll probably need GPU for that, correct? If you're training the model locally, you're going to need GPU. So it can get expensive and difficult. Anytime you say the word G as in GPU, in a DevOps context, the words expensive and difficult come to mind. I did this for a while whenever I was in a past life, in a previous role. I helped set up machine learning infrastructure at a Fortune 500 company and it can get pricey really fast. So that's one of the benefits to using the Gretel cloud is you don't have to worry about setting up your own stuff, but 100%, we knew going in that there were going to be data privacy laws, that there are going to be people who would not be legally allowed to or just would not want to share their data into the cloud, rightfully so. I completely understand that. So we have a solution there for you.", "Andrew:", "Yep. That's a great question.", "Mason:", "Yeah, great question. Love answering the questions. So-", "Andrew:", "Yeah, it's trained.", "Mason:", "It's trained. So as you see, it only took about four minutes based on wall time. Pretty good. We only ran this through one epoch and one with a batch size of one, which we'll talk about later, because I trained this model a lot yesterday and I learned a lot about what happens. And you train with different ... Is it epochs or epochs? Is there actually a [inaudible 00:31:34]-", "Andrew:", "Depends on who you ask.", "Mason:", "Okay, good. Then I'm going to say epoch because epoch makes me think of like JIRA and I don't want to think about JIRA, right? Okay. So we've trained the model and now all we have to do whenever we want to actually just create more synthetic data for ourselves, this create record handler object. So basically, it just is going to create some records for us.", "Mason:", "We're going to tell that we want 20 records and we're going to give it a maximum length of 128. We set this up here to ... This is just Pandas overhead, setting how Pandas will display the data. Then we basically just take those record handle that we've created. We submit it to the cloud and we wait and then we just read the CSV of what we get back and we'll see it come up.", "Mason:", "So let's go ahead and submit this. I'm going to log in to LinkedIn real quick because I don't think we're getting ... If there are any questions in LinkedIn, they're not coming in via StreamYard. So if you have questions on any of the platforms, please feel free to ask. I don't see any on this one. But, yeah, so that was a great question from Saeid and now we wait.", "Mason:", "This also usually doesn't take too long. As you can see, it has allocated a worker in the backend for us and now it's going to create some synthetic lyrics. And ... Okay. So now, this is going to be like synthetic lyrics with, I want to say, no guidance, but we just said, \"Give us what you got.\" We didn't say, \"Oh, I want something that has\" ... We didn't start with the auto text predict, like I want to go to the. We didn't do that.", "Mason:", "So would you say, Andrew, that these are probably going to be more of the Wild West responses since we didn't give them any prompts?", "Andrew:", "They certainly could be. Now, you could prompt it with something totally crazy and the model wouldn't know what to do. But in this case, it's just going to go with the flow in some sense. It's going to start from a blank slate and it's going to generate what it thinks lyrics should look like. And so sometimes they rhyme. Sometimes there's recognizable bits and it's pretty cool. I'm excited to see what we produce here.", "Mason:", "Yes. Every time we ran this yesterday, we got some really interesting stuff. Some of them were ... Okay, I don't know if they would win any Grammys. It's also hard to evaluate lyrics without music for me. But ... And then there were somes where it literally just sounded like Lady Gaga was making noises into the microphone. And those made me laugh.", "Andrew:", "Yeah, [inaudible 00:34:03]. You mentioned at the beginning that her vocalization is pretty good. We saw a lot of just a, a, a, a, a, but it's funny that it was like three a's and then one a and then two a's and two a's. So there's a beat to it. So I bet if you would to, as you say, get music to it, it probably would sound pretty good.", "Mason:", "Yeah. Yeah. That's fascinating. This stuff. I'm still in the phase where this just feels like magic to me. So I just enjoy reading it. Okay. So ... Okay. Are we-", "Andrew:", "I think we skipped one. So we're up.", "Mason:", "Did I do ... Yeah, I always do that. Okay. Again, I'm still learning how to scroll on ... Okay, here we go. We've got some ... Oh, no, we've got some weird code stuff. So I told myself I'd never lose the day of my life and the world would tell you the truth and all my life would be wrong for me. That's actually not bad. I could ... That sounds very much what angsty Mason listened to in eighth grade.", "Mason:", "But as you can see, some of them, when you leave things to their wild devices, they go a little bit off. But, yeah, I'm pretty happy with this. This one decided it wanted to swear. I'm glad we cleaned that up.", "Andrew:", "Yeah. So something that's really nice about this is you can generate as many as you want and you can do quality controls on your side. So you can maybe have something that detects rhymes based on rhyme dictionary and you can generate just thousands and thousands of these and choose the best ones that you want. And we say it didn't take that long. And so, yeah, it's a really nice workflow.", "Mason:", "And I think you had mentioned it earlier or maybe I just read it somewhere, but usually when you're doing GPT generative stuff, there still needs to be a human in the middle to filter out stuff. Is that still the case? You need to verify that the text data that you've generated is actually valuable, and that's pretty much true of all GPT models at this point, right?", "Andrew:", "That's right.", "Mason:", "Okay. Okay, cool. So Saeid has another question for us. How different are Gretel models to the OpenAI GPT-2 or GPT-3 ones?", "Andrew:", "Yeah. Again, great question. So the GPT-2 model is 1.5 billion parameters trained on a proprietary data set that OpenAI holds. And GPT-3 is 175 billion parameters, again, trained on a proprietary data set.", "Andrew:", "The model that we have is 175 million, so it's a 10th of the size and it was trained on an open data set. So you can expect the data that it was trained on. The combination of those two factors simply means it's much faster to fine-tune on your own data, much faster to generate, and you have a better idea of what's going to come out of it, but there's no difference architecturally aside from size.", "Mason:", "Fantastic. Awesome. So now, let's go play with the prompts. So ... And we're going to do something different today because I feel like this is weird. I feel like I don't know why corn kept coming to my brain. So I would say let's do a Lady Gaga song here, but I'm really afraid of getting just all the vocalizations.", "Mason:", "So I think the next thing we're going to do is we're going to actually prompt the model. So Andrew, can you explain just what prompting the model is really quick?", "Andrew:", "Yeah, we've done some examples of it so far, but to get a little bit technical and exact with it, if we want the model to have some context, just have something that it's already thinking about, and thinking is maybe the wrong word to use here, but we need to prompt the model. And so you prompt it in the same way you structured the data. So we have artist, song name, and then the clean lyrics from before.", "Andrew:", "And so in this case, we say, \"Well, we want a Taylor swift song and we want the song name to be something that we choose.\" And the first few words will be something else, right? Yeah. So we want Taylor Swift to sing a song about happy ducks. And you can say nothing or you can add a few lyrics.", "Mason:", "Okay, let's try this. Let's go with ... We're going to say Bilbo was a happy duck ...", "Andrew:", "Yep.", "Mason:", "... who, and I want to see if it's going to verb properly here. This is what happens when you make me do things off the top of my head. You get weird stuff like this. So now, let's go ahead and just run this and see what we get.", "Andrew:", "So you can see in the config now, there's a few extra options. So we already talked about the number of records and the maximum text length, but then there's top P, top K, number beams, what have you. We try to expose as much control as we can to the user of these models. And so if you have some specific hyperparameters that you want to try out, you can do that through the Gretel config as well.", "Andrew:", "So in this case, top P, we've chosen a number that we've found to be really, really good in general. But if you think top P of 0.9 is better, you're welcome to use that in your generation.", "Mason:", "Sounds good. I have no idea what top P and top K are.", "Andrew:", "Yeah. It's just a way to steer the model a little better to hopefully keep it from going off the rails. But sometimes it works, sometimes it doesn't.", "Mason:", "But I definitely would say that one of the benefits of this is, is like ... So I'm able to generate synthetic data that you can see is actually pretty valuable. I'm not a machine learning or a data scientist. This ... One of the really big benefits of the Gretel platform itself is it is allowing more people access to synthetic data that probably wouldn't have understood it before. That's why we have Andrew here because I don't know enough to explain all this deep-level stuff to you. I'm just an average old DevOps developer. But as you can see, I can create synthetic data and I can see use cases where I would've used this if I was still in an engineering world.", "Mason:", "So it's one of my favorite parts about Gretel is whether you're an expert or a novice, you can still get tremendous amounts of value out of it. It's like the ... You get a high level of value and then it only goes up from there the more that you know.", "Mason:", "Okay. So Bilbo was a happy duck who got a chance to shine in the sunlight, so happy. They can be happy. Okay. This does sound like Taylor Swift. Whenever I can ... I don't know, it's still because [inaudible 00:40:14]-", "Andrew:", "So I'm a huge Taylor Swift fan. And so [inaudible 00:40:15].", "Mason:", "You tell me, you go right ahead. Go right ahead.", "Andrew:", "Yeah, I would jam to this. This is so good. Bilbo was a happy duck who could talk about love like a duck who could kiss a happy duck [inaudible 00:40:26] world.", "Andrew:", "So I think there's something important here that I want to point out. So we're doing this fun example, it's a little bit silly, but the idea here is you can get the model on topic and it can generate things on topic. So these are song lyrics. They're repetitive, of course, but they are about a duck, they're by Taylor Swift, and they're happy, right? And I think we're so used to magic on the internet that we skim over just how groundbreaking this is in the past few years. This was impossible just five years ago. And now, you can just add the touch of a button, get it to work.", "Andrew:", "And so there's tons of other use cases. I highlighted one at the beginning, but say instead of artist name, song name, whatever, you have number of stars in a review, sentiment of that review, and then the review text. So you can change the sentiment how you want. You can generate a three-star review that's positive. You can generate a three-star review that's negative. You can do anything like that.", "Andrew:", "Really, it's up to your imagination. And it's just cool that it works. I mean, as you say, it's not going to win any awards, but it's coherent and it's relevant.", "Mason:", "I would like it known that if Taylor Swift wins a Grammy for any of these songs, everyone keep an eye out.", "Andrew:", "Came here first.", "Mason:", "We at least get the credit. And maybe half, maybe we cut the Grammy in half, put it somewhere. But if she comes in and steals these lyrics, then we know that ... That would actually be really interesting. I would be really curious to see. I know there has to be some techno grunge musician out there that's generating all their lyrics via GPT or something. But that would be really interesting to do. But, yeah.", "Mason:", "So we have this. Now, there is one question that I wanted to ask because this is something that I experienced yesterday and we briefly touched on earlier is that epochs are ... I think you had mentioned earlier, but I'm going to reiterate, epochs are the number of times that the model sees the data.", "Mason:", "So in my brain, my naive brain was like, \"Oh, that would make the model better.\" But yesterday, as I added more and more epochs, the quality of the text actually devolved to a point where ... I went on one, five, and 10. And by the time I got to 10 epochs, literally, it was spitting out source code legit if bullying statements and stuff. So why would it's seeing the model more? Why would that cause the text quality to maybe decrease in some instances?", "Andrew:", "Yeah. I'm glad you asked that. So the first answer is a lot of the machine learning world is still somewhat of an art instead of a science. And we're working really hard to develop first principles approaches to understanding why models do what they do. But we have a lot of intuition, a lot of answers there.", "Andrew:", "In this case, you mentioned there's about 6,000 rows. In the grand scheme of things, that's not very many. And so we're training this new model that was pre-trained already in the internet on a small amount of data. And the learning rate is also pretty high. And so essentially, what's happening is we are forcing the model to forget a lot of what it's already learned. And if you do that over and over and over again, there's not enough data to be reabsorbed into the model. And so it will just degenerate.", "Andrew:", "Now, in this case, just doing one pass over the data, it still gets to remember a lot of what it knows about the world that it got from its pre-training. And then we just add a teeny bit of extra lyrics, Taylor Swift, blah, blah, blah, so that it can use all of that together to generate this stuff.", "Andrew:", "Now, if we just push its brain full of lyrics, it will get confused and there's not enough lyrics. And so it degenerates in that way.", "Mason:", "Oh, that's absolutely fascinating. This is still magic to me. And it's a pleasure every day to get to play with this, but I never would've thought of that. It's so cool.", "Andrew:", "Yeah. I think it's surprising how some of these models behave at times.", "Mason:", "So basically, just play with the parameters, see what happens. If it goes the wrong direction, then go back the way you came. Maybe one pass is better than 10 passes as we could see here. We don't have time to retrain because the more epochs you add, the longer it takes to train.", "Mason:", "So ... Okay, so we have this question here from Laura just came in. Is there a minimum and/or maximum data size you recommend using and still being effective?", "Andrew:", "Yeah, really great question. So one of the surprising findings of the original GPT paper was that you maybe don't have to actually train the model at all. You can just prompt it. And so they found that as few as 32 examples was sufficient to prompt the model in a certain direction. So we got down to 25 and it still was working a little bit, but, yes, you can prompt a model and use very few examples.", "Andrew:", "Now, if you're actually getting to train the model instead of just prompting, yeah, we recommend something in the high hundreds, low thousands as a very, very minimum. And then you want to do low, really low learning rate, really low number of efforts. As far as a maximum, in some sense, as long as you're willing to wait. But ... Yeah. So adding more data just increases the training time. And so if you're doing it locally, really, as long as you want, there are some time limits on the cloud that are in the documentation. And I don't have a good number there for you, I'm sorry, but you could do quite a few.", "Mason:", "Yeah. We actually saw that yesterday when we first started playing with this model. I tried ... And so you saw how we truncated. We didn't really talk about this, but we truncated at 256. I originally sent all of the song data to it and it started allocating gigs upon gigs upon gigs of data, which was just too much for what we were trying to do. And we were getting errors because we had sent too much data. So you can choke it with too much data for certain.", "Mason:", "I would imagine the more data you add, the more GPU and more hardware resources you're going to need, which makes it an interesting problem to solve, for sure.", "Andrew:", "Yeah. But the original model was trained on a ton of data. And so, really, the answer is however long you're willing to wait.", "Mason:", "Yes. How long did ... Out of random curiosity, do you know how long it took for us to train Gretel GPT?", "Andrew:", "Yeah. So Gretel GPT was trained EleutherAI, which is an open-source collective of researchers who train these models. And ... Yeah. So this particular model was trained for 300 billion tokens, which you can think of as words, which typically takes something on the order of weeks to train.", "Mason:", "Okay.", "Andrew:", "So it was trained for quite some time on quite a lot of data.", "Mason:", "That's really cool. Okay. So now, we're going to move on. We're done making Taylor Swift say silly things. But what we're going to do really quickly, I ... And we're going to go ahead and go over the chatbot example that we have. And as you're going to see, this code looks almost identical. I mean, this line, obviously, changed because we changed what CSV we're getting.", "Mason:", "So we're going to go ahead and do the exact same thing, but we're going to do it with some slightly more relevant or maybe more useful chatbot data. And it's not even really chatbot data as you'll see. This data is a full conversation. So it's basically a call and response back and forth. Maybe think like Facebook messenger or something.", "Mason:", "So we have that. We've done the pip install. As you can see, none of this code here ... We're actually going to do side-by-side comparisons. I love doing side by side.", "Mason:", "So if you look at this code from this code, it basically, excuse me, doesn't change at all. I need to get my Gretel key again. I probably still had it in my clipboard, but never just paste random stuff into a thing. Always verify you have what you need. We're going to log in to Gretel. Again, we haven't changed the config either for this. So it's the exact same config. So we're going to go ahead and create that. The data set is the same.", "Mason:", "Now, the part of the code that we had to change here is we did have to change this line of code. For one, we didn't have the same columns, so you're going to have to change this to create your own columns. So instead of it being artist and song, it's call and response. And then we ... Because these are relatively short, we didn't have the need to truncate this time. And because there was no random null data that was making me cry, I decided that we didn't need to do it here either. But as you can see from the rest of it, it's the exact same code.", "Andrew:", "Yeah. And we have blueprints on our website on our GitHub for these sorts of things.", "Mason:", "Yes. And I will definitely link and go over those here in a second. Well, actually, we'll probably do that right now. We're going to train the model and we're going to go ahead and do that right now. So if we go to, Let me pull it up over here, github.com/gretelai/gretelblueprints, we have a whole plethora of notebooks that are, basically, almost production ready that you can use to replace with your data set. And you can do local ... So Saeid had asked earlier about what to do if you can't use your data to do the row privacy. Well, we have these local ones which allow you to do it locally on your machine.", "Mason:", "We have notebooks on doing walkthroughs, using differential privacy, retain values with conditional data, and a whole bunch of other really interesting ones. Is there anyone in here that you want me to click on or explore [inaudible 00:50:15]?", "Andrew:", "We don't have to click on any, but we had a community question earlier.", "Mason:", "Oh, yes. Which one ... From our chats.", "Andrew:", "Yeah. So the community question about using three years of data to generate 10 years of data.", "Mason:", "Okay. Yeah. Let me go ahead and get that here really quick, so people can ... There we go. So we have this question, how can we use three years of data to create the previous 10 years of data? For example, I have data from 2019 to 2022, and I want to be able to create synthetic data from 2010 to 2022.", "Andrew:", "Yeah. So we have something in our model or in our suite of tools called seeding. And it's essentially just conditional generation or, as we've seen today, prompting. And so what you do is you train what's called a seed model on your data, where the seed field is your dates. And then you prompt the model with new dates from 2010 to 2022.", "Andrew:", "And so the model can learn all of the distributions and patterns and everything from your three years of data and then generate back to 2010. And it will look realistic and it will have a lot of the same qualities that you care about. And so it's as simple ... And so you use the blueprint on conditional synthetic generation by maintaining values or by seeding, and it works great. So, yeah, this was a great question and, honestly, a really great use case for Gretel.", "Mason:", "Yes, that's awesome. I love it. And then let's go ahead and check back and see. It looks like we're still training a little bit, so we'll continue to go through. So we have these notebooks. So gretel.ai/gretelblueprints, you can also find them from our docs, docs.gretel. ... I would think I could type, but that's a skill that's beyond me.", "Mason:", "So for anything you want to do, so for synthetics and stuff, we have these SDK notebooks in here, which basically link to the same thing. So these are another way of finding them. You can find all of our documentation notebooks on both synthetics, transforms, and classify, which we haven't really talked about transform and classify. These are privacy, basically, helper ... You can think of them as helper functions or helper products. Transform allows you to transform PII into a different format. So that way, you're not leaking any data. And then classify helps you detect it, I think, using natural language processing.", "Mason:", "So these are really cool. They're really cool actually. I love transform. Whenever I was working in DevOps, that's the one I would've loved the most is just transforming customer data so I don't have to deal with it. And ... Yeah.", "Andrew:", "So there's a natural question here. When would you use transforms over synthetics?", "Mason:", "Yes, it's a great question.", "Andrew:", "Because it seems like they do the same thing.", "Mason:", "Yes.", "Andrew:", "Transforms is just so much faster because you're using your data already and just transforming it directly into new data. The drawback is, of course, you can't generate more of the same, but it's guaranteed in some sense to be protected.", "Mason:", "Fantastic. And then ... But synthetics is you want more of the same or when you use them together, you get an extra layer of privacy, right? Transforms is great by itself. Synthetics is great by itself. Together, they're even better by themselves. That's a ... It didn't make sense, but it worked in my brain. So that's all that matters. So we want to use those. And then something like classify, I would've loved to have used classify on incoming log streams and stuff. It would've been really cool.", "Andrew:", "That's great. Yep.", "Mason:", "So ... And then another repository, where you'll be able to find this lyrics generator, which I haven't uploaded yet. We have fun with synthetic data repository. We have another GPT model in here, which you can use what we call Gretel Bartender, which allows us to create synthetic cocktail recipes. It's pretty fun. It was a fun little one. But I will also be uploading this lyric generator to this repository as well. So if you want to get that after the fact, so the Gretel blueprints repository, I will drop this in chat for anyone who wants it.", "Mason:", "So it's there in the YouTube chat. And then see if there's ... We'll drop it here in the LinkedIn chat as well.", "Mason:", "Okay. Let's go back and check on our model. Looks like we're done. Okay. So we've created our model. And now, all we're going to do is we're just going to generate five data sets, just to make this go a little bit faster, because we're almost coming up on time. But we're going to go ahead and generate five rows of this data and then we could also prompt it. We'll also try to prompt it here in a second, too.", "Mason:", "If you have any questions, please feel free to post them in the chat. We're happy to answer them.", "Andrew:", "Yeah. I just want to call back to something you said at the very beginning. You said we're going to go through two examples. The code's essentially the same.", "Mason:", "Yeah.", "Andrew:", "And I just think I have seen these before, but I'm like, \"Oh, wow, the code really is-", "Mason:", "Yeah, no, I 100% copy pasted. And I was like, \"This would be ... If I got five of these assignments in school, I'd be like hahahahaha.\"", "Andrew:", "Yeah, exactly.", "Mason:", "Change CSV file, change couple of variables done.", "Andrew:", "That's it.", "Mason:", "They really are exactly the same. Okay, I want to go ... Okay. So really, I want to go to the beach. That sounds like a great idea. I like it. I don't want to go to the beach like that. I love it. It sounds so bad. Okay.", "Mason:", "So as we see, as it gets a little bit longer, gets a little bit more loopy, but this is legitimately valid. I mean, are you going to win orator of the year? No. But is it valid text data? 100%. So now, let's come down here and let's prompt it with, how are you today? I'm doing fine. Thank you. And then let's see what it spits out after that.", "Andrew:", "Yeah. So one use case we've been seeing is people want to generate text data for their logging infrastructure. And so logs are text, right? So you have something in your application that is spitting out, this event happened at this time, blah, blah, blah. If you wanted to generate more logs that fit some criteria, this is absolutely the way to do it. And they may not be the perfect logs, but they will look like logs. And that's really what matters in this case.", "Mason:", "Very valuable, too. That's awesome. What if ... I'm just going to keep hitting with the hard questions. What if there's a really rare log event that happens and I want more of that specific log event so I can test against it? Is that something that can happen here, too?", "Andrew:", "Yeah, absolutely. So you can just prompt it to generate more of the rare log event. So say info is a really common log event you see all the time, but maybe error is less common, you can just prompt it to generate error logs. And given that there are sufficiently many error logs in your actual training data, it will easily be able to generate more varied error logs for you.", "Mason:", "Yeah. Awesome. I love that. It's so cool. I'm going to play with that next. So as we can see, we've preceded the conversation with people of like, how are you today? I'm doing fine. Thank you. And we just get more basic ... Well, this poor one got laid off. I feel sorry for this synthetic data worker. But everything else is pretty spot on, pretty straightforward.", "Andrew:", "Yeah, we're not going to lay you off, GPT. Don't worry.", "Mason:", "No, we're not. No, you're good. Okay. So we have one more question from the community and then we have some swag giveaway for those of you that have stayed or those of you that are watching this after the fact. So the last question that we have in the community, and as you saw, we tried to sneak these in, but this one didn't really fit in, but we're going to answer it anyway we got. Can data be loaded onto the platform to constantly update our input into the synthetic data sets?", "Andrew:", "Yeah. So I think this community member is talking about something like active learning or online learning. So let's say, for example, that people are still writing new songs and how do you incorporate those songs into your data set? And so the easiest way is to append them to your CSV and train your model. And we saw it only took four minutes. And so this is something that you can run a cron job every hour to update the model. And it takes almost no time.", "Andrew:", "And honestly, that's what I would do if I were trying to build a system like that. Just append to your CSV, retrain, and you're good to go.", "Mason:", "Fantastic. Okay. So let's zoom this in. Thank you for all your help today, Andrew. And we're almost out of time. So everyone, thank you for attending. We hope ... If there's any more questions, we've got about two minutes left, you can drop them in the chat. But if you don't have any questions, you can go to this QR code, you can scan it, or go to the link that you'll see here in the banner box below, grtl.ai/gpt-workshop. And we'll send you some stickers.", "Mason:", "You may have to wait a teensy bit because we're still getting them printed. But if you've enjoyed it today and you want some Gretel stickers, we have a lot of really fun designs that we've come up with that people, I think, are really going to enjoy. I've taken them to one conference already and people loved them. So go ahead and you can either scan this QR code or go to grtl.ai/gpt-workshop. You have a week.", "Mason:", "So before July 20th at 12:00 Eastern Time. So that way, for anyone who's watching this, if you didn't get a chance to watch it live, but you did get a chance to watch this and made it by the end of the week, you're still able to get some stickers as well. And if you aren't able to get them this time or you miss out, we're going to be doing more of these live events in the future. So keep coming back, we'll probably keep giving away some stickers and some cool swagger. Who knows what fun things we'll come up with? I enjoy different things from time to time.", "Mason:", "Ah, well, we made it, Andrew. That was pretty fun. How's ...", "Andrew:", "I love it. See, the thing is, I could talk about this for five more hours. This is what I've lived and breathed. So I really appreciate you having me on. I appreciate you asking me the tough questions.", "Mason:", "Hey, I'm glad that the questions that I asked actually made sense. I feel like every day, a little bit, I understand more and more about this space every day, which is really fun because I've been in DevOps for so long. You learn some new things, but when you do a hard shift over into machine learning and data science, it's a completely new world. It's literally the Aladdin gift of a whole new world.", "Andrew:", "There we go.", "Mason:", "So that's how it's been for me, but I'm glad that I was able to do this. I don't see any other questions in any of the chat. So I think we're going to go ahead and exit out.", "Mason:", "Thank you, everyone, again for attending. And we will see you ... Keep an eye out. If you haven't already followed the YouTube channel, follow the YouTube channel, get notifications. You'll get notified when we go live, when we add more stuff. We'll definitely be going live again here within a couple of weeks to a month. So we'll be back around and we hope to see you next time. See you all later.", "\u200d", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/developing-multi-modal-generative-ai-programs-for-enterprises", "page_title": "Developing Multi-Modal Generative AI Programs for Enterprises - Video", "headers": ["Developing Multi-Modal Generative AI Programs for Enterprises", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Good afternoon everyone. Glad to be here. My name is John Myers and I am the CTO at Gretel ai, and we are a generative AI platform that specializes in creating synthetic data. All right, so what I hope everyone here can take away today is to kind of aggregate a lot of the awesome talks we had this morning that talked about a lot of different use cases for generative ai, and then think about how to actually build a generative AI program within the enterprise. And so what I want to do today is talk about the different types of modalities for generative ai, the different types of data you can actually operate on. I'd like to walk you through some example workflows that we see with our customers today, and then talk about some common vendor deployment models so you can help think about the different characteristics of tools and products that you'll want to deploy inside of the enterprise and incorporate synthetic data into the business. ", "So what we're going to talk about, we're going to give some definitions of synthetic data and generative AI again, and talk about different multimodality that exist within that domain. We'll walk through a couple of machine learning and synthetic data example workflows. And these are examples that we pulled from our customers on how they integrated synthetic data into their ML ops processes. Talk about some deployment considerations, and then we'll go into q and a. So let's start with some high level definitions. So synthetic data is generated data that mirrors the statistical properties of real world data and enables building on top of that data while it simulates real world situations without having to have access to the original data. Synthetic data can be generated through these generative AI methods across a variety of modalities. These would be different modalities that help you process lots of different types of data that could be tabular, multi table and relational databases, natural language and time series data. ", "And we do this through a variety of different types of models, which are all deployable through gretel, either in our cloud or you can run it on a cloud prem model, what we call hybrid inside of your enterprise. And then the benefit is that you have this flexibility of where you can actually deploy. If you need to get started very quickly and experiment, you can jump right into grettel Cloud. Once you want to deploy its scale, you can actually pull things into your own environment and kind of build your own internal corpus of models that are yours and yours only, and you're not actually now sending any of your sensitive data to any cloud provider. ", "So let's talk about modalities of synthetic data. So these are the kind of core modalities that we support at Gretel, and you'll find that there's a lot of different tools out there that support these modalities through a variety of different ways. The most common one we have is tabular data. This can come in a couple of different flavors. This can come in single table, and typically the use cases we have with single table tabular data is actually taking machine learning training data sets, creating a synthetic version of them. So they're basically safe to share more broadly across the enterprise. It's really easy to get started with that. If you already have these types of data sets curated, you can come right into a Gretel product, create a synthetic version of it, and then start basically sharing it and then using it across the enterprise without worrying about those privacy concerns. ", "Some examples we have here with one of our actual customers today is making use of opt-out data. And so typically what we found with a lot of our customers is in order to make data shareable, they had to apply different types of transformation techniques to the data. So maybe you're bucketing certain values in the data or you're kind of watering it down a little bit, so you don't have a lot of fidelity in that data. The way we solve that is by allowing 'em to create a synthetic version of that data, then you don't have to use that overly bucketed data. And then you have something that can be more broadly shared for your classification and regression use cases, stepping further down the use case line. In more advanced situations, we have the ability to allow folks to augment their data sets with new records, and this allows you to either increase the amount of training samples or create training samples of a certain class within the dataset. ", "So when it comes to moving into your ML ops platforms, a lot of 'em have different requirements. You have to have a minimum number of training records, and they also suggest that you have a certain balance within your data. If that doesn't exist in your original training data, you can actually then use synthetic data to actually fill in those gaps, so to speak. The next modality that we're most common with our customers is time series data. So this is basically operating on data that is linear across time. And the most common use case we have here is to fill in a lot of gaps for data that is generally pretty expensive to collect. So when one of our customers has kind of physical sensors out there collecting information, they're expensive to deploy, and so they might have a handful of them out there and they're starting to collect data points within a certain range. ", "And so in order to kind of fulfill their simulations for their needs, they're using synthetic data to actually fill in some of the gaps in the time series. The next one would be natural language. There's a lot of that today that we talked about earlier. Really this comes down to being able to build your own internal G P T models. So earlier we had the talk from the Federal Reserve, and one of the questions that was asked was, Hey, are you using pre-trained models that are out there and open source to kind of build your own internal L L M? So with gretel, what we allow you to do is to actually pull some of those models out from the ecosystem that are out there, bring them into your environment, you can fine tune them on your own data, and then you can use those G P T models to fulfill a bunch of different use cases. ", "So some of the most common use cases we have with our customers today, it's creating additional training samples for N L P modeling, some of the more concrete use cases here, creating additional samples for toxic language detection, spam and fraud detection, and helping bots for training and development like chat bots and personal virtual assistants and stuff like that. And then our last modality of that is out there available on GRETEL today through a free public preview is synthetic images. So this allows us to build on top of tools like stable diffusion. You can show up, you can bring prompts, and then you can generate images for ML training that are generally pretty expensive to acquire one way or another. So think about insurance and medical tech. We have use cases where insurance companies want to create images of really catastrophic accidents so then they can be automatically analyzed for different types of things like fault and stuff like that. So since those are really hard to acquire, since I don't think anyone wants to go out and intentionally crash a bunch of cars, we can generate those types of images and then you can bring them right back into your ML ops pipeline. ", "So whenever we're going to get into some example workflows on how to kind of integrate synthetic data into your ML ops process, so there's two use cases that I'll walk through. The first one is kind of focused on, we already have ML training sets, they don't perform well because of imbalances in the data or there's not enough training samples. And then the second use case I'll walk through is we want to introduce ML into our environment, but we don't have any training sets yet, but we just need to get started. How do we explore our production data to trying to again get to that point where we can create high fidelity ML training sets? ", "Alright, so here's kind of a walkthrough at a high level of what we see inside of a couple of our customers that are actually already building ML AI models, but they have a challenge with imbalances in the data. So we'll start with the ML AI data sets usually then we'll have the teams already starting to analyze those data sets for imbalances in the data. Once they're able to identify what their kind of gaps are in that data, you can take the ML AI dataset that you already have and train a synthetic model on it. In this case, we're training a tabular data model. So we have a whole variety of models that you can use for that depending on the need, language based, GaN based, statistical based. We have models that focus on differential privacy. So we have four or five different models you can actually choose from. ", "And you train that synthetic model. Now you can start generating additional data using what we call conditioning. So let's say you are collecting data from a bunch of different locations and you don't have a lot of training samples from the city of New York, for example. So what you can do is once you train that model, you can say, Hey, you know what? Give me 500 more records, but make sure the location is New York. And then what the model does is it fills in the rest of the record for you based off of what it's already learned about the records that are derived from New York. And so once that happens, you can add that data back your training set, continue with their M L AI experimentation and see how your classification or your aggression models are performing. And then you can kind of keep iterating and tuning on that to make sure that you're building an M L AI dataset that is balanced in the way that you need it. ", "Here we're going to kind of walk back to an example where you might have an enterprise that's just getting started with an ML AI program for a specific set of use cases. And in this situation, what we commonly see is you have applications that are out there, all your data is residing in these production databases. It's highly normalized, it's spread out across data warehouses or relational databases that are designed for transactions, right? Because you're building an application that users are kind of interacting with and you're just building this huge corpus of data. You might not have any ML AI training sets yet. So what do you do in this situation? Well, in this kind of situation, typically what you would do is you would explore all that data and then start kind of crafting the kind of queries you need to start building those training sets. ", "The problem here is privacy. How do you make a safe version of that production database that you can actually comb through and analyze? So what we allow you to do is be able to take that production database and create a subset of it, right? Because at this point you're just doing data exploration. So if you can create a subset of that database, you can start exploring with it, kind of build out the different query patterns you need. For example, you might finally develop your data set, save that as a materialized view, push that back to the production database, and now you can harvest the data from the production database with that materialized view, kind of gives you that single table that you'll want to train on 'em in your ML ops platform. Once you have that single table, then you can start synthesizing that table into a new data set and start using that in your ML ops process. So this is really common for companies that are really trying to kind of break m l AI into new areas and they just completely blocked because they don't have access to any of the data to begin with. ", "Alright? All right. Now we'll get into some deployment considerations. So the deployment considerations here, there's kind of three different varieties. Gretel actually offers all three of these. You'll find that as you explore different tools, some are in one bucket, some are in multiple buckets. And so we'll start with the open source or semi permissive licensing. What I mean by semi permissive licensing is you have different packages out there that are available to use, but you have to read the fine print. A lot of licenses are moving to source available licenses or restrictive licenses such that you might be able to use them in the enterprise, but you might only be allowed to use them for experimentation or you might only be able to use them internally. You can't turn around and build a synthetic data service with an open source tools and resell it. So first thing is when you're starting to pick up an open source package, red the find and print on the license and understand exactly what your restrictions are. ", "At Gretel, our open source packages are under what we call a source available license. So if you want to explore with gretel's like core Python packages and our free variance, they are free for any enterprise to build anything with. Our only restriction is that you can't go and build a competing service and resell that service to others. Now with most open source tools, some of the drawbacks are you're mostly relying on community support and then that licensing could change at any moment. Most of these are also in the forms of SDKs, mostly in Python, just because that is where all the core packages are coming from, like the underlying libraries like TensorFlow and PyTorch, most of these SDKs, much like grettel are built on top of that. That does mean it does require a little bit of experience from your developers and operating with those packages. ", "And then the last part of thinking about using open source tools in your environment is scaling deployment and infrastructure. It's kind of on you, right? You have to figure out how to deploy those packages on the right hardware, how to scale it, how to resource it. And if you're resourced to do that, that's great. I would recommend that if you're going to look at those types of tools, you kind of use them in a sandbox environment. But when it comes down to actually operationalization, you might want to consider moving into more of a vendor cloud hosted or like a cloud prem or a hybrid mode. So vendor cloud hosted like a full SaaS component is great. Easy to get started an experiment, assuming free tiers exist. Gretel has a free tier. You can get in, we have demo data for you, so you don't even have to worry about uploading your sensitive information. And then the, I'll go right to the hybrid mode is the hybrid mode is kind of this mode where you can actually deploy the compute resources in your own cloud environment. And in Gretel, we support deploying a hybrid mode into a W S G C P and Azure. And once you're there, our data plane operates inside of the confines of your virtual cloud. Your data never leaves your environment. The models you train stay in your environment. You can fine tune those models and use them pretty much in an unlimited fashion. ", "Wrapping up what I covered was kind of different things you used to consider when building a generative I AI program. Step one, think about what modalities you need. You're dealing with tabular data, relational data, image data, time series data. Make sure you're looking around for a vendor that gives you support for everything you need, so you're not kind of having to build stove pipes for each type of modality. Second one is where in your existing workflows do you want to plug in? Make sure that you have the tools to plug it into your existing ML ops workflows and it fits into the processes you already have. And then finally, deployment considerations. Am I doing open source? Am I going full SaaS hosted, or do I need something in between that gives me the ability to keep the data in my environment with a light footprint of compute resources in a cloud that I already use? And that should wrap it up.", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/videos/accelerate-data-access-in-healthcare-a-gretel-fireside-chat", "page_title": "Accelerate Data Access in Healthcare: A Gretel Fireside Chat  - Video", "headers": ["Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Video description", "Read the blog post", "Predicting Patient Stay Durations in the ER with Safe Synthetic Data", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/transforms-and-synthetics-on-relational-databases", "page_title": "Transforms and Synthetics on Relational Databases", "headers": ["Transforms and Synthetics on Relational Databases", "Introduction\u00a0\u00a0\u00a0", "Transform Notebook", "Synthetics Notebook", "Our Database", "Gathering Data Directly From a Database", "Take a Look at the Data", "Defining Configurations", "Training and Generating Data", "Take a Look at the Final Data", "Load Final Data Back into Database", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["In a recent blog we walked you through how to use transforms directly on a relational database while keeping the referential integrity of primary and foreign keys intact. In this blog, we\u2019ll simultaneously step you through both our new streamlined version of our multi-table transform notebook as well as our new multi-table synthetics notebook. The notebooks share much of the same process flow and can each be used independently or in tandem. As shown in the image below, there are three main flows you can take through Gretel when anonymizing data:", "If you\u2019re interested in just de-identifying data for a pre-processing or demo environment, then the transform notebook is the way to go. If you want to augment your data or if you intend to run a statistical or ML analysis on your data, then the synthetics notebook is the way to go. However, if you\u2019re looking for the highest level of privacy, then the use of both notebooks is in order. The transform notebook is run first to remove PII and other sensitive information from the database tables. The synthetics notebook is then run to add another layer of protection and enable the creation of more or less records than in the original dataset. You can follow along with our Gretel multi-table notebooks located ", "or here:", "The relational database we'll be using is a mock e-commerce one shown below. The lines between tables represent primary-foreign key relationships. Primary and foreign key relationships are used in relational databases to define relationships between tables. To assert referential integrity, a table with a foreign key that references a primary key in another table should be able to be joined with that other table. Below we will demonstrate that referential integrity exists both before and after the data is either transformed or synthesized.", "After cloning the multi-table repo and inputting your Gretel API key, we first grab our SQLite mock database from S3. In the transform notebook we use the original \u201cecom\u201d database. However, in the synthetics notebook (as shown below) we\u2019re using the \u201cecom_xf\u201d database which is actually the final results of having first run the transform notebook on \u201cecom\u201d. Both notebooks create an engine using SQLAlchemy. We then use that engine to gather data, and crawl the schema to gather primary and foreign key information.", "These notebooks can be run on any database SQLAlchemy supports such as PostgreSQL or MySQL. For example, if you have a PostgreSQL database, simply swap the `sqlite:///` connection string above for a `postgres://` one in the `create_engine` command.", "Both notebooks then take an initial look at the data by joining the order_items table with the users table using the user_id.", "Below is the output (not all fields returned are viewable in the snapshot). Note how every record in the order_items table matches a distinct record in the users table. A primary goal of both notebooks is to show how we can run transforms or synthetics on a relational database and maintain these relationships.", "Both transform and synthetic models require configuration setup to define how the models will be built. In transforms, policies are built which specify how PII or sensitive information is to be de-identified. For example, you could choose to replace all names with fake names or to offset every date by a random number of days. See our ", " for more information on how to set up transform configurations. In our multi-table transform notebook, we specify the location of policies as shown in the following code snippet. Note that not all tables in the database may require transforms.", "In synthetic models, we use configurations to specify the hyperparameters used in the training. We have a variety of ", " to choose from depending on the characteristics of your dataset. Below we show how the default configuration is grabbed and assigned to each table in our synthetic notebook.", "In both notebooks, we then proceed to train models, generate data and align the primary and foreign keys. Behind the scenes, all training and generating is done in parallel with logging output letting the user follow the progress. The code in the transform notebook is:", "While the code in the synthetic notebook, looking very similar, is as shown below. Note the synthesize_tables command also returns the models that were created so that later they can be used to view the synthetic performance reports.", "To view the final data in either notebook, we again take the same join on the order_items and users table ", ", but now applied to the new data. Once again, each record in the order_items table matches to a distinct record in the users\u2019 table. Below we show the final data from the synthetics notebook:", "In the synthetics notebook, you can also then take a look at the synthetic performance report for any of the tables with the following code:", "Below we show the header of the synthetic performance report for the table \u201cusers\u201d. As you can see, we do quite well.", "To wind things up, the last step in both notebooks is to now load the final data back into the database. In the transform notebook, we save the new data to \u201cecom_xf\u201d using the schema in \u201cecom\u201d, and in the synthetics notebook (as shown below) we save the new data to \u201cecom_synth\u201d using the schema from \u201cecom_xf\u201d.", "As demonstrated above, the Gretel multi-table transform and synthetics notebooks are simple but powerful ways for anonymizing a relational database. Depending on your use case, you can use the notebooks independently to de-identify relational databases for testing environments (Transform) or to anonymize and augment data while maintaining statistical integrity (Synthetics); or you can use them together for maximum privacy protections. Whichever route you choose, the referential integrity of the primary and foreign keys will remain intact. With synthetics, you are also ensured that the ratio of new table sizes is a perfect reflection of the original data. Thank you for reading! Please reach out to me if you have any questions, ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/videos/ai-business-interview-synthetic-data-and-generative-ai", "page_title": "AI Business Interview: Synthetic Data and Generative AI - Video", "headers": ["AI Business Interview: Synthetic Data and Generative AI", "Video description", "Read the blog post", "Transcription", "More Videos", "AI Business Interview: Synthetic Data and Generative AI", "Developing Multi-Modal Generative AI Programs for Enterprises", "Accelerate Data Access in Healthcare: A Gretel Fireside Chat ", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel.ai's Chief Product Officer Alex Watson joined Junior Editor Ben Wodecki at the AI Summit London 2023", "Gretel CTO and cofounder John Myers explores building a Generative AI program, with a focus on synthetic data, geared towards multi-business-unit support, focusing on the theme of 'building for posterity'.", "Hear from leading researchers and scientists for a captivating fireside chat on unlocking data access in healthcare with synthetic data"]},
{"url": "https://gretel.ai/blog/gretel-ai-raises-12-million-in-series-a-to-safely-share-build-with-data", "page_title": "Gretel.ai Raises $12 Million in Series A to Safely Share, Build with Data", "headers": ["Gretel.ai Raises $12 Million in Series A to Safely Share, Build with Data", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["After announcing ", " round from Moonshots Capital, Greylock Partners, Village Global and a group of strategic angel investors in February, we are thrilled to share that Gretel.ai raised $12 million in Series A funding, led by Greylock. You can read more about the round in ", ". ", "This funding and our TechCrunch story are more evidence that Gretel is gaining strong momentum in our mission to help developers create safe data. We expected as much! With safe data, developers can build faster and collaborate more, and that\u2019s good for everyone. ", "While we\u2019re excited by the value we\u2019ve been able to offer developers so far, this is only the beginning for Gretel. Today, a few large companies dominate the data economy, mining your personal data, cornering the private data market, and reaping huge profits for themselves. Gretel is all about busting this wide open. We\u2019re working to enable a new \u2018open data economy\u2019 where open access to synthetic data democratizes the playing field and unleashes record innovation.", "If you\u2019re a developer interested in learning more, check out our free ", " and our open source ", ". You can get started in minutes with one of our guides for ", ", or generating ", ". We are building Gretel for developers like you, so don\u2019t be shy. Please follow us on Twitter, submit a PR for a new feature on ", ", or start a conversation on our community ", "! And if you\u2019d like to join us on this journey, please check out our ", " page!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/conditional-text-generation-by-fine-tuning-gretel-gpt", "page_title": "Conditional Text Generation by Fine Tuning Gretel GPT", "headers": ["Conditional Text Generation by Fine Tuning Gretel GPT", "Why not just collect more real-world data?", "Training setup", "Model Configuration", "Train the synthetic model", "Generate annotated data from the model", "Load GPT results into a tabular format", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Updated on June 15, 2023 to reflect ", ".", "Whether you are training a chatbot, creating a model to perform sentiment analysis on customer reviews, or even building the next voice model for Alexa, there will always be new commands or utterances that the model struggles to understand.", "In this post, we will demonstrate how synthetic data creates a scalable, fast, and cost-effective alternative to manual techniques for data augmentation. To get started, we will fine-tune a ", " model on a financial intent classification dataset called `", "`. With some clever encoding and a technique called ", ", we can then use the GPT model to generate new and unique annotated examples for any of the intent classes.\u00a0", "Using ", " is more privacy-preserving, scalable, and orders of magnitude less expensive than previous approaches of collecting and labeling data from customers, or manually generating and annotating data. In fact, Gartner estimates that by 2030, the ", ".", "Let\u2019s start with loading the `banking77` dataset into a Pandas Dataframe. The dataset comprises 13,083 customer service queries labeled with 77 intents from the banking domain. There are two columns, \u201cintent\u201d, and \u201ctext\u201d. For example, for the user ", " query \u201cWhat can I do if my card has not arrived after two weeks?\u201d the corresponding ", " would be \u201ccard arrival\u201d.\u00a0", "Our goal is to generate coherent text examples for any of the 77 different intent labels in the training data. We\u2019ll need to encode the intent labels and text into a single field to train the GPT model. To do this, we will concatenate the two fields with a special character as a separator.", "For example:", "[intent] + \u201c,\u201d + [text]", "\u200d", "This example relies on the keywords in the intent to prompt the model to generate related text. One way to improve this model could be to use ", " to extract relevant keywords for each class, which could be automatically added to the intent keywords.", "In this experiment, we will use ", " as our base model via Gretel.ai\u2019s APIs. Check out our ", " for a list of models we support. We'll use our ", ", though you can modify parameters as you need.", "Save the combined intents + texts to a single column CSV, and submit to the Gretel API service using the config above.", "Finally, we can use our newly fine-tuned model to generate new text examples for a given intent by ", " with examples from the class. Hint: We have found prompting the model with ~25 examples for the class you wish to generate to work well in practice. Once again, since GPT does not natively handle columnar data, we will need to encode our intents and texts using a comma as a separator, and an `\\n` carriage return between records.", "In this case, each `record` is prompted with 25 examples from the source data and then tasked to create up to 1000 additional tokens of similar examples. We create a record handler to submit the job to the Gretel APIs.", "We can now pull down the data generated in the previous step. It is returned by the Gretel API as a text string, which we can now load into a DataFrame (tabular) format. To do this, split the text record by new lines and the intent.", "There you have it! The final result and synthetic texts are coherent and human-like. Quite impressive! If you\u2019d like to take this a step further, try generating additional keywords for each intent category, and try running with different GPT models to compare their outputs. Here is a snapshot of our final, synthetically generated and annotated data.", "We are super excited to launch support for generative pre-trained transformer models at Gretel, and found the capabilities of these models to be really promising! If this is exciting to you, feel free to reach out at hi@gretel.ai or join and share your ideas in our ", ".\u00a0 If you\u2019d like to explore other sample notebooks check out ", ".\u00a0", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/generate-synthetic-data-in-3-lines-of-code", "page_title": "Generate synthetic data in 3 lines of code", "headers": ["Generate synthetic data in 3 lines of code", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We are super excited to introduce the gretel-trainer, an interface designed to be the simplest way to generate synthetic data, and a preview of new features coming to Gretel\u2019s production ", " & ", ". Jason Warner, formerly ", " and VP of Engineering at Heroku, has always had some advice that stuck with me \u2014 \u201c", "\u201d. ", "In today\u2019s post, we\u2019ll walk through a code example for generating synthetic data using gretel-trainer and how you can use it to create synthetic data for anything from the CSV you have on your computer to datasets with ", ". Try out the code below, or follow along step-by-step with our notebook in ", ".", "First, start with installing the gretel-trainer library.", "Below is the simplest path to creating synthetic data. This code uses ", " to train a deep learning generative model on the popular ", " dataset and to create an artificial, synthetic equivalent version.", "Want to load an existing trained model, and use it to generate more data? That\u2019s easy too!", "Need to customize your synthetic model? Selecting an ", " and tuning ", " is easier than ever.", "Since the gretel-trainer uses Gretel\u2019s fully managed cloud service for model training and generation, you can create state-of-the-art synthetic data without needing to set up or manage infrastructure and GPUs. Try running our ", ", and for the next steps, try running on one of your own datasets or CSVs. Have questions? Ask for help on ", "\u2019s ", ". ", "What\u2019s next: In part 2 of this post, we will dive into more advanced use cases using gretel-trainer to modify distributions in underlying data \u2014 a valuable technique for improving ", " in cyber and financial datasets using ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/machine-learning?094d394d_page=3", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "Compare Synthetic and Real Data on ML Models with the new Gretel Synthetic Data Utility Report", "Augmenting ML Datasets with Gretel and Vertex AI", "Teaching large language models to zip their lips", "Diffusion models for document synthesis", "Downstream ML classification with Gretel ACTGAN and PyCaret", "How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "Creating Synthetic Time Series Data for Global Financial Institutions \u2013 a POC Deep Dive", "Transforms and Synthetics on Relational Databases", "Generate synthetic data in 3 lines of code", "Synthetic Image Models for Smart Agriculture", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/generate-time-series-data-with-gretels-new-dgan-model", "page_title": "Generate time-series data with Gretel\u2019s new DGAN model", "headers": ["Generate time-series data with Gretel\u2019s new DGAN model", "Intro", "Data", "Model training", "Synthetic data and analysis", "Summary", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel is excited to release our first dedicated time-series model in our API. Time-series data, where correlations across time are crucial, are found everywhere, from smart home sensors to financial data. With our new DGAN model, you can use the DoppelGANger model to generate synthetic time-series with the convenience of Gretel\u2019s API.", "For details on the model, please see our ", " about our PyTorch implementation of DoppelGANger. Now, you can use that implementation directly through Gretel\u2019s API, the same way you use Gretel-LSTM or Gretel-GPT now \u2013 just modify the YAML configs as detailed below. Please note this is an open beta release, so there may be a few rough edges and we\u2019re eager to hear your feedback to improve the experience (join our ", " communities).", "The rest of this blog walks through an example demonstrating basic usage of the DGAN model with our Python SDK. We\u2019ll show a few key highlights of the code and process in this blog. Follow along and see every line of code in our ", ".", "Let\u2019s use daily crude oil prices for the running example in this blog. Suppose you\u2019re an analyst at an energy company tasked with assessing the risk of different strategies for extracting, storing, and selling crude oil. You don\u2019t know what\u2019s going to happen next week or year, but you can look at historical trends. But there\u2019s only one history to look at, so you want to generate synthetic data for oil prices to get a larger set of ", " to evaluate.", "First, we need some existing data to train the synthetic model. The WTI and Brent spot price history from ", " will work nicely. These are reference prices for two types of crude oil: West Texas Intermediate and Europe Brent. See Figure 1 for a snippet of the data in tabular form. There are three columns: the date, and daily prices for WTI and Brent from 1987 to 2022. The entire 35 year history of prices are shown in Figure 2.", "DGAN requires many examples of a time series to train (for best results we recommend at least 100 examples), but we just have one historical set of prices here. The most common way to address this is to split the long history into smaller subsequences, and this is directly supported in our API via the max_sequence_len parameter. Here, we chose 10-day sequences to train and generate with two weeks of prices, see Figure 3 for examples. With 35 years of daily price data, we have sufficient two-week examples for training (~900) and two weeks seems like a plausible starting point for a short-term risk analysis by our hypothetical analyst.", "Other data setups are possible, and the downstream task and goals for the synthetic data should inform the data setup. For example, if recent price trends are critical, then you might look for a data set with prices every minute (or faster) for the last 12 months and split by day. We\u2019re happy to discuss these different data setups if you have any questions.", "To train the model, we\u2019ll use the config below. This is based on our ", " with a few tweaks.", "The key places to change to use different training data are the time_column and the sample_len and max_sequence_len. For detailed explanations of these and other parameters, see the ", ".", "The code snippet below uses that config and starts a model training in our Gretel Cloud. Run the cell and go brew some coffee - training for this oil data takes about 10 minutes.", "As part of training the model, we automatically generated some synthetic data. Let\u2019s download that data into a Pandas DataFrame for analysis (see Figure 4 for excerpt from the table).", "A few things to note in this synthetic data output: the date column is just repeating the first 10 days from the original data and there\u2019s a 4th column called example_id. Both are a product of the model generating 10-day long chunks at a time. First, we reuse an arbitrary date range from the training data for the date column. And the example_id values identify the different 10-day sequences in the generated data. Any downstream analysis should focus within those 10-day chunks.", "While returned all together as a single CSV file (or read into a DataFrame), we do not recommend treating this as a single, long history like the original input data because there are discontinuities every 10 days. If longer sequences of coherent data are required, try using a larger max_sequence_len in the config. The ", " documentation contains more info.", "Selecting a few example_id values to plot, Figure 5 contains some synthetic oil price data produced by the model.", "These simulated price movements are a great start for a more robust risk assessment by our analyst. Of course, some iteration is usually needed as this DGAN model produces price histories that might have come anytime in the last 35 years and only for two weeks.", "Besides visually checking if the synthetic data \u201clook similar, but not identical\u201d to the training data, there are many quantitative ways to evaluate and use the generated data. Just remember that the best temporal correlations are always within the max_sequence_len rows of a single example_id value. A few visualizations of the quality of the synthetic data are provided in the notebook. We\u2019ll just show one evaluation of the temporal dynamics of the generated data compared to the training data. Figure 6 uses the ", " of each variable with itself at different time lags.", "We see the autocorrelations for real and generated data are similar, so the DGAN model is effectively learning how prices usually change every one, two, three, etc. days. This is one indicator that we have high quality synthetic data that is appropriate for a day-to-day risk assessment task.\u00a0", "This oil data and config above demonstrates the simplest usage of our DGAN model. More advanced setups are also supported, such as providing your own example_id column as mentioned above, using fixed attributes that do not vary over time, and categorical variables. See our ", " for more info on how to take advantage of these features.", "Also please note this is an open beta model, and thus has a few rough edges. One issue we\u2019ve encountered is the training can be insatiable, so if a run does not produce nice results like the plots shown above, sometimes just retraining with the same config will help.", "With ", ", you can use our APIs to generate synthetic time-series data in minutes. Sign up for a free account at ", ", load the ", " with a Jupyter runtime, and start generating synthetic time-series data today.", "As an open beta release, we would love to hear how the DGAN model is working for you. Several improvements for DGAN are planned, including ", " support, integration with ", ", better default configuration to address training instability, and more.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/machine-learning-accuracy-using-synthetic-data", "page_title": "Machine Learning Accuracy Using Synthetic Data", "headers": ["Machine Learning Accuracy Using Synthetic Data", "Our Experiments", "Results", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["One of the questions we hear often from developers working with synthetic data is \u201chow well does synthetic data work for ML use cases versus the original dataset?\u201d The question refers to whether the synthetic data can really be used to produce a machine learning model in par with what could have been created if the original data had been used. Let\u2019s dive in and find out.", "To narrow in on test datasets to use, we queried the data section of Kaggle with the \u201cClassification\u201d filter turned on, searching for datasets with significant activity. Our criteria was the dataset had to have at least 4000 records and be textual in base (vs image or audio). \u00a0Below is a table listing the datasets we used, as well each dataset's row and column count. When training the synthetic model, all rows and columns were used with no modifications. All training parameters used default settings for gretel-synthetics, and we generate 5000 synthetic records for each dataset.", "The complete code used in these experiments can be found ", ", and all original datasets and the generated synthetic records can be found ", ".", "We start by building the list of machine learning algorithms we\u2019ll be testing, as shown below.", "We next read in both the original and the synthetic data, then use sampling to ensure both sets are the same size and have the same ratio of positive and negative samples (you can see those details in the full code). For each dataset/model combination, we'll run a 5-fold stratified cross-validation 5 times. As shown below, we set up a pipeline to handle missing fields, encode categorical fields, standardize all fields and then run the model.", "We then graph each dataset showing the relative performance of each machine learning algorithm on both the original data and the synthetic data. As you can see in the graphs \u00a0below, the synthetic data runs do quite well!", "And finally, we graph a quick synopsis of all the datasets:", "These datasets were chosen with no attempt to highlight only the successes. At Gretel.ai, we\u2019re really proud of how well our synthetic data generation holds onto the statistical integrity of the original data. While tougher datasets might exist, there are also many ways to tune a synthetic model that were unused in this set of experiments.", "Synthetic data plays an important role in the future of Artificial Intelligence. Beyond the hurdle of swift access to sensitive data, companies often lack enough relevant data to effectively train a model. To remedy this, synthetic data can be used to augment the original training data. This is particularly true (as described in our earlier ", ") when instances of the positive class are rare (such as in fraud or cybersecurity). Synthetic data can also be used to broaden the variety of examples used in pre-production testing scenarios.", "Whatever your use case, Gretel provides privacy engineering as a service delivered to you as APIs. You can synthesize and transform data in minutes enabling safe, trusted AI systems without sacrificing utility.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/machine-learning?094d394d_page=1", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "Data Simulation: Tools, Benefits, and Use Cases", "Gretel announces partnership with Microsoft Azure and joins Microsoft for Startups Pegasus Program", "AWS + Gretel Synthetic Data Accelerator Program for Generative AI", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Machine Learning Accuracy Using Synthetic Data", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Optuna Your Model Hyperparameters", "Test Data Generation: Uses, Benefits, and Tips", "Prompting Llama-2 at Scale with Gretel", "How to Safely Query Enterprise Data with Langchain Agents + SQL + OpenAI + Gretel", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/amy-steier?b2715911_page=1", "page_title": "Author: Amy Steier - Gretel.ai", "headers": ["Amy Steier", "Machine Learning Accuracy Using Synthetic Data", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Optuna Your Model Hyperparameters", "Comprehensive Data Cleaning for AI and ML", "Transforms and Synthetics on Relational Databases", "How accurate is my synthetic data?", "Automatically Reducing AI Bias With Synthetic Data", "Innovating With FastText and Table Headers", "Transforms and Multi-Table Relational Databases", "Introducing Gretel's Privacy Filters", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-are-good-epsilon-e-and-delta-d-values-in-differential-privacy", "page_title": "What are good epsilon (\u03b5) and delta (\u03b4) values in differential privacy?", "headers": ["What are good epsilon (\u03b5) and delta (\u03b4) values in differential privacy?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Epsilon is the quantitative privacy guarantee. It puts a ceiling on how much the probability of a particular output can increase if you were to add or remove a single training example. Stringent privacy needs usually require an epsilon value of less than one. However, in some domains it\u2019s not uncommon to see epsilons of up to 10 being used. Delta is a bound on the external risk that won\u2019t be restricted by epsilon. External risk is that which inherently exists no matter what you do with your dataset. By default Gretel will initialize this value to be much smaller than 1/#training samples to severely limit this risk.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/license/source-available-license", "page_title": "Source Available License", "headers": ["Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["\u200d", "THIS GRETEL.AI SOURCE AVAILABLE LICENSE AGREEMENT, VERSION 1.0 (THE \u201c", "\u201d) SETS FORTH THE TERMS AND CONDITIONS UNDER WHICH GRETEL LABS, INC. (\u201c", "\u201d) MAKES AVAILABLE CERTAIN OF ITS PROPRIETARY SOFTWARE SOURCE CODE, AS INDICATED BY A COPYRIGHT NOTICE THAT GRETEL.AI INCLUDES IN OR ATTACHES TO SPECIFIC VERSIONS OF SUCH SOFTWARE SOURCE CODE (THE \u201c", "\u201d). PLEASE READ THE FOLLOWING TERMS AND CONDITIONS CAREFULLY BEFORE DOWNLOADING, INSTALLING OR USING THE SOFTWARE, AS THEY GOVERN USE OF THE SOFTWARE.", "Gretel.ai is willing to license the Software to the individual or entity that agrees to this Agreement (\u201c", "\u201d), only upon the condition that you accept all the terms contained in this Agreement. By downloading, installing or using the Software, you indicate that you understand this Agreement and accept all of its terms. If you are downloading, installing or using the Software on behalf of a company or other legal entity, you represent that you have the authority to bind such entity to this Agreement, in which case \u201cyou\u201d or \u201cyour\u201d shall refer to such entity. If you do not agree to the terms of this Agreement, you may not download, install or use the Software. ", "Unless and until you have entered into a separate license agreement with Gretel.ai, this Agreement constitutes the final, complete, and exclusive agreement between the parties regarding the Software licensed hereunder, and supersedes all prior or contemporaneous agreements, understandings, and communication, whether written or oral, with respect to such Software.", "\u200d", " ", " ", "Conditioned on your compliance with the terms and conditions of this Agreement, Gretel.ai grants you a non-exclusive, royalty-free, worldwide, non-transferable, non-sublicensable license under Gretel.ai\u2019s copyrights in the Software and during the term of this Agreement to:", "in each case, (i) solely on servers and equipment owned or controlled by you, (ii) solely for your internal business purposes or for your own non-commercial academic, research or other personal use, and (iii) in all events, in accordance with the obligations and restrictions set forth in this Agreement, including Sections 1.2 - 1.5.", "\u200d", "The license granted to you in Section 1.1 does not include the right to, and you will not, and you will not permit any other individual or entity to:", "\u200d", "You will reproduce all of Gretel.ai\u2019s and its licensors\u2019 copyright notices and any other proprietary rights notices contained in the Software (and not remove or alter any of the foregoing) in all copies of the Software and Modifications that you make.", "\u200d", "Your rights in the Software are limited to those expressly granted in Section 1.1. Gretel.ai and its licensors reserve all right, title and interest in and to the Software and all intellectual property rights therein not expressly granted to you in this Agreement. You agree to comply fully with all U.S. export laws and regulations to ensure that neither the Software nor any technical data related thereto nor any direct product thereof are exported or re-exported directly or indirectly in violation of, or used for any purposes prohibited by, such laws and regulations.", "\u200d", "Gretel.ai may package or otherwise make the Software available with software or other material that is distributed as \u201cfree software,\u201d \u201copen source software\u201d or under similar licensing or distribution models, including the GNU General Public License, GNU Lesser General Public License, Mozilla Public License, BSD licenses, the MIT license, the Apache License and other such licenses, whether or not recognized or approved by the Free Software Foundation or the Open Source Initiative (\u201cOpen Source Software\u201d), as noted in Gretel.ai\u2019s documentation for the Software. All such Open Source Software is licensed to you exclusively under the terms of their applicable licenses, including all warranty disclaimers and limitations on liability therein, and the usage limitations and restrictions set forth in this Agreement do not apply to such Open Source Software.", "Notwithstanding the foregoing, the license granted to you in Section 1.1 does not include the right to, and you will not, and you will not permit any other individual or entity to, modify, combine, integrate or otherwise use the Software or your Modifications with Open Source Software or any other software or materials in such a manner that requires, or could require, the Software or your Modifications, in whole or part, to be (a) disclosed or distributed to third parties in source code form, (b) licensed to third parties for the purpose of making derivative works, or (c) redistributable by third parties at no charge, including under the terms of the GNU General Public License version 3, the GNU Affero General Public License version 3, the GNU Lesser General Public License version 3, or any prior or successor versions or equivalents of the foregoing.", "\u200d", "To the extent you provide Gretel.ai with suggestions, proposals, ideas, recommendations or other feedback relating to the Software (\u201cFeedback\u201d), you hereby grant to Gretel.ai a non-exclusive, perpetual, irrevocable, royalty-free, fully-paid, sub-licensable, transferable, worldwide right and license to make, have made, use, sell, offer for sale, import, export, rent, lease, reproduce, distribute, publicly display, publicly perform, modify, create derivative works of, disclose and otherwise exploit such Feedback in any manner without restriction (whether of confidentiality, compensation or otherwise).", "\u200d", "As between you and Gretel.ai, you acknowledge and agree that Gretel.ai owns all right, title, and interest in and to the Software, including all intellectual property rights therein. Gretel.ai reserves all rights not expressly granted to you in this Agreement, including with respect to its trademarks, service marks and brand features.", "\u200d", "You acknowledge and agree that the Software is provided \u201cAS IS.\u201d GRETEL.AI DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT, AND ANY WARRANTIES ARISING OUT OF COURSE OF DEALING OR USAGE OF TRADE. For the avoidance of doubt, Gretel.ai has no obligation to provide maintenance, support or any other services to you for the Software under this Agreement.", "\u200d", "TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL GRETEL.AI BE LIABLE TO YOU OR TO ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, DIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THIS AGREEMENT, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, AND WHETHER OR NOT GRETEL.AI HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. You expressly acknowledge that Gretel.ai has entered into this Agreement in reliance upon the limitations of liability specified herein, which allocate the risk between Gretel.ai and you, and form a basis of the bargain between us.", "\u200d", "This Agreement is effective until terminated. You may terminate this Agreement at any time by destroying all copies of the Software and your Modifications. This Agreement will terminate immediately, without the requirement of written notice to you, in the event that you breach the terms of this Agreement or infringe or otherwise violate Gretel.ai\u2019s intellectual property rights. Upon any termination of this Agreement, the rights and licenses granted to you in this Agreement will automatically terminate and you will, at your expense, destroy all copies of the Software and your Modifications. Sections 2, 3, 4, 5, 6 and 7 will survive any termination of this Agreement. ", "\u200d", "If you have any questions regarding this Agreement, you may contact Gretel.ai at hi@gretel.ai."]},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=2", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Test Data Generation: Uses, Benefits, and Tips", "Introducing Gretel Benchmark", "Prompting Llama-2 at Scale with Gretel", "Automate Synthetic Data Pipelines with Gretel Workflows", "Gretel GPT Sentiment Swap", "Anonymize tabular data to meet GDPR privacy requirements", "Synthetic Data and the Data-centric Machine Learning Life Cycle", "Gretel is now available in the AWS Marketplace", "Gretel is live on Google Cloud Marketplace \ud83c\udf89", "Fine-tune a MPT-7B LLM with Gretel GPT", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-is-stochastic-gradient-descent-sgd-modified-to-be-differentially-private", "page_title": "How is Stochastic Gradient Descent (SGD) modified to be differentially private?", "headers": ["How is Stochastic Gradient Descent (SGD) modified to be differentially private?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["SGD works by stochastically sampling a set of training examples, computing the loss (difference between predicted value and real value), computing the gradient of the loss, then after modifying these gradients by the learning rate, uses the resulting values to update the model parameters. The iteration of this process is what\u2019s meant by descent. There are few main changes to this process to make it differentially private. First the gradients are clipped such that no single training example can unduly impact the model, and second, random noise is added to the clipped gradients to make it impossible to deduce which examples were included in the training. Additionally, instead of clipping gradients at a batch level, they are clipped in micro-batches. The more clipping, noise adding and micro-batching you have, the more differentially private your model will be. As there is often a trade-off between privacy and utility, Gretel-synthetics exposes each of these elements as modifiable parameters in the training.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-does-rdp-order-mean", "page_title": "What does RDP order mean?", "headers": ["What does RDP order mean?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel uses a variation of differential privacy referred to as R\u00e9nyi differential privacy (RDP). RDP makes use of the R\u00e9nyi divergence to measure the distance between distributions. R\u00e9nyi divergence is a generalization of Kullback-Leibler divergence that works in the notion of a parameter referred to as it\u2019s \u201corder\u201d. In RDP, the idea is to search for the order that optimizes epsilon (e.g. your privacy guarantee). When running Gretel-synthetics, the \u201coptimal RDP order\u201d will be printed along with epsilon and delta once training completes.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-many-epochs-should-i-train-my-model-with", "page_title": "How many epochs should I train my model with?", "headers": ["How many epochs should I train my model with?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The right number of epochs depends on the inherent perplexity (or complexity) of your dataset. A good rule of thumb is to start with a value that is 3 times the number of columns in your data. If you find that the model is still improving after all epochs complete, try again with a higher value. If you find that the model stopped improving way before the final epoch, try again with a lower value as you may be overtraining. If you have only a small number of records in your dataset or are having a large number of records fail validation, you may need to increase the number of epochs significantly to help the neural network learn the structure of the data.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-does-gretel-synthetics-implement-differential-privacy", "page_title": "How does Gretel-synthetics implement differential privacy?", "headers": ["How does Gretel-synthetics implement differential privacy?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The TensorFlow team has taken on a lot of the heavy lifting of implementing and releasing TensorFlow Privacy, an extension to TensorFlow that allows differentially private learning. Gretel synthetics implements TensorFlow\u2019s open source code for DP-SGD in the Tensorflow-Privacy library with slight modifications to adapt it to recurrent neural networks, and improved the baseline performance by replacing the plain SGD optimizer with an RMSProp optimizer as it often gives higher accuracy than vanilla SGD (Tijmen Tieleman and Geoffrey Hinton, COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012).", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/if-my-model-trained-in-batches-using-differential-privacy-what-is-my-final-epsilon-privacy-guarantee", "page_title": "If my model trained in batches using differential privacy, what is my final epsilon (privacy guarantee)?", "headers": ["If my model trained in batches using differential privacy, what is my final epsilon (privacy guarantee)?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["When differential privacy models are trained on disjoint subsets of a private database, their combined use has an epsilon value equal to the maximum across all models.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-many-lines-of-input-data-do-i-need-to-train-a-synthetic-model", "page_title": "How many lines of input data do I need to train a synthetic model?", "headers": ["How many lines of input data do I need to train a synthetic model?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We generally recommend 5000+ examples. If you have a smaller dataset that is only a few hundred lines, try training for 100+ training epochs to learn the structure. If you are working with a highly dimensional dataset (e.g. 15+ columns) we recommend 15000+ examples.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-does-gretel-synthetics-leverage-differential-privacy", "page_title": "How does Gretel-synthetics leverage differential privacy?", "headers": ["How does Gretel-synthetics leverage differential privacy?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel-synthetics uses differential privacy to defend against memorization while learning on a private dataset. Imprecisely speaking, the output of a synthetic model trained over a dataset D that contained one occurrence of a secret training record X versus another synthetic model D1 that did not contain X should be nearly identical. Thus, we have mathematical assurances that our model did not memorize the secret.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-is-gretel-synthetics-differential-privacy-different-from-traditional-implementations", "page_title": "How is Gretel-synthetics differential privacy different from traditional implementations?", "headers": ["How is Gretel-synthetics differential privacy different from traditional implementations?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Several companies including Uber have built libraries that help apply differential privacy to SQL queries, by injecting noise into the results of a query aggregation. This approach is powerful but requires you to know what questions that you want to ask of data, without the ability to see or inspect sensitive data directly. Gretel-synthetics is a sequence-to-sequence model that trains on a source dataset, injects noise during the learning process rather than at query time, and creates a secondary dataset that can be shared and viewed directly by data scientists or developers or queried using any database technology.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/blog/advanced-data-privacy-gretel-privacy-filters-and-ml-accuracy", "page_title": "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "headers": ["Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Gretel Privacy Filters", "Our Experiments", "Results", "Digging Deeper", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Last year, we ", " the efficacy of using synthetic data in machine learning by creating synthetic versions of eight\u00a0 popular Kaggle datasets and testing them on a variety of popular ML algorithms. We were excited to find that the use of synthetic data had minimal impact on model accuracy. Back in ", ", we introduced Gretel\u2019s new Privacy Filters. The Privacy Filters are but one of several available advanced data privacy mechanisms. The use of these mechanisms results in a Privacy Protection Level (PPL) score now shown in the Synthetic Performance Report. Now, on top of the privacy inherent in the use of synthetic data, users can add further protection to ensure their synthetic data is safe from adversarial attacks. In this article, we explore the impact of these new Privacy Filters on machine learning accuracy.", "The Gretel Privacy Filters were the culmination of much research on the nature of adversarial attacks on synthetic data. The Privacy Filters prevent the creation of synthetic data with weaknesses commonly exploited by adversarials. We have two Privacy Filters, the first is the Similarity Filter, and the second is the Outlier Filter. The Similarity Filter prevents the creation of synthetic records that are overly similar to a training record. These are prime targets of adversarials seeking to gain insights into the original data. The second Privacy Filter is the Outlier Filter. This prevents the creation of synthetic records that would be deemed an outlier in the space defined by the training data. Outliers revealed in a synthetic dataset can be exploited by Membership Inference Attacks, Attribute Inference, and a wide variety of other adversarial attacks. They are a serious privacy risk.", "The Gretel Privacy Filters can each be set to either `null` (meaning off), `med`, or `high`. These values correspond to thresholds over which a synthetic record is deemed too similar or too much of an outlier. Choosing the right Privacy Filter setting depends highly on the intended use of the synthetic data. For example, if you intend to share the data publicly, we recommend that both filters be set to `high`. If you intend to only share internally in your organization, then you\u2019re fine using the default setting of `med` for both filters. If the whole point of your intended analysis is to study outliers, then we recommend turning the Outlier Filter off. Typically, we find that a setting of `med` has little to no impact on synthetic data utility.\u00a0 In fact, sometimes it even helps! With a setting of `high` however, not always, but sometimes there is a modest hit on utility.", "We use the same datasets that were used in our earlier machine learning accuracy blog: ", "When training the synthetic model, all rows and columns were used with no modifications. This time, however, we got a little fancy and used our publicly available ", " to find the ideal set of hyperparameters for each dataset. You can find more information on the notebook in our recent blog: ", ". Below are the improvements Optuna gave us over the synthetic default configuration. A score of 80 or above is considered excellent. As you can see, the models were pretty good to begin with but Optuna gave them each a nice lift.", "We then tuned a Gretel Synthetics model and created synthetic data first with our Privacy Filters turned off, then\u00a0 with them set to \u201cmed\u201d and lastly\u00a0 with them set to \u201chigh\u201d. The original training data and all synthetic data can be found in our ", " on GitHub.", "We start by building the list of machine learning algorithms we\u2019ll be testing, as shown below. The complete code used in these experiments can be found ", ".", "We next read in both the original and the synthetic data, and for each dataset/model combination, we'll run a 5-fold stratified cross-validation five times. As shown below, we set up a pipeline to handle missing fields, encode categorical fields, standardize all fields and then run the model.", "Let\u2019s first cut to the chase and look at\u00a0 how the average machine learning accuracy varied across tests:", "Not bad! You can see that the synthetic data average accuracy with no Privacy Filters is usually not far from the original data\u2019s average accuracy. When we turn the Privacy Filters to \u201cmed\u201d the accuracy remains similar and sometimes even exceeds the original data accuracy. When we turn the Privacy Filters to \u201chigh\u201d, the results are variable but still quite good. Sometimes there\u2019s a modest hit on accuracy, sometimes there\u2019s no impact and sometimes it actually helps.", "Let\u2019s take a closer look at an example where accuracy improves, as well as one where there\u2019s a modest degradation. We\u2019ll start with\u00a0the UCI Credit Card Default Dataset, where we consistently see a modest impact on accuracy when the Privacy Filters are set to high.", "If we visualize the similarity of synthetic records to training records, we can see that nothing gets filtered. If you\u2019re testing this yourself, you can click on the \u201cFiltered Records Only\u201d button to verify this. Note that we show both training-to-training and synthetic-to-training similarity in the graph for context, but only the synthetic set gets filtered, never the training set.", "If we visualize the outlier scores for our synthetic records, however, we can see that quite a few do get filtered when our Privacy Filters are set to high. Click on the \u201cFiltered Records Only\u201d to verify.", "The classification task in the UCI Credit Card Default dataset is to determine who will and who won\u2019t default on their payment next month. You could imagine that someone with unusual billing and payment characteristics might be more prone to defaulting. In fact, in the outlier records, 62.5% default on their credit card as compared to only 20.7% in the entire dataset. Clearly, the outliers are helping the model build strong classification accuracy.\u00a0", "Now let\u2019s look at a dataset where the opposite is true.", "If we visualize the similarity of synthetic records to training records, we can see that several records get filtered. You can click on the \u201cFiltered Records Only\u201d button to verify this. ", "And when we visualize the outlier scores for this data, we also can see that several get filtered.", "The classification task in the Data Scientist Job Candidates model is to determine who will get hired. In this case, the outlier candidates (and possibly the overly similar candidates) actually hinder the model\u2019s ability to make an accurate prediction. Outliers are often problematic in any statistical analysis and it is not uncommon to remove them in machine learning. Thus, the Privacy Filtering of these records both improves the data\u2019s privacy as well as the ML accuracy.", "One of the many insightful comments made in a recent ", " by OpenMined, is that the tradeoff between privacy and accuracy is more complicated than you would expect. If the data wasn't made private, then less people would be inclined to share their data. There's also the risk that people would be less honest about their data. Both of these aspects also have a direct impact on accuracy. At Gretel, we\u2019re both passionate about making data private and at the same time, preserving data utility. ", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-many-columns-of-training-data-can-i-have", "page_title": "How many columns of training data can I have?", "headers": ["How many columns of training data can I have?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel works best on learning models for densely-packed datasets with 50 or less columns of data. However, there is no limit on the columns (dimensionality) of your dataset. Gretel-synthetics clusters highly correlated columns into batches to be trained independently and then joins the results. We have tested for datasets up to 1,500 columns of sparse data.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/category/machine-learning?094d394d_page=4", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "How to use Weights & Biases with Gretel.ai", "Evaluating Data Sampling Methods with a Synthetic Quality Score", "What We\u2019re Reading: Trends & Takeaways from the NeurIPS 2021 Conference", "Innovating With FastText and Table Headers", "Gretel Synthetics: Introducing v0.10.0", "Reducing AI bias with Synthetic data", "What is Synthetic Data?", "What is Model Soup?", "Improving massively imbalanced datasets in machine learning with synthetic data", "Create high quality synthetic data in your cloud with Gretel.ai and Python", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-are-gretel-synthetics-premium-features", "page_title": "What are gretel-synthetics premium features?", "headers": ["What are gretel-synthetics premium features?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["With Gretel API access, developers can access premium features such as automated data format validation to ensure that all data generated by the neural network matches the structure and distributions of the source data, field correlation and clustering to improve insights captured by the synthetic data model, and a reporting module that generates an HTML report assessing the quality and correlations between the synthetic data and the original dataset.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-is-synthetic-data-different-from-the-original-source-data-it-was-trained-on", "page_title": "How is synthetic data different from the original source data it was trained on?", "headers": ["How is synthetic data different from the original source data it was trained on?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel-synthetics creates artificial data by training a machine learning model to create data just like the input data it was trained on. For example, if you train on a CSV dataset the output will be CSV.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-is-differential-privacy", "page_title": "What is differential privacy?", "headers": ["What is differential privacy?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Differential privacy is a framework for measuring the privacy guarantees provided by an algorithm. Through the lens of differential privacy, we can design machine learning algorithms that responsibly train models on private data. Learning with differential privacy provides provable guarantees of privacy, mitigating the risk of exposing sensitive training data in the synthetic data model or its output. Intuitively, a model trained with differential privacy should not be affected by any single training example, or small set of training examples in its data set.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/does-training-a-synthetic-model-require-a-gpu", "page_title": "Does training a synthetic model require a GPU?", "headers": ["Does training a synthetic model require a GPU?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["A GPU is highly recommended by not required to get started with gretel-synthetics. For a rule of thumb, you can expect training the synthetic model to be 10x faster or more on GPU. However, inference is not nearly as parallelizable as training and we recommend CPUs and Gretel\u2019s available multi-processing support for text generation.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/is-there-an-architecture-diagram", "page_title": "Is there an architecture diagram?", "headers": ["Is there an architecture diagram?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This flow diagram walks through the process of loading source data, training a generative machine learning model, and using the model to create a synthetic dataset using ", ".", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=3", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Conditional Text Generation by Fine Tuning Gretel GPT", "Unlocking Adapted LLMs on Enterprise Data", "How to safely work with another company's data", "Red Teaming Synthetic Data Models", "Introducing Gretel Tabular DP: A fast, graph-based synthetic data model with strong differential privacy guarantees", "Scale Synthetic Data to Millions of Rows with ACTGAN", "Compare Synthetic and Real Data on ML Models with the new Gretel Synthetic Data Utility Report", "Gretel and Google Cloud partner on synthetic data", "Generate Synthetic Databases with Gretel Relational", "Bringing AI-generated images to enterprise use cases", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=1", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Filling in sparse tables with Gretel\u2019s Tabular LLM", "Data Simulation: Tools, Benefits, and Use Cases", "Gretel Demo Day: Exploring the Future of Synthetic Data", "Optimize the Llama-2 Model with Gretel\u2019s Text SQS", "Machine Learning Accuracy Using Synthetic Data", "Advanced Data Privacy: Gretel Privacy Filters and ML Accuracy", "Why Nonprofits Should Care About Synthetic Data", "What is Data Anonymization?", "Generate time-series data with Gretel\u2019s new DGAN model", "Optuna Your Model Hyperparameters", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/data-processing-addendum", "page_title": "Data Processing Addendum", "headers": ["DATA PROCESSING ADDENDUM", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["This Data Processing \u00a0Addendum (including its Exhibits) (\u201c", "\u201d) is incorporated into and forms part of and is subject to the terms and conditions of the Gretel.ai Subscription Services Agreement available at ", " (the \u201c", "\u201d) by and between Customer and Gretel.ai. \u00a0This Addendum will become legally binding upon the effective date of the Agreement. Capitalized or other terms not defined in this Addendum have the meaning set forth in the Agreement.", "This Addendum reflects the parties\u2019 commitment to abide by Data Protection Laws concerning the transfer of Customer Personal Data originating in the European Economic Area, Switzerland, and/or the United Kingdom to a country that has not been found to provide an adequate level of protection under applicable Data Protection Laws. All obligations in this Addendum are in addition to, not in lieu of, any other contractual, statutory, and other obligations of Gretel.ai. Notwithstanding the foregoing, this Addendum shall supersede and replace any inconsistent or conflicting language related to international transfers of Customer Personal Data in the Agreement. We update the terms of this Addendum from time to time. If Customer has an active Gretel.ai subscription, Gretel.ai will let Customer know when it does via email (if Customer has subscribed to receive email notifications from Gretel.ai) or via notification through the Subscription Services. ", "For the purposes of this Addendum, the following terms apply. ", " Gretel.ai shall implement and maintain reasonable administrative, technical, and physical safeguards designed to protect Customer Personal Data.", "Upon becoming aware of a Security Incident, Gretel.ai agrees to provide written notice without undue delay and within the time frame required under Data Protection Laws to Customer\u2019s Designated POC. Where possible, such notice will include all available details required under Data Protection Laws for Customer to comply with its own notification obligations to regulatory authorities or individuals affected by the Security Incident. ", "Where Data Protection Laws afford Customer an audit right, Customer (or its appointed representative) may carry out an audit of Gretel.ai\u2019s policies, procedures, and records relevant to the Processing of Customer Personal Data. Any audit must be: (i) conducted during Gretel.ai\u2019s regular business hours; (ii) with reasonable advance notice to Gretel.ai; (iii) carried out in a manner that prevents unnecessary disruption to Gretel.ai\u2019s operations; and (iv) subject to reasonable confidentiality procedures. In addition, any audit shall be limited to once per year, unless an audit is carried out at the direction of a government authority having proper jurisdiction.", ". At the expiry or termination of the Agreement, Gretel.ai will delete all Customer Personal Data (excluding any back-up or archival copies which shall be deleted in accordance with Gretel.ai\u2019s data retention schedule), except where Gretel.ai is required to retain copies under applicable laws, in which case Gretel.ai will isolate and protect that Customer Personal Data from any further Processing except to the extent required by applicable laws.", "Customer represents and warrants that: (i) it has complied and will comply with Data Protection Laws; (ii) it has provided data subjects whose Customer Personal Data will be Processed in connection with the Agreement with a privacy notice or similar document that clearly and accurately describes Customer\u2019s practices with respect to the Processing of Customer Personal Data; (iii) it has obtained and will obtain and continue to have, during the term, all necessary rights, lawful bases, authorizations, consents, and licenses for the Processing of Customer Personal Data as contemplated by the Agreement; and (iv) Gretel.ai\u2019s Processing of Customer Personal Data in accordance with the Agreement will not violate Data Protection Laws or cause a breach of any agreement or obligations between Customer and any third party. ", "\u200d", "This Exhibit A forms part of the Addendum. Capitalized terms not defined in this Exhibit A have the meaning set forth in the Addendum or in the Agreement. ", "The parties agree that the following terms shall supplement the Standard Contractual Clauses: ", "The parties agree that: (i) a new Clause 1(e) is added the Standard Contractual Clauses which shall read: \u201cTo the extent applicable hereunder, these Clauses also apply mutatis mutandis to the Parties\u2019 processing of personal data that is subject to the Swiss Federal Act on Data Protection. Where applicable, references to EU Member State law or EU supervisory authorities shall be modified to include the appropriate reference under Swiss law as it relates to transfers of personal data that are subject to the Swiss Federal Act on Data Protection.\u201d; (ii) a new Clause 1(f) is added to the Standard Contractual Clauses which shall read: \u201cTo the extent applicable hereunder, these Clauses, as supplemented by Annex III, also apply mutatis mutandis to the Parties\u2019 processing of personal data that is subject to UK Data Protection Laws (as defined in Annex III).\u201d; (iii) the optional text in Clause 7 is deleted; (iv) Option 1 in Clause 9 is struck and Option 2 is kept, and data importer must notify data exporter of new subprocessors in accordance with Section 3(d) of the Addendum; (v) the optional text in Clause 11 is deleted; and (vi) in Clauses 17 and 18, the governing law and the competent courts are those of Ireland (for EEA transfers), Switzerland (for Swiss transfers), or England and Wales (for UK transfers). \u00a0", "Annex I to the Standard Contractual Clauses shall read as follows: ", ": Customer.", ": The address for Customer associated with Customer\u2019s Gretel.ai account or as otherwise specified in the Agreement.", "Customer\u2019s contact details associated with Customer\u2019s account or as otherwise specified in the Agreement.", "The Subscription Services.", "Controller.", "\u200d", "Gretel.ai.", "PO Box 70097, Sunnyvale, California 94086.", "Gretel.ai\u2019s contact details as set forth in the Agreement.", "The Subscription Services.", "Processor.", "\u200d", "The supervisory authority mandated by Clause 13. If no supervisory authority is mandated by Clause 13, then the Irish Data Protection Commission (DPC), and if this is not possible, then as otherwise agreed by the parties consistent with the conditions set forth in Clause 13.", "Annex II of the Standard Contractual Clauses shall read as follows: ", "Data importer shall use commercially reasonable efforts to implement and maintain appropriate technical and organisational measures designed to protect personal data in accordance with the Addendum.", "Pursuant to Clause 10(b), data importer will provide data exporter assistance with data subject requests in accordance with the Addendum. \u00a0", "4. ", "A new Annex III shall be added to the Standard Contractual Clauses and shall read as follows: ", "The ", " (\u201c", "\u201d) is incorporated herein by reference. ", "\u200d", " The start date in Table 1 is the effective date of the Addendum. All other information required by Table 1 is set forth in Annex I, Section A of the Clauses. ", " The UK Addendum forms part of the version of the Approved EU SCCs which this UK Addendum is appended to including the Appendix Information, effective as of the effective date of the Addendum.", " The information required by Table 3 is set forth in Annex I and II to the Clauses. ", " The parties agree that Importer may end the UK Addendum as set out in Section 19. "]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-kinds-of-privacy-protections-can-gretel-synthetics-help-with", "page_title": "What kinds of privacy protections can Gretel Synthetics help with?", "headers": ["What kinds of privacy protections can Gretel Synthetics help with?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel-synthetics is designed to help developers and data scientists create safe, artificial datasets with many of the same insights as the original dataset, but with greater guarantees around protecting personal data or secrets in the source data. Gretel\u2019s implementation of differential privacy helps guarantee that individual secrets or small groups of secrets, such as a credit card number inside structured and unstructured data fields will not be memorized or repeated in the synthetic dataset. Gretel\u2019s synthetic data library also helps to defend against re-identification and joinability attacks, where traditionally anonymized data can be joined with another dataset, even ones that have not been created yet, to re-identify users.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/if-you-only-keep-50-000-records-how-do-i-look-at-more-of-my-labeled-records", "page_title": "If you only keep 50,000 records, how do I look at more of my labeled records?", "headers": ["If you only keep 50,000 records, how do I look at more of my labeled records?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["You can utilize Gretel APIs in a publish / subscribe model. After you ingest data to Gretel, you may consume the labeled records from the stream and re-integrate into your workflows. Please see ", " for a simple example."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/do-i-still-need-to-de-identify-sensitive-data-when-using-gretel-synthetics", "page_title": "Do I still need to de-identify sensitive data when using gretel-synthetics?", "headers": ["Do I still need to de-identify sensitive data when using gretel-synthetics?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel synthetics will memorize and replay repeated data that it observes in the training set. When anonymizing sensitive identifying data types within a dataset, such as fields containing social security or credit card numbers, it is best to anonymize data as best as possible before training the synthetic model. Gretel helps you automate this process with our data labeling APIs and transformation SDKs.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/what-happens-to-the-records-i-send", "page_title": "What happens to the records I send?", "headers": ["What happens to the records I send?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Records are labeled for known entities and each record will get a specific Gretel Metadata object attached to it. This object shows all the entities we found in your data, and provides pointers back to the original record to find where those entities exist. \u00a0Records are written into a project stream and older records are automatically aged out based on maximum stream length."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/how-does-gretel-synthetics-create-artificial-data", "page_title": "How does Gretel Synthetics create artificial data?", "headers": ["How does Gretel Synthetics create artificial data?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel Synthetics utilizes a sequence-to-sequence architecture to train on a text dataset and learn to predict the next characters in the sequence. Gretel-synthetics uses a Long-Short Term Memory (LSTM) artificial neural network to learn and create new synthetic examples from any kind of text or structured data.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-is-synthetic-data", "page_title": "What is synthetic data?", "headers": ["What is synthetic data?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["While synthetic data can mean many things, Gretel\u2019s definition of synthetic data is artificial data created from training a machine learning model to re-create a source dataset. The synthetic model outputs artificial data that contains many of the insights and correlations of the original data without memorizing any records from the original data.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-are-the-outputs-from-gretel-synthetics", "page_title": "What are the outputs from Gretel Synthetics?", "headers": ["What are the outputs from Gretel Synthetics?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["The outputs include a synthetic data model that can be used to generate synthetic data records, an initial set of synthetic data records, and an available premium reporting module that assesses the quality and correlations between the synthetic data and the original dataset.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/can-i-run-gretel-synthetics-on-premises", "page_title": "Can I run Gretel Synthetics on premises?", "headers": ["Can I run Gretel Synthetics on premises?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Some customers need to train synthetic data models within a compliance-approved environment. Gretel-synthetics is open source and can be deployed as a Python package or Docker container. However, some premium features require an API key and connection to Gretel\u2019s APIs.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-synthetics-faqs/what-kinds-of-data-can-i-send-to-gretel-synthetics", "page_title": "What kinds of data can I send to Gretel Synthetics?", "headers": ["What kinds of data can I send to Gretel Synthetics?", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["You can create synthetic data from any kind of text data, whether structured or unstructured. The simpler the data format the better, we recommend CSV, Pandas DataFrames, or unstructured text delimited by line. Gretel-synthetics thrives on patterns. For example, it does particularly well on machine learning datasets as it\u2019s able to replicate both within field distributions as well as cross field correlations. Conversely, if all your dataset field values are highly unique, the model will struggle to find exploitable patterns.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/blog", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/what-is-a-project-in-gretel-cloud", "page_title": "What is a project in Gretel Cloud?", "headers": ["What is a project in Gretel Cloud?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["A project in Gretel Cloud is like a lightweight container to hold a stream of records. A project must have its own unique name, similar to a S3 bucket name. Don't worry - we aren't storing any data in S3 (or anywhere to disk).", "A project can then have a display name, description, and a README. Each data contains one primary record stream. Records you send to Gretel are written into this stream and go through entity detection and metadata extraction. By default, project streams only store the last 50,000 records that were received. This is so you can quickly examine the composition of your data."]},
{"url": "https://gretel.ai/solutions/public-sector", "page_title": "Generate safe government data at scale \u2014 Gretel.ai", "headers": ["Generate safe government data at scale", "Unlock limitless possibilities", "Generate Safe Synthetic Data for ", ", ", ", and ", ".", "Synthesize Training Data for ", " and ", "Boost Model Robustness with ", "Data challenges in Government", "Driving Government innovation with synthetic data", "Why synthetic data?", "Public sector resources", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Utilize privacy-first synthetic data to conduct Safe analytics within federal agencies, ensuring that sensitive and classified information remains protected.", "Make informed decisions and enhance strategic planning without exposing actual data to potential vulnerabilities or breaches.", "Harness the power of privacy-first synthetic data to safely share anonymized information with external research partners or contractors.", "Unlock data access for non-production environments increasing development cycles and productivity while eliminating sensitive data exposure.", "Train large language models (LLMs) using synthetic data to provide enhanced assistance in analyzing and interpreting vast amounts of unstructured data, deriving actionable insights securely without utilizing sensitive or classified information.", "Deliver cutting-edge government services through LLMs, guaranteeing that citizens receive the highest quality of care and support from Government applications.", "Implement AI with safe, high-quality training data across public and private sectors to improve prediction models for both known and unknown threats", "Improve model robustness across mission critical mandates: from natural disasters and threat models to fraud detection and more.", "Multiple data classifications, personnel clearance levels, and classified networks present significant challenges for government data sharing.", "Government clouds are complex with multiple cleared and uncleared environments and rules for cross-environment data transfer.", "With a global race for AI dominance, governments need to rapidly build AI training datasets to maintain tactical advantage.", "Synthetic data accelerates government innovation and collaboration while protecting citizen and national privacy. By generating artificial records that contain critical insights without exposing any actual sensitive information, synthetic data removes privacy risks.", "This allows civil and defense agencies to rapidly share datasets across departments, cloud environments, classified networks, clearance levels, and with external partners. Governments also leverage synthetic data to augment their existing AI training datasets and anonymize sensitive insights in order to accelerate the adoption of AI applications.", "Backed by multiple generative AI models and proven across government agencies, scale your data without compromising on quality or privacy.", "Backed by differential privacy\u2014complete with mathematical guarantees, ensure safe data sharing across classified networks, clearance-levels, and partners.", "Synthetic data makes sensitive data assets safe and better so agencies can quickly operationalize data for mission-critical outcomes.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/solutions/healthcare", "page_title": "Generate safe EHR data at scale \u2014 Gretel.ai", "headers": ["Generate safe EHR data at scale", "Unlock limitless possibilities", "Generate Safe Synthetic Data for ", ", ", ", and ", ".", "Synthesize Training Data for ", " and ", "Boost Model Robustness with ", "The data bottleneck", "Power innovation with synthetic EHR data", "Why synthetic data?", "Healthcare Resources", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Synthetic data can help in simulating patient data for clinical trials, facilitating more efficient initial testing phases and protocol development.", "Enable the safe analysis of electronic health records (EHR) by using synthetic data, maintaining patient privacy while extracting valuable insights for healthcare improvement.", "Develop personalized treatment plans using synthetic data that employs differential privacy, ensuring safe and privacy-preserving data analysis.", "Unlock data access for non-production environments increasing development cycles and productivity while eliminating sensitive data exposure.", "Improve patient interaction bots through synthetic data, enhancing automated patient support and communication.", "Generate synthetic conversations for training AI in understanding diverse medical queries, supporting the development of AI-driven consultation systems.", "Boost the accuracy and reliability of disease prediction models by integrating synthetic data to cover a wide spectrum of patient conditions and histories.", "Improve models that optimize hospital operations and resource allocation by incorporating synthetic data for varied scenarios and patient influxes.", "Sensitive patient information is regulated and its use requires adherence to HIPAA, GDPR, and other privacy frameworks.", "EHR records are often imbalanced due to factors like social inequalities in healthcare \u00a0and low adverse clinical outcomes.", "Patient data is difficult to find for some medical conditions due to issues in reporting or infrequency of rare diseases.", "Synthetic health data accelerates clinical research while protecting patient privacy by generating artificial records that resemble real EHR without including any actual patient information.", "Organizations also use synthetic data to augment AI training datasets by boosting low sample sizes, balancing between classes, filling in missing fields, and \u00a0simulating new examples for underrepresented medical conditions.", "Backed by multiple generative AI models and proven across healthcare use cases, scale your data needs as required without compromising on quality or privacy.", "With tunable privacy filters and models built with differential privacy\u2014complete with mathematical guarantees, rest assured EHR data is safe.", "Make your data asset safe and better so teams can stop worrying about data quality and access and focus on improving patient outcomes.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/solutions/finance", "page_title": "Generate safe financial data at scale \u2014 Gretel.ai", "headers": ["Generate safe financial data at scale", "Unlock limitless possibilities", "Generate Safe Synthetic Data for ", ", ", ", and ", ".", "Synthesize Training Data for ", " and ", "Boost Model Robustness with ", "The data bottleneck", "High-quality, safe financial data unlocked", "Why synthetic data?", "Finance & Banking Resources", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Utilize synthetic data to simulate various economic conditions and market fluctuations to enhance risk models and decision-making.", "Improve fraud detection models by generating diverse synthetic data to simulate various types of financial fraud.", "Create personalized banking services using synthetic data, ensuring customer privacy through differential privacy, allowing for innovation without compromising sensitive information.", "Unlock data access for non-production environments increasing development cycles and productivity while eliminating sensitive data exposure.", "Improve customer service bots by training them with synthetic data, enabling better handling of client queries and issues related to banking and finance.", "Develop sophisticated conversational AI for investment advice by using synthetic training data to simulate various customer needs and market conditions.", "Enhance the robustness of trading algorithms by using synthetic data to simulate different market conditions and outliers.", "Improve credit scoring models by introducing synthetic data that replicates various customer profiles and credit histories, ensuring models are resilient and unbiased.", "Predictive analytics and modeling in financial organizations require vast amounts of data to train models, and even more data is required for edge-cases and anomalies like fraud or black swan events. Access to safe, diverse datasets at meaningful scale remains a bottleneck across the industry.", "With ever-evolving data privacy regulations across the world, such as FINRA and the SEC, data compliance remains a top priority for banks and financial institutions globally. Protecting customer data and eliminating sensitive data exposure is an ongoing challenge.", "The emergence of new technologies like generative AI require businesses to innovate or fall behind. Without access to safe training data, financial institutions cannot experiment and drive new technologies forward.", "Synthetic financial data allows institutions to meet the nuanced needs of modern banking and finance by providing private, safe data at scale. With Gretel you don\u2019t have to compromise between data utility and data privacy.", "Our synthetic data platform provides all the features you need to unlock the value of your data while maintaining high privacy and quality standards. Whether it\u2019s preventing data leaks, sharing data across a vast organization, or reducing data compliance overhead, Gretel brings best-in-class privacy to your business.", "Multiple generative AI models built for the enterprise, so you can rest assured that your data mirrors the statistical properties of real-world data without compromising on privacy.", "Align your data privacy strategies with the ever-evolving regulatory environment and ensure stringent compliance while mitigating legal and reputational risks.", "Harness the power of the latest technologies, like generative AI, to drive innovation in services, customer interactions, and operational efficiencies without sensitive data exposure.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/how-do-i-get-started", "page_title": "How do I get started?", "headers": ["How do I get started?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Use a Google or GitHub account and sign up at ", ". Then create your first project and upload a dataset or use one of our sample datasets to get started."]},
{"url": "https://gretel.ai/data-types/relational-databases", "page_title": "Gretel Relational Synthetics and Transforms", "headers": ["Gretel Relational", "Why Gretel Relational?", "Connect to your enterprise database.", "Detect and transform sensitive entities.", "Create synthetic versions of your data.", "Generate synthetic subsets of your data.", "Ensure referential integrity, accuracy, and privacy.", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Connect to your enterprise database with 30+ connectors.", "Detect and transform sensitive entities.", "Create synthetic versions of your data.", "Generate synthetic subsets that always beat random sampling.", "Ensure referential integrity, accuracy, and privacy.", "No more complex data pipelining. Use Gretel to easily connect to any normalized SQL database and write back high-quality synthetic data to the environment of your choice. ", "And, with automatic key detection, Gretel magically detects primary and foreign key pairs, giving you more time for coffee.", "Say goodbye to manual data transformations. With Gretel, you can effortlessly extract sensitive columns and apply a range of transformations at scale, such as masking, hashing, tokenization, or even replacement. ", "Establish your organizational policies to be enforced across tables, then let Gretel's advanced models handle the rest.", "Your enterprise databases are built on objects and relationships that define your business. With Gretel, you can generate top-notch synthetic versions of your data while maintaining the referential integrity of your original records. ", "Take advantage of our expanding collection of generative models to create synthetic data that's just as good, if not better, than the real thing.", "Shrink from petabytes to gigabytes with anonymized data that looks and feels like production data\u2014all without risking privacy. Don\u2019t take chances with random sampling, synthetics lets you accurately subset your data so you can innovate with confidence and speed.", "To use data, you need to trust it. Verify synthetic data quality with our unique accuracy and privacy scores developed especially for relational data. Measure how well in-table and cross-table relationships are maintained with consumable reports. Build ambitiously, knowing your data is accurate and secure.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/evaluate", "page_title": "Validate your synthetic data performance with Gretel.ai", "headers": ["Validate the quality of your ", "Understand your data.", "Benefits for Developers", "Verify data meets quality standards.", "Analyze data accuracy and performance.", "Benchmark model performance.", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel Evaluate provides an out-of-the-box report on every model you train, so you can easily understand how your data performs on key metrics and compare results side by side.", "Measure how closely the generated data maintains the statistical properties of the original dataset.", "Show the tunable privacy levels of each synthetic model.", "Assess the performance of synthetic data on classification and regression models.", "An easy-to-use low-code tool that benchmarks multiple datasets against multiple synthetic models.", " Accelerate model development with faster data feedback loops.", " Get a deep understanding of the technical metrics that matter most in your data.", " Easily compare synthetic models and datasets throughout the ML lifecycle.", " Gain rapid insight into your synthetic data performance with a high-level composite Synthetic Data Quality Score (SQS).", " Ensure your synthetic data is as good\u2014or better than\u2014real-world data with statistical assurance verified with Principal Component Analysis (PCA).", " Correlation between every pair of fields is computed and compared between real-world training data and generated synthetic data.", " See how your synthetic data compares to real-world data with a Machine Learning Quality Score (MQS).", " Synthetic and real data are trained and evaluated on AutoML classification and regression models and the top models are compared.", "Track experiments or monitor data drift by comparing multiple models and datasets with just 5 lines of code.", "Get valuable measurements like Synthetic Data Quality Scores (SQS), dataset statistics, and training time all in one place.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/classify", "page_title": "Gretel Classify: Know your data, so you can protect it.", "headers": ["Know your data, so you can protect it.", "Identify and label sensitive data easily", "Benefits for ", "Learn Classify with Blueprints", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/transform", "page_title": "Gretel Transform: Anonymize your data in real-time.", "headers": ["Anonymize your data in ", "Protect sensitive data automatically", "Benefits for ", "Learn Transform with Blueprints", "Gretel Transform in Action", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/synthetics", "page_title": "Gretel Synthetics: Build smarter with the right data.", "headers": ["Gretel", "Build smarter with ", "the right data.", "Your end-to-end synthetic data needs all in one platform", "On-Demand data delivery", "Unbeatable data accuracy", "Tunable data privacy", "Verifiable data quality", "Rapidly expanding support of popular data types", "Synthesize and transform multiple tables or entire relational databases.", "Synthesize tabular data, think rows and columns.", "Synthesize unstructured text, think human language and chatbots.", "Synthesize time-series data, think sensors and financial data.", "Synthesize high-quality, domain-specific images today", "What can synthetic data do for me?", "Unlock the power of ", "Jumpstart your experiments with ", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Unlock unlimited possibilities with synthetic data. Share, create, and augment data with cutting-edge generative AI.\u00a0", "With Gretel, you synthesize the ", "right data to power downstream workflows.", "\u200d", "Mitigate GDPR and CCPA risks, promote safe data access.", "Accelerate CI/CD workflows, performance testing, and staging.", "Augment AI training data, including minority classes and unique edge cases.", "Amaze prospects with personalized product experiences.", "Reach out to us today and see how Gretel can bring your use case to life.", "We believe no one model rules them all. Use one or a combination of state-of-the-art Gretel models.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai", "page_title": "Synthetic Data Generation Platform for Developers", "headers": ["Gretel", "The ", " platform for developers.", "We help developers unlock synthetic data.", "Generate accurate and safe synthetic data, on demand.", "Get started with synthetic data in less than five minutes.", "Deploy Gretel for enterprise use cases", "Using AI to Create Safe Synthetic Datasets for Genomics.", "Creating Synthetic Time Series Data for Global Financial Institutions.", "Run Gretel ", " ", "Implement privacy ", "Gretel Cloud", "...or in your environment.", "Ready to try Gretel?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Train generative AI models that learn the statistical properties of your data.", "Validate your models and use cases with our quality and privacy scores.", "Generate as much data as you need, when you need it.", "Harness the power of generative AI in your enterprise with Gretel and Google Cloud.", "Unlock the power of multi-modal synthetic data in your enterprise with AWS.", "Unlock generative AI in Azure with custom AI models that are trained on safe, secure data.", "Using our cloud GPUs makes it radically more effortless for developers to train and generate synthetic data.", "Scale workloads automatically with no infrastructure to set up and manage.", "Invite team members to collaborate on cloud projects and share data across teams.", "Your data never leaves your environment. Runners can generate and transform locally, orchestrated by our APIs.", "Track progress and manage local workers from anywhere with the Gretel Console.", "Join our Discord to connect with the Gretel team and engage with our community.", "Set up your environment and connect to our SDK."]},
{"url": "https://gretel.ai/blog?9032491c_page=2", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/generate-synthetic-data-using-gretel-hybrid", "page_title": "Generate Synthetic Data Using Gretel Hybrid", "headers": ["Generate Synthetic Data Using Gretel Hybrid", "Resource Overview \ud83d\udd0d", "Walkthrough", "1. Install Prerequisite CLI Tools \u2328\ufe0f", "AWS", "Terraform", "Gretel", "curl and jq", "2. Deploy Gretel Hybrid on AWS \u26a1", "Authenticate with AWS", "Clone the Gretel Hybrid Repository", "Configure Terraform Variables and Provider", "Setup Gretel API Key", "Deploy Gretel Hybrid on AWS", "3. Generate Synthetic Data Within Your Cloud Environment \u2601\ufe0f", "Configure the Gretel CLI", "Upload Sample Data to Your Source Bucket", "Create a Gretel Project", "Generate Synthetic Data", "Review Sample Data\u00a0", "(Optional) Generate More Synthetic Data", "Cleanup \ud83e\uddf9", "Wrapping Up", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["One common characteristic of production data, regardless of industry or sphere of business is this: data is sensitive. Customer names, addresses, payment information, and other personally identifiable information (PII) are stored in production databases with strict security (and often regulatory) requirements. Gretel provides a way for customers to keep sensitive data within the boundaries of their own cloud tenant through Gretel Hybrid, which has been live and in use by customers ", ". Customers are able to train models and generate ", " synthetic data from infrastructure they own and maintain, and their data never leaves their environment.", "Kubernetes is the core compute service powering Gretel Hybrid. We support a \u201cbring your own compute\u201d approach across 3 major cloud providers\u2013", ", ", ", and ", "\u2013via their managed Kubernetes services (EKS, AKS, and GKE). ", " is a cloud agnostic infrastructure as code tool we\u2019ve chosen to enable rapid hybrid deployments and feature delivery. Recently Gretel has released ", " which empower customers to deploy Gretel Hybrid faster than ever.\u00a0 Our publicly available modules allow a user to deploy a \u201csandbox\u201d or \u201cproof of concept\u201d environment in less than 30 minutes. They are also production ready and can be utilized by customers who are ready to fully operationalize Gretel Hybrid.", "In this post, we will walk you through the steps to deploy Gretel Hybrid into your own AWS environment and generate some synthetic health data using a demo dataset. This will allow you to iterate and test Gretel Hybrid against your own sensitive datasets without sending any data to Gretel Cloud. Stick with us and follow along!", "Here is the dependency diagram showing our available Terraform modules for Gretel Hybrid on AWS. We will be deploying all the below resources as part of this example, but you\u2019re also able to utilize the individual modules in case you already have an existing VPC or EKS Cluster.", "As a prerequisite for following along, you will need an ", " with the proper permissions. Your AWS IAM User or IAM Role will need to be able to create and manage all of the resources laid out in the previous section\u2019s diagram. For following along with this guide, we recommend using an AWS Sandbox account with admin level access.", "First, we need to install the AWS CLI, the Terraform CLI, and the Gretel CLI. Here\u2019s how:", "Official installation instructions for Linux, MacOS, and Windows are ", ". Confirm the CLI has been installed correctly by running the following command from your terminal.", "Official installation instructions for Linux, MacOS, and Windows are ", ". Confirm the CLI has been installed correctly by running the following command from your terminal.", "The most common way to ", ". You may run the below command to quickly install it if you already have pip installed. If you need help installing pip, please consult the ", ".", "We make use of curl to download a CSV file with some demo data to your local machine. We utilize jq to filter the json response from some Gretel CLI commands. If your operating system does not come with these utilities installed by default you will need to install ", " and ", ".", "Now we\u2019re ready to deploy Gretel Hybrid on AWS.\u00a0", "From your terminal, start by configuring your AWS CLI to use the appropriate credentials for your sandbox account. You may follow the ", " to authenticate your shell session appropriately. Your organization may have specific guidance on how to access an organization owned account. You can confirm proper AWS access by running the below command and verifying its output.", "Clone the Gretel Hybrid git repository with the following command.", "Change directory into the full-deployment example.", "Here is what our working directory looks like.", "Open the terraform.tfvars file and edit the list of variables as desired. ", "After customizing the bucket names and any desired variables you may move on.", "Edit the main.tf file and add the following on line 10 to define a blank AWS provider for Terraform which will cause Terraform to use the default AWS credentials which you have already configured.", "If you haven\u2019t already, sign up for a free Gretel account at ", ".\u00a0", "You may directly retrieve your API Key after signing up by visiting ", " and copying the key.", "We must pass the API Key to Terraform for deployment. We don\u2019t want to define a sensitive value in a cleartext file, so we will pass this variable to Terraform using an environment variable. Replace the below text inside the quotes with your copied Gretel API Key.", "Run these terraform commands from the full_deployment directory.", "Initialize terraform. This is an idempotent operation and is always safe to do (resources will not be created/destroyed).", "View the changes Terraform will make upon deployment. Use this any time you make changes to take a closer look at what is going on.", "Deploy the module. This will require user confirmation so don't walk away from your shell until you confirm by typing \"yes\" to start the deployment.", "It will take 10-20 minutes for all the necessary resources to be deployed. Congratulations! You've deployed everything necessary to run Gretel Hybrid within your own cloud tenant.", "We\u2019re almost ready to start synthesizing data.", "Since we will be running a test job with the Gretel CLI, we must first ", " appropriately. Run the below command.", "The configuration process will take you through step by step prompts to configure your Gretel CLI. You may hit \u201center\u201d to use the value in square brackets included in the prompt.", "The values should look like this.", "You\u2019ll need to upload the sample data to the source AWS S3 Bucket using the AWS CLI. You can do so with the following command. ", "terraform.tfvars ", " The sample healthcare CSV is located at ", ".", " can be thought of as repositories that hold models. Projects are created by single users and can be shared with various permissions. Run the below command to create a project.", "We will take advantage of the GPU configuration within your Gretel Hybrid deployment and run our ", " against the demo data. Run the below command to schedule your Gretel Job. This can take anywhere from 5-15 minutes to complete.", "Now that your Gretel Hybrid job has finished running, we can take a closer look at the sample synthetic data that was generated as part of the model training job. Make sure you set the SINK_BUCKET variable appropriately with the value from your terraform.tfvars file.", "Now we can list the output artifacts from our model training job.", "We will copy the sample data as well as the synthetic data quality report over to our local environment.", "You can open the quality report with your browser of choice to take a closer look at ", ". You can compare the data_preview.csv which is a small set of synthetic data against the source demo dataset contained in sample-synthetic-healthcare.csv.", "Want to generate more synthetic data? You can run the following command and choose the number of synthetic records you\u2019d like to generate based on the previously trained model. This will result in another Gretel Job being scheduled to run within your Kubernetes cluster and may take ~5 minutes.", "When the job finishes running we can take a closer look at the output artifacts using the below commands. First let\u2019s list the run IDs within the sink bucket.", "Now we will pass that ID into a variable as shown below. We will copy the synthetic data the same way we have done in the previous section.", "Now you can compare the data.csv dataset against the source demo dataset contained in sample-synthetic-healthcare.csv.\u00a0", "If you would like to clean up the AWS resources you provisioned while following along, the following commands will delete all provisioned resources. Run them from the full_deployment directory. The destroy command will ask for confirmation before proceeding.", "In this walkthrough we created all of the AWS resources necessary to deploy Gretel Hybrid and generate synthetic data, while keeping data within your own cloud tenant. We demonstrated the\u00a0 process using a sample healthcare dataset for model training. You can check out our ", " for further reading, or watch this ", "!", "Now that you\u2019ve tried Gretel Hybrid, the next step is to try it out with your own data. You can be confident that your data remains secure in your environment , and is never passed to Gretel Cloud at any point. You can export a dataset as a CSV and follow along with the above guide, replacing the sample dataset with your own.", "In addition to regular tabular data Gretel also supports relational data sources. Check out ", " which are fully compatible with Gretel Hybrid.", "Thanks for following along and happy synthesizing!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog?9032491c_page=3", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=4", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Diffusion models for document synthesis", "The Evolution of Gretel's Developer Stack for Synthetic Data", "Measure the Quality of any Synthetic Dataset with Gretel Evaluate", "How to Generate Synthetic Data: Tools and Techniques to Create Interchangeable Datasets", "Build a synthetic data pipeline using Gretel and Apache Airflow", "Creating Synthetic Time Series Data for Global Financial Institutions \u2013 a POC Deep Dive", "Transforms and Synthetics on Relational Databases", "How accurate is my synthetic data?", "Generate synthetic data in 3 lines of code", "Introducing Gretel Amplify", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/gretel-cloud-faqs/what-types-of-entities-can-gretel-identify", "page_title": "What types of Entities can Gretel identify?", "headers": ["What types of Entities can Gretel identify?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["View a full list of supported entities, as well as instructions to create your own detections in our ", "."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/what-is-gretel-cloud", "page_title": "What is Gretel Cloud?", "headers": ["What is Gretel Cloud?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel Cloud is a fully managed service that allows for easy data discovery and analysis, and creation of safe data. We are launching our service as a ", "starting with automatic data labeling and access to our open source SDKs for data transformation and synthetic data generation."]},
{"url": "https://gretel.ai/gretel-cloud-faqs/what-are-the-features", "page_title": "What can I do with Gretel Cloud?", "headers": ["What can I do with Gretel Cloud?", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel enables the creation of safe data by focusing on three pillars:"]},
{"url": "https://gretel.ai/blog?9032491c_page=4", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/machine-learning?094d394d_page=5", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "Create a Location Generator GAN", "README.V2", "How we accidentally discovered personal data in a popular Kaggle dataset", "Q&A Series: Solving Privacy Problems with Synthetic Data", "Create artificial data with Gretel Synthetics and Google Colaboratory", "Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "Exploring NLP Part 1: Why Should a Privacy Engineering Company Care About NLP?", "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "Common misconceptions about differential privacy", "ML Models: Understanding the Fundamentals", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog?9032491c_page=5", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog?9032491c_page=6", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog/bring-your-own-cloud", "page_title": "Bring Your Own Cloud (BYOC): Transforming & Synthesizing Data with Gretel Hybrid", "headers": ["Bring Your Own Cloud (BYOC): Transforming & Synthesizing Data with Gretel Hybrid", "The challenge", "Our insight \ud83d\udca1", "Setting the stage\u00a0\u00a0", "The Kubernetes deployment", "Preparing the data", "Running the training and transforms", "Giving the transformed data a new home", "What have we learned?", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Gretel has a snappy SaaS offering that runs in the cloud, but some of our customers (particularly those working in heavily regulated industries like banking and insurance) have ", ". They can\u2019t simply move their data into our managed cloud offering. The data and models have to remain within ", " cloud throughout the process. This is a problem that we have solved in a couple of different ways.\u00a0", "Our initial offering is a single machine setup that ", " for the on-premise component of the model training and data generation. We call this setup the \u201cGretel Agent.\u201d", "While good for smaller workloads, larger datasets would take a while to process on a single machine (months to years). Also, this setup introduces a single point of failure. (Hopefully Tom doesn\u2019t kick the power cable on the Gretel Agent!) For that reason, it\u2019s being phased out in support of our new approach.", "We settled on a Kubernetes deployment to be both cloud agnostic and to enable us to scale our hybrid workflows based on the number of nodes we can run in a customer\u2019s environment. We call this our \u201cGretel Kubernetes Agent.\u201d It\u2019s currently in closed beta, so ", " for more information on getting started.", "Both the Docker and Kubernetes-based options represent the on-premise piece of what we call \u201cGretel Hybrid.\u201d\u00a0", "The main idea is that customers can use Gretel\u2019s code and our scheduling APIs, but only metadata goes back and forth between their cloud and Gretel\u2019s. The data, the trained models, and the synthetic data all remain within your cloud during the process.", "For this use case, the customer had a cloud database (7TB), and they wanted to be able to use this data in a lower environment. A lower environment would be something like a performance testing environment, where changes can be verified before going out to the production environment. This enables testing to be reflective of the ", " of data as well as the scale, something that is hard to do by simply inserting a bunch of generated or duplicate rows.", "The main concern is exposing PII (Personal Identifiable Information, e.g. names, phone numbers, and emails) to developers or other folks who have access to the lower environment.", "In addition to allowing you to generate an entirely synthetic dataset, Gretel also offers a ", " package which makes it easy to remove or alter sensitive fields while keeping the rest of the data the same. This is useful for lower environments, where data will not be shared externally. We used that as part of this hybrid data pipeline.", "So how did we transform 7 TB of data? Let\u2019s get into it.", "After a few iterations with the customer, we got our helm chart installed just the way they wanted it. For those who haven\u2019t worked with Kubernetes much, a ", " is one of the mechanisms to package and redistribute software within Kubernetes clusters, while allowing customer-specific customizations. We had a very simple chart to start with, for folks who wanted to run it in a cluster specifically set up for model training and data generation, but we added more hooks to the chart to allow the customer to install it in a multi-purpose Kubernetes cluster. The main lesson learned was that sensible defaults paired with options tailored to the customer\u2019s specific needs make for the best installation strategy.", "These hooks meant adding options to set the appropriate Node Selectors (i.e., a way to select which Nodes to use by label), Taints/Tolerations (i.e. a way to designate which nodes are usable by which jobs), and resources for the Gretel Kubernetes Agent and the workers that it spawns. The end product was simply a `helm install` that a customer could run on their shared cluster.", "The above diagram shows how this helm chart allows us to control scheduling ", " Gretel Cloud, while keeping all the data within a customer\u2019s cloud.\u00a0", "The Kubernetes Gretel Agent is a process that ", " scheduled via our APIs. The Agent can schedule as many jobs in parallel as the customer\u2019s plan and environment allow.", "But before we can start running these workloads, we have to prepare the source buckets.", "Along with Kubernetes, the common denominator for our deployment strategy is object storage (S3, Azure, GCS, etc).\u00a0", "The Gretel training and transform jobs will need access to a bucket as part of the deployment, and the helm chart has hooks for passing in credentials or a Kubernetes Service Account, so the jobs can access the source and sink buckets. One of the key pieces to getting this working is to test the policy setup from within the cluster (by running a sample job) and by using things like AWS\u2019 policy simulator, to ensure the service account will ", " be able to access any buckets it needs.", "allows us to take the data directly out of a source database (RDBMS), but in this instance, we used a utility that exported the dataset into parquet files. Gretel supports csv, jsonl, and parquet, along with a built-in S3 connector that makes it easy to set up data sources in instances where direct access to the database might not be available.\u00a0", "Once we had the data in our source bucket, it was simply a matter of setting up some scripts, which scheduled the jobs that would run in the Gretel Kubernetes Agent deployment.", "While scaling up the workloads, we fixed some issues that limited how quickly the Gretel Kubernetes Agent could place jobs, as well as worked with the customer to change the CIDR block range, since we were running out of Pod IPs from running so many jobs. CIDR, or Classless Inter-Domain Routing, defines the range of IP addresses that you are allowed to use.", "Once we got over those scaling hurdles, we were able to run 100 concurrent workloads in parallel for this customer, getting through 1000s of these files every hour. The scheduling was all done via Python scripts that used our ", " to schedule the jobs that the on-premise deployment would poll.\u00a0", "Because everyone\u2019s data is different, we had to tweak the ", " for certain tables, but for the most part we could use the same configuration to transform PII into fake data across many tables. We also made some improvements to our transform APIs to help with future customers, such as handling entity recognitions that can take a very long time, or enabling a few other ways to fake different data types.", "Once we had all of the rough edges ironed out, we were ready to load the data into a lower environment.", "The end goal of this journey was to enable production-like data in a test database. The last hurdle we covered by using the gretel-client SDK once again, this time to locate all the transformed files per table (our mapping was 1-to-1 of table to model) and load them into the performance database.", "Once we had the access necessary to the sink bucket and the test RDBMS instance, it was straightforward to load the parquet files in with Pandas and SqlAlchemy. The 7TB of transformed data had found a new home.", "Gretel lets you transform data in your own cloud using Kubernetes and object storage as the common ground.", "We\u2019re always looking to improve this setup and hope to have new customers looking to generate and transform data in hybrid environments with us, so that we can offer the best tools to transform data in a way that enables our customers to innovate safely with data while respecting their sensitive information and complying with privacy regulations.", "If you\u2019re interested in what it takes to set up hybrid environments, check out our ", " on setting up Gretel Hybrid.", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog?9032491c_page=7", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/ben-mccown", "page_title": "Author: Ben McCown - Gretel.ai", "headers": ["Ben McCown", "Generate Synthetic Data Using Gretel Hybrid", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=5", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Conditional data generation in 4 lines of code", "Synthetic Image Models for Smart Agriculture", "Walkthrough: Create Synthetic Data from any DataFrame or CSV", "How to use Weights & Biases with Gretel.ai", "Deep dive on generating synthetic data for Healthcare", "Evaluating Data Sampling Methods with a Synthetic Quality Score", "Automatically Reducing AI Bias With Synthetic Data", "Gretel Synthetics: Introducing v0.10.0", "How To Create Differentially Private Synthetic Data", "Synthetic Data Configuration Templates", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog/announcing-the-synthetic-data-community-discord", "page_title": "Announcing the Synthetic Data Community Discord", "headers": ["Announcing the Synthetic Data Community Discord", "Why Discord?", "How to join", "Conclusion", "Connect with the Gretel Community.", "Get started with Gretel", "Want to learn more?", "Generate synthetic data at scale", "Stay connected", "Similar posts", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["We\u2019re excited to announce the launch of the Synthetic Data Community Discord server \u2013 a forum where all those interested in synthetic data can congregate, communicate, and collaborate.\u00a0 We envision this Discord server to be a place for sharing information, such as news, meetups and events, research papers, personal projects, and more. We plan on hosting hackathons and code challenges to help more people get involved in synthetic data. We\u2019re even planning on using this channel as the primary discussion forum for the upcoming ", " conference.\u00a0", "Over the past few years, many tech and open source communities have migrated to Discord for its stellar voice and video channels. Discord supports a variety of platforms and can even be embedded into web pages. Most importantly, Discord doesn\u2019t have a message retention limit, so we\u2019ll never lose our conversations.", "To join a Discord server, you\u2019ll need to create a ", ". Once you have your account, you can join the server by clicking on the invite link ", ". You'll need to accept the Community ", " and Discord rules before being allowed to post.\u00a0", "Gretel is excited to continue building out resources and spaces for the Synthetic Data Community. We hope to chat with you on the Discord server!", "Join our Discord to connect with the Gretel team and engage with our community.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/machine-learning?094d394d_page=6", "page_title": "Machine Learning blog posts - Gretel.ai", "headers": ["Machine Learning", "Using generative, differentially-private models to build privacy-enhancing, synthetic datasets from real data.", "Community Insights: Overcoming Medical Class Imbalance with Synthetic Data", "Generate synthetic Taylor Swift-like lyrics using Gretel GPT", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog?9032491c_page=1", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog?9032491c_page=8", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog?9032491c_page=9", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/author/mason-egger", "page_title": "Author: Mason Egger - Gretel.ai", "headers": ["Mason Egger", "Announcing the Synthetic Data Community Discord", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/blog?9032491c_page=10", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/blog?9032491c_page=11", "page_title": "The Gretel.ai Blog - Gretel.ai", "headers": ["Gretel Blog", "No results found.", "Stay connected", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": ["Try a different search query, or select another topic.", "Subscribe to our newsletter to receive Gretel news and blog posts directly to your inbox."]},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=6", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "What is Synthetic Data?", "Introducing Gretel's Privacy Filters", "Improving massively imbalanced datasets in machine learning with synthetic data", "Gretel's New Synthetic Performance Report", "Creating synthetic time series data", "Q&A Series: Solving Privacy Problems with Synthetic Data", "Create artificial data with Gretel Synthetics and Google Colaboratory", "Gretel Synthetics Frequently Asked Questions (FAQs)", "Create Synthetic Time-series Data with DoppelGANger and PyTorch", "ML Models: Understanding the Fundamentals", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/category/synthetics?094d394d_page=7", "page_title": "Synthetics blog posts - Gretel.ai", "headers": ["Synthetics", "Practical Privacy with Synthetic Data", "Generate synthetic Taylor Swift-like lyrics using Gretel GPT", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []},
{"url": "https://gretel.ai/author/matt-kornfield", "page_title": "Author: Matt Kornfield - Gretel.ai", "headers": ["Matt Kornfield", "Bring Your Own Cloud (BYOC): Transforming & Synthesizing Data with Gretel Hybrid", "Product", "Developers", "Company", "Resources", "Social", "Community", "FAQs"], "content": []}
]